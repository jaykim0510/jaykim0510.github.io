[
  
    {
      "title"    : "Apache Spark Series [Part6]: 스파크 SQL",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series6",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series6'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part6]: 스파크 SQL'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series6'>Apache Spark Series [Part6]: 스파크 SQL</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part5]: 스파크 데이터프레임(DataFrame)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series5",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series5'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part5]: 스파크 데이터프레임(DataFrame)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series5'>Apache Spark Series [Part5]: 스파크 데이터프레임(DataFrame)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part4]: 스파크의 클러스터 매니저(Cluster Manager)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series4",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents  스탠드얼론  얀  메소스  쿠버네티스스탠드얼론얀메소스쿠버네티스",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series4'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part4]: 스파크의 클러스터 매니저(Cluster Manager)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series4'>Apache Spark Series [Part4]: 스파크의 클러스터 매니저(Cluster Manager)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part3]: 스파크의 클러스터 환경",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series3",
      "date"     : "Feb 21, 2022",
      "content"  : "Table of Contents  클러스터 환경  스파크의 분산처리 아키텍처  스파크 애플리케이션 동작 순서  참고클러스터 환경스파크는 본질적으로 분산처리 프레임워크입니다. 그래서 단순히 테스트를 위한 용도로는 단일 로컬 서버만으로도 가능하지만, 실제 배포 단계에서 스파크를 제대로 활용하기 위해서는 여러 대의 서버를 이용한 클러스터 환경을 구축할 필요가 있습니다.클러스터란 여러 대의 서버가 네트워크를 통해 연결되어 마치 하나의 서버인 것처럼 동작하는 방식을 의미합니다. 하지만 여러 서버들을 이 같은 방식으로 동작시키는 것은 쉬운 일이 아닙니다. 그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다.스파크에서는 자체 구현한 클러스터 매니저도 제공하고 외부 클러스터 매니저를 임포트해서 사용할 수도 있습니다. 이렇게 여러 종류의 클러스터 매니저를 지원하게 되면 선택의 폭이 넓어진다는 장점도 있긴 하지만 클러스터 매니저마다 동작 방식이나 용어가 다르면 혼동이 될 수 있습니다. 스파크에서는 이런 혼란을 없애고자 추상화된 클러스터 모델을 제공함으로써 사용하는 클러스터의 종류에 관계없이 일관된 방법으로 프로그램을 작성하고 클러스터를 관리할 수 있게 해줍니다.내용에 들어가기 전에 한 가지 알아둘 것은 클러스터 환경이라고 해서 로컬 환경에서 사용하던 스파크 애플리케이션 코드를 새로 작성해야 할 필요는 없습니다. 다만 클러스터 환경에서는 여러 서버를 마치 하나의 서버인 것처럼 다뤄야 하기 때문에 하나의 작업을 여러 서버에 분산해서 실행하고 그 결과를 취합할 수 있는 분산 작업 관리 기능이 추가되어야 할 것입니다.따라서 이번 포스트의 목적은 분산처리를 위한 시스템 아키텍처를 이해하고, 이를 구현하기 위해 필요한 설정과 매개변수를 이해하는 것입니다.스파크의 분산처리 아키텍처아래 그림은 분산처리를 위한 스파크의 전형적인 아키텍처입니다.보시다시피 클러스터 매니저는 가운데에서 분산처리를 위한 매니저 역할을 하고 있습니다. 각각의 컴포넌트에 대한 설명은 앞의 포스트에서 다룬 적이 있음으로 여기서는 간단하게만 요약하도록 하겠습니다.  드라이버 프로그램: 스파크 컨텍스트를 생성하고 클러스터 매니저와 연결시켜주는 프로그램  스파크 컨텍스트: 클러스터와 연결되는 객체로 스파크 애플리케이션 코드를 작성하는데 필요한 거의 모든 기능을 제공  클러스터 매니저: 워커 노드를 모니터링하며 최적의 자원(CPU, 메모리) 할당  워커 노드: 분산된 데이터를 할당받고 요청된 작업을 처리하는 서버  익스큐터: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위  잡(Job): 액션 연산의 수  태스크: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위스파크 클러스터는 이와 같이 드라이버, 클러스터 매니저, 워커 노드의 조합으로 구성됩니다. 여기서 실행 모드, 클러스터 매니저의 종류에 따라 약간의 다른 점이 있지만 큰 맥락에서는 같습니다.실행 모드의 경우 두 가지가 있습니다. 클러스터 모드, 클라이언트 모드입니다.두 모드 가운데 어떤 것을 선택하든 수행 결과는 동일합니다. 하지만 클러스터 모드의 경우 드라이버 프로그램과 익스큐터 간의 네트워크 비용이 상대적으로 낮아져서 성능 향상을 기대할 수 있습니다. 하지만 스파크 셸과 같은 인터랙티브 환경을 이용한 디버깅이 어려워서 정형화된 작업에만 주로 사용하고, 클라이언트 모드의 경우 사용성이 편리하지만 드라이버 프로그램과 워커 노드가 네트워크 상에서 너무 많이 떨어져 있으면 전체적인 성능에 영향을 줄 수 있으므로 가급적 동일 네트워크 상에 존재하는 서버로 선택하는 것이 좋습니다.클러스터 매니저에 대해서는 다음 포스트에서 조금 더 자세히 다루도록 하겠습니다.스파크 애플리케이션 동작 순서지금까지 스파크의 클러스터 환경에서 갖게되는 아키텍처와 컴포넌트에 대해 살펴봤습니다. 지금부터는 아키텍처에서 실제로 스파크 애플리케이션이 구동되는 과정을 살펴보도록 하겠습니다.  가장 먼저 스파크 애플리케이션 코드를 작성합니다. 이 때 코드에는 스파크컨텍스트를 생성하는 드라이버 프로그램이 포함돼 있어야 합니다.  작성한 코드를 빌드하고 관련 라이브러리와 함께 jar나 zip 파일 등으로 패키징합니다.  패키지 파일을 스파크에서 제공하는 spark-submit 셸 스크립트를 이용해 클러스터에 배포하고 실행합니다.  코드에 있는 드라이버 프로그램이 실행되고 스파크컨텍스트가 클러스터 매니저와 연동되어 워커 노드에 익스큐터를 생성합니다.  드라이버 프로그램은 작성된 코드에서 액션 연산의 수만큼 잡(Job)을 생성합니다.  잡(Job)을 셔플링이 가장 적게 일어나는 방법으로 스테이지를 나누고 각 스테이지 단계를 여러 개의 태스크로 나눕니다.  태스크를 익스큐터에 적절히 분배하여 분산 처리합니다.참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  What is SparkContext? Explained",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-21T21:01:35+09:00'>21 Feb 2022</time><a class='article__image' href='/spark-series3'> <img src='/images/spark_13.png' alt='Apache Spark Series [Part3]: 스파크의 클러스터 환경'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series3'>Apache Spark Series [Part3]: 스파크의 클러스터 환경</a> </h2><p class='article__excerpt'>그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part2]: 블록체인의 동작 원리",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series1",
      "date"     : "Feb 21, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-21T21:01:35+09:00'>21 Feb 2022</time><a class='article__image' href='/blockchain-series1'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part2]: 블록체인의 동작 원리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series1'>BlockChain Series [Part2]: 블록체인의 동작 원리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part2]: 스파크 개발환경 구축하기",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series2",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  스파크 설치          pyspark                  자바, 파이썬 설치          pyspark 설치                    Spark        로컬 개발 환경  클러스터 환경  참고스파크 설치스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다. 자바가 필요한 이유는 스파크가 JVM 위에서 실행되기 때문입니다.하지만 실무에서는 대부분의 빅데이터 소프트웨어들이 클러스터 환경에서 동작하기 때문에 제대로 활용하기 위해서는 여러 가지 준비할 것도 많고 설정해야 할 것들도 많습니다. 그래서 스파크는 개발/테스트를 위한 용도로 간단하게 사용할 때에는 단독 서버에서 동작하는 로컬 모드를, 배포를 위한 용도로 클라이언트, 클러스터 모드를 지원합니다.스파크 애플리케이션 코드는 자바, 스칼라, 파이썬, R언어로 작성할 수 있습니다.pyspark우선 저는 파이썬을 주언어로 사용하기 때문에 pyspark를 이용해 파이썬으로 스파크 애플리케이션 코드를 작성할 예정입니다. pyspark의 장점은 만약 개발/테스트를 위한 목적으로만 스파크를 사용할 예정이라면 스파크를 설치할 필요가 없다는 것입니다. 스파크를 사용하는데 스파크를 설치할 필요가 없다? 무슨 뜻이냐면 pyspark를 설치하기만 해도 스파크를 실행하기 위해 필요한 최소한의 파일을 함께 설치해줍니다.하지만 여전히 자바는 설치해주어야 합니다.  To run Spark, you only require a Java runtime environment (JRE) but you may also download the Java development kit (JDK) which includes the JRE.저는 파이썬이 설치되어 있는 도커 이미지를 이용해 컨테이너 안에서 실습을 진행해 보았습니다.자바, 파이썬 설치# 파이썬이 설치된 컨테이너 생성docker run -it python:3.8-buster# JDK 설치apt-get updateapt-get install openjdk-11-jdk# JAVA_HOME 변수 설정, 경로 추가export JAVA_HOME=/etc/openjdk-11-jdk     # 본인의 자바 설치 경로export PATH=$JAVA_HOME/bin:$PATH. /etc/profile # bash쉘이면 source /etc/profilepyspark 설치# pyspark 설치pip install pyspark# 잘 설치되었는지 확인import pysparksc = pyspark.SparkContext(appName=&quot;SparkContext&quot;)sc--------------------------------SparkContextVersionv3.2.1Masterlocal[*]AppNameSparkContextSpark이번에는 파이썬에 국한되지 않는 조금 더 일반적인 방법으로 스파크를 설치해보겠습니다. 이번에는 리눅스 운영체제만 가지는 컨테이너 위에서 실습을 진행하도록 하겠습니다.처음에는 우분투 이미지를 바로 컨테이너로 띄우고 그 위에서 자바를 설치하려 했지만, 오라클에서 다운받는 방법을 제한하고 있어서 아래의 방법으로 진행했습니다. (우분투 이미지에 로컬에서 다운받은 자바를 하나의 이미지로 새로 빌드)그래서 사실 위에서 진행한 pyspark만 설치하는 방법에서도 python이미지에 로컬 자바로 한 번 이미지를 빌드한 후 사용하는 것이 좋을 것 같습니다. 저도 아직 본격적으로 사용해보지는 않아서 에러가 있는지는 확인해보지 않았지만 로컬에서 자바를 다운 받고 빌드하는 방법은 확실히 안전합니다.🦊 자바 설치자바 라이센스를 소유하고 있는 오라클에서 2019년 4월부터 자바를 외부의 허용하지 않은 방법으로 다운받는 것을 금지시켰습니다. 그래서 wget과 같은 방식으로 자바8 버전을 더이상 다운받을 수 없게 되고 무조건 오라클에 로그인을 한 후 로컬에 먼저 다운을 받아야합니다. 자바 17은 가능한데 스파크에서 자바 17로 설치하니까 오류가 난다. 구글링에서는 자바를 다운그레이드 하라고 나와있다. 자바8로 해보니까 된다. 그래서 자바8을 지금 다운 받으려고 하는 것이다.그래서 저같은 경우에는 왠만한 작업들은 무조건 도커 컨테이너에서 진행하는 편이라 처음에는 도커허브에서 자바8이 설치되어 있는 이미지를 찾아봤지만 뭔가 세부설정들이 마음에 들지 않게 되어 있어서 이미지를 직접 빌드하기로 결정했습니다. 제가 사용한 방법의 과정은 다음과 같습니다.# 자바를 로컬에 다운로드# 다운로드 페이지 접속https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html# 저는 M1 칩을 사용하고 있어서 ARM64 전용 파일을 다운로드 받았습니다.jdk-8u311-linux-aarch64.tar.gz# 다운로드 받은 폴더에서 압축해제tar -xzvf jdk-8u311-linux-aarch64.tar.gz# Dockerfile 작성# 로컬에 설치한 자바를 컨테이너로 옮기고 스파크까지 설치해주었습니다FROM ubuntu:latestCOPY jdk1.8.0_321 ./jdk1.8.0_321RUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim \&amp;amp;&amp;amp; apt-get -y install wget \&amp;amp;&amp;amp; wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz \&amp;amp;&amp;amp; tar -xzvf spark-3.2.1-bin-hadoop3.2.tgzvim /etc/profile# 환경 변수설정해줍니다. 이부분은 Dockerfile에서 ENV로 설정해 줄 수도 있습니다export JAVA_HOME=/jdk1.8.0_321export PATH=$JAVA_HOME/bin:$PATH. /etc/profilecd spark-3.2.1-bin-hadoop3.2ls--------------------------------------------------------------------------------------------------------------LICENSE  NOTICE  R  README.md  RELEASE  bin  conf  data  examples  jars  kubernetes  licenses  python  sbin  yarn# 스파크에서 제공하는 실행 파일cd binls# 스파크 셸 실행./bin/spark-shell# 셸 종료:q위의 과정은 이미지에서 매번 스파크를 다운받는 방식이기 때문에, 스파크를 다운받은 컨테이너를 다시 한 번 이미지로 만들면 그 다음부터는 새로 만든 이미지를 이용하면 컨테이너를 띄우는 속도가 더 빨라지게 됩니다. 그래서 docker commit 명령어를 이용해 한번 더 이미지를 빌드하는 것을 권장드립니다.# 로컬 터미널에서 docker commit 명령어로 이미지 생성# dockere commit &amp;lt;원하는 컨테이너 이름&amp;gt; &amp;lt;생성할 이미지 이름&amp;gt;docker commit thirsty_galois spark_container로컬 개발 환경위의 설치과정을 완료한 후 스파크의 설정 정보를 확인해 보겠습니다../bin/spark-shell --verbose다른 부분은 일단 신경쓰지 말고 master 부분만 보도록 하겠습니다. 현재 master가 local[*]로 설정되어 있습니다. 이는 현재 드라이버 프로그램을 실행하는 서버를 포함해 워커 노드까지 모두 로컬 서버를 이용하고 있다는 뜻입니다. *는 로컬 서버의 모든 스레드를 사용하겠다는 뜻입니다.따라서 여기까지만 설정하게 되면 로컬에서 테스트 목적으로 사용하기 위한 최소한의 준비는 끝난 것입니다. 이 외에도 여러 가지 설정들을 직접하고 싶을 때에는 ./conf에 설정을 위한 여러가지 파일의 템플릿을 이용할 수 있습니다.클러스터 환경참고  Pyspark 코드는 어디서 실행되는가?  Pyspark만으로 스파크 애플리케이션 실행할 수 있나?  Pyspark의 한계  bin/sh: 1: source: not found  [Linux] 우분투에 자바 설치  Unable to download Oracle JDK 8 using Wget command  자바(JDK, JRE) 모든 버전 다운로드( 6,7,8,9,10,11,12,13,14,15, 16, 17..)  How to Set Up a Multi Node Apache Spark Cluster with Quobyte  Updating the Apache Spark configuration files  [spark] Spark 3 클러스터 설치",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/spark-series2'> <img src='/images/spark_10.png' alt='Apache Spark Series [Part2]: 스파크 개발환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series2'>Apache Spark Series [Part2]: 스파크 개발환경 구축하기</a> </h2><p class='article__excerpt'>스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "HyperLedger Fabric Series [Part1]: What is HyperLedger Fabric?",
      "category" : "",
      "tags"     : "Hyperledger_fabric",
      "url"      : "/hyperledger_fabric-series0",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/hyperledger_fabric-series0'> <img src='/images/hyperledger_fabric_logo.png' alt='HyperLedger Fabric Series [Part1]: What is HyperLedger Fabric?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hyperledger_fabric-series0'>HyperLedger Fabric Series [Part1]: What is HyperLedger Fabric?</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(2): Java 관련 용어",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/dev_know-series2",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents아직 글을 작성 중입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series2'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(2): Java 관련 용어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series2'>Development Knowledge Series(2): Java 관련 용어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(1): Scripting Language vs Compile Language",
      "category" : "",
      "tags"     : "Javascript, Python, and Java",
      "url"      : "/dev_know-series1",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contentshttps://well-made-codestory.tistory.com/30?category=978350",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series1'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(1): Scripting Language vs Compile Language'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series1'>Development Knowledge Series(1): Scripting Language vs Compile Language</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part1]: 블록체인을 공부하기 전에",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series0",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  블록체인이란  블록체인에 대한 오해  블록체인 용어 바로 알기          분산 vs 탈중앙화      디지털화 vs 블록체인      가상 화폐 vs 암호 화폐      거래소 vs 중개소      트랜잭션 vs 거래내역      디지털 자산        블록체인의 역사          사이퍼펑크      비트코인의 탄생        블록? 블록체인? 비트코인?          블록      블록체인      비트코인        마치며  참고블록체인이란블록체인(blockchain)이란 다수의 거래내역을 묶어 블록을 구성하고, 해시를 이용하여 여러 블록들을 체인처럼 연결한 뒤, 다수의 사람들이 복사하여 분산 저장하는 알고리즘이다.블록체인은 비트코인과 이더리움 등 암호화폐에 사용된 핵심 기술이다. 은행 등 제3의 중개기관이 없더라도 블록체인 기술을 이용하면 누구나 신뢰할 수 있는 안전한 거래를 할 수 있다. 블록체인은 암호화폐뿐 아니라, 온라인 거래내역이 있고 이력관리가 필요한 모든 데이터 처리에 활용할 수 있다. 블록체인 기반의 스마트 계약, 물류관리 시스템, 문서관리 시스템, 의료정보관리 시스템, 저작권관리 시스템 등 다양한 활용이 가능하다.블록체인은 간략히 ‘분산원장’(分散元帳, distributed ledger) 기술이라고 한다. 즉, 거래내역을 기록한 원장을 다수의 사람들에게 분산하여 저장·관리하는 기술이다. 자세히 설명하면, 블록체인이란 다수의 온라인 거래 기록을 묶어 하나의 데이터 블록(block)을 구성하고, 해시(hash) 값을 이용하여 이전 블록과 이후 블록을 마치 체인(chain)처럼 연결한 뒤, 이 정보의 전부 또는 일부를 피투피(P2P) 방식으로 전 세계 여러 컴퓨터에 복사하여 분산 저장·관리하는 기술이다.블록체인에 대한 오해위의 내용은 (해시넷: 블록체인)에서 작성한 글의 일부를 가져온 것입니다.‘제 3자가 필요 없어지고, 이에 따라 수수료도 사라지는 이상적인 플랫폼’. 많은 사람들은 블록체인을 이용하면 그동안의 거래 시스템 전반에 많은 혁신을 가져다 줄 것으로 기대했습니다. 하지만 현실은 그렇지 않았습니다. 블록체인의 등장은 그동안 불필요했던 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다. 또한 독립적인 화폐를 내세웠던 암호 화폐는 선채굴을 악용한 채굴업자들이 암호화폐 대부분을 장악한 후 전 세계 중개소를 통해 일반인들을 선동, 호도하며 내다팔아 막대한 수익을 얻었습니다.저는 이번 BlockChain Series를 준비하며 앞으로 블록체인의 미래가 어떻게 될지 한 번 알아보려고 합니다.블록체인 용어 바로 알기분산 vs 탈중앙화블록체인을 공부하다 보면 분산, 탈중앙화라는 단어가 많이 사용되는 것을 보게 됩니다. 하지만 블록체인과 일반적인 분산 시스템과는 약간 다른 점이 있습니다. 분산 시스템은 데이터를 여러 노드에 분산해 나누어 처리하기 때문에 효율적입니다. 하지만 블록체인은 같은 데이터(거래 내역 등)를 모든 노드가 중복 처리합니다. 이 방법은 데이터의 신뢰도를 높일 수는 있지만 비효율적입니다.            특징      분산      탈중앙화              저장      여러 노드에 분산 저장      여러 노드에 중복 저장              처리      여러 노드가 분산 처리      여러 노드가 중복 처리              장점      높은 효율성      높은 신뢰성      디지털화 vs 블록체인블록체인을 공부하면서 블록체인의 장점으로 시스템의 효율성과 안전한 데이터 저장이라는 글을 본 적이 있습니다. 하지만 이는 디지털화(digitalization)의 장점이며 블록체인은 정확히 이것과 반대입니다. 블록체인은 효율성을 포기하고 데이터의 신뢰성을 높인 것이며, 데이터는 보안되지 않고 모든 사용자에게 공개되어 저장됩니다. 따라서 블록체인은 보안과 효율을 위한 도구가 아니고 신뢰받는 제 3자 없이 거래가 가능한 플랫폼을 만들기 위한 실험적 과정에 있는 개념이라고 생각하면 됩니다.가상 화폐 vs 암호 화폐가상 화폐는 어떤 실물의 가치를 디지털화한 것을 의미하며, 암호 화폐는 가상 화폐의 일부로 블록 체인을 기반으로 만들어진 화폐를 의미합니다. 암호화 되었다는 뜻은 암호 화폐를 보내는 송신인과 수신인이 암호화된 공개키를 통해서만 거래를 하기 때문으로 이는 금융 시스템의 투명성을 해치고 악용될 여지가 많습니다.거래소 vs 중개소거래소는 금융 거래소에서 가져온 단어로 금융 거래소는 거래 시스템에 필요한 환경과 제도가 잘 갖춰져 있습니다. 하지만 암호 화폐 거래소의 경우 암호 화폐의 가격이 단일화 되어 있지도 않고, 거래소 안에 엄격한 규정이 적용되어 있지도 않습니다. 그래서 정확한 명칭은 암호 화폐 중개소 정도가 적합합니다. 그리고 온라인 상에 있는 많은 중개소들은 블록체인과는 별 관련이 없습니다. 중개소는 그저 암호 화폐를 이용하는 지갑이라는 소프트웨어와 온라인 주식 매매에 이용되는 HTS 기능 중 일부를 사용해 거래를 중개하는 브로커일 뿐입니다.트랜잭션 vs 거래내역IT 분야에서는 트랜잭션을 보통 업무 처리의 단위로 얘기 하고 특히 데이터베이스 분야에서는 더 이상 쪼갤 수 없는(또는 더 쪼개면 심각한 오류가 발생할 수 있는)최소한의 업무 처리 단위를 의미합니다. 블록체인에 있어서 트랜잭션은 ‘정의된 이벤트가 발생하는 것’을 의미하며 거래 내역을 포함한 더 포괄적인 의미입니다.디지털 자산암호 화폐는 디지털 자산이 아닙니다. 디지털 자산은 저작권, 소유권 등의 권리를 디지털화 한 것으로 이는 실질적인 가치를 내재하고 있습니다. 하지만 암호 화폐는 내재 가치가 0인 디지털 숫자에 불과합니다. 하지만 내재 가치가 0인 경우에도 가치를 가질 수 있습니다. 그러기 위해 필요한 것이 바로 ‘신뢰’입니다. 따라서 암호 화폐가 실제로 가치를 가지기 위해서는 사람들로부터의 신뢰가 필요합니다. 예를 들어 지폐가 있습니다. 지폐는 종이에 불과하지만 그 뒤에 그 지폐를 발행한 국가의 법령과 신뢰가 뒷받침하기 때문에 실질적인 가치를 가질 수 있는 것입니다. 주식도 온라인 증명서에 불과하지만 주식을 발행한 기관의 신뢰 덕분에 가치를 가지게 됩니다.블록체인의 역사사이퍼펑크블록체인은 사이퍼펑크(cypherpunk) 운동에 뿌리를 두고 있습니다. 사이퍼펑크란 중앙집권화된 국가와 거대 기업들에 대항하여 개인의 프라이버시를 보호하기 위해 암호기술을 이용하여 익명성을 보장하는 탈중앙화 시스템을 만드려는 행동주의자들을 말합니다.비트코인의 탄생2008년 사토시 나카모토란 가명으로 암호화 커뮤니티에 논문이 하나 올라왔습니다. 제목은 비트코인: P2P 전자 캐시 시스템으로 사토시는 이 논문에서 코인을 그 누구의 간섭도 받지 않는 결제 수단이라고 설명했습니다. 그리고 2009년 1월 비트코인의 개념을 실제로 소프트웨어로 구현되어 최초의 블록(제네시스 블록 또는 0번 블록)이 생성되었습니다. 비트코인은 현재까지 10분에 하나꼴로 블록이 만들어지고 있으며 2019년 10월 기준 70만개의 블록이 만들어졌습니다. (참고로 사토시의 정체는 아직까지 밝혀지지 않았다고 합니다)블록? 블록체인? 비트코인?블록체인을 공부하다 보면 블록의 정체가 무엇인지 또 나는 블록체인을 공부하는데 왜 자꾸 나도 모르는 사이에 비트코인이라는 단어가 등장하는 건지 의아했었습니다. (참고로 사토시가 공개한 비트코인에 관한 논문에는 ‘블록’과 ‘체인’이라는 명사가 독립적으로 사용되기는 했지만 ‘블록체인’이라는 단어는 한 번도 등장하지 않았습니다.)블록블록은 전산학에서 보통 한꺼번에 처리되는 논리적 데이터 단위를 일컫습니다. 비트코인에서는 트랜잭션들을 1메가바이트를 넘지 않는 선에서 계속 묶었다가 1메가바이트 직전이 되면 블록으로 만듭니다. 보통 이렇게 하나의 블록이 만들어지는데 10분 정도가 걸립니다.            블록      설명              정의      한꺼번에 처리되는 데이터 단위              크기      최대 1MB              트랜잭션 수      2000~3000개              생성 시간      평균 10분      블록체인트랜잭션 데이터를 포함하는 블록을 앞의 블록의 해시값을 이용해 연결한 것을 블록체인이라고 합니다. 블록은 각각의 노드에서 자신이 가지고 있는 트랜잭션들을 묶다가 블록을 만들 수 있는 시점이 되면 비동기적으로 블록을 만들기 위해 경쟁합니다. 이 때의 경쟁을 채굴(mining)이라고 하며 가장 먼저 블록을 만든 노드의 블록만을 모든 노드의 블록체인에 추가합니다.비트코인비트코인 생태계(네트워크)에서 채굴로 얻게되는 보상을 ‘비트코인’이라고 합니다. 이 보상을 얻기 위해 채굴자들이 그렇게 앞다투어 채굴을 했던 것입니다. 여기서 채굴은 블록을 만들 때 블록에 필요한 해시값을 찾는 과정을 일컫습니다. 채굴에 대한 보상으로 얻게되는 비트코인의 수량은 어떻게 책정될까요? 보상금은 보조금과 수수료의 합으로 이루어지는데, 보조금은 채굴로 얻게되는 고정 수익을 말하고, 수수료는 비트코인 거래 시 송신자로부터 얻게 되는 것으로 수수료는 송신자가 스스로 정하게 됩니다. 그래서 보통 더 높은 수수료를 지불한 트랜잭션이 먼저 블록에 포함되게 됩니다. 보조금의 경우 제네시스 블록이 생성됐을 때에는 50BTC 였으나, 보조금은 블록이 21만개 생성될 때마다 반감되도록 설정되어 현재는 블록을 하나 생성할 때마다 6.25BTC를 얻게 됩니다.블록은 평균 10분 마다 한 개씩 생성되고 보조금은 21만개 마다 계속 반감되기 때문에 시간이 지나면 블록을 생성해도 더 이상 비트코인이 (거의) 발행되지 않게 될 것입니다. 이론적으로 2033년에 거의 포화되는 시점에 다다를 것으로 보며 그 때까지 누적된 비트코인의 양은 2100만BTC라고 합니다. 이렇게 채굴에 대한 보상이 극도로 낮아질 경우, 채굴자가 급격히 줄어들게 될 것이고, 이는 비트코인 시스템의 안전성을 급격하게 저하시키게 됩니다.마치며지금까지 블록체인에 대해 공부하기 전에 알고가면 좋은 지식들에 대해 살펴보았습니다. 다음 포스트에서는 블록체인의 동작원리에 대해 알아보겠습니다.참고  블록체인 해설서 책  해시넷: 블록체인",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/blockchain-series0'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part1]: 블록체인을 공부하기 전에'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series0'>BlockChain Series [Part1]: 블록체인을 공부하기 전에</a> </h2><p class='article__excerpt'>하지만 현실은 블록체인의 등장으로 불필요한 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Bitcoin Series [Part1]: What is Bitcoin?",
      "category" : "",
      "tags"     : "Bitcoin",
      "url"      : "/bitcoin-series0",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/bitcoin-series0'> <img src='/images/bitcoin_logo.jpeg' alt='Bitcoin Series [Part1]: What is Bitcoin?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/bitcoin-series0'>Bitcoin Series [Part1]: What is Bitcoin?</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-memory_allocation",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  메모리 관리          Python Memory Manager      Garbage Collection        메모리 할당          Stack 할당      Heap 할당        참고요즘에는 컴퓨터, 스마트폰을 사용할 때 한가지 프로그램/어플리케이션만 실행하는 사람은 없을 것입니다. 그렇기 때문에 내가 만든 프로그램/어플리케이션이 메모리를 효율적으로 사용하도록 개발하는 것은 중요합니다.메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다. 메모리 할당은 내가 사용하는 프로그래밍 언어, 운영체제, 컴퓨터 아키텍처에 따라 조금씩 다르지만 전체적인 과정은 비슷합니다.파이썬의 메모리 관리는 대부분 Python Memory Manager에 의해 수행되지만, Python Memory Manager를 공부하면 프로그래밍 전반에 대한 이해와 코드 최적화, 디버깅을 더욱 잘 할 수 있게 될 것입니다.메모리 관리파이썬에서의 메모리 관리는 크게 두 가지 레벨로 나누어서 생각할 수 있습니다. 첫 번째로 운영체제 레벨에서는 각 프로세스에 메모리를 얼마나 할당할지를 정하고, 각각의 프로세스가 다른 프로세스에 접근하지 못하도록 관리합니다.두 번째로 파이썬 내에서의 메모리 관리입니다.파이썬에서는 컴파일 단계에서 스택 영역에 메모리를 정적으로 크기를 정하고, 실행 단계에서는 Python memory manager를 이용해 힙 영역에 동적으로 메모리를 할당하고 그 외의 역할(공유, 할당, 제거 등)들을 수행함으로써 메모리를 관리합니다.Python Memory ManagerPython memory manager는 모든 Python objects와 data structures를 포함하는 private 힙을 포함합니다. Python memory manager는 공유(sharing), 분할(segmentation), 사전 할당(preallocation) 또는 캐싱(caching)과 같은 다양한 동적 스토리지 관리 측면을 다루는 다양한 구성 요소를 가지고 있습니다.가장 낮은 수준에서 raw memory allocator는 운영 체제의 메모리 관리자와 상호 작용하여 모든 파이썬 관련 데이터를 저장할 수 있는 충분한 공간을 private 힙에 확보합니다. raw memory allocator 외에도 여러 object-specific allocators가 동일한 힙에서 object의 특성에 맞는 고유한 메모리 관리 정책을 구현합니다.파이썬 힙의 관리는 인터프리터 자체에 의해 수행되며 힙 내부의 메모리 블록에 대한 객체 포인터를 사용자가 직접 제어할 수 없다는 것을 의미합니다. Python objects를 위한 힙 공간 할당은 파이썬/C API 함수를 통해 파이썬 메모리 관리자에 의해 수행됩니다.Garbage Collection가비지 컬렉션은 인터프리터가 프로그램을 사용하지 않을 때 프로그램의 메모리를 비우는 것입니다. 파이썬이 이렇게 할 수 있는 것은 파이썬 개발자들이 백엔드에서 우리를 위해 가비지 컬렉터를 구현했기 때문입니다. 파이썬 가비지 컬렉터는 reference counting 방법으로 객체에 더 이상 참조가 없을 때는 객체가 차지하고 있던 메모리의 할당을 취소하고 메모리를 비우게 됩니다.메모리 할당메모리 할당(memory allocation)은 프로그램이 컴퓨터 메모리의 특정 빈 블록에 할당되거나 할당되는 과정을 의미합니다. 파이썬에서 이 모든 것은 Python memory manager에 의해 수행됩니다.Stack 할당스택 할당은 정적 메모리를 저장하는데, 정적 메모리는 특정 함수나 메서드 호출 내에서만 필요한 메모리입니다. 함수가 호출되면 프로그램의 호출 스택에 추가됩니다. 변수 초기화 같은 특정 함수 내부의 모든 로컬 메모리 할당은 함수 호출 스택에 임시로 저장되며, 함수가 돌아오면 삭제되고 호출 스택이 다음 작업으로 이동합니다. 연속적인 메모리 블록에 대한 이 할당은 미리 정의된 루틴을 사용하여 컴파일러에 의해 처리되기 때문에 개발자들은 이것에 대해 걱정할 필요가 없습니다.(그러면 이 부분은 함수, 메서드, 그리고 함수나 메서드 안에서 사용되는 지역변수들을 컴파일 단계에서 확인하고 이에 알맞은 메모리 크기를 예측해서 정적으로 할당하는 건가?)(그리고 코드 실행 단계에서 함수가 호출될 때마다 스택에 Call stack이 쌓인다?)Heap 할당힙 할당은 프로그램에서 전역 범위로 사용되는 메모리인 동적 메모리를 저장합니다. 이러한 변수들은 특정 메서드나 함수 호출 외부에 필요하거나 전역적으로 여러 함수 내에서 공유됩니다. 스택 할당과 달리 힙 할당은 힙 데이터 구조와 관련이 없습니다. 힙 영역은 단순히 할당하고 어느 정도 임의의 주소에서 자유롭게 사용할 수 있는 큰 메모리 공간이며, 저장되는 객체에 필요한 공간을 기반으로 합니다.(힙 할당은 코드 실행 단계에서 객체의 타입에 맞게 동적으로 메모리 할당된다?)(Python memory manager의 가비지 컬렉션 기능에 의해 더이상 참조되지 않는 객체는 제거되고 메모리 비운다?)참고  How does Memory Allocation work in Python (and other languages)?  Python 공식문서: Memory Management  What’s the difference between a stack and a heap?  RealPython: Memory Management in Python  muchogusto.log: 파이썬 런타임과 메모리 관리  python의 메모리 할당과 관리 (Stack &amp;amp; Heap Memory)  파이썬 메모리 영역",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-memory_allocation'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-memory_allocation'>Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)</a> </h2><p class='article__excerpt'>메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-cpython_pypy",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  파이썬 언어의 특징  파이썬은 인터프린터 언어?  컴파일 언어  인터프리터 언어  파이썬 인터프리터의 종류          CPython      Jython      PyPy        참고파이썬 언어의 특징파이썬(Python)은 1980년대 후반 귀도 반 로섬이 개발하고 1991년에 출시한 high-level 범용 프로그래밍 언어입니다. 동적 타입 언어이므로 변수의 타입을 선언할 필요가 없으며, 코드가 실행되어 메모리 관리가 자동으로 수행됩니다.이번 포스트에서는 파이썬 코드를 실행할 때 어떤 동작이 내부적으로 일어나는지 알아보도록 하겠습니다.저희가 CLI 환경에서 파이썬 코드를 실행하는 경우를 생각해봅시다.python my_code.py여기서 python은 바로 파이썬 인터프리터 프로그램을 의미합니다. 따라서 파이썬 코드는 파이썬 인터프리터를 통해 실행하는 것입니다.  (Python Interpreter - Stanford Computer Science 참고)파이썬은 인터프린터 언어?위에서 파이썬 인터프리터를 통해 파이썬 코드를 실행한다고 했습니다. 그러면 파이썬은 인터프리터 언어일까요? 과거에 C는 컴파일 언어이고, 쉘 프로그래밍 언어는 인터프리터 언어였기 때문에 이 경계가 비교적 명확했지만, 비교적 최근에 나온 언어들은 그 경계가 모호합니다.파이썬 또한 어느 정도의 컴파일 언어적 특성과, 인터프리터 언어의 특성을 모두 가지고 있어서 이분법적으로 나누기가 힘들지만 프로그래밍 언어의 대표인 C언어가 완전한 컴파일 언어이기 때문에 이와 구분하기 위해 그냥 편하게 인터프리터 언어라고 하는 것 같습니다. 정리하면 파이썬은 컴파일 언어이기도 하면서 인터프리터 언어이기도 합니다.컴파일 언어먼저 간단하게 컴파일 언어의 뜻과 컴파일 언어가 코드를 실행하는 과정에 대해 살펴보겠습니다.  컴파일은 소스코드를 다른 타겟 언어(기계어, 자바, C, 파이썬)로 변환하는 과정을 의미  코드를 실행할 때 코드 전체를 인풋으로 사용  코드는 컴파일 단계에서 한 번 기계어로 변환되어 저장되고 나면 언제든 바로 실행가능  컴파일러는 실행하는 역할이 아니고 기계어로 변환하는 역할인터프리터 언어  인터프리터 언어는 소스 코드를 바로바로 실행하게 됩니다.  인터프리터는 코드를 한 줄씩 입력으로 사용해 실행합니다.  인터프리터에는 종류마다 다른 실행 방법이 있습니다.  A. 소스 코드를 파싱해서 설정한 방법에 따라 실행  B. 소스 코드를 먼저 중간 단계의 바이트 언어로 변환하고 1의 과정을 수행  C. 컴파일러에 의해 먼저 변환된 코드를 이용해 1의 과정을 수행파이썬 인터프리터는 이 중 B에 해당합니다.1. 소스 코드를 컴파일러를 이용해 중간 단계의 바이트 언어의 파일(.pyc)로 변환2. Python Virual Machine 위에서 바이트 언어 파일 한 줄씩 실행  파이썬 인터프리터의 종류CPythonJythonPyPyPyPy는 파이썬 언어(RPython: 파이썬의 일부)로 작성된 인터프리터입니다. 디폴트인 CPython과의 호환성을 유지하면서도 CPython 보다 속도가 빠르다고 알려져있습니다.참고  Shalini Ravi, How Does Python Code Run: CPython And Python Difference  elhay efrat, Python » CPython  geeksforgeeks, Difference between various Implementations of Python  파이썬 언어의 특징  파이썬 인터프리터 고르기  Wireframe: 파이썬은 인터프리터언어입니까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-cpython_pypy'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-cpython_pypy'>Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part3]: Javascript 표현식과 문",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series3",
      "date"     : "Feb 6, 2022",
      "content"  : "Table of Contents  값  리터럴  표현식  문  세미콜론 자동 삽입  표현식인 문과 표현식이 아닌 문값값은 표현식이 평가되어 생성된 결과를 말한다. 평가란 식을 해석해서 값을 생성하거나 참조하는 것을 의미한다.10 + 20;x - 1;리터럴리터럴은 사람이 이해할 수 있는 문자 또는 약속된 기호를 사용해 값을 생성하는 표기법을 말한다.            리터럴      예시              정수      100              부동소수점      0.4              2진수      0b010001              문자열      ‘Hello’              불리언      true, false              null      null              undefined      undefined              객체      { name: ‘Lee’, address: ‘Seoul’ }              배열      [1, 2, 3]              함수      function() {}              정규 표현식      /[A-Z]+/g      표현식표현식(expression)은 값으로 평가될 수 있는 문이다. 위에서 살펴본 리터럴은 값으로 평가될 수 있기 때문에 리터럴도 표현식이라 할 수 있다.10&#39;Hello&#39;person.namearr[1]10 + 20sum = 10sum !== 10square()person.getName()x + 3문문(statement)은 프로그램을 구성하는 기본 단위이자 최소 실행 단위다. 문은 여러 토큰으로 구성된다. 토큰이란 문법적인 의미를 가지며, 문법적으로 더 이상 나눌 수 없는 코드의 기본 요소를 의미한다. 예를 들어, 키워드, 식별자, 연산자, 리터럴, 세미콜론, 마침표 등의 특수기호는 모두 토큰이다.문은 선언문, 할당문, 조건문, 반복문 등으로 구분할 수 있다.// 선언문var x;// 할당문x = 5;// 함수 선언문funciton foo() {}// 제어문if (x &amp;gt; 1) { console.log(x); }// 반복문for (var i = 0; i &amp;lt; 2; i++) { console.log(i); }세미콜론 자동 삽입세미콜론(;)은 문의 종료를 나타낸다. 즉 자바슼립트 엔진은 세미콜론으로 문이 종료한 위치를 파악하고 순차적으로 하나씩 문을 실행한다. 단 괄호로 묶은 코드 블록 뒤에는 세미콜론을 붙이지 않는다. 블록은 문의 종료를 의미하는 자체 종결성을 갖기 때문이다.세미콜론은 생략 가능하다. 이는 자바스크립트 엔진이 소스코드를 해석할 때 문의 끝이라고 예측되는 지점에 세미콜론을 자동으로 붙여주는 세미콜론 자동 삽입 기능(ASI:Automatic Semicolon Insertion)이 암묵적으로 수행되기 때문이다.하지만 개발자의 의도와 일치하지 않는 경우도 생기기 때문에 자바스크립트 커뮤니티에서는 세미콜론의 사용을 권장한다.표현식인 문과 표현식이 아닌 문값으로 평가될 수 있으면 표현식인 문이고, 그렇지 않으면 표현식이 아닌 문이다.// 표현식이 아닌 문var x;// 표현식인 문x = 1 + 2;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-06T21:01:35+09:00'>06 Feb 2022</time><a class='article__image' href='/javascript-series3'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part3]: Javascript 표현식과 문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series3'>Javascript Series [Part3]: Javascript 표현식과 문</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part2]: Javascript 변수",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  변수란 무엇인가? 왜 필요한가?  변수 선언  변수 호이스팅  값의 할당  변수 네이밍  참고변수란 무엇인가? 왜 필요한가?사람은 계산과 기억을 모두 두뇌에서 하지만, 컴퓨터는 연산과 기억을 수행하는 부품이 나눠져 있다. 컴퓨터는 CPU를 사용해 연산하고, 메모리를 사용해 데이터를 기억한다.메모리는 데이터를 저장할 수 있는 메모리 셀의 집합체다. 메모리 셀 하나의 크기는 1바이트이다. 각 셀은 고유의 메모리 주소를 갖는다. 모든 값은 메모리 상의 임의의 위치에 저장되고 CPU는 이 값을 읽어들여 연산을 수행한다. 연산 결과로 생성된 값도 메모리 상의 임의의 위치에 저장된다. 하지만 문제는 이 값을 재사용하기 위해서는 어디 저장되어야 하는지 알아야 하는데 모른다는 것이다. 설령 안다고 하더라도 개발자가 직접적으로 메모리에 접근하는 것은 위험하다. 그래서 자바스크립트는 직접적인 메모리 제어를 허용하지 않는다.프로그래밍 언어는 기억하고 싶은 값을 메모리에 저장하고, 저장된 값을 읽어 들여 재사용하기 위해 변수라는 메커니즘을 제공한다.변수는 하나의 값을 저장하기 위해 확보한 메모리 공간을 식별하기 위해 붙인 이름을 말한다.변수는 컴파일러 또는 인터프리터에 의해 메모리 공간의 주소로 치환되어 실행된다. 따라서 개발자가 직접 메모리 주소를 통해 값을 저장하고 참조할 필요가 없고 변수를 통해 안전하게 값에 접근할 수 있다.변수 선언변수 선언이란 변수를 생성하는 것을 말한다. 좀 더 자세히 말하면 메모리에는 값을 저장하기 위한 공간을 확보하고, 변수 이름을 메모리 공간의 주소로 연결(name binding)하는 것을 말한다.변수를 사용하려면 반드시 선언이 필요하다. 변수를 선언할 때는 var, let, const 키워드를 사용한다. ES6에서 let, const 키워드가 도입되기 전까지는 var 키워드가 변수 선언을 위한 유일한 키워드였다.🦄 **var 키워드의 단점**  var 키워드의 가장 대표적인 단점은 블록 레벨 스코프를 지원하지 않고, 함수 레벨 스코프만 지원한다는 것이다.  자바스크립트 엔진은 변수 선언을 다음과 같은 2단계에 걸쳐 수행한다.  선언 단계: 변수 이름을 등록해서 자바스크립트 엔진에 변수의 존재를 알린다.  초기화 단계: 값을 저장하기 위한 메모리 공간을 확보하고 암묵적으로 undefined를 할당해 초기화한다.이렇게 초기화 단계를 거침으로써 이전에 다른 애플리케이션이 사용했던 값(garbage value)이 재사용되는 것을 방지해줄 수 있다.변수 호이스팅console.log(score);var score;위의 코드를 보면 변수 선언문보다 변수를 참조하는 코드가 앞에 있다. 그럼에도 결과는 에러가 아닌 undefined다.자바스크립트에서는 변수 선언이 런타임(소스코드가 실행되는 시점)이 아니라 그 이전 단계에서 먼저 실행된다.자바스크립트 엔진은 소스코드를 실행하기에 앞서 먼저 소스코드의 평가 과정을 거치면서 소스코드를 실행하기 위한 준비를 한다. 이 과정에서 엔진은 변수 선언을 포함한 모든 선언문을 소스코드에서 찾아서 먼저 실행한다. 그리고 이 과정이 끝나면 비로소 모든 선언문을 제외한 나머지 코드를 한 줄씩 순차적으로 실행한다.이처럼 모든 선언문(var, let, const, function, function*, class이 코드의 선두로 끌어 올려진 것처럼 동작하는 자바스크립트의 특징을 변수 호이스팅이라 한다.값의 할당변수에 값을 할당할 때는 연산자 =를 사용한다.var score; // 변수 선언score = 80; // 값의 할당var score = 80; // 변수 선언과 값의 할당자바스크립트 엔진은 변수 선언과 값의 할당을 하나의 문으로 단축 표현해도 2개의 문으로 나누어 각각 실행한다. 이 때 주의할 점은 변수 선언과 값의 할당의 실행 시점이 다르다는 것이다. 변수 선언은 값의 할당이 일어나는 런타임 이전에 이루어진다.console.log(score); // undefinedvar score = 80;console.log(score); // 80또한 변수의 선언과 값의 할당을 하나의 문으로 단축 표현해도 자바스크립트 엔진은 나누어 실행하므로, 변수에 먼저 undefined가 할당되어 초기화되는 것은 변함이 없다. 그리고 값이 할당될 때에는 새로운 메모리 공간을 확보하고 그 곳에 할당 값을 저장한다.이렇게 되고나면 undefined는 가비지 컬렉션에 의해 메모리에서 자동 해제된다.변수 네이밍자바스크립트는 변수 네이밍을 할 때에 카멜 케이스와 파스칼 케이스를 권장한다.참고  name-binding",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/javascript-series2'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part2]: Javascript 변수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series2'>Javascript Series [Part2]: Javascript 변수</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Airflow Series [Part1]: What is Airflow",
      "category" : "",
      "tags"     : "Airflow",
      "url"      : "/airflow-series1",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/airflow-series1'> <img src='/images/airflow_logo.png' alt='Airflow Series [Part1]: What is Airflow'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/airflow-series1'>Airflow Series [Part1]: What is Airflow</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 네트워크 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series9",
      "date"     : "Feb 4, 2022",
      "content"  : "Table of Contents  Bridge Network Driver  Overlay Network Driver  도커 네트워크 실습          도커 네트워크의 몇 가지 특징      Bridge 드라이버 사용해보기      Overlay 드라이버 사용해보기        참고도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다.이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.당연히 다음 질문은 어떤 네트워크 드라이버를 사용해야 하는가 하는 것입니다. 각 드라이버는 트레이드오프를 제공하며 사용 사례에 따라 다른 장점이 있습니다. 도커 엔진과 함께 제공되는 내장 네트워크 드라이버가 있으며 네트워킹 벤더와 커뮤니티에서 제공하는 플러그인 네트워크 드라이버도 있습니다. 가장 일반적으로 사용되는 내장 네트워크 드라이버는 bridge, overlay, macvlan입니다. 이번 포스트에서는 비교적 간단한 드라이버인 bridge와 overlay에 대해서만 살펴보겠습니다.Bridge Network Driverbridge 네트워크 드라이버가 우리 목록의 첫 번째 드라이버입니다. 이해하기 쉽고, 사용하기 쉽고, 문제 해결이 간단하기 때문에 개발자와 Docker를 처음 접하는 사람들에게 좋은 네트워킹 선택이 됩니다. bridge 드라이버는 private 네트워크를 호스트 내부에 생성해 컨테이너들이 생성한 네트워크 안에서 통신할 수 있도록 합니다. 컨테이너에 포트를 노출함으로써 외부 액세스가 허용됩니다. 도커는 서로 다른 도커 네트워크 간의 연결을 차단하는 규칙을 관리하여 네트워크를 보호합니다.내부적으로 도커 엔진은 리눅스 브리지, 내부 인터페이스, iptables 규칙 및 호스트 경로를 만들어 컨테이너 간의 연결을 가능하게 합니다. 아래 강조 표시된 예에서는 도커 브리지 네트워크가 생성되고 두 개의 컨테이너가 이 네트워크에 연결됩니다. 도커 엔진은 별도의 설정 없이 필요한 연결을 수행하고 컨테이너에 대한 서비스 디스커버리를 제공하며 다른 네트워크와의 통신을 차단하도록 보안 규칙을 구성합니다.우리의 애플리케이션은 현재 호스트 8000번 포트에서 서비스되고 있습니다. 도커 브리지는 컨테이너 이름으로 web이 db와 통신할 수 있도록 하고 있습니다. 브릿지 드라이버는 같은 네트워크에 있기 때문에 자동으로 우리를 위해 서비스 디스커버리를 합니다.브리지 드라이버는 로컬 범위 드라이버이므로 단일 호스트에서 서비스 디스커버리, IPAM 및 연결만 제공합니다. 다중 호스트 서비스 검색을 수행하려면 컨테이너를 호스트 위치에 매핑할 수 있는 외부 솔루션이 필요합니다. 이 때 필요한 것이 바로 overlay 드라이버입니다.Overlay Network Driveroverlay 네트워크 드라이버는 multi-host 네트워킹의 많은 복잡성을 획기적으로 단순화합니다. Swarm 스코프 드라이버로, 개별 호스트가 아닌 전체 Swarm 또는 UCP 클러스터에서 작동합니다.overlay 드라이버는 컨테이너 네트워크를 물리적 네트워크와 분리해주는 VXLAN data plane을 사용합니다. 덕분에 다양한 클라우드, 온-프레미스 네트워크 환경 속에서 최고의 이식성을 제공해줍니다.도커 네트워크 실습(라우드 엔지니어 Won의 성장 블로그 참고)도커 네트워크의 몇 가지 특징  도커는 컨테이너에 내부 IP(eth0)를 순차적으로 할당  컨테이너 외부에 노출시킬 엔드포인트로 veth(Virtual Ethernet) 생성  컨테이너마다 veth 네트워크 인터페이스 자동 생성  docker0는 기본 생성되는 디폴트 브리지로 각 veth 인터페이스와 호스트의 기본 네트워크인 eth0와 연결Bridge 드라이버 사용해보기Overlay 드라이버 사용해보기참고  도커 공식문서  MARK CHURCH, Understanding Docker Networking Drivers and their use cases  클라우드 엔지니어 Won의 성장 블로그, 06. 도커 네트워크 포스트  DaleSeo: Docker 네트워크 사용법  Julie의 Tech블로그, 도커 - 네트워킹 / bridge와 overlay",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-04T21:01:35+09:00'>04 Feb 2022</time><a class='article__image' href='/docker-series9'> <img src='/images/docker_network_logo.png' alt='Docker의 네트워크 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series9'>Docker의 네트워크 이해하기</a> </h2><p class='article__excerpt'>이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part1]: What is Flink",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series1",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  참고아직 작성 전 입니다…참고  Flink and Kafka Streams: a Comparison and Guideline for Users",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/flink-series1'> <img src='/images/flink_logo.png' alt='Flink Series [Part1]: What is Flink'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series1'>Flink Series [Part1]: What is Flink</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 볼륨 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series8",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  도커에서 데이터 관리하기  마운트 종류  Volume          Volume을 사용하기 좋은 경우        Bind Mount          Bind mount를 사용하기 좋은 경우        tmpfs Mount          tmpfs mount를 사용하기 좋은 경우        사용 방법          Dockerfile      docker run command      docker compose        참고도커에서 데이터 관리하기기본적으로 컨테이너 안에서 생성된 모든 파일은 컨테이너 레이어에 저장됩니다. 그래서, 해당 컨테이너가 삭제되면 데이터도 함께 사라집니다. 따라서 컨테이너의 생명 주기와 관계없이 데이터를 영구적으로 저장하기 위한 방법이 필요합니다. 또한 여러 컨테이너가 데이터를 공유할 수 있으면 데이터를 컨테이너별로 중복 저장할 필요가 없어 컨테이너를 더욱 효율적으로 관리할 수 있게 될 것입니다.이러한 이유로 다음과 같은 기능을 가진 옵션을 도커에서는 제공해주고 있습니다.  데이터 영구 저장  컨테이너간 데이터 공유도커에서 볼륨을 위해 제공해주는 옵션 중 volume 또는 bind mount를 사용하면 컨테이너가 중지된 후에도 파일을 호스트 머신에 파일을 저장함으로써 유지할 수 있습니다.참고로 도커에서 tmpfs mount를 사용하면 호스트의 시스템 메모리에 인-메모리 형식으로 파일을 저장할 수 있습니다.마운트 종류어떤 종류의 마운트를 사용하더라도 컨테이너 안에서 데이터의 모습은 같습니다. 각 마운트 종류의 차이를 이해하는 쉬운 방법은 데이터가 도커 호스트 내에서 어디 존재하는지 생각해보는 것입니다.      Volume을 사용하면 도커에 의해 관리되는 호스트 파일 시스템(Linux기준 /var/lib/docker/volumes/)에 데이터가 저장됩니다. 비-도커 프로세스들은 여기 파일들을 수정해서는 안됩니다.        Bind mount는 호스트 머신 어디든 저장될 수 있습니다. 비-도커 프로세스들도 여기 파일들을 언제든 수정할 수 있습니다.        tmpfs mount는 호스트 메모리 시스템에만 저장됩니다. 호스트 파일 시스템에는 절대 저장되지 않습니다.  VolumeVolume은 완전히 도커에 의해서만 관리되어 호스트 머신의 디렉토리 구조나 OS에 독립적인, 도커에서 데이터를 유지하기 위한 권장되는 메커니즘입니다.또한 볼륨이 컨테이너를 사용하는 컨테이너의 크기를 늘리지 않고 컨테이너의 라이프사이클 외부에 존재하기 때문에 컨테이너 레이어에서 데이터를 유지하는 것보다 더 나은 선택인 경우가 많습니다.Volume은 docker volume create 명령어를 이용해 명시적으로 볼륨을 생성할 수도 있고, 컨테이너를 생성할 때 같이 볼륨을 생성할 수도 있습니다.볼륨은 도커 호스트 내의 디렉토리에 저장됩니다. 컨테이너에 마운트되는 볼륨이 바로 이것 입니다. volume은 bind mount와 비슷하지만 다른 점은 도커에 의해 관리 되고 호스트 머신으로부터 isolated 되었다는 점입니다.주어진 볼륨은 여러 컨테이너에 동시에 마운트 될 수 있습니다. 볼륨을 사용해 실행 중인 컨테이너가 없더라도 볼륨이 저절로 제거되지 않습니다. 만약 사용하지 않는 볼륨을 제거하고 싶다면 docker volume prune 명령어를 사용하면 됩니다.볼륨 드라이버를 사용해 클라우드 또는 리모트 호스트에 데이터를 저장할 수도 있습니다.Volume을 사용하기 좋은 경우  여러 컨테이너에 마운트하고 싶은 경우. 명시적으로 표현한 볼륨이 없으면 자동으로 생성하고 마운트 해줍니다.  도커 호스트의 파일 구조를 모르는 경우. bind mount와 달리 Volume은 볼륨 명으로 관리(bind mount는 디렉토리 경로)  로컬이 아닌 리모트 호스트 또는 클라우드 서버에 저장하고 싶은 경우  높은 성능의 I/O을 요구하는 경우. 볼륨은 호스트가 아닌 Linux VM에 저장. 따라서 읽고 쓰는데 있어 성능이 훨씬 뛰어납니다.  Docker Desktop에서 완전히 네이티브한 파일 시스템이 필요한 경우  백업, 데이터 통합이 필요한 경우Bind Mountbind mount는 volume에 비해 기능이 제한됩니다. bind mount를 사용하면 호스트 머신의 파일 또는 디렉토리가 컨테이너에 마운트됩니다. 파일 또는 디렉토리는 호스트 시스템의 절대 경로에서 참조됩니다. 반대로 volume을 사용하면 호스트 머신의 Docker 스토리지 디렉토리 내에 새 디렉토리가 생성되고 Docker가 해당 디렉토리의 내용을 관리합니다.파일 또는 디렉토리가 도커 호스트에 아직 존재하지 않아도 됩니다. 아직 존재하지 않는 경우 요청 시 생성됩니다. bind mount는 성능이 매우 뛰어나지만 사용 가능한 특정 디렉토리 구조가 있는 호스트 시스템의 파일 시스템에 의존합니다.bind mount를 사용하면 컨테이너에서 실행되는 프로세스를 통해 호스트 파일 시스템을 변경할 수 있습니다. 이는 호스트 시스템에 Docker가 아닌 프로세스에 영향을 미치는 등 보안에 안좋은 영향을 미칠 수 있는 강력한 기능입니다.Bind mount를 사용하기 좋은 경우  호스트 머신에 있는 설정 파일을 컨테이너와 공유하고 싶은 경우  파일 또는 디렉토리 구조가 항상 일관될 수 있는 경우tmpfs Mounttmpfs 마운트는 도커 호스트 또는 컨테이너 내에서 디스크에 유지되지 않습니다. 컨테이너 수명 동안 컨테이너가 비영구 상태 또는 중요한 정보를 저장하는 데 사용할 수 있습니다. 예를 들어, 내부적으로, swarm 서비스는 tmpfs 마운트를 사용하여 서비스의 컨테이너에 비밀을 탑재한다.tmpfs mount를 사용하기 좋은 경우  보안상의 이유 또는 대용량 데이터로 인한 성능 저하 우려로 데이터가 호스트 머신 또는 컨테이너에 유지되길 원하지 않는 경우.사용 방법Dockerfile  Dockerfile에는 볼륨 기능을 위해 VOLUME을 제공합니다.  이미지가 빌드되는 호스트 머신은 사용자에 따라 달라지므로 source는 지정할 수 없습니다.  VOLUME에서 표기하는 것은 오직 컨테이너안에 있는 볼륨의 destination입니다.VOLUME /myvolume  데이터가 사라지지 않도록 저장해두는 source는 컨테이너 생성/실행 시 표기할 수 있습니다.docker run -it -v $(pwd)/src:/src my-image  사실 Dockerfile에서 VOLUME은 사용하지 않는 것이 좋습니다. (어차피 source를 지정할 수 없으므로)docker run command컨테이너를 생성하거나 실행할 때 -v(--volume) or --mount 옵션을 이용해 볼륨을 마운트할 수 있습니다.  -v(--volume)# first field# volume옵션을 사용할 때는 명명된 볼륨이면 볼륨의 이름, 익명 볼륨이면 생략 가능# bind mount옵션의 경우 호스트 머신의 디렉토리 경로# second field는 컨테이너내의 마운트하고자 하는 경로# third field는 옵셔널, 예를 들어 ro(read only라는 의미)-v &amp;lt;first field&amp;gt;:&amp;lt;second field&amp;gt;:&amp;lt;thrid field&amp;gt;# 예시docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  --mount# type=volume, bind, tmpfs# source(src)# target(destination, dst)# readonly--mount &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, ..# 예시--mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app \  nginx:latest  볼륨 생성docker volume create my-voldocker volume ls-------------------------------DRIVER              VOLUME NAMElocal               my-voldocker volume inspect my-vol[    {        &quot;CreatedAt&quot;: &quot;2022-02-02T17:03:46Z&quot;,        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: {},        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;,        &quot;Name&quot;: &quot;my-vol&quot;,        &quot;Options&quot;: {},        &quot;Scope&quot;: &quot;local&quot;    }]docker volume rm my-vol  볼륨과 함께 컨테이너 실행# --mount flag 이용docker run -d \  --name devtest \  --mount source=myvol2,target=/app \  nginx:latest# -v flag 이용docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  읽기 전용 볼륨# --mount flag 이용docker run -d \  -it \  --name devtest \  --mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app,readonly \  nginx:latest# -v flag 이용docker run -d \  -it \  --name devtest \  -v &quot;$(pwd)&quot;/target:/app:ro \  nginx:latestdocker composedocker compose에 관한 포스트는 여기를 참고해주시면 감사드리겠습니다.version: &quot;3.9&quot;services:  frontend:    image: node:lts    volumes:      - myapp:/home/node/appvolumes:  myapp:참고  도커 공식문서: Manage data in Docker  도커 공식문서: Docker-compose volume configuration  DaleSeo: Docker 컨테이너에 데이터 저장 (볼륨/바인드 마운트)  stack overflow: Understanding “VOLUME” instruction in DockerFile",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series8'> <img src='/images/docker_17.png' alt='Docker의 볼륨 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series8'>Docker의 볼륨 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 아키텍처 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series7",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Desktop, Engine, Server, Host, Daemon  Docker Server          Docker Daemon      Docker Driver                  Storage Driver          Network Driver          Execdriver                      참고Desktop, Engine, Server, Host, Daemon도커를 공부하면서 Docker Desktop, Engine, Server, Host, Daemon이라는 용어들의 관계가 조금 헷갈렸었습니다. 지금까지 배웠던 내용을 토대로 다음 용어들을 짧게 정리해볼까 합니다.      Docker DesktopMac, Windows 환경에서의 도커 애플리케이션입니다. 도커는 리눅스 기반의 운영체제에서 동작하는 어플리케이션이지만 Docker Desktop을 통해 Mac, Windows에서도 사용할 수 있도록 해줍니다. 또한 Docker Engine뿐 아니라 Docker Compose도 기본적으로 함께 설치되며 Kubernetes도 클릭 한 번으로 설치가능하도록 해줍니다.        Docker Engine일반적으로 저희가 도커를 생각할 때 가지고 있는 기능들을 곧 Docker Engine이라고 합니다. 다시 말해 도커 컨테이너를 생성하기 위해 요청하는 Client, 실제 컨테이너를 생성하고 관리하는 Server를 포함하는 Client-Server Application을 말합니다.        Docker ServerDocker Client로 부터 REST API 형태로 요청을 받았을 때 그 요청을 토대로 실제로 컨테이너를 생성하고 관리하는 부분을 말합니다.        Docker HostDocker Host는 Docker Engine이 설치된 곳을 말합니다. 제 컴퓨터가 Linux기반의 운영체제였다면 제 컴퓨터 자체가 Host가 되었을 것이고, 만약 클라우드 환경에서 서버를 하나 빌려서 거기에 도커를 설치했다면 빌린 서버가 Host가 될 것입니다.    참고로 제가 지금 사용하고 있는 환경은 Mac입니다. Mac의 운영체제는 Unix 계열의 운영체제로 Linux와는 사용하는 커널이 약간 달라서 결론적으로 도커를 다이렉트로 설치할 수 없습니다. 그래서 macOS 위에 Linux Virtual Machine을 하나 더 띄우고 그 위에서 도커를 설치 사용하게 됩니다. 이렇게 사용하면 기본적으로 Linux VM에 2GB 정도의 메모리가 사용된다고 합니다.  (참고: How much overhead from running Docker on a Mac?)        Docer DaemonDocker Daemon은 Docker Server 안에 있는 핵심 요소 중 하나로 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.  지금까지 도커의 큰 그림에서의 구성요소에 대해 살펴보았습니다. 지금부터는 그 중 Docker Server의 내부에 대해서 조금 더 살펴보려고 합니다. 위의 그림을 보면 Docker Client가 요청을 하면 나머지는 Docker Server에서 실행이 이루어지는데 Docker Server가 요청을 수행하기 위해 내부적으로 어떤 과정을 거치는지 한 번 알아보겠습니다.Docker Server아래 그림은 Docker Server의 아키텍처를 보여주는 좋은 그림입니다. 비록 2014년도에 그려진 그림이어서 최근 버전의 도커와는 차이가 있을 수 있지만 도커의 기본 구성요소를 공부하는 데에는 좋은 자료라고 생각합니다.크게 두 개의 사각형 덩어리가 각각 Docker Daemon과 Docker Driver입니다.(개인적으로 Engine이라고 적힌 부분은 마치 엔진과 같은 역할을 한다는 뜻일 뿐 저희가 위에서 배운 Docker Engine을 뜻하는 건 아니라고 생각합니다.)Docker DaemonDocker daemon 은 docker engine 내에서 주로 client 및 registry, driver 의 중심에서 작업의 분배를 담당하는 중심점이라고 보면 됩니다. client 로부터의 HTTP 요청을 내부 job 단위(가장 기본적인 작업 실행 단위)로 처리할 수 있도록 분배합니다. 즉, HTTP server 의 역할과 함께 client 요청을 분배(route and distribute), scheduling 하고, 요청에 대한 적합한 Handler 를 찾습니다. 요청에 대해 실질적인 처리는 Handler 를 통해 다른 모듈 들에게 전달하여 수행하고 그 결과를 응답으로 작성하여 client 에게 제공합니다.Docker DriverDocker Driver 는 크게 세 가지 범주로 나눌 수 있습니다.  graphdriver : container image 관리  networkdriver : 가상 bridge 등 container 의 network 관리  execdriver : container 생성 관리Storage Drivergraphdriver는 Storage Driver 라고 이해하면 됩니다. /var/lib/docker 내에 저장되어 있는 container, image 관련 정보들을 이용하여 사용자에게 통합된 File System으로 제공하는 드라이버입니다. built-in graphdriver 로는 btrfs, vfs, auts, devmapper, overlay2 등이 있습니다. Storage Driver에 관한 내용은 이 포스트를 참고하시면 됩니다.Network Driver도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다. CNM을 구성하는 요소는 크게 다음과 같이 3가지가 있습니다.  Sandbox: 컨테이너의 Network의 많은 Endpoint를 설정하는 곳으로 Linux network namespace와 비슷한 개념으로 구현  Endpoint: 컨테이너 내의 eth 와 외부의 vth의 페어  Network: 네트워크는 직접적으로 통신을 할 수 있는 엔드포인트를 연결하는 역할2개의 Sandbox 안에 각각 Endpoint 요소를 하나 씩 만들고, 그 Endpoint 둘을 Network 이라는 요소에 연결해 컨테이너 간의 통신을 위한 네트워크를 구현할 수 있습니다. 이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver 입니다.Libnetwork provides the network control and management plane (native service discovery and load balancing). It accepts different drivers to provide the data plane (connectivity and isolation).Some of the network drivers that we can choose are:  bridge: it creates single-host bridge networks. Containers connect to these bridges. To allow outbound traffic to the container, the Kernel iptables does NAT. For inbound traffic, we would need to port-forward a host port with a container port.🦊 **Note**  Every Docker host has a default bridge network (docker0).  All new container will attach to it unless you override it (using --network flag).     MACVLAN: Multi-host network. Containers will have its own MAC and IP addresses on the existing physical network (or VLAN). Good things: it is easy and does not use port-mapping. Bad side: the host NIC has to be in promiscuous mode (most cloud provider does not allow this).  Overlay: it allows containers in different hosts to communicate using encapsulation. It allows you to create a flat, secure, layer-2 network.Note: Docker creates an Embedded DNS server in user-defined networks. All new containers are registered with the embedded Docker DNS resolver so can resolve names of all other containers in the same network.ExecdriverExecdriver는 컨테이너 생성 및 관리에 관한 역할을 담당합니다. 즉, 커널의 격리 기술을 이용하여 컨테이너를 생성하고 실행하는 역할을 합니다. Execdriver의 하위 드라이버인 Runtime driver로는 예전에는 리눅스의 LXC를 이용했지만 최근버전의 도커는 도커내에서 개발한 Docker native runtime driver인 libcontainer나 runc를 이용합니다.Execdriver에서 선택된 LXC 또는 native driver는 Linux Kernel 에서 제공하는 cgroups, namespace 등의 기능을 이용할 수 있는 interface를 제공하고, 이를 통해 도커는 컨테이너 생성 및 관리에 필요한 실질적인 기능들을 제공합니다.docker run 을 실행하면 이는 결국 execdriver -&amp;gt; runtime driver -&amp;gt; cgroups, namespace 등의 기능을 이용하는 인터페이스에 의해 container 환경이 마련되고 기동되는 것이다.참고  Rain.i님의 도커 컨테이너 까보기(4) – Docker Total Architecture 포스트  Maria Valcam, Docker: All you need to know — Containers Part 2",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series7'> <img src='/images/docker_13.png' alt='Docker의 아키텍처 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series7'>Docker의 아키텍처 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker 컨테이너에 저장된 데이터는 어떻게 될까?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series6",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Container Layer  UFS  CoW  Storage Driver  정리  참고도커를 공부하면서 궁금했던 것 중에 하나가 컨테이너에서 생성된 파일은 어디에 저장되어 있는걸까? 였습니다. 그동안 저는 도커에 다른 저장소를 마운트하면 컨테이너에서 생성된 데이터를 저장할 수 있고 그렇지 않다면 컨테이너가 삭제되면서 같이 사라진다라고 알고 있었는데 그러면 컨테이너가 사라지기 전까지는 어디에 저장되어 있는지 궁금해졌습니다.그러던 중 좋은 글을 공유해 놓은 블로그를 알게되어 이와 관련해 정리해보았습니다. (참고: Rain.i 블로그)Container Layer도커 컨테이너는 도커 이미지로부터 만들어진 인스턴스입니다. 도커 이미지를 토대로 여러 개의 컨테이너를 만들 수 있습니다. 예를 들어 우분투 운영체제를 제공하는 이미지를 이용해 어떤 컨테이너에는 파이썬을 설치하고, 어떤 곳에는 nginx를 설치해 웹 서버로 사용할 수도 있습니다. 이렇게 새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다. 이걸 보면 도커는 각각의 서비스를 컨테이너화 했을 뿐 아니라 컨테이너도 또 컨테이너화 한 것 같은 느낌이 드네요.도커가 컨테이너를 이런식으로 구현한 이유는 이미지의 상태를 최대한 그대로 보존하여 컨테이너를 계속 생성하더라도 토대가 변하지 않아 예상치 못한 오류를 예방할 수 있고 관리하기도 편합니다. 사용하는 입장에서도 어차피 컨테이너를 삭제하면 원래 기본 이미지 상태로 돌아가니까 걱정없이 컨테이너를 조작할 수 있을 것 입니다.우선 컨테이너를 생성하고 새로운 데이터를 생성하면 도커 상에서는 Container Layer에 저장된다는 것을 알았습니다. 그런데 Container Layer도 결국 도커를 띄운 호스트의 자원을 이용하기 때문에 제 컴퓨터(로컬이라면 데스크탑이나 노트북, 리모트라면 AWS의 EC2 정도) 어딘가에 저장이 되어 있을 것입니다. 이렇게 컨테이너들이 사용하는 이미지나 변경사항들은 모두 호스트 File system 의 /var/lib/docker 디렉토리 내에 저장된다. 이 영역을 Docker area 또는 Backing Filesystem 이라고 부르기도 한다.만약 컨테이너에서 생성된 파일을 버리지 않고 저장하고 싶다면 다음의 두 가지 방법을 사용할 수 있습니다.  Commit: 컨테이너 상에서 변경을 수행한 후 새로운 이미지로 만들어둔다.  Volume: 변경사항을 로컬 또는 외부 볼륨에 저장하도록 한다.UFS위의 내용을 읽다보면 이러한 의문이 생길 수 있습니다. ubuntu 이미지가 가지고 있던 Filesystem이 아닌 별도의 Filesystem에 Container Layer의 데이터가 저장이 되는데 왜 우리는 컨테이너를 사용할 때 이러한 사실을 몰랐을까? 그 이유는 바로 도커에서는 UFS(Union File System)라는 방식을 이용해 Image Layer와 Container Layer의 Filesystem을 하나로 통합해서 저희에게 제공해줍니다.이러한 UFS 방식의 장점은 무엇일까요? 가장 큰 장점은 Image Layer의 데이터를 여러 컨테이너가 공유할 수 있다는 점입니다. 공유한다는 것은 여러 개의 컨테이너를 띄우더라도 Image Layer의 데이터 용량은 단 1개만큼만 저장된다는 말입니다.CoW위의 그림과 같이 Image Layer의 a라는 파일을 a&#39;으로 수정할 때 Image Layer에서 파일이 수정되지 않고 Container Layer 위에서 새로 파일을 복사한 후 수정하는 것을 CoW(Copy on Write)라고 합니다. 이러한 기법을 통해 기존의 이미지에 대한 변경을 막을 수 있습니다. 하지만 Copy-on-Write 기법은 그 동작 구조 상 다음의 단점이 있습니다.  Performance Overhead: data 를 먼저 복제(Copy)한 후 변경을 수행해야함  Capacity Overhead: 원본 데이터 뿐 아니라, 변경된 데이터도 저장해야함따라서 되도록이면 중복 사용되고 수정되지 않을만한 데이터들을 이미지 레이어로 구성하는 것이 좋습니다.Storage Driver위에서 그동안 배운 UFS와 CoW 방식을 도커에서 쉽게 이용할 수 있는 것은 도커의 Storage Driver 덕분입니다. Storage Driver는 컨테이너 내에서의 파일 I/O 처리를 담당하는 드라이버입니다. Storage Driver는 Pluggable한 구조로 되어 있고 특성도 다릅니다. 또한 리눅스 배포판마다 지원하는 드라이버도 다르므로 자신의 workload에 맞는 Storage Driver를 선택해아 합니다.Storage Driver의 종류(참고: 도커 공식문서)리눅스 배포판별 지원하는 Storage Driver(참고: 도커 공식문서)Storage Driver와 Backing File SystemStorage Driver는 Container Layer의 데이터를 Backing filesystem(/var/lib/docker)으로 저장하고 사용자에게 layered filesystem으로 제공해 줍니다.   (참고로 볼륨 마운트는 이러한 Storage Driver의 도움없이 직접 Host의 Filesystem에 접근 가능합니다.)참고로 Storage Driver와 Backing filesystem 간에도 종속성이 있습니다.(참고: 도커 공식문서)Storage Driver와 graphDBStorage Driver는 사용자에게 최적의 통합된 파일 시스템을 제공하기 위해서는 layer 별 관계를 조회하고 key를 통해 특정 image를 검색하는 등, 이러한 일련의 정보 검색 및 관리하는 데이터베이스가 필요합니다. 이런 정보를 저장하고 있는 데이터베이스를 graphDB라고 합니다. (graphDB는 Storage Driver의 뇌와 같은 역할?)정리  UFS: Container Layer와 Image Layer의 파일이 통합되어 보인다  CoW: Image Layer 내의 파일을 원본은 유지하는 방향으로 파일을 수정할 수 있다  Storage Driver: 위의 기능들을 실제로 수행하는 드라이버  graphDB: Storage Driver가 최적의 실행을 하는데 필요한 정보를 저장하고 있는 SQLite기반 DB참고  도커 공식문서 About Storage Driver  Rain.i님의 도커 컨테이너 까보기(2) – Container Size, UFS 포스트  Davaom’s Tech Blog, [Docker] 컨테이너의 구조 포스트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series6'> <img src='/images/docker_5.png' alt='Docker 컨테이너에 저장된 데이터는 어떻게 될까?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series6'>Docker 컨테이너에 저장된 데이터는 어떻게 될까?</a> </h2><p class='article__excerpt'>새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series6",
      "date"     : "Jan 31, 2022",
      "content"  : "Table of Contents  Connection Client To Broker          Scenario 0: Client and Kafka running on the same local machine      Scenario 1: Client and Kafka running on the different machines      Scenario 2: Kafka and client running in Docker      Scenario 3: Kafka in Docker container with a client running locally                  Adding a new listener to the broker                    Scenario 4: Kafka running locally with a client in Docker container      원문: Confluent블로그Connection Client To Broker클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.  브로커와의 초기 연결. 연결이 되면 브로커는 클라이언트에게 연결 가능(resolvable and accessible from client machine)한 브로커의 엔드포인트 제공(advertised.listeners)  클라이언트와 연결 가능한 브로커와의 연결초기 연결은 producer = KafkaProducer(bootstrap_servers=[&quot;localhost:9092&quot;]) 와 같이 bootstrap_servers 중 하나의 서버와 초기 연결된다. 그러면 연결된 서버는 클라이언트에게 advertised.listeners를 노출해 연결되도록 한다.예시로 클라이언트와 카프카가 서로 다른 머신에 있는 경우를 보자.연결이 성공되는 경우는 다음과 같다.연결이 실패되는 경우는 다음과 같다.이러한 경우에는 advertised.listeners를 localhost:9092로 설정하면 안된다.Scenario 0: Client and Kafka running on the same local machinebootstrap_servers = &#39;localhost:9092&#39;advertised_listeners = &#39;localhost:9092&#39;  잘 동작한다.클라이언트에 전달되는 메타데이터는 192.168.10.83이다. 이 값은 로컬 머신의 IP 주소이다.Scenario 1: Client and Kafka running on the different machines카프카 브로커가 다른 머신에서 동작하는 경우를 살펴보자. 예를 들면 AWS, GCP와 같은 클라우드에서 생성한 머신여기 예제에서 클라이언트는 나의 노트북이고 카프카 브로커가 동작하고 있는 머신의 LAN은 asgard03이라고 해보자.초기 연결은 성공한다. 하지만 메타데이터에서 돌려주는 노출된 리스너는 localhost이다. 하지만 클라이언트의 localhost에는 카프카 브로커가 없으므로 연결은 실패한다.이 문제를 해결하기 위해서는 server.properties에서 advertised.listeners 값을 수정해 클라이언트에서 접근 가능한 올바른 호스트네임과 포트를 제공해주어야 한다.# advertised.listeners 수정 전advertised.listeners=PLAINTEXT://localhost:9092listeners=PLAINTEXT://0.0.0.0:9092# advertised.listeners 수정 후advertised.listeners=PLAINTEXT://asgard03.moffatt.me:9092listeners=PLAINTEXT://0.0.0.0:9092Scenario 2: Kafka and client running in Docker도커를 이용할 때 기억해야할 점은 도커는 컨테이너를 통해 그들만의 작은 세상을 만든다는 것이다. 컨테이너는 자체적인 호스트네임, 네트워크 주소, 파일 시스템을 가지고 있다. 따라서 컨테이너를 기준으로 localhost는 더이상 나의 노트북이 아니다. 도커 컨테이너에서 localhost는 컨테이너 자기 자신이다.여기서는 카프카와 클라이언트를 모두 각각 도커 호스트 위에 컨테이너로 만들어 본다.클라이언트를 컨테이너로 만들어주는 Dockerfile이다.FROM python:3# We&#39;ll add netcat cos it&#39;s a really useful# network troubleshooting toolRUN apt-get updateRUN apt-get install -y netcat# Install the Confluent Kafka python libraryRUN pip install confluent_kafka# Add our scriptADD python_kafka_test_client.py /ENTRYPOINT [ &quot;python&quot;, &quot;/python_kafka_test_client.py&quot;]위의 메니페스트를 이용해 클라이언트 이미지를 만든다.docker build -t python_kafka_test_client .카프카 브로커를 생성하자.docker network create rmoff_kafkadocker run --network=rmoff_kafka --rm --detach --name zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:5.5.0docker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0쥬키퍼와 카프카 브로커가 컨테이너로 돌아가고 있다.$ docker psIMAGE                              STATUS              PORTS                          NAMESconfluentinc/cp-kafka:5.5.0        Up 32 seconds       0.0.0.0:9092-&amp;gt;9092/tcp         brokerconfluentinc/cp-zookeeper:5.5.0    Up 33 seconds       2181/tcp, 2888/tcp, 3888/tcp   zookeeper위에서 우리는 우리만의 도커 네트워크를 만들었고 이제 이 네트워크를 통해 클라이언트와 브로커가 통신하도록 해보자$ docker run --network=rmoff_kafka --rm --name python_kafka_test_client \        --tty python_kafka_test_client broker:9092결과를 보면 초기 연결은 성공하지만, 메타데이터로 localhost를 돌려주기 때문에 프로듀서와 클라이언트의 연결은 실패된다.이를 해결하려면 advertise.listeners의 호스트네임을 컨테이너 이름으로 바꿔줘야 한다.# 수정 전-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \# 수정 후 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \최종적으로 브로커 설정을 다음과 같이 고칠 수 있다.docker stop brokerdocker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0Scenario 3: Kafka in Docker container with a client running locally위의 Scenario 2와 비교하여 클라이언트가 컨테이너화 되어 있다가 여기서는 따로 컨테이너화 되지 않고 로컬 머신 위에 있다. (이러한 차이로 위에서 하던 방식이 왜 안되는 건지 모르겠다…)로컬에 실행하는 클라이언트는 따로 네트워크가 구성되어 있지 않다. 그렇기 때문에 따로 특정 트래픽을 받기 위해서는 로컬의 포트를 열어 이를 통해 통신해야 한다. 아래 그림과 같이 9092:9092 포트를 열었다고 해보자. 클라이언트가 로컬의 9092포트 엔드포인트로 접근하기 위해서는 bootstrap_servers=&#39;localhost:9092&#39;로 해야 한다. advertised.listeners는 broker:9092로 해야 한다(클라이언트와 localhost관계가 아니므로).문제는 클라이언트 입장에서 broker:9092는 resolvable하지 않다.Adding a new listener to the broker이 문제를 해결하는 방법은 다수의 리스너를 만드는 것이다....    ports:      - &quot;19092:19092&quot;    environment:      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT...Scenario 4: Kafka running locally with a client in Docker container이런 상황이 잘 있지는 않지만, 어쨋든 이런 경우에 대한 해결책은 있다. 다만 좀 임시방편적일 뿐이다.만약 맥에서 도커가 동작하고 있다면, host.docker.internal을 이용할 수 있다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-31T21:01:35+09:00'>31 Jan 2022</time><a class='article__image' href='/kafka-series6'> <img src='/images/kafka_54.png' alt='Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series6'>Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]</a> </h2><p class='article__excerpt'>클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part5]: Kafka Listeners – Explained[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series5",
      "date"     : "Jan 30, 2022",
      "content"  : "Table of Contents  Kafka Listeners  Why can I connect to the broker, but the client still fails?  HOW TO: Connecting to Kafka on Docker  HOW TO: Connecting to Kafka on IaaS/Cloud          Option 1: External address is resolvable locally      Option 2: External address is NOT resolvable locally        Exploring listeners with Docker원문: Confluent블로그읽어보면 좋은 포스트Kafka Listeners카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners(또는 도커 이미지를 사용할 경우 KAFKA_ADVERTISED_LISTENERS)를 external IP 주소로 설정해야 합니다.아파치 카프카는 분산 시스템입니다. 데이터는 리더 파티션으로부터 쓰고 읽어지며 리더 파티션은 어떤 브로커에도 있을 수 있습니다. 그래서 클라이언트가 카프카에 연결되기 위해서는 해당 리더 파티션을 가지고 있는 브로커가 누구인지에 대한 메타데이터를 요청합니다. 이 메타데이터에는 리더 파티션을 가지는 브로커의 엔드포인트 정보를 포함하고 있으며 클라이언트는 이 정보를 이용해 카프카와 연결될 것입니다.만약 카프카가 도커와 같은 가상머신이 아닌 bare metal 위에서 동작한다면 이 엔드포인트는 그저 hostname이나 localhost 정도가 될 것입니다. 하지만 조금 더 복잡한 네트워크 환경 또는 멀티 노드 환경으로 오게 되면 조금 더 주의가 필요하게 됩니다.초기에 브로커가 연결되면 실제로 리더 파티션을 가지는 브로커의 host와 IP의 정보를 돌려줍니다. 이러한 과정은 단일 노드 환경에서도 마찬가지입니다.KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXTKAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOBserver.properties  KAFKA_LISTENERS: 카프카가 리스닝하기 위해 노출하는 host/IP와 port  KAFKA_ADVERTISED_LISTENERS: 클라이언트에게 알려주는 리스너의 host/IP와 port 리스트  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 각 리스너들이 사용하는 security protocol  KAFKA_INTER_BROKER_LISTENER_NAME: 브로커들 간의 통신을 위해 사용하는 리스너브로커에 연결되면 연결된 리스너가 반환됩니다. kafkacat은 이러한 정보를 알아보는 유용한 툴입니다. -L을 이용하면 연결된 리스너에 관한 메타데이터를 얻을 수 있습니다.# 9092포트로 연결시, localhost:9092 리스너가 반환$ kafkacat -b kafka0:9092 \           -LMetadata for all topics (from broker -1: kafka0:9092/bootstrap):1 brokers:  broker 0 at localhost:9092# 29092포트로 연결시, kafka0:29092 리스너가 반환$ kafkacat -b kafka0:29092 \           -LMetadata for all topics (from broker 0: kafka0:29092/0):1 brokers:  broker 0 at kafka0:29092Why can I connect to the broker, but the client still fails?초기 브로커 연결에 성공했다고 하더라도, 브로커가 반환하는 메타데이터 안에 있는 주소로 여전히 클라이언트가 접근하지 못하는 경우가 있습니다.      AWS EC2 인스턴스에 브로커를 만들어 로컬 머신에서 EC2에 있는 브로커로 메세지를 보내보려고 합니다. external hostname은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com입니다. 로컬 머신과 EC2가 포트포워딩을 통해 연결되었는지 확인해보겠습니다.        우리의 로컬 머신은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com을 54.191.84.122으로 성공적으로 리졸브(resolve) 합니다.  $ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -LMetadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):1 brokers:  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092      hostname이 ip-172-31-18-160.us-west-2.compute.internal인 리스너를 반환합니다.        하지만 인터넷을 통해 ip-172-31-18-160.us-west-2.compute.internal은 not resolvable해서 클라이언트는 브로커에 메세지 전송을 실패합니다.  $ echo &quot;test&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time  브로커가 설치된 서버의 클라이언트로는 문제없이 동작한다.$ echo &quot;foo&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginningfoo이러한 일이 발생하는 이유는 9092포트로 연결하는 리스너가 내부 리스너이기 때문이라고 한다. 그래서 브로커가 설치된 서버의 내부에서만 resolvable한 hostname인 ip-172-31-18-160.us-west-2.compute.internal을 리턴한다.HOW TO: Connecting to Kafka on Docker도커에서 동작하기 위해서는 카프카의 두 개의 listener를 지정해야 한다.      도커 네트워크 내에서의 통신: 이것은 브로커간의 통신 또는 도커 안의 다른 컴포넌트와의 통신을 의미한다. 이를 위해서는 도커 네트워크 안에 있는 컨테이너의 호스트네임을 사용해야 한다. 각각의 브로커는 컨테이너의 호스트네임을 통해 서로 통신하게 될 것이다.        도커가 아닌 네트워크로부터의 트래픽: 이것은 도커를 실행하는 서버에서 로컬로 동작하는 클라이언트가 될 수 있다. 이러한 경우 도커를 실행하는 서버(localhost)에서 컨테이너의 포트에 연결할 수 있다. 아래의 도커 컴포즈 스니펫을 한 번 보자.  […]kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      […]      # For more details see See https://rmoff.net/2018/08/02/kafka-listeners-explained/      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB[…]      도커 네트워크 내에 클라이언트가 있다면 클라이언트는 호스트네임 kafka0, 29092 포트를 이용한 BOB 리스너를 통해 브로커와 통신할 것입니다. 각각의 컨테이너(클라이언트, 브로커)는 kafka0를 도커 내부 네트워크를 통해 resolve합니다.        도커를 실행하는 호스트 머신(VM)에 있는 외부 클라이언트의 경우, 호스트 네임 localhost, 9092 포트를 이용한 FRED 리스너를 통해 브로커와 통신한다.        도커를 실행하는 호스트 머신(VM) 밖에 있는 외부 클라이언트는 위의 리스너를 통해 통신할 수 없다. 왜냐하면 kafka0도 localhost도 모두 resolvable하지 않기 때문이다.  HOW TO: Connecting to Kafka on IaaS/Cloud도커와의 차이점은, 도커에서 외부의 연결은 단순히 localhost에서 이루어진 반면, 클라우드 호스트 기반의 카프카는 클라이언트가 localhost에 존재하지 않는다는 것이다.더 복잡한 것은 도커 네트워크가 호스트의 네트워크와는 크게 분리되어 있지만 IaaS에서는 외부 호스트 이름이 내부적으로 확인 가능한 경우가 많기 때문에 이러한 문제가 실제로 발생할 경우 호스트 이름이 잘못될 수 있다.브로커에 연결할 외부 주소가 브로커에게 로컬로 확인할 수 있는지 여부에 따라 두 가지 방법이 있다.Option 1: External address is resolvable locallyEC2 인스턴스의 IP 주소는 기본적으로 External IP. 만약 local에서 resolvable하다면, 로컬 내의 클라이언트, 외부 클라이언트 모두 이를 통해 통신 가능. 다만 외부 클라이언트는 밑의 설정만 추가해주면 된다.advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092Option 2: External address is NOT resolvable locally만약 로컬 내에서 resolvable하지 않다면, 두 가지 리스너가 필요하다.  VPC 내에서의 통신을 위해 local에서 resolvable한 Internal IP를 통해 내부에서 리슨한다  VPC 밖, 예를 들어 나의 노트북에서 접속하려는 경우 인스턴스의 External IP가 필요하다listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTadvertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092inter.broker.listener.name=INTERNALExploring listeners with Docker  Listener BOB (port 29092) for internal traffic on the Docker network  Listener FRED (port 9092) for traffic from the Docker host machine (localhost)  Listener ALICE (port 29094) for traffic from outside, reaching the Docker host on the DNS name never-gonna-give-you-up---version: &#39;2&#39;services:  zookeeper:    image: &quot;confluentinc/cp-zookeeper:5.2.1&quot;    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000  kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      KAFKA_BROKER_ID: 0      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot;      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100  kafkacat:    image: confluentinc/cp-kafkacat    command: sleep infinity",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-30T21:01:35+09:00'>30 Jan 2022</time><a class='article__image' href='/kafka-series5'> <img src='/images/kafka_36.png' alt='Kafka Series [Part5]: Kafka Listeners – Explained[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series5'>Kafka Series [Part5]: Kafka Listeners – Explained[번역]</a> </h2><p class='article__excerpt'>카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners를 external IP 주소로 설정해야 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part2]: 컴퓨터의 기본 구조",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series2",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  컴퓨터의 기본 구성          하드웨어의 구성      폰 노이만 구조      하드웨어 사양 관련 용어        CPU          CPU의 기본 구성      CPU의 명령어 처리 과정        컴퓨터 성능 향상 기술          버퍼      캐시      인터럽트        참고컴퓨터의 기본 구성하드웨어의 구성컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다. 이 중에 CPU와 메모리는 필수장치로 구분되고 나머지는 주변장치로 구분됩니다.폰 노이만 구조오늘날 대부분의 컴퓨터는 CPU, 메모리, 저장장치, 입출력장치가 버스로 연결되어 있는 폰 노이만 구조를 따르고 있습니다. 폰 노이만 구조 이전에는 컴퓨터가 하드와이어링(hard wiring) 형태로 용도에 맞게 매번 컴퓨터의 전선을 새로 연결해야 했습니다. 이러한 문제를 해결하기 위해 수학자 존 폰 노이만(John von Neumann)은 프로그램만 교체하여 메모리에 올리는 방법을 제안했습니다. 이러한 폰 노이만 구조 덕분에 오늘날에는 프로그래밍 기술을 이용해 컴퓨터로 다양한 작업을 할 수 있게 되었습니다.폰 노이만 구조의 가장 중요한 특징은, 모든 프로그램은 메모리에 올라와야 실행할 수 있다는 것입니다. 예를 들어 하드디스크에 워드 프로그램과 문서가 저장되어 있어도 실행을 하기 위해서는 메모리에 올라와야 합니다. 운영체제 또한 프로그램이기 때문에 메모리에 올라와야 실행이 가능합니다.하드웨어 사양 관련 용어  CPU 클럭(clock): 초당 CPU내의 트랜지스터가 열고 닫히는 횟수(사이클 수) 하나의 사이클에 여러 개의 명령어가 완료되는 경우도 있고, 하나의 명령어가 여러 사이클에 걸쳐서 완료되기도 함      바이트(byte): 저장장치의 기억 용량 단위                            용량 단위          용량                          1B          1byte                          1KB          2^10byte = 1024B                          1MB          2^20byte = 1024KB                          1GB          2^30byte = 1024MB                          1TB          2^40byte = 1024GB                      버스(bus): 시스템 버스는 메모리와 주변장치를 연결하는 버스로 메인보드의 클럭속도를 나타내는 지표이며, CPU 내부 버스는 CPU 내부 부품들을 연결하는 버스로 CPU 클럭 속도와 같음. CPU 버스 속도가 시스템 버스의 속도보다 훨씬 빠름CPUCPU(Central Processing Unit)은 중앙처리장치라고 하며 메모리에 올라온 프로그램의 명령어를 해석하여 실행하는 장치입니다. 따라서 중앙 처리 장치(CPU)는 컴퓨터 부품과 정보를 교환하면서 컴퓨터 시스템 전체를 제어하는 장치로, 모든 컴퓨터의 작동과정이 중앙 처리 장치(CPU)의 제어를 받기 때문에 컴퓨터의 두뇌에 해당한다고 할 수 있습니다.🦊 32bit CPU흔히 CPU를 얘기할 때 32bit CPU, 64bit CPU라고 하는데 이 때 32bit는 CPU가 메모리에서 데이터를 읽거나 쓸 때 한 번에 처리할 수 있는 데이터의 최대 크기를 말합니다.CPU의 기본 구성  산술논리 연산장치(ALU): 산술 연산(덧셈, 뺄셈 등)과 논리 연산(AND, OR 등)을 수행하는 부분  제어장치(control unit): 명령어를 해석해 제어 신호를 보냄으로써 작업을 지시하는 부분  레지스터(resister): CPU 내에 데이터를 임시로 보관하는 부분CPU의 명령어 처리 과정CPU는 메모리에 올라온 프로그램을 실행하기 위해서는 컴파일러를 이용해 코드를 기계어로 바꿔줘야 합니다. 이 기계어를 사람이 이해하기 쉽게 일대일 대응시켜 기호화한 어셈블리어가 있는데 어셈블리어를 살펴보면 CPU가 어떤 식으로 명령어를 내리고 처리하는지 볼 수 있습니다.# C언어int D2 = 2, D3 = 3, sum;sum = D2 + D3;# 어셈블리어LOAD mem(100), register 2; # 메모리 100번지에 있는 값을 레지스터2에 로드LOAD mem(120), register 3; # 메모리 120번지에 있는 값을 레지스터3에 로드ADD register 5, resister 2, register 3; # 레지스터2와 레지스터3에 저장된 값을 더해 레지스터5에 저장MOVE register 5, mem(160); # 레지스터5에 저장된 값(5)을 메모리 160번지로 이동위의 명령어는 명령어 레지스터에 저장되고 제어장치는 저장된 명령어를 해석하고 알맞은 제어 신호를 보냄으로써 동작을 수행합니다. 이러한 제어 신호는 제어버스를 통해 메모리와 주변장치에 전달합니다.컴퓨터 성능 향상 기술현재 컴퓨터 구조의 가장 큰 문제는 CPU와 다른 장치간의 작업 속도가 다르다는 것입니다. CPU 내부 버스의 속도가 시스템 버스의 속도보다 빠르기 때문에, 메모리를 비롯한 주변장치의 속도가 CPU의 속도를 따라가지 못하고 있습니다. 여기서는 이러한 속도 차이를 개선하기 위해 개발된 기술 중 운영체제와 관련된 기술을 살펴보겠습니다.버퍼버퍼(buffer)는 속도에 차이가 있는 두 장치 사이에서 그 차이를 완화하는 역할을 합니다. 예를 들어 저장장치에서 메모리로 데이터를 읽어올 때 데이터를 하나씩 전송하는 것보다 일정량의 데이터를 모아서 한꺼번에 전송하면 속도를 향상시킬 수 있습니다. (일상생활에서 물건을 하나씩 나르는 것보다 바구니에 물건을 일정량 담아서 옮기는 것이 더 빠릅니다. 특히 거리가 먼 경우에는 그 차이가 더 클 것입니다.) 버퍼는 이러한 바구니 역할을 합니다.캐시캐시(cache)는 메모리와 CPU간의 속도 차이를 완화하기 위한 용도로 메모리의 데이터를 미리 가져와 저장해두는 임시 장소입니다. 캐시 또한 버퍼의 일종으로 CPU가 앞으로 사용할 것으로 예상되는 데이터를 미리 가져다 놓습니다(prefetch).캐시는 CPU 안에 있으며 CPU 내부 버스의 속도로 동작합니다. 캐시는 메모리의 내용 중 일부를 미리 가져오고, CPU는 메모리에 접근하기 전에 캐시를 먼저 방문해 원하는 데이터가 있는지 찾아봅니다. 캐시에서 원하는 데이터를 찾은 경우를 캐시 히트(cache hit)라고 합니다. 일반적인 컴퓨터의 캐시 적중률은 약 90%입니다.캐시 적중률을 높이기 위해 캐시는 내부적으로 현재 위치와 가까이 위치한 데이터를 가져옵니다. 캐시 용량이 높은 캐시를 구매할 수도 있지만 가격이 비쌉니다.인터럽트초기의 컴퓨터 시스템에는 주변장치가 많지 않아 CPU가 직접 입출력장치에서 데이터를 가져오거나 보냈는데 이러한 방식을 폴링(polling)이라고 합니다. 오늘날에는 주변장치가 많아 CPU가 모든 입출력에 관여하면 작업 효율이 현저하게 떨어집니다. 이러한 문제를 해결하기 위해 등장한 것이 인터럽트(interrupt) 방식입니다.CPU는 데이터를 가져오거나 보낼 때 직접하지 않고, 입출력 관리자에게 명령을 보냅니다. 입출력 관리자가 메모리에 가지고 오거나 메모리의 데이터를 저장장치로 옮기는 동안 CPU는 계속 다른 작업을 할 수 있습니다. 입출력 관리자가 데이터 전송을 완료하고 나면 완료 신호를 CPU에 보내는데 이를 인터럽트라고 합니다.인터럽트 방식을 이용하면 데이터의 입출력이 이루어지는 동안 CPU는 다른 작업을 하고 있을 수 있습니다.참고  쉽게 배우는 운영체제 책 참고  i’m developer, not coder블로그 참고  인텔 홈페이지  위키백과",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/os-series2'> <img src='/images/os_4.png' alt='OS Series [Part2]: 컴퓨터의 기본 구조'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series2'>OS Series [Part2]: 컴퓨터의 기본 구조</a> </h2><p class='article__excerpt'>컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part7]: StatefulSet과 Headless의 조합",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series7",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series7'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part7]: StatefulSet과 Headless의 조합'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series7'>Kubernetes Series [Part7]: StatefulSet과 Headless의 조합</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part2]: IP주소와 DNS 서버",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series2",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents  IP(Internet Protocol) 주소          IP 주소      서브넷 마스크(Subnet Mask)        DNS(Domain Name System) 서버IP(Internet Protocol) 주소IP 주소IP 주소는 인터넷에 연결하고자 하는 디바이스가 가지고 있는 NIC(Network Interface Controller)의 고유한 주소를 뜻합니다. 편지를 주고 받기 위해서는 서로의 주소가 필요한 것처럼 디바이스간 통신을 위해서는 IP주소가 필요합니다. IP주소는 네트워크 번호와 호스트 번호로 이루어진 32비트 숫자입니다.(IPv4 기준)서브넷 마스크(Subnet Mask)DNS(Domain Name System) 서버DNS 서버는 도메인 네임을 IP주소로 매핑하여 보관하고 있는 서버입니다. 하지만 모든 도메인 정보를 저장할 수는 없고 저장한다고 해도 IP주소를 가지고 오는데 많은 시간이 소요됩니다. 이를 해결하기 위해 DNS 서버를 계층적으로 구성해 IP 주소를 가져오도록 했으며 한 번 가져온 정보는 캐시에 저장해둡니다. 하지만 캐시에 저장된 후 정보가 변경될 수 있기 때문에 캐시에 저장된 정보는 유효기간이 지나면 캐시에서 삭제됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/network-series2'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part2]: IP주소와 DNS 서버'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series2'>Network Series [Part2]: IP주소와 DNS 서버</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part6]: ConfigMap과 Secret",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kubernetes-series6'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part6]: ConfigMap과 Secret'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series6'>Kubernetes Series [Part6]: ConfigMap과 Secret</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part1]: 운영체제의 개요",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series1",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  프로세스 관리  자원(CPU, 메모리) 관리 및 분배  파일시스템 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/os-series1'> <img src='/images/os_1.png' alt='OS Series [Part1]: 운영체제의 개요'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series1'>OS Series [Part1]: 운영체제의 개요</a> </h2><p class='article__excerpt'>운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part1]: 웹브라우저의 동작(Application Layer)",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series1",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  어플리케이션 계층(Application Layer)  웹 브라우저의 동작원리          HTTP 리퀘스트 작성                  URL 입력          URL에 관한 문법          HTTP 리퀘스트 작성                    DNS 서버에 웹 서버의 IP주소 조회                  DNS Resolver를 이용해 DNS 서버 조회                    프로토콜 스택에 메시지 송신 요청        참고어플리케이션 계층(Application Layer)어플리케이션 계층은 인터넷으로 연결 가능한 두 디바이스의 OSI layer 가장 끝단에 있는 계층으로, 웹 브라우저, 게임, 메일과 같은 것들이 있습니다. 그 중에서도 웹 브라우저(사파리, 크롬 등)는 웹 서버로의 접근, 파일 업로드/다운로드, 메일 전송과 같은 다양한 클라이언트 기능을 겸비한 복합적인 클라이언트 소프트웨어입니다. 그래서 저는 이번 포스트에서 웹 브라우저의 동작원리에 대해 집중적으로 알아보도록 하겠습니다.디바이스간 연결을 위해서는 계층별로 지켜야 규약들이 있습니다. 만약 이러한 규약들이 없다면 세상에 존재하는 다양한 디바이스들을 연결시키기 어렵습니다. 어플리케이션에서도 이러한 규약들이 있는데 대표적으로 다음과 같은 것들이 있습니다.Application Layer Protocols  HTTP: HyperText Transfer Protocol의 약자. 하이퍼링크로 연결된 html 문서(사실상 거의 모든 데이터)를 전송할 때의 규약  FTP: File Transfer Protocol의 약자. 파일을 업로드/다운로드 할 때 사용되는 규약  SMTP: Simple Mail Transfer Protocol의 약자. 메일을 전송할 때 사용되는 규약웹 브라우저의 동작원리우리가 웹 브라우저(크롬, 사파리 등)에서 뉴스 보기를 클릭하거나 유튜브 비디오를 시청할 때 내부적으로 어떤 일들이 일어나는지 한 번 알아보겠습니다.HTTP 리퀘스트 작성우리는 보통 웹 브라우저에서 URL을 입력하거나 어떤 버튼을 클릭하는 식으로 웹 서버와 상호작용 하게 되는데 이 때 웹 브라우저는 내부에서 HTTP 리퀘스트라는 것을 웹 서버에 전송합니다.URL 입력URL에 관한 문법# URL 문법scheme://[userinfo@]host[:port][/path][?query][#fragment]예: https://www.google.com/search?q=hello&amp;amp;hl=ko# scheme예: https- 주로 프로토콜이 사용됩니다.- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 (https, http, ftp)- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트- https는 http에 보안 추가 (HTTP Secure)# host예: www.google.com- 도메인명 또는 IP주소# port예: 8888- 접속 포트# path예: /search- 리소스 경로 (계층적 구조)- 디렉토리명/파일명# query예: ?q=hello&amp;amp;hl=ko- key=value 형태- ?로 시작, &amp;amp;로 추가 가능- query parameter 또는 query string으로 보통 불림# fragment예: #getting-started-introducing-spring-boot- html 내부 북마크 등에 사용- 서버에 전송하는 정보는 아님HTTP 리퀘스트 작성URL을 입력하고 나면 웹 브라우저는 URL을 바탕으로 HTTP 리퀘스트 메시지를 만듭니다.HTTP 리퀘스트 메시지의 형태는 다음과 같습니다.(joie-kim님 블로그 참고)DNS 서버에 웹 서버의 IP주소 조회HTTP 리퀘스트를 작성하고 나면 이제 OS에게 이것을 웹 서버로 전송해달라고 요청합니다. (웹 브라우저가 직접 전송하지 않는 이유는 메시지를 송신하는 기능은 하나의 애플리케이션에만 종속되는 기능이 아니므로 OS에서 전송 기능을 담당하는 것이 더 좋다고 합니다.)OS에서는 리퀘스트 메시지를 전송하기 전에 먼저 도메인 네임을 IP 주소로 변환하는 과정을 거칩니다. 이를 네임 레졸루션(name resolution)이라고 합니다.DNS Resolver를 이용해 DNS 서버 조회네임 레졸루션을 시행하는 것이 DNS 리졸버(DNS Resolver)입니다. 리졸버는 Socket 라이브러리에 들어있는 부품화된 프로그램입니다. Socket 라이브러리는 네트워크 관련 기능을 하는 프로그램을 모아놓은 라이브러리입니다.프로토콜 스택에 메시지 송신 요청DNS Resolver가 IP주소를 찾아오면 이제 진짜 웹 서버로 보낼 준비가 완료되었습니다. 이렇게 준비된 HTTP Request 메시지는 OS의 내부에 포함된 프로토콜 스택을 호출하여 실행을 요청합니다.참고  성공과 실패를 결정하는 1%의 네트워크 원리 책  imperva 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/network-series1'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part1]: 웹브라우저의 동작(Application Layer)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series1'>Network Series [Part1]: 웹브라우저의 동작(Application Layer)</a> </h2><p class='article__excerpt'>웹 브라우저는 웹 서버로의 접근, 파일 업로드/다운로드, 메일 전송과 같은 다양한 클라이언트 기능을 겸비한 복합적인 클라이언트 소프트웨어입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part4]: Kafka on Kubernetes",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series4",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  Kafka on Kubernetes  쿠버네티스 클러스터 구축          단일 노드 클러스터                  minikube          Docker Desktop                    멀티 노드 클러스터                  kind          클라우드 환경(GKE, EKS)                    쿠버네티스 GUI 도구: Lens        카프카 메니페스트 작성          LoadBalancer 생성      Zookeeper 설치      Kafka Broker 설치        카프카 클라이언트  카프카 모니터링  참고자료Kafka on Kubernetes쿠버네티스 클러스터 구축쿠버네티스 클러스터를 구축하는 방법에 대해서는 Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기를 참고하시면 됩니다.단일 노드 클러스터minikube미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)멀티 노드 클러스터kindkind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드 환경(GKE, EKS)쿠버네티스 GUI 도구: Lensbrew install lens카프카 메니페스트 작성LoadBalancer 생성kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yamlkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yamlkubectl create -f ./metallb/configmap.yaml# configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: default      protocol: layer2      addresses:      - 192.168.72.102Zookeeper 설치# deployment.yamlkind: DeploymentapiVersion: apps/v1metadata:  name: zookeeper-deployspec:  replicas: 2  selector:    matchLabels:      app: zookeeper-1  template:    metadata:      labels:        app: zookeeper-1    spec:      containers:      - name: zoo1        image: zookeeper:latest        ports:        - containerPort: 2181        env:        - name: ZOOKEEPER_ID          value: &quot;1&quot;---# zooservice.yamlapiVersion: v1kind: Servicemetadata:  name: zookeeper-service  labels:    app: zookeeper-1spec:  ports:  - name: client    port: 2181    protocol: TCP  - name: follower    port: 2888    protocol: TCP  - name: leader    port: 3888    protocol: TCP  selector:    app: zookeeper-1Kafka Broker 설치# kafkaservice.yamlapiVersion: v1kind: Servicemetadata:  name: kafka-service  annotations:    metallb.universe.tf/address-pool: default    metallb.universe.tf/allow-shared-ip: shared  labels:    name: kafkaspec:  type: LoadBalancer  ports:  - name: kafka-port    protocol: TCP    port: 9092    targetPort: 9092  selector:    app: kafka    id: &quot;0&quot;---# deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: kafka-broker0spec:  replicas: 1  selector:    matchLabels:        app: kafka        id: &quot;0&quot;  template:    metadata:      labels:        app: kafka        id: &quot;0&quot;    spec:      hostname: kafka-host0      containers:      - name: kafka        image: wurstmeister/kafka        ports:        - containerPort: 9092        env:        - name: KAFKA_LISTENERS          value: INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092        - name: KAFKA_ADVERTISED_LISTENERS          value: INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092        - name: KAFKA_INTER_BROKER_LISTENER_NAME          value: INTERNAL_LISTENER        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP          value: INTERNAL_LISTENER:PLAINTEXT, EXTERNAL_LISTENER:PLAINTEXT        - name: KAFKA_ZOOKEEPER_CONNECT          value: zookeeper-service:2181        - name: KAFKA_BROKER_ID          value: &quot;0&quot;        - name: KAFKA_CREATE_TOPICS          value: admintome-test:1:1# 위치: opt/kafka_버전# 주키퍼 실행bin/zookeeper-server-start.sh ./config/zookeeper.properties# 카프카 실행bin/kafka-server-start.sh ./config/server.properties# 토픽 생성bin/kafka-topics.sh --create --zookeeper zookeeper-service:2181 --replication-factors 1 --partitions 1 --topic test-topic카프카 클라이언트# 카프카 클라이언트 파이썬 버전 설치pip install kafka-python# producer.pyfrom kafka import KafkaProducerproducer = KafkaProducer(security_protocol=&quot;PLAINTEXT&quot;, bootstrap_servers=[&#39;192.168.111.2:9092&#39;], api_version=(0,1,0))producer.send(&#39;test&#39;, b&#39;finally working kafka&#39;) # 현재 이부분에서 안넘어감 (bootstrap_servers의 host에 어떤거 넣어야 할지 모르겠음)producer.flush()카프카 모니터링참고자료  옥탑방의 일상로그 블로그  Towards Data Science 블로그  pcjayasinghe 깃허브  PharosProduction 깃허브  Deploy Apache Kafka and Zookeeper Cluster on Kubernetes 블로그 글",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/kafka-series4'> <img src='/images/kube_24.png' alt='Kafka Series [Part4]: Kafka on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series4'>Kafka Series [Part4]: Kafka on Kubernetes</a> </h2><p class='article__excerpt'>Kubernetes 환경에서 Kafka를 띄워보려고 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose를 이용해 데이터 파이프라인 구축하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series5",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  참고참고  Docker 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T00:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/docker-series5'> <img src='/images/docker_logo.png' alt='Docker Compose를 이용해 데이터 파이프라인 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series5'>Docker Compose를 이용해 데이터 파이프라인 구축하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose 속성 알아보기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series4",
      "date"     : "Jan 26, 2022",
      "content"  : "Table of Contents  도커 컴포즈(Docker Compose)  Top-level keys  services  참고도커 컴포즈(Docker Compose)보통 하나의 프로젝트에는 여러 가지 서비스를 포함하고 있습니다. 예를 들어 웹 서버 기반의 프로젝트를 진행한다고 할 때 프로젝트의 기본적인 아키텍처는 다음과 같습니다.(Netflix Tech Blog 참고)위의 그림을 보면 웹 서버뿐만 아니라 데이터 웨어하우스, 데이터베이스, 검색 서비스 등과 같은 것들이 서로 유기적으로 연결되어 있습니다. 이런 경우 프로젝트를 상용화 하기 위해서는 어떻게 해야 할까요? 각각의 서비스를 도커를 이용해 컨테이너로 띄우고 서로를 연결시켜주어야 합니다. 이는 앞에서 배웠던 도커 이미지를 만들기 위한 Dockerfile 단계에서는 할 수 없고 도커 컨테이너를 실행(docker run)하는 단계에서 컨테이너를 띄우는 순서, 데이터 공유, 환경변수 등을 설정해서 실행해야 합니다.우리가 도커를 배우는 이유는 서비스를 컨테이너화하고 간결한 코드로 관리하여 어디서든 배포가능 하도록 하기 위한 것인데 컨테이너 실행 단계에서 이렇게 복잡한 과정이 필요하다면 생각보다 힘이 빠질 것 같습니다.이러한 문제를 해결하기 위해 도커에서는 도커 컴포즈라는 도구를 제공하였습니다. 도커 컴포즈는 기존의 여러 서비스를 연결하기 위해 컨테이너 실행 단계에서 복잡한 옵션을 주는 상황에서 벗어나 처음부터 YAML 파일 형태로 관리하여 docker-compose.yml 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜주는 것입니다. 다음은 웹 서버와 데이터베이스를 연결해 하나의 프로젝트로 배포하도록 해주는 도커 컴포즈 파일의 예시입니다.version: &#39;3&#39;services:  db:    image: postgres    volumes:      - ./docker/data:/var/lib/postgresql/data    environment:      - POSTGRES_DB=sampledb      - POSTGRES_USER=sampleuser      - POSTGRES_PASSWORD=samplesecret      - POSTGRES_INITDB_ARGS=--encoding=UTF-8  django:    build:      context: .      dockerfile: ./compose/django/Dockerfile-dev    environment:      - DJANGO_DEBUG=True      - DJANGO_DB_HOST=db      - DJANGO_DB_PORT=5432      - DJANGO_DB_NAME=sampledb      - DJANGO_DB_USERNAME=sampleuser      - DJANGO_DB_PASSWORD=samplesecret      - DJANGO_SECRET_KEY=dev_secret_key    ports:      - &quot;8000:8000&quot;    command:       - python manage.py runserver 0:8000    volumes:      - ./:/app/(44bits 블로그 참고)지금부터 도커 컴포즈 파일의 구성요소를 한 번 살펴보도록 하겠습니다.Top-level keys도커 컴포즈 파일의 최상위 키값에는 version, services, networks, volumes과 같은 항목이 있습니다.  version: 도커 컴포즈 파일 포맷의 버전을 지정 (도커 엔진과의 호환성 체크 문서)  services: 컨테이너 각각에 대한 세부사항 설정으로 도커 컴포즈에서 가장 많은 비중 차지  networks: 컴포즈가 제공하는 디폴트 네트워크가 아닌 커스텀 네트워크 생성을 원할 때 지정 (참고)  volumes: 볼륨에 이름을 지정할 수 있으며, 여러 컨테이너에서 공유하는 볼륨을 제공services  image원하는 컨테이너의 이미지를 이미지 저장소에서 불러와 설정합니다. 불러오는 방법은 다음과 같은 방법이 있습니다.# 이미지 이름image: redis# 이미지 이름:태그image: ubuntu:18.04# 이미지 작성자/이름image: tutum/influxdb# 이미지 urlimage: example-registry.com:4000/postgresql  buildbuild를 사용하면 이미지를 불러오지 않고, Dockerfile을 이용해 빌드할 수 있습니다.          context: Dockerfile이 위치한 디렉토리 또는 깃 레포지토리의 url      dockerfile: context에 Dockerfile이 없을 경우 사용할 대체 Dockerfile (단독으로 못쓰고 context필요)      args: 이미지 빌드 단계에서만 사용되는 환경변수의 값 (Dockerfile ARG 명령어에 미리 명시되어야함)      ARG buildnoRUN echo &quot;Build number: $buildno&quot;services:  webapp:    build:      context: ./dir      dockerfile: Dockerfile-alternate      args:        buildno: 1build와 image가 함께 표기된 경우 build로 이미지를 만들고 이미지의 이름에 image 값을 사용합니다.build: ./dirimage: webapp:tag  environment컨테이너에 환경변수를 추가해줍니다. 아래의 SESSION_SECRET와 같이 value를 표기하지 않으면 컴포즈가 실행중인 머신 안에서 정의된 value로 해석가능 합니다. 이것은 value를 secret하게 할 수 있으며 또한 host-specific value로 사용할 수 있습니다.environment:  RACK_ENV: development  SHOW: &#39;true&#39;  SESSION_SECRET:  ports포트를 노출해줍니다. 간단한 short syntax와 추가적으로 필드를 추가할 수 있는 long syntax가 있습니다.          Short Syntax 표기법보통 HOST PORT: CONTAINER PORT 방법으로 표기합니다번호 하나만 작성되면 CONTAINER PORT를 의미합니다IP 주소를 표기하여 해당 IP 주소의 트래픽만 허용할 수도 있습니다 (default: 0.0.0.0)쌍따옴표를 안쓰면 시간으로 인식되기 때문에 반드시 쌍따옴표로 감싸줍니다          ports:    - &quot;3000&quot;    - &quot;3000-3005&quot;    - &quot;8000:8000&quot;    - &quot;9090-9091:8080-8081&quot;    - &quot;49100:22&quot;    - &quot;127.0.0.1:8001:8001&quot;    - &quot;127.0.0.1:5000-5010:5000-5010&quot;    - &quot;127.0.0.1::5000&quot;    - &quot;6060:6060/udp&quot;    - &quot;12400-12500:1240&quot;                    Long Syntax 표기법                  target: the port inside the container          published: the publicly exposed port          protocol: the port protocol (tcp or udp)          mode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.                  ports:    - target: 80      published: 8080      protocol: tcp      mode: host                      volumes호스트 경로의 디렉토리 또는 네임드 볼륨을 컨테이너에 마운트합니다. 하나의 서비스를 위한 볼륨으로는 호스트 경로를 사용해도 괜찮지만, 여러 서비스가 공유하는 볼륨이 필요하다면 top-level volumes에 네임드 볼륨을 정의해야 합니다.          Short Syntax 표기법                              일반적인 표기방법: [SOURCE:]TARGET[:MODE]              volumes:      # Just specify a path and let the Engine create a volume      - /var/lib/mysql      # Specify an absolute path mapping      - /opt/data:/var/lib/mysql      # Path on the host, relative to the Compose file      - ./cache:/tmp/cache      # Named volume      - datavolume:/var/lib/mysql                                          Long Syntax 표기법                  type: 마운트 타입. volume, bind, tmpfs, npipe (참고)                          volume: 도커에 의해 관리되는 볼륨을 마운트하는 경우              bind: 호스트 머신의 파일 또는 디렉토리를 컨테이너에 마운트하는 경우                                source: 마운트 하고자 하는 호스트 경로의 디렉토리 또는 네임드 볼륨                      target: 볼륨이 마운트 될 컨테이너에서의 경로              version: &quot;3.9&quot;  services:    web:      image: nginx:alpine      ports:        - &quot;80:80&quot;      volumes:        - type: volume          source: mydata          target: /data          volume:            nocopy: true        - type: bind          source: ./static          target: /opt/app/static  networks:    webnet:  volumes:    mydata:                                            depends_on서비스간의 실행순서를 통해 디펜던시를 지키도록 해줍니다. 이 설정은 종속되는 다른 서비스가 실행되기만을 기다릴 뿐 준비(ready)가 되었는지 까지는 고려하지 않습니다. 준비 상태가 필요하다면 추가적인 설정이 필요합니다. (참고)아래 예시는 web 서비스 컨테이너가 실행되기 전에 db와 redis의 실행을 기다립니다. 보통 데이터베이스, 주키퍼와 같은 선행되어야 하는 서비스가 있는 경우 많이 사용합니다.version: &quot;3.9&quot;services:  web:    build: .    depends_on:      - db      - redis  redis:    image: redis  db:    image: postgres  command기본 커맨드 명령어를 오버라이딩합니다.command: bundle exec thin -p 3000command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;thin&quot;, &quot;-p&quot;, &quot;3000&quot;]참고  Docker Compose 공식문서  44bits님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-26T21:01:35+09:00'>26 Jan 2022</time><a class='article__image' href='/docker-series4'> <img src='/images/docker_8.jpeg' alt='Docker Compose 속성 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series4'>Docker Compose 속성 알아보기</a> </h2><p class='article__excerpt'>도커 컴포즈는 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series3",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  WORKDIR  VOLUME  COPY  ADD  참고Dockerfile instruction  COPY  ADD  VOLUME  WORKDIRWORKDIRWORKDIR 명령은 Docker 파일에서 이어지는 모든 RUN, CMD, ENTRIPOINT, COPY 및 ADD 명령에 대한 작업 디렉토리를 설정합니다. WORKDIR이 존재하지 않으면 이후 Dockerfile 명령어에 사용되지 않더라도 생성됩니다.WORKDIR 명령은 Docker 파일에서 여러 번 사용할 수 있습니다. 상대 경로가 제공되는 경우 이전 WORKDIR 명령의 경로에 상대적입니다. 예를 들어 다음 명령어의 결과는 /a/b/c입니다.WORKDIR /aWORKDIR bWORKDIR cRUN pwd또한 ENV를 이용해 Dockerfile에서 명시한 환경 변수의 경우 WORKDIR 명령어에서 해석할 수 있습니다. 아래 예를 보면 DIRPATH는 Dockerfile에서 정의를 했기 때문에 /path로 인식되고, DIRNAME은 해석되지 않아 /path/$DIRNAME과 같은 결과가 나옵니다.ENV DIRPATH=/pathWORKDIR $DIRPATH/$DIRNAMERUN pwdVOLUMEVOLUME 명령은 지정된 이름으로 마운트 지점을 생성하고 네이티브 호스트 또는 다른 컨테이너와 마운트됩니다.docker run 명령어를 실행하면 기본 이미지 내의 디렉토리 중 명시된 디렉토리에 있는 파일들로 마운트된 디렉토리를 초기화합니다.VOLUME 명령어로 볼륨을 생성한 뒤 이후의 빌드과정에서 생기는 볼륨의 변경값은 모두 무시됩니다.호스트 디렉터리는 컨테이너를 생성하거나 실행할 때 지정해야 합니다.  호스트 디렉토리(마운트 지점)는 본질적으로 호스트에 종속됩니다. 이는 지정된 호스트 디렉토리를 모든 호스트에서 사용할 수 있다고 보장할 수 없기 때문에 이미지 이식성을 유지하기 위한 것입니다. 따라서 Dockerfile 내에서 호스트 디렉토리를 마운트할 수 없습니다.COPYThe COPY instruction copies new files or directories from  and adds them to the filesystem of the container at the path .The  is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.# t로 시작하는 모든 txt파일을 &amp;lt;WORKDIR&amp;gt;/relativeDir/ 로 복사한다COPY t*.txt relativeDir/# test.txt, teso.txt, tesi.txt과 같은 파일을 /absoluteDir/ 로 복사한다COPY tes?.txt /absoluteDir/ADD참고  Docker 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series3'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series3'>Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series2",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  RUN  CMD  ENTRYPOINT  CMD vs ENTRYPOINT  참고Dockerfile instruction  RUN  CMD  ENTRYPOINTRUNRUN 명령어 작성요령은 다음과 같이 2가지 형태가 있습니다.  shell form: RUN &amp;lt;command&amp;gt; (the command is run in a shell, Linux default: /bin/sh -c)  exec form: RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]RUN instruction은 어떠한 명령어든 최근 이미지에 새로운 레이어에서 실행됩니다. 그리고 실행 결과는 이미지에 커밋됩니다. 커밋된 새로운 이미지는 Dockerfile의 다음 단계에 계속 사용됩니다.원한다면 RUN 명령어 중 만들어지는 커밋된 이미지를 이용해 컨테이너를 생성할 수 있습니다.  shell form: RUN &amp;lt;command&amp;gt;shell form의 기본 shell은 /bin/zsh -c echo Test 과 같이 직접 표기를 통해 바꿀 수 있습니다. 또한 \를 통해 여러 개의 RUN 명령어를 하나로 압축할 수 있습니다.RUN /bin/zsh -c echo $HOMERUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim  exec formexec form은 /bin/sh -c 가 필요하지 않은 경우 사용 가능한 형태입니다.RUN pip install -r requirements.txtCMDCMD 명령어 작성요령은 다음과 같이 3가지 형태가 있습니다.  CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec form, this is the preferred form)  CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT)  CMD command param1 param2 (shell form)CMD 명령어는 오직 한 개의 명령어만 효과가 있습니다. 만약 아래와 같이 여러 번에 걸쳐서 작성하면 마지막 명령어 CMD echo &quot;B&quot;만 실행됩니다.CMD echo &quot;A&quot; CMD echo &quot;B&quot; CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다. 예를 들어 설명해보겠습니다.  exec form: CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]# executable과 params의 조합이 하나의 디폴트CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;Hello&quot;] --------------# 컨테이너를 실행할 때 별다른 명령어를 입력하지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello# 명령어를 입력하면 CMD의 디폴트는 실행되지 않습니다docker run -it --rm &amp;lt;image-name&amp;gt; echo &quot;Good morning&quot;-&amp;gt; Good morning참고로 exec form은 shell processsing을 지원하지 않습니다. 그래서 CMD [ &quot;echo&quot;, &quot;$HOME&quot; ]은 $HOME을 대체해서 출력하지 않습니다.🦊shell processing이 필요한 경우 두 가지 방법이 있습니다.# shell을 직접 실행한다CMD [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]# shell form을 사용한다CMD echo $HOME  only params: CMD [&quot;param1&quot;,&quot;param2&quot;]이 경우에는 반드시 ENTRYPOINT 명령어를 명시해줘야 합니다. 왜냐하면 인자값만 줬을 뿐 아무런 실행 가능한 것도 표기하지 않았기 때문입니다. 이 방법은 ENTRYPOINT 명령어에 디폴트 파라미터를 제공하기 위한 것입니다.ENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]--------------  # 컨테이너를 실행할 때 디폴트 인자값 주지 않아 CMD 명령어가 실행된 경우 docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# 실행 시 인자 값을 주어 CMD 명령어가 실행되지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello ME  shell form: CMD command param1 param2shell form을 사용하면 command가 /bin/sh -c 를 통해 실행되게 됩니다. 그래서 만약 .py와 같은 파이썬 파일을 실행할 때는 shell form이 아닌 exec form을 사용해야 합니다.CMD echo &quot;Hello&quot;--------------docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hellodocker run -it --rm &amp;lt;image-name&amp;gt; echo Bye-&amp;gt; ByeCMD는 보다시피 컨테이너 실행 시 디폴트 값을 줄 뿐 반드시 실행된다는 보장을 할 수 없다. 항상 실행을 보장하고 싶을 때에는 ENTRYPOINT를 사용하면 된다.ENTRYPOINTENTRYPOINT에도 2가지 표현 방법이 있습니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]  shell form: ENTRYPOINT command param1 param2ENTRYPOINT 명령어는 docker run --entrypoint을 사용하는 경우를 제외하고는 오버라이딩 되지 않고 반드시 실행된다는 특징이 있습니다. 예를 들어 만약 docker run &amp;lt;image&amp;gt; -d 식으로 컨테이너를 실행했다면 -d는 ENTRYPOINT의 exec form 뒤에 붙게 됩니다.shell form은 어떠한 CMD 명령어나 run 커맨드라인 인자값도 사용되지 않도록 합니다. 단점은 CMD의 경우와 마찬가지로 무조건 /bin/sh -c로 시작할 수 밖에 없다는 점입니다.ENTRYPOINT 명령어도 마지막 것만 실행됩니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]FROM ubuntuENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]----------------------# ENTRYPOINT, CMD 모두 실행docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# ENTRYPOINT, run argument 실행docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello MECMD vs ENTRYPOINT  CMD, ENTRYPOINT 명령어는 마지막 하나만 실행된다  CMD 명령어는 도커 컨테이너 실행할 때 디폴트 값을 주기 때문에 오버라이딩 될 수 있다  항상 실행되는 명령어를 원한다면 ENTRYPOINT를 사용하자  항상 실행되는 명령어와 오버라이딩 되는 인자를 원한다면 CMD와 ENTRYPOINT를 함께 써보자  CMD와 ENTRYPOINT의 조합 결과는 다음과 같다참고  Docker 공식문서  스뎅(thDeng)님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series2'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series2'>Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series1",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  Dockerfile  Dockerfile Instructions          FROM                  Multi-Stage Builds                    LABEL      ARG      ENV        참고Dockerfile instruction  FROM  LABEL  ARG  ENVDockerfileDockerfile InstructionsFROMFROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다. 그렇기 때문에 유효한 Dockerfile은 반드시 FROM 명령어로부터 시작해야 합니다.  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt; [AS &amp;lt;name&amp;gt;]  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt;[:&amp;lt;tag&amp;gt;] [AS &amp;lt;name&amp;gt;]# python:3.8-buster 이미지를 기본 이미지로 만듭니다FROM python:3.8-buster# 현재 Dockerfile이 있는 위치에 있는 모든 파일을 새로 만든 이미지의 디렉토리 위치로 복사합니다COPY . .# 복사된 requirements.txt 파일에 있는 라이브러리를 설치RUN pip install -r requirements.txt# deeplearning:pytorch 라는 새로운 이미지를 만듭니다docker build -t deeplearning:pytorch .Multi-Stage Builds이미지를 빌드할 때 가장 중요한 것은 이미지의 사이즈를 줄이는 것입니다. Dockerfile에서 각각의 명령어는 이미지의 layer를 하나씩 늘려나가게 됩니다. 이를 경량화하는 방법으로 RUN 명령어 사용시 Bash에서 &amp;amp;&amp;amp; 연산자를 사용할 수 있습니다.또한 만약 여러 개의 이미지로부터 새로운 이미지를 불러와야 하는 상황이라면 FROM 과 COPY를 사용해 이미지를 경량화 할 수 있습니다. 이를 이용하면 각각의 이미지에서 원하는 파일만 선택적으로 복사해 다음 이미지로 전달시키고 필요없는 파일(다운로드 과정에서 필요한 코드와 같은 부수적인 파일들)은 제거할 수 있습니다. 이 방법을 Multi-Stage Builds라고 하는데 이 방법은 여러 개의 이미지로 부터 새로운 이미지를 생성할 때 여러 개의 Dockerfile이 필요없이 하나의 파일에 관리할 수 있다는 장점도 있습니다.Multi Stage Builds 방법으로 이미지를 만드는 코드의 형태는 다음과 같습니다.# &amp;lt;image&amp;gt; 를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# &amp;lt;image2&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image2&amp;gt;...# &amp;lt;image&amp;gt;의 빌드 결과로 생성된 파일 중 원하는 파일만 복사COPY --from=apple /dir/you/want/from/apple /dir/of/image2# 이 방법은 기본 이미지는 &amp;lt;image&amp;gt; 하나만 필요하지만 원하는 파일만 가져오고 싶은 경우 사용하는 것 같다# &amp;lt;image&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# apple로 생성된 이미지를 기본 이미지로 한다FROM apple AS apple_juice...# &amp;lt;image&amp;gt;에서 필요한 파일만 복사COPY /dir/you/want/from/apple /dir/of/apple_juice# 사용 예시FROM golang:1.16 AS builderWORKDIR /go/src/github.com/alexellis/href-counter/RUN go get -d -v golang.org/x/net/html  COPY app.go    ./RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROM alpine:latest  RUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=builder /go/src/github.com/alexellis/href-counter/app ./CMD [&quot;./app&quot;]  LABELLABEL 명령어는 이미지에 메타데이터를 추가하기 위해 사용됩니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot;LABEL com.example.label-with-value=&quot;foo&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;This text illustrates \that label-values can span multiple lines.&quot;하나의 명령어에 여러 데이터를 추가하면 이미지 크기를 줄일 수 있습니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; \com.example.label-with-value=&quot;foo&quot; \version=&quot;1.0&quot; \ description=&quot;This text illustrates \that label-values can span multiple lines.&quot;ARG빌드 단계에서만 사용하기 위한 변수입니다. 밑에서 배울 ENV와 같은 변수를 지정하게 되면 ENV가 ARG를 오버라이딩합니다.ENVENV 명령어는 환경 변수를 키:밸류 형태로 지정하도록 해줍니다. 설정된 환경 변수는 설정 이후의 모든 빌드 단계와 런타임 단계에서 사용됩니다.만약 밸류로 띄어쓰기가 필요하다면 쌍따옴표로 감싸주면 됩니다.ENV MY_NAME=&quot;John Doe&quot;ENV MY_CAT=fluffy참고  Docker 공식문서  Docker 공식문서2  stack overfolw Nagev 답변  EARTHLY 블로그: Docker Multistage Builds  geeksforgeeks 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series1'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series1'>Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Fault tolerance in Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series3",
      "date"     : "Jan 24, 2022",
      "content"  : "Table of Contents  Fault tolerance in Kafka          카프카 리플리케이션(Replication)      리더(Leader)와 팔로워(Follower)      컨트롤러(Controller)      리플리케이션 과정        참고자료Fault tolerance in Kafka카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.카프카 리플리케이션(Replication)카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, 다른 브로커에 파티션을 복제해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 리더와 팔로워가 있습니다.(shwitha B G 블로그 참고)리더(Leader)와 팔로워(Follower)카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.컨트롤러(Controller)리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 컨트롤러라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.(shwitha B G 블로그 참고)컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. 주키퍼(Zookeeper) 개념이 잠깐 등장합니다.(Zookeeper is the centralized service for storing metadata of topic, partition, and broker)  주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.  브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다  컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.  컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.리플리케이션 과정마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 하이워터마크(high water mark)라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message 3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 message 3을 읽어갔지만, 이제는 더이상 message 3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.참고자료  Hackernoon 블로그  Ashwitha B G 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-24T21:01:35+09:00'>24 Jan 2022</time><a class='article__image' href='/kafka-series3'> <img src='/images/kafka_15.png' alt='Kafka Series [Part3]: Fault tolerance in Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series3'>Kafka Series [Part3]: Fault tolerance in Kafka</a> </h2><p class='article__excerpt'>이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part5]: Kubernetes networking for developers [번역]",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series5",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Basics  Localhost (IP address 127.0.0.1)  Pod network  Service network원문: Kubernetes networking for developers - IBM developers불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.Kubernetes Basics컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 파드(Pod)로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)Localhost (IP address 127.0.0.1)같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 공유되는 네트워크 네임스페이스를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.Pod network파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.사용되는 IP 주소는 파드 네트워크라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.Service network쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해 트래픽을 전송하도록 해줍니다.kind: ServiceapiVersion: v1metadata:  name: web  namespace: my-appspec:  selector:    app: web-server  ports:  - name: web    protocol: TCP    port: 80    targetPort: 80위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.  ClusterIP  NodePort  LoadBalancer  ExternalName",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series5'> <img src='/images/kube_35.png' alt='Kubernetes Series [Part5]: Kubernetes networking for developers [번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series5'>Kubernetes Series [Part5]: Kubernetes networking for developers [번역]</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series4",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  로컬환경          미니큐브(minikube)      Docker Desktop      kind(Kubernetes in Docker)        클라우드환경          GKE(Google Kubernetes Engine)      EKS(Elastic Kubernetes Service)        참고자료로컬환경쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 테스트, 개발 환경에 적합합니다.미니큐브(minikube)미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)kind(Kubernetes in Docker)minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)brew install kindkind version--------------------kind v0.11.1 go1.17.2 darwin/arm64잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)# kind로 클러스터 구축을 위한 kind.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦 ✓ Configuring the external load balancer ⚖️ ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✗ Joining worker nodes 🚜 실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드환경GKE(Google Kubernetes Engine)EKS(Elastic Kubernetes Service)참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series4'> <img src='/images/kube_24.png' alt='Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series4'>Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기</a> </h2><p class='article__excerpt'>쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part3]: Kubernetes Service",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series3",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Network  ClusterIP  NodePort  LoacBalancer  Ingress          클러스터 외부의 로드 밸런서만을 이용한 Ingress      클러스터 내부의 Ingress 파드를 곁들인 Ingress        참고자료Kubernetes Network쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 localhost로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 파드의 IP 주소로 통신하면 됩니다. 또한 노드 간의 통신은 VXLAN이나 L2 Routing을 이용할 수 있습니다.이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러나 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.  로드 밸런싱  서비스 디스커버리  클러스터 내부 DNSClusterIPClusterIP는 서비스의 기본 타입입니다. ClusterIP 서비스를 생성하면 클러스터 내부에서만 통신 가능한 가상 IP가 할당됩니다. kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송합니다.NodePortNodePort는 모든 노드의 IP주소:포트에서 수신한 트래픽을 컨테이너에 전송하는 형태로 외부와 통신할 수 있습니다. NodePort는 전체 노드 N개 중 임의의 노드의 IP주소를 외부에 노출합니다. 그럼에도 ClusterIP를 통해 다른 노드의 파드로 통신하는데에는 문제 없습니다. 그러나 노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)이 되기 때문에 NodePort만을 이용해 외부와 통신하는 것은 분명한 한계점이 있습니다. 또한 NodePort는 쿠버네티스에서 지정한 범위(30000~32767) 안에서만 지정할 수 있기 때문에 서비스로 활용하기에는 포트 번호가 예쁘지는 않습니다. 노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양합니다.LoacBalancerLoadBalancer에서는 NodePort와 다르게 별도로 외부 로드 밸런서를 사용하기 때문에 노드 장애가 발생해도 크게 문제가 되지 않습니다. 노드에 장애가 발생한 경우 해당 노드를 목적지에서 제외 처리하고 트래픽을 전송하지 않게됩니다. LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 할당됩니다. 실제 서비스 운영 환경에서는 외부로부터 요청을 수신하는 External IP 주소를 DNS 설정 등의 이유로 고정하는 것을 선호하고 LoadBalancer 서비스는 이를 지원합니다.Ingress인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, kind: Ingress타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.클러스터 외부의 로드 밸런서만을 이용한 Ingress  GKE 인그레스외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.클러스터 내부의 Ingress 파드를 곁들인 Ingress  Nginx 인그레스클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서  NodePort vs LoadBalancer stackoverflow  Google Kubernetes Engine 가이드  Confluent 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series3'> <img src='/images/kube_26.png' alt='Kubernetes Series [Part3]: Kubernetes Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series3'>Kubernetes Series [Part3]: Kubernetes Service</a> </h2><p class='article__excerpt'>쿠버네티스에서는 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part2]: Main elements of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series2",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kafka의 주요 구성요소          Topic, Partition, Segment      Producer                  메세지 전송과정          라운드 로빈(Round-Robbin) 방식          스티키 파티셔닝(Sticky Partitioning) 방식          중복 없는 전송          정확히 한 번 전송                    Broker      Consumer                  컨슈머 오프셋 관리          그룹 코디네이터          파티션 할당 전략                          라운드 로빈 파티션 할당 전략              스티키 파티션 할당 전략              협력적 스티키 파티션 할당 전략                                            마치며  참고자료Kafka의 주요 구성요소Kafka는 크게 3가지로 이루어 있습니다.  Producer: Kafka로 메시지를 보내는 모든 클라이언트  Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버  Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트(참고: cloudkarafka)Topic, Partition, SegmentKafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.  Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)  Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)  Segment: 메시지가 실제로 저장되는 파일(참고: cloudkarafka)카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.Producer프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.(Dzone 블로그 참고)메세지 전송과정프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 ProducerRecord라고 하는 형태로 전송되며, Topic과 Value는 필수값이며, Partition과 Key는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은  프로듀서에서 send() 메소드 호출  Serializer는 JSON, String, Avro 등의 object를 bytes로 변환  ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달  Partition이 지정되지 않았을 때, Key값이 지정되었다면 Partitioner가 Key값을 바탕으로 해당 파티션에 전달  Partition, Key값이 모두 없으면 라운드 로빈(Round-Robbin)방식 또는 스티키 파티셔닝(Sticky Partitioning) 방식으로 메세지를 파티션에 할당  파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 배치로 전송할 수도 있음라운드 로빈(Round-Robbin) 방식프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.스티키 파티셔닝(Sticky Partitioning) 방식라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)중복 없는 전송정확히 한 번 전송Broker브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.Consumer컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.  파티션 할당 전략  프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가  컨슈머의 개수가 파티션보다 많지는 않은가  컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가컨슈머 오프셋 관리컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 __consumer_offsets라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, 컨슈머가 다음으로 읽어야 할 위치를 말합니다.참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.그룹 코디네이터컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.이렇게 컨슈머 그룹내의 변화를 감지하기 위해 트래킹하는 것이 바로 그룹 코디네이터입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.파티션 할당 전략라운드 로빈 파티션 할당 전략라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.스티키 파티션 할당 전략스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.협력적 스티키 파티션 할당 전략이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.(컨플루언트 블로그 참고)마치며이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 장애 발생에 대처 가능한 안정적인 서비스를 제공해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 높은 처리량, 데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활하게 해주는 것이 좋습니다.참고자료  실전 카프카 개발부터 운영까지 책  Dzone 블로그  CodeX 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kafka-series2'> <img src='/images/kafka_30.png' alt='Kafka Series [Part2]: Main elements of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series2'>Kafka Series [Part2]: Main elements of Kafka</a> </h2><p class='article__excerpt'>프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker란 무엇인가?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series0",
      "date"     : "Jan 22, 2022",
      "content"  : "Table of Contents  도커 소개  도커의 장점  도커의 구조  도커의 구성요소          도커 데몬      도커 클라이언트      도커 오브젝트                  이미지          컨테이너                      참고도커 소개Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다. 컨테이너 기술은 가상화를 위한 방법 중 하나인데 이에 관한 더 자세한 내용은 추후에 다루어 보도록 하겠습니다.개발자들에게 있어 골칫거리 중 하나는 새로 만든 애플리케이션을 개발 환경에서 테스트 환경으로, 테스트 환경에서 운영환경으로 옮길 때마다 온갖 이상한 오류를 만난다는 것입니다. 그 이유는 인프라 환경마다 네트워크 기술과 보안 정책, 스토리지가 모두 제각각이어서 그렇습니다. 그래서 ‘소프트웨어를 한 컴퓨팅 환경에서 다른 컴퓨팅 환경으로 이동하면서도 안정적으로 실행하는 방법이 없을까?’라는 고민이 커졌고 그 대답이 바로 컨테이너였습니다.개념은 간단합니다. 애플리케이션과 그 실행에 필요한 라이브러리, 바이너리, 구성 파일 등을 패키지로 묶어 배포하는 것입니다. 이렇게 하면 노트북-테스트 환경-실제 운영환경으로 바뀌어도 실행에 필요한 파일이 함께 따라다니므로 오류를 최소화할 수 있습니다. 운영체제를 제외하고 애플리케이션 실행에 필요한 모든 파일을 패키징한다는 점에서 운영체제 위에서 구현된 가상화, 즉 ‘운영체제 레벨 가상화’라고 부르기도 합니다.참고로 도커 이전에도 컨테이너 기술을 이용한 운영체제 레벨의 가상화는 있었습니다. 구글에서는 도커가 등장하기 전부터 이러한 기술을 회사 내부적으로 이용하고 있었다고 합니다. 그러나 기술적으로 높은 진입 장벽 때문에 대중화되지 않았던 것 뿐입니다.이러한 상황 속에서 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되었고, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 필요한 모든  애플리케이션들을 도커파일을 이용해 이미지를 만들고 컨테이너로 배포하는 게 흔한 개발 프로세스가 되었습니다.도커의 장점  애플리케이션을 인프라 환경에 상관없이 빠르게 배포할 수 있습니다.  어플리케이션을 실행하기 위한 독립적인 컨테이너 환경을 제공해 서비스간 디펜던시 오류를 해결해줍니다.  별다른 운영체제 소프트웨어가 필요없어 가볍습니다.도커의 구조  도커의 아키텍처는 클라이언트-서버 아키텍처입니다.  도커 클라이언트(docker)는 도커 (REST) API를 사용해 도커 데몬(dockerd)에게 요청 메시지를 보냅니다.  dockerd은 요청을 받으면 이미지, 컨테이너, 네트워크, 볼륨과 같은 도커 오브젝트를 생성하고 관리합니다.  도커 레지스트리는 public한 곳(docker hub)도 있고, private(AWS의 ECR)한 곳도 있습니다.도커의 구성요소도커 데몬도커 데몬(dockerd)은 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.도커 클라이언트도커 클라이언트(docker)는 사용자가 도커 데몬과 통신하는 주요 방법입니다. docker run과 같은 명령을 사용하면 도커 클라이언트는 해당 명령을 도커 데몬에게 전송하고 도커 데몬은 명령을 수행하게 됩니다.도커 오브젝트(출처: 오웬의 개발 이야기)이미지도커 이미지는 도커 컨테이너를 생성하기 위한 읽기 전용 템플릿입니다. 기본 이미지 위에 원하는 커스터마이징을 통해 새로운 이미지를 만들 수도 있으며, 이렇게 만들어진 이미지는 Docker Registry에 Push하여 공유할 수 있습니다. 이미지를 만들때에는 Dockerfile에 필요한 명령어를 정의하여 만들 수 있습니다. Dockerfile에 정의된 각각의 명령어들은 이미지의 Layer를 생성하며, 이러한 Layer들이 모여 이미지를 구성합니다. Dockerfile을 변경하고 이미지를 다시 구성하면 변경된 부분만 새로운 Layer로 생성됩니다. 이러한 Image의 Layer구조는 Docker가 타 가상화 방식과 비교할 때, 매우 가볍고 빠르게 기동할 수 있는 요인이 됩니다.컨테이너컨테이너는 Docker API 사용하여 생성, 시작, 중지, 이동 또는 삭제 할 수 있는 이미지의 실행가능한 인스턴스를 나타냅니다. 컨테이너를 하나 이상의 네트워크에 연결하거나, 저장 장치로 묶을 수 있으며, 현재 상태를 바탕으로 새로운 이미지를 생성할 수도 있습니다. 기본적으로 컨테이너는 Host 또는 다른 컨테이너로부터 격리되어 있습니다. 컨테이너가 제거될 때는 영구 저장소에 저장되지 않은 변경 사항은 모두 해당 컨테이너와 같이 사라집니다.참고  도커 공식문서  Rain.i님의 도커 컨테이너 까보기(1) – Protocol, Registry 포스트  yjs0997님의 [Docker 기본(2/8)] Docker’s Skeleton 포스트  ITWorld 용어풀이: 컨테이너(container), IT World",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-22T21:01:35+09:00'>22 Jan 2022</time><a class='article__image' href='/docker-series0'> <img src='/images/docker_3.svg' alt='Docker란 무엇인가?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series0'>Docker란 무엇인가?</a> </h2><p class='article__excerpt'>Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part1]: What is Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series1",
      "date"     : "Jan 17, 2022",
      "content"  : "Table of Contents  Apache Kafka 소개  Event  기업 사례: 잘란도(Zalando)  Kafka의 핵심 기능          순서 보장      적어도 한 번 전송 방식      Pull based approach      강력한 Partitioning      비동기식 방식        정리  참고자료Apache Kafka 소개  Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.  실시간 데이터를 스트리밍하는 분산환경의 publish-subscribe 메시지 플랫폼  복잡한 데이터 파이프라인 구조를 간단하게 해주며 파이프라인의 확장성을 높여준다Event모든 기업에게 있어 데이터는 중요한 자산입니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 그 가치가 더욱 큽니다. 이러한 데이터에는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동(배송, 결제, 송금 등) 그 밖에 모든 것들이 데이터가 될 수 있습니다.Kafka에서는 Event, Data, Record, Message를 모두 혼용해서 쓰고 있습니다. Event는 어떠한 행동이나, 사건도 모두 될 수 있습니다. 다음과 같은 것 들이 있습니다.  웹 사이트에서 무언가를 클릭하는 것  센서의 온도/압력 데이터  청구서  배송 물건의 위치 정보이렇게 세상에 있는 모든 정보를 실시간으로 저장하고, 처리하기 위해서는 높은 throughput, 낮은 latency가 요구됩니다. Kafka는 최대 600MB/s의 throughput과 200MB에 대해 5ms의 낮은 latency를 제공하고 있습니다.(Benchmarking Kafka vs. Pulsar vs. RabbitMQ: Which is Fastest? 참고)지금까지는 Kafka가 높은 throughput과 낮은 latency로 엄청난 양의 데이터를 실시간으로 처리해주는 플랫폼이라고 배웠습니다. 이제 이러한 개념을 가지고 조금 더 앞으로 나가보겠습니다. 다음은 Kafka를 설명하는 좋은 문장이라고 생각되어 가져와 봤습니다. (Apache Kafka Series [Part 1]: Introduction to Apache Kafka)  Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.Kafka를 이용하면 특정 Source에서 특정 Destination으로 데이터를 흘려보내는 것이 아니라, Publisher들이 실시간으로 언제든 데이터를 저장할 수 있으며, Subscriber는 언제든 저장된 데이터를 가지고 올 수 있습니다. 이러한 구조를 Pub/Sub 모델이라고 합니다. Pub/sub은 Messaging platform의 architecture를 훨씬 간단하게 만들고, 확장성을 용이하게 해줍니다.기업 사례: 잘란도(Zalando)Kafka는 현재 Fortune 100대 기업 중 80% 이상이 사용하고 있는 데이터 플랫폼의 핵심 기술입니다. 해외의 링크드인, 트위터, 아마존, 넷플릭스, 우버를 포함해 국내에서는 대표적으로 카카오와 라인 등이 Kafka를 이용하고 있습니다. 제가 여기서 소개드릴 사례는 유럽의 대표 온라인 쇼핑몰 잘란도(Zalando)입니다. (참고: Event First Development - Moving Towards Kafka Pipeline Applications)잘란도는 회사의 규모가 점점 커지고 사업이 다각화되면서 내부적으로 데이터에 대한 문제가 점점 대두되었습니다. 처리해야 할 데이터 양의 증가, 복잡해져가는 데이터 파이프라인(데이터를 Produce하는 곳과 Consume하는 곳의 다양화), 데이터 수집 장애로 인한 신뢰도 하락과 같은 문제로 잘란도에서는 이벤트 드리븐 시스템을 도입하기로 결정하였습니다.  The aim here was the democratization of data for all potential users on the new platform.(참고: https://realtimeapi.io/hub/event-driven-apis/)결과적으로 잘란도는 Kafka를 도입함으로써 내부의 데이터 처리 파이프라인을 간소화하고, 확장을 용이하게 했으며, 스트림 데이터 처리량을 높일 수 있었습니다. 이러한 결과를 얻을 수 있었던 것은 Kafka에서 제공하는 몇 가지 핵심 기능 덕분이었습니다.Kafka의 핵심 기능순서 보장이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.적어도 한 번 전송 방식분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(itempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다. 최근에는 ‘정확히 한 번’ 전송 방식이 도입되어 카프카내에서 중복성을 제거하는 방법이 많이 사용되고 있습니다.Pull based approach카프카에서 데이터를 소비하는 클라이언트는 풀 방식으로 동작합니다. 풀 방식의 장점은 자기 자신의 속도로 데이터를 처리할 수 있다는 점입니다. 푸시 방식은 브로커가 보내주는 속도에 의존해야 한다는 한계가 있습니다.강력한 Partitioning파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.비동기식 방식데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)정리  Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.  데이터를 분산처리하여 높은 throughput과 낮은 latency를 제공한다.  심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.참고자료  실전 카프카 개발부터 운영까지 책  CodeX 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-17T21:01:35+09:00'>17 Jan 2022</time><a class='article__image' href='/kafka-series1'> <img src='/images/kafka_12.png' alt='Kafka Series [Part1]: What is Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series1'>Kafka Series [Part1]: What is Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part1]: What is Apache Spark?",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series1",
      "date"     : "Jan 15, 2022",
      "content"  : "Table of Contents  Spark Introduction  RDD          파티션(Partition)      불변성(Immutable)      게으른 연산(Lazy Operation)        Cluster Mode          드라이버 프로그램      클러스터 매니저      워커 노드        알아두면 좋은 것들          Shuffling      Passing Functions to Spark        질문Spark Introduction스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다. 쉽게 말해 대용량 데이터를 여러 컴퓨터에 나누어서 동시에 처리한다고 할 수 있습니다. 이런 방법이 스파크 이전에 없었던 것은 아닙니다. 스파크 이전에 하둡(Hadoop)이 이와 유사한 기능을 제공했었습니다. 참고로 하둡은 더그 커팅(Doug Cutting)이라는 사람이 구글이 발표했던 두 개의 논문(The Google File System_2003, MapReduce: simplified data processing on large clusters_2008)을 직접 구현해 만든 프레임워크입니다. 이처럼 구글에서는 예전부터 대용량의 데이터를 고속 분산처리 하기 위해 노력했었고, 현재 스파크는 대부분의 기업들이 사용하고 있는 소프트웨어입니다.지금부터는 스파크와 하둡을 비교하며 스파크의 특징에 어떤 것이 있는지 알아보겠습니다.            차이점      하둡      스파크              기반      디스크 기반      메모리 기반              처리방식      Map-Reduce      RDD              프로그래밍언어      자바      스칼라, 자바, 파이썬, R              라이브러리      -      다양한 라이브러리(Spark streaming, MLlib, GraphX 등) 제공      처리방식에 대해 조금 더 이야기 해보겠습니다. 맵리듀스(MapReduce)는 2004년 구글에서 대용량 데이터 처리를 분산환경에서 처리하기 위한 목적의 소프트웨어 프레임워크입니다. 맵리듀스는 함수형 프로그래밍에서 일반적으로 사용되는 Map과 Reduce라는 함수를 기반으로 만들어졌습니다. Map은 각각의 분산된 환경에서 독립적으로 실행되는 함수의 일종, Reduce는 분산 환경에서의 데이터를 하나로 모으는 함수라고 생각할 수 있습니다.맵리듀스는 분산된 환경에서 데이터가 처리되는데 필요한 많은 함수들을 제공해주지만, 현업에서 필요한 기능들을 모두 커버하기에는 무리가 있었습니다. 그래서 이러한 단점을 보완하기 위해 2009년 UC Berkeley 대학에서 연구를 시작해 2012년 미국 NSDI 학회에서 스파크의 핵심 개념인 RDD(Resilient Distributed Dataset) 에 대한 논문을 발표하였습니다.RDD  RDD is a fault-tolerant collection of elements that can be operated on in parallel.(아파치 스파크 공식문서 참고)다시 말하면 RDD란 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소로 구성되며 데이터를 처리하는 과정에서 장애가 발생하더라도 스스로 복구할 수 있는 능력을 가진 데이터 모델이라는 뜻입니다. RDD는 분산 데이터에 대한 모델로서 단순히 값으로 표현되는 데이터만 가리키는 것이 아니고, 분산된 데이터를 다루는 방법까지 포함하는 일종의 클래스와 같은 개념입니다.RDD에서 중요한 특징은 다음과 같습니다.  파티션(Partition)  불변성  게으른 연산(Lazy operation)파티션(Partition)RDD는 분산 데이터 요소로 구성된 데이터 집합입니다. 여기서 분산 데이터 요소를 파티션이라고 합니다. 스파크는 작업을 수행할 때 바로 이 파티션 단위로 나눠서 병렬로 처리합니다. 여기서 제가 헷갈렸던 것은 파티션이 분산처리와 병렬처리 중 어떤 것을 기준으로 나뉘어진 단위인가 라는 것 이었습니다. 공식문서(아파치 스파크 공식문서 참고)를 살펴본 결과 파티션은 병렬 처리가 되는 기준이었습니다. 여러 서버에 분산할 때 보통 하나의 서버 당 2~4개 정도의 파티션을 설정합니다. 이 기준은 개인의 클러스터 환경에 따라 기본 설정 값이 다르며 이 값은 원하는 값으로 바꿀 수 있습니다. 구글에서 이미지를 살펴보았을 때는 다들 task당 한개의 파티션이라고 합니다.불변성(Immutable)한 개의 RDD가 여러 개의 파티션으로 나뉘고 다수의 서버에서 처리되다 보니 작업 도중 일부 파티션 처리에 장애가 발생해 파티션 처리 결과가 유실될 수 있습니다. 하지만 스파크에서 RDD는 불변성이기 때문에 생성 과정에 사용되었던 연산들을 다시 실행하여 장애를 해결할 수 있습니다. 여기서 불변성이라는 말은 RDD에서 어떤 연산을 적용해 다른 RDD가 될 때 무조건 새로 RDD를 생성합니다(RDD는 불변이다). 이러한 방식 덕분에 장애가 발생해도 기존의 RDD 데이터에 다시 연산을 적용해 장애를 해결할 수 있는 것입니다(RDD는 회복 탄력성이 좋다(resilient)).게으른 연산(Lazy Operation)RDD의 연산은 크게 트랜스포메이션 과 액션이라는 두 종류로 나눌 수 있습니다.  트랜스포메이션: RDD1 -&amp;gt; RDD2 이런식으로 새로운 RDD를 만들어내는 연산, 대표적으로 map 함수  액션: RDD -&amp;gt; 다른 형태의 데이터를 만들어내는 연산, 대표적으로 reduce 함수(아파치 공식문서 참고)트랜스포메이션 연산은 보통 분산된 서버 각각에서 독립적으로 수행할 수 있는 연산입니다. 그리고 액션은 분산된 서버에 있는 데이터가 서로를 참조해야 하는 연산입니다. 그래서 액션은 서버 네트워크간의 이동이 발생하게 됩니다. 이런 현상을 셔플링(Shuffling)이라고 하고, 보통 네트워크에서 읽어오는 연산은 메모리에 비해 100만배 정도 느립니다.그렇기 때문에 셔플링이 발생하는 연산을 할 때에는 그 전에 최대한 데이터를 간추리는 것이 중요한데 스파크의 중요한 특징 중 하나가 바로 게으른 연산을 한다는 것입니다. 게으른 연산이라는 말은 RDD가 액션연산을 수행할 때에 비로소 모든 연산이 한꺼번에 실행된다는 것입니다. 이러한 방식의 장점은 데이터를 본격적으로 처리하기 전에 어떤 연산들이 사용되었는지 알 수 있고, 이를 통해 최종적으로 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그중에서 가장 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다. 이렇게 되면 셔플링이 최대한 작은 사이즈로 발생할 수 있도록 합니다.스파크는 RDD를 사용함으로써 처리 속도도 높이고, 장애 복구도 가능해졌다Cluster Mode이번에는 스파크를 구동시키는 환경에 대해서 알아보겠습니다. 스파크는 단일 서버로 동작시키는 로컬 모드와, 클러스터 환경에서 동작시키는 클러스터 모드가 있습니다.로컬 모드는 위의 클러스터 환경에 있는 구성 요소들을 모두 하나의 서버에 놓는 것(Executor는 1개)과 같기 때문에 여기서는 분산 처리를 가능하게 해주는 클러스터 모드에 대해서 조금 더 자세히 알아보겠습니다. 구성 요소는 크게 다음과 같습니다.  드라이버 프로그램  클러스터 매니저  워커 노드여기서 드라이버 프로그램과 워커 노드를 보통 애플리케이션이라고 하고, 클러스터 매니저는 외부 서비스로 애플리케이션과 연동합니다.드라이버 프로그램  Spark driver is a program that runs on the master node of the machine which declares transformations and actions on knowledge RDDs. In easy terms, the driver in Spark creates SparkContext, connected to a given Spark Master.It conjointly delivers the RDD graphs to Master, wherever the standalone cluster manager runs.드라이버 프로그램의 역할은 다음과 같습니다.  클러스터 매니저와의 connection을 위한 스파크 컨텍스트 객체를 생성  스파크 컨텍스트를 이용해 RDD 생성  스파크 컨텍스트를 이용해 연산 정의  정의된 연산은 DAG 스케줄러에게 전달되고 스케줄러는 연산 실행 계획 수립 후 클러스터 매니저에 전달클러스터 매니저클러스터 매니저에는 다음과 같은 것들이 있습니다.  Standalone: a simple cluster manager included with Spark that makes it easy to set up a cluster  YARN: the resource manager in Hadoop 2  Kubernetes: an open-source system for automating deployment, scaling, and management of containerized applications클러스터 매니저의 종류마다 지원하는 범위가 세부적으로 다르지만 대략적인 역할은 다음과 같습니다.  스파크 컨텍스트 생성시 설정된 Executer의 개수, Executer의 메모리를 바탕으로 자원을 할당  이용 가능한 워커에 태스크를 할당하기 위해 노드를 모니터링워커 노드스파크 컨텍스트는 워커 노드에 Executer를 생성하도록 클러스터 매니저에 요청을 하고 클러스터는 그에 맞춰 Executer를 생성합니다. Executer가 생성되면 드라이버 프로그램은 정의된 연산을 수행합니다. 이 때 작업을 실제로 수행하는 것은 아니고 액션 연산의 수만큼 잡(Job)을 생성하고 잡은 셔플링이 최대한 적게 일어나는 방향으로 스테이지(Stage)를 나눕니다. 나누어진 스테이지는 다시 여러 개의 태스크(Task)로 나누어진 후 워커 노드에 생성된 Executer에 할당됩니다.워커 노드는 Executer를 이용해 태스크를 처리하고, 데이터를 나중에 재사용 할 수 있도록 메모리에 저장도 합니다.  Executer: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위, 하나의 노드에 여러 개 Executer 가능  Job: 액션 연산의 수  Task: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위알아두면 좋은 것들ShufflingPassing Functions to Spark질문  RDD가 파티션으로 나뉘어지는 시점은 RDD가 생성되는 순간일까 아니면 연산이 실행되는 순간일까?  어떤 기준으로 RDD를 파티셔닝할까?  셔플링은 액션 연산에서만 발생할까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-15T21:01:35+09:00'>15 Jan 2022</time><a class='article__image' href='/spark-series1'> <img src='/images/spark_6.png' alt='Apache Spark Series [Part1]: What is Apache Spark?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series1'>Apache Spark Series [Part1]: What is Apache Spark?</a> </h2><p class='article__excerpt'>스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Github Actions Series [Part1]: Understanding GitHub Actions",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/github_action_series1",
      "date"     : "Jan 13, 2022",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-13T21:01:35+09:00'>13 Jan 2022</time><a class='article__image' href='/github_action_series1'> <img src='/images/github-actions_logo.png' alt='Github Actions Series [Part1]: Understanding GitHub Actions'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action_series1'>Github Actions Series [Part1]: Understanding GitHub Actions</a> </h2><p class='article__excerpt'>GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part2]: Kubernetes Resource",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series2",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  쿠버네티스의 리소스          Workload Resources                  Pod          ReplicaSet          Deployment                    Service관련 리소스                  Service          Ingress                    Config and Storage관련 리소스                  ConfigMap          Volume                      참고자료쿠버네티스의 리소스Workload Resources  Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.(Rancher문서 참고)PodPod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다. Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다. 컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.Pod가 생성되는 과정은 다음과 같습니다.Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 할당합니다. 그러면 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.🐨 오브젝트 생성을 위한 YAML파일Pod를 포함해 쿠버네티스의 오브젝트를 만들기 위해서는 YAML파일이 필요합니다. YAML파일에 오브젝트를 위한 설정들을 작성할 수 있는데, 이 때 필수적으로 사용되는 key값들이 있습니다.            Key      설명      예              apiVersion      오브젝트 버전      v1, app/v1, ..              kind      오브젝트 종류      Pod, ReplicaSet, Deployment, ..              metadata      메타데이터      name, label, ..              spec      오브젝트 별 상세 설정      오브젝트마다 다름      apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1Pod의 spec에는 containers, volumes, restartPolicy, hostname, hostNetwork 등이 있습니다.(Pod공식문서 참고)ReplicaSetReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트입니다. 단일 노드 환경이면 Pod는 모두 단일 노드에서 생성되고, 여러개의 노드를 가지고 있는 상황이면, Pod는 노드에 각각 분산되어 배포됩니다.(노드 장애 대비) 이 때 Pod를 어떤 노드에 배치할지는 스케줄러가 결정하게 됩니다. 보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많습니다.ReplicaSet은 다음과 같이 동작합니다.ReplicaSet controller가 desired state에 맞춰 Pod를 생성합니다. 그러면 Scheduler는 생성된 Pod를 노드에 할당해줍니다.apiVersion: apps/v1kind: ReplicaSetmetadata:  name: echo-rsspec:  replicas: 3  selector:    matchLabels: # app: echo이고 tier: app인 label을 가지는 파드를 관리      app: echo      tier: app  template: # replicaset이 만드는 pod의 템플릿    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v1ReplicaSet의 spec에는 replicas, selector, template, minReadySeconds가 있습니다.(ReplicaSet 공식문서 참고)DeploymentDeployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트입니다. ReplicaSet을 이용하여 Pod을 업데이트하고 이력을 관리하여 롤백Rollback하거나 특정 버전revision으로 돌아갈 수 있습니다.Deployment 오브젝트가 Pod의 버전을 관리하는 과정은 다음과 같습니다.Deployment Controller가 Deploy 조건을 체크하면서 원하는 버전에 맞게 Pod의 버전을 맞춥니다. 이 때 ReplicaSet에 있는 Pod들을 보통 한 번에 바꾸지 않고 조건에 맞게(예를 들어, 25%씩) 바꿔나감으로써 버전을 바꾸더라도 중간에 서비스가 중단되지 않도록 합니다. (무중단배포)apiVersion: apps/v1kind: Deploymentmetadata:  name: echo-deployspec:  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 0  replicas: 4  selector:    matchLabels:      app: echo      tier: app  template:    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v2spec에는 replicas, selector, template, strategy  등이 있습니다.(Deployment 공식문서 참고)Service관련 리소스  In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.ServiceService는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod와 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.  ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할  NodePort: 외부에서 접근가능하도록 하는 포트 역할  LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.# ClusterIP# redis라는 Deployment 오브젝트에 IP할당apiVersion: v1kind: Servicemetadata:  name: redisspec:  ports:    - port: 6379 # clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)      protocol: TCP  selector: # 어떤pod로 트래픽을 전달할지 결정    app: counter    tier: db# NodePortapiVersion: v1kind: Servicemetadata:  name: counter-npspec:  type: NodePort  ports:    - port: 3000 # ClusterIP, Pod IP의 포트      protocol: TCP      nodePort: 31000 # Node IP의 포트  selector:    app: counter    tier: app# LoadBalancerapiVersion: v1kind: Servicemetadata:  name: counter-lbspec:  type: LoadBalancer  ports:    - port: 30000      targetPort: 3000      protocol: TCP  selector:    app: counter    tier: appIngressIngress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 nginx, haproxy, traefik, alb등이 있습니다.minikube를 이용할 경우 다음 명령어로 설치할 수 있습니다.# nginx ingress controllerminikube addons enable ingressapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: echo-v1spec:  rules:    - host: v1.echo.192.168.64.5.sslip.io      http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: echo-v1                port:                  number: 3000# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라spec에는 rules, defaultBackend(어느 rule에도 속하지 않을 경우) 등이 있습니다.(Ingress 공식문서 참고)Config and Storage관련 리소스ConfigMapConfigMap은 설정, 환경 변수들을 담는 오브젝트입니다. 예를 들어 개발/운영에 따라 환경 변수값이 다른 경우, ConfigMap 을 활용해 Pod 생성시 넣어줄 수 있습니다.ConfigMap을 다양한 방법으로 만들 수 있습니다.  ConfigMap yaml 파일로 오브젝트 생성  환경 변수 설정을 담고 있는 yaml파일을 ConfigMap 오브젝트로 생성  그냥 환경 변수를 담고 있는 임의의 파일을 ConfigMap 오브젝트로 생성# ConfigMap yaml파일apiVersion: v1 # 참고로 v1이면 core API groupkind: ConfigMapmetadata:  name: my-configdata:  hello: world  kuber: neteskubectl apply -f config-map.yml# 환경 변수 설정을 담고 있는 yaml파일global:  scrape_interval: 15sscrape_configs:  - job_name: prometheus    metrics_path: /prometheus/metrics    static_configs:      - targets:          - localhost:9090# yaml 파일로 ConfigMap 파일 생성kubectl create cm my-config --from-file=config-file.yml# ConfigMap 적용kubectl apply -f my-config.yml# config-env.yml파일 (yml파일 아니지만 그냥 확장자 yml로 해놓아도됨)hello=worldhaha=hoho# 임의의 파일로 ConfigMap 파일 생성kubectl create cm env-config --from-env-file=config-env.yml# ConfigMap 적용kubectl apply -f env-config.yml여러 가지 방법으로 ConfigMap을 Pod에 적용할 수 있습니다.  디스크 볼륨 마운트  환경변수로 사용# ConfigMap yaml파일이 있는 볼륨 마운트apiVersion: v1kind: Podmetadata:  name: alpinespec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      volumeMounts:        - name: config-vol          mountPath: /etc/config  volumes:    - name: config-vol      configMap:        name: my-config# ConfigMap yaml파일 직접 환경변수로 설정apiVersion: v1kind: Podmetadata:  name: alpine-envspec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      env:        - name: hello          valueFrom:            configMapKeyRef:              name: my-config              key: helloVolumeVolume은 저장소와 관련된 오브젝트입니다. 지금까지 만들었던 컨테이너는 Pod을 제거하면 컨테이너 내부에 저장했던 데이터도 모두 사라집니다. MySQL과 같은 데이터베이스는 데이터가 유실되지 않도록 반드시 별도의 저장소에 데이터를 저장하고 컨테이너를 새로 만들 때 이전 데이터를 가져와야 합니다.저장소를 호스트 디렉토리를 사용할 수도 있고 EBS 같은 스토리지를 동적으로 생성하여 사용할 수도 있습니다. 사실상 인기 있는 대부분의 저장 방식을 지원합니다.저장소의 종류에는 다음과 같은 것들이 있습니다.  임시 디스크          emptyDir                  Pod 이 생성되고 삭제될 때, 같이 생성되고 삭제되는 임시 디스크          생성 당시에는 아무 것도 없는 빈 상태          물리 디스크(노드), 메모리에 저장                      로컬 디스크          hostpath                  노드가 생성될 때 이미 존재하고 있는 디렉토리                      네트워크 디스크          awsElasticBlockStore, azureDisk 등      # emptydirapiVersion: v1kind: Podmetadata:  name: shared-volumes spec:  containers:  - name: redis    image: redis    volumeMounts:    - name: shared-storage      mountPath: /data/shared  - name: nginx    image: nginx    volumeMounts:    - name: shared-storage      mountPath: /data/shared  volumes:  - name : shared-storage    emptyDir: {}# hostpathapiVersion: v1kind: Podmetadata:  name: host-logspec:  containers:    - name: log      image: busybox      args: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]      volumeMounts:        - name: varlog          mountPath: /host/var/log  volumes:    - name: varlog      hostPath:        path: /var/log참고자료  subicura님의 kubenetes안내서  하나씩 점을 찍어나가며 블로그  Kubernetes 공식문서  Rancher 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/kubernetes-series2'> <img src='/images/kube_22.png' alt='Kubernetes Series [Part2]: Kubernetes Resource'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series2'>Kubernetes Series [Part2]: Kubernetes Resource</a> </h2><p class='article__excerpt'>Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part1]: Kubernetes Intro",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  들어가기 전에          도커의 등장        Kubernetes          쿠버네티스 소개      쿠버네티스 아키텍쳐      Desired State        마치며  참고자료들어가기 전에도커의 등장2013년 도커가 등장하기 전까지 서버 관리는 굉장히 어렵고 컨트롤하기 어려운 것으로 여겨졌습니다. 하나의 서비스를 제공하기 위해서는 보통 수십에서 수백개의 애플리케이션이 서로 연결되어 동작하는데, 이 때 오류가 나게 되면 어디서 문제가 생긴건지 파악하기가 쉽지 않았습니다.이러한 문제를 해결하기 위해 사람들은 가상화 기술을 이용해 서버를 애플리케이션별로 격리시키고자 하였습니다. 이 때 크게 두가지 방법으로 접근할 수 있는데, 하나는 가상머신을 이용해 컴퓨팅 리소스를 따로 분리하여 사용하도록 하는 것이었습니다. 하지만 이 방법은 컴퓨팅 성능을 떨어트립니다.두 번째 방법은 LXC(LinuX Containers)라는 리눅스 커널 기술로 기존의 하드웨어 레벨에서 하던 방식을 운영체제 레벨에서 해결하도록 했습니다. 이렇게 하면 컴퓨팅 성능도 떨어트리지 않으면서, 파일시스템, 리소스(CPU, 메모리, 네트워크)를 분리할 수 있습니다. 하지만 이 방법은 사용하기에는 운영체제에 대한 깊은 이해를 필요로 해서 많은 개발자들이 쉽게 쓰기는 힘들었습니다.이 때 등장한 것이 바로 도커입니다. 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되자, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 수많은 애플리케이션이 컨테이너로 배포되고 도커파일을 만들어 이미지를 빌드하고 컨테이너를 배포하는 게 흔한 개발 프로세스가 되었습니다.(도커에 관한 더 자세한 내용은 여기를 참고하시기 바랍니다)이제 모든 것들을 컨테이너화하기 시작하면서 우리의 서비스는 다음과 같은 모습을 가지게 되었습니다.이렇게 서비스 하나를 배포하기 위해 수많은 컨테이너를 띄우고, 연결하고, 버전업을 해야하는 상황이 생긴겁니다. 그래서 개발자들은 이제 컨테이너들을 동시에 띄우고 관리까지 해주는 컨테이너 오케스트레이션기술이 필요해지게 되었습니다.Kubernetes쿠버네티스 소개쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 합니다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작합니다.쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화합니다.쿠버네티스 아키텍쳐  마스터: 전체 클러스터를 관리하는 서버  노드: 컨테이너가 배포되는 서버쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.  컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 Desired 상태를 유지, API서버 요청 처리  스케줄러: 새로 생성되는 Pod(컨테이너와 비슷)가 있는지 계속 체크, 생성되면 가장 적절한 노드 선택컨트롤러는 자신이 맡고 있는 오브젝트의 상태를 계속 체크하고 상태를 유지합니다. 또한 API 서버에서 어떤 새로운 상태를 요구할 경우, 맞춰서 또 상태를 바꿔서 유지하고 이 때 새롭게 Pod가 생성되거나 삭제되면 스케줄러가 그에 맞춰서 노드에서 삭제, 할당합니다.Desired State쿠버네티스에서 가장 중요한 것은 desired state(원하는 상태)라는 개념입니다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말합니다.쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있습니다.이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청합니다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 됩니다.마치며쿠버네티스는 여러 컨테이너를 자동으로 배포해주고 관리해준다는 점에서 정말 좋은 기술입니다. 그리고 마이크로서비스, 클라우드 환경과도 정말 잘 어울리기 때문에 배워두면 정말 쓸모가 많을 것 같습니다. 하지만 쿠버네티스는 많은 영역을 커버하다보니 배워야할 것들이 굉장히 많습니다. 그리고 컨테이너들을 띄우는 서버를 관리하기 위한 서버를 더 사용하게 되는 것이기 때문에, 컴퓨팅 자원이 충분하지 않다면 사용하는 것이 적절하지 않을 수도 있습니다. (쿠버네티스를 운영환경에 설치하기 위해선 최소 3대의 마스터 서버와 컨테이너 배포를 위한 n개의 노드 서버가 필요)다음 포스트에서는 서버가 넉넉하지 않은 상황에서 사용할 수 있는 minikube를 설치, 그리고 쿠버네티스에 명령어를 전달할 때 사용하는 kubectl 설치해보겠습니다.그리고 도커에서는 컨테이너를 띄우지만 쿠버네티스에서는 컨테이너를 관리할 수 있도록 조금 더 패키징한 다양한 오브젝트를 띄우게 되는데 이 때 어떠한 오브젝트들이 있는지도 배워보도록 하겠습니다.참고자료  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/kubernetes-series1'> <img src='/images/kube_23.svg' alt='Kubernetes Series [Part1]: Kubernetes Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series1'>Kubernetes Series [Part1]: Kubernetes Intro</a> </h2><p class='article__excerpt'>쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part1]: Javascript Intro",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  자바스크립트의 탄생  자바스크립트의 표준화  자바스크립트의 역사          Ajax      jQuery      자바스크립트 엔진 V8      Node.js      SPA        ECMAScript  자바스크립트의 특징  자바스크립트 실행 환경자바스크립트의 탄생1995년 웹 브라우저 시장을 지배하고 있던 넷스케이프 커뮤니케이션즈는 웹 페이지의 보조적인 기능을 수행하기 위해 브라우저에서 동작하는 경량 프로그래밍 언어를 도입하기로 결정한다. 그래서 탄생한 것이 브렌던 아이크(Brendan Eich)가 개발한 자바스크립트다.자바스크립트의 표준화1996년 마이크로소프트에서 마이크로소프트에서 인터넷 익스플로러의 점유율을 높이고자 자바스크립트의 파생 버전인 Jscript를 인터넷 익스플로러에 탑재했다. 이로 인해 브라우저에 따라 웹페이지가 정상적으로 동작하지 않는 크로스 브라우징 이슈가 발생하기 시작했다.넷스케이프 커뮤니케이션즈는 컴퓨터 시스템의 표준을 관리하는 비영리 표준화 기구인 ECMA 인터내셔널에 자바스크립트의 표준화를 요청한다.1997년 ECMA-262라 불리는 표준화된 자바스크립트 초판이 완성되었고, 상표권 문제로 자바스크립트는 ECMAScript로 명명되었다.            버전      출시연도      특징              ES1      1997      초판              ES2      1998      ISO/IEC 16262 국제 표준과 동일한 규격을 적용              ES3      1999      정규표현식, try … catch              ES5      2009      HTML5와 함께 출연한 표준안, JSON, strict mode, 접근자 프로퍼티, 프로퍼티 어트리뷰트 제어, 향상된 배열 조작 기능(forEach, map, filter, reduce, some, every)              ES6(ECMAScript 2015)      2015      let/const, 클래스, 화살표 함수, 템플릿 리터럴, 디스트럭처링 할당, 스프레드 문법, rest파라미터, 심벌, 프로미스, Map/Set, 이터러블, for…of, 제너레이터, Proxy, 모듈 import/export      자바스크립트의 역사초창기 자바스크립트는 웹페이지의 보조적인 기능을 수행하기 위한 한정적인 용도로 사용되었다. 이 시기에 대부분의 로직은 주로 웹 서버에서 실행되었고, 브라우저는 서버로부터 전달받은 HTML과 CSS를 단순히 렌더링하는 수준이었다.Ajax1999년, 자바스크립트를 이용해 서버와 브라우저가 비동기방식으로 데이터를 교환할 수 있는 통신 기능인 Ajax(Asynchoronous JavaScript and XML)가 XMLHttpRequest라는 이름으로 등장했다.이전의 웹페이지는 html 태그로 시작해서 html 태그로 끝나는 완전한 HTML 코드를 서버로부터 다시 전송받아 웹페이지 전체를 렌더링하는 방식으로 동작했다. 이러한 방식은 변경할 필요가 없는 부분까지 서버로부터 코드를 다시 전송받기 때문에 성능면에서 부족한 점이 있었다.Ajax의 등장 이후, 웹 페이지에서 변경할 필요가 없는 부분은 다시 렌더링하지 않고, 필요한 부분만 렌더링하는 방식이 가능해졌다. 이로써 웹 브라우저에서도 데스크톱 애플리케이션과 유사한 빠른 성능과 부드러운 화면 전환이 가능해졌다.jQuery2006년 jQery의 등장으로 다소 번거로웠던 DOM(Document Object Model)을 더욱 쉽게 제어할 수 있게 되었고, 크로스 브라우징 이슈도 어느 정도 해결되었다. jQuery는 많은 사용자 층을 확보하게 되었고, 다소 배우기 까다로웠던 자바스크립트보다 jQuery를 더 선호하는 개발자가 양산되기도 했다.자바스크립트 엔진 V8그동안 웹 애플리케이션은 데스크톱 애플리케이션에 비해 성능상의 한계점이 있다는 인식이 있어왔지만 Ajax의 등장으로 웹 애플리케이션의 가능성을 확인하게 되었고, 이 후 자바스크립트로 웹 애플리케이션을 구축하려는 시도가 늘면서 자바 스크립트를 구동하는 자바스크립트 엔진의 성능을 더 높이고자 하는 요구가 생기게 되었다.이에 구글은 2008년 V8이라는 자바스크립트 엔진을 개발하였고 V8의 등장으로 자바스크립트를 이용해 개발한 웹 애플리케이션이 기존의 데스크톱 애플리케이션과 유사한 UX를 제공할 수 있게 되었다.Node.jsNode.js는 라이언 달(Ryan Dahl)이 2009년 개발한 자바스크립트 엔진 V8로 빌드된 자바스크립트 런타임 환경이다.Node.js는 브라우저의 자바스크립트 엔진에서만 동작하던 자바스크립트를 브라우저 이외의 환경에서도 동작할 수 있도록 했다.Node.js는 다양한 플랫폼에 적용할 수 있지만 서버 사이드 애플리케이션 개발에 주로 사용되며, 이에 필요한 모듈, 파일 시스템, HTTP 등 빌트인 API를 제공한다.프론트엔드와 백엔드 영역을 모두 자바스크립트로 개발할 수 있다는 동형성(isomorphic)은 개발 속도를 향상시켰다.그동안 브라우저에서만 동작하는 반쪽짜리 프로그래밍 언어 취급을 받았지만 Node.js의 등장으로 서버 사이드 애플리케이션 개발에도 사용할 수 있게 됨에따라 현재는 프론트영역 백엔드 영역을 아우르는 웹 프로그래밍 언어의 표준으로 자리잡았다.Node.js는 비동기 I/O을 지원하며, 단일 스레드 이벤트 루프 기반으로 동작함으로써 요청 처리 성능이 좋다. 따라서 Node.js는 데이터를 실시간으로 처리하기 위해 I/O이 빈번하게 발생하는 SPA(Single Page Application)에 적합하다.SPA자바스크립트의 발전으로 웹 어플리케이션을 이용한 개발이 활발해지다보니 복잡한 규모의 개발에 점점 대처하기가 어려워졌다. 이러한 요구에 발맞춰 여러 기업에서는 CBD(Component Based Development) 방법론을 기반으로 하는 SPA가 대중화 되면서 Angular, React, Vue.js등 다양한 프레임워크/라이브러리가 등장하게 되었다ECMAScriptECMAScript는 자바스크립트의 표준 사양인 ECMA-262를 말합니다. 각 브라우저 제조사는 ECMAScript 사양을 준수해 브라우저의 자바스크립트 엔진을 구현한다.자바스크립트는 ECMAScript와 브라우저가 별도 지원하는 클라이언트 사이드 Web API(DOM, XMLHttpRequest, fetch 등)을 아우르는 개념이다.자바스크립트의 특징자바스크립트는 웹브라우저에서 동작하는 유일한 프로그래밍 언어다.자바스크립트는 개발자가 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어다.자바스크립트는 명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어다.자바스크립트 실행 환경자바스크립트를 실행하기 위해서는 자바스크립트 엔진이 필요한데 이는 브라우저와 Node.js에만 있다.자바스크립트를 개발/테스트할 때는 주로 크롬의 개발자 도구, Node.js, 비주얼 스튜디오 코드의 Live Server 확장 플러그인을 사용한다. Live Server를 사용하면 별도의 가상 서버가 기동되고 서버에 있는 브라우저에 HTML 파일을 로딩한다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/javascript-series1'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part1]: Javascript Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series1'>Javascript Series [Part1]: Javascript Intro</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part4]: ElasticSearch 검색",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-search",
      "date"     : "Jan 7, 2022",
      "content"  : "Table of Contents  검색 API  Query DSL검색 APIQuery DSL",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/elasticsearch-search'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part4]: ElasticSearch 검색'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-search'>ElasticSearch Series [Part4]: ElasticSearch 검색</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part1]: AWS Intro",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-intro",
      "date"     : "Jan 7, 2022",
      "content"  : "Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64/v8 OS architecture만 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/aws-intro'> <img src='/images/aws_logo.png' alt='AWS Series [Part1]: AWS Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-intro'>AWS Series [Part1]: AWS Intro</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part3]: ElasticSearch Modeling",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-modeling",
      "date"     : "Jan 6, 2022",
      "content"  : "Table of Contents  Elasticsearch 모델링          데이터 타입                  Keyword 데이터 타입          Text 데이터 타입                    매핑 파라미터      Elasticsearch 모델링엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.  필드에 지정할 데이터 타입  필드별 매핑 파라미터예를 들어, 영화 정보를 엘라스틱서치를 이용해 저장하고 싶다면,            필드명      필드 타입              movieTitle      text              jenre      keyword              nation      keyword              produceYear      integer              releaseDate      date              actor      keyword      이렇게 필드별로 text, keyword, integer, date 등의 데이터 타입을 설정할 수 있습니다.데이터 타입# 대표적인 데이터 타입- 문자열 관련한 데이터 타입: keyword, text- 일반적인 데이터 타입: integer, long, double, boolean- 특수한 데이터 타입: date, ip, geo_point, geo_shapeKeyword 데이터 타입Keyword 데이터 타입은 문자열 데이터를 색인할 때 자주 사용하는 타입 중 하나로, 별도의 분석기 없이 원문 그대로가 저장된다는 것이 특징입니다. 예를 들어 ‘elastic search’라는 문자열을 keyword 타입으로 저장한다면, ‘elastic’이나 ‘search’로는 검색이 되지 않고 정확히 ‘elastic search’라고해야만 검색됩니다. 이러한 데이터 타입은 주로 카테고리형 데이터의 필드 타입으로 적절하며, 문자열을 필터링, 정렬, 집계할 때는 keyword타입을 이용해야 합니다.Text 데이터 타입반지의 제왕 영화 시리즈에는 ‘반지의 제왕: 반지 원정대’, ‘반지의 제왕: 두 개의 탑’, ‘반지의 제왕: 왕의 귀환’이 있습니다. 근데 저는 부제목까지는 기억이 안나고 ‘반지의 제왕’만 기억이 납니다. 그래서 저는 ‘반지의 제왕’이라고만 검색해도 위의 영화들이 나왔으면 좋겠습니다. 이럴 때는 text데이터 타입을 이용합니다. Text타입은 전문 검색이 가능하다는 점이 가장 큰 특징입니다. Text타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 역색인(inverted index)됩니다.더 자세한 내용은 공식문서를 참고해주시면 좋을 것 같습니다. (엘라스틱서치 공식문서 참고)매핑 파라미터문서를 색인하는 과정은 당연 엘라스틱서치에서 가장 중요한 부분입니다. 그렇기 때문에 엘라스틱서치에서는 매핑 파라미터를 통해 색인 과정을 커스텀하도록 도와줍니다. 예를 들어 어떤 영화 데이터는 장르가 없다고 하면 색인할 때 필드를 생성하지 않습니다. 이럴 때 null_value를 ‘데이터 없음’이라고 설정했다면, 필드가 생성 되고 값에 ‘데이터 없음’이라는 값이 들어갑니다. 또 다른 예시는 특정 필드를 색인에 포함할지 말지를 결정하는 enabled, index 파라미터도 있습니다. (둘의 차이는 stack overflow 참고)# 매핑 파라미터- 문자열에 자주 사용: analyzer, search_analyzer, similarity, term_vector, normalizer- 저장 관련: enabled, index, store, copy_to, doc_values- 색인 방식과 관련: ignore_above, ignore_malformed, coerce, dynamic, null_value- 필드 안의 필드를 정의할 때 사용: properties, fields- 그 밖: position_increment_gap, format(엘라스틱서치 공식문서 참고)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-06T21:01:35+09:00'>06 Jan 2022</time><a class='article__image' href='/elasticsearch-modeling'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part3]: ElasticSearch Modeling'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-modeling'>ElasticSearch Series [Part3]: ElasticSearch Modeling</a> </h2><p class='article__excerpt'>엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-element",
      "date"     : "Jan 5, 2022",
      "content"  : "Table of Contents  Elasticsearch를 구성하는 개념          인덱스      샤드      타입      문서      필드      매핑        Elasticsearch에서 제공하는 주요 API          인덱스 관련 API      문서 관련 API      검색 API      집계 API      Elasticsearch를 구성하는 개념“Elasticsearch에서 데이터는 인덱스 안의 특정 타입 에 문서 로 저장되는데 이 때 문서는 필드 를 가지고 있으며, 이러한 필드는 매핑 프로세스로 정의된 이름과 속성을 보통 따른다. 그리고 이 때 모든 문서들은 정의된 샤드 의 개수에 각각 골고루 분배되어 분산처리된다. 또한 장애를 대비해 레플리카 의 개수만큼 복제해 놓기도 한다.”위의 내용은 Elasticsearch의 데이터 저장방식에 관한 글입니다. 하지만 Elasticsearch가 익숙하지 않은 사람에게는 낯선 용어들도 있고 익숙하지만 이해하기 힘든 용어들도 있습니다. 그래도 Elasticsearch에서 사용하는 용어와 RDBMS에서 사용하는 용어를 비교하며 살펴보고 다시 한번 읽어보면 조금 더 이해가 갈 것입니다.            Elasticsearch      RDBMS              인덱스      데이터베이스              타입      테이블              문서      행              필드      열              매핑      스키마              샤드      파티션      인덱스인덱스는 논리적 데이터 저장 공간을 뜻하며, 하나의 물리적인 노드에 여러 개의 인덱스를 생성할 수도 있습니다. 이는 곧 멀티테넌시를 지원한다는 뜻이기도 합니다. 만약 Elasticsearch를 분산 환경으로 구성했다면 하나의 인덱스는 여러 노드에 분산 저장되며 검색 시 더 빠른 속도를 제공합니다.샤드분산 환경으로 저장되면 인덱스가 여러 노드에 분산 저장된다고 했는데, 이렇게 물리적으로 여러 공간에 나뉠 때의 단위를 샤드라고 합니다. 이 때 샤드는 레플리카의 단위가 되기도 합니다.타입타입은 보통 카테고리와 비슷한 의미로 노래를 K-pop, Classic, Rock처럼 장르별로 나누는 것과 같습니다. 하지만 6.1 버전 이후 인덱스 당 한 개의 타입만 지원하고 있습니다.문서한 개의 데이터를 뜻하며, 기본적으로 JSON 형태로 저장됩니다.필드필드는 문서의 속성을 나타내며 데이터베이스의 컬럼과 비슷한 의미입니다. 다만 컬럼의 데이터 타입은 정적이고, 필드의 데이터 타입은 좀 더 동적이라고 할 수 있습니다.매핑매핑은 필드와, 필드의 타입을 정의하고 그에 따른 색인 방법을 정의하는 프로세스입니다.Elasticsearch에서 제공하는 주요 APIfrom elasticsearch import ElasticsearchES_URL = &#39;localhost:9200&#39;ES_INDEX = &#39;first_index&#39;DOC_TYPE = &#39;_doc&#39;es = Elasticsearch(ES_URL)인덱스 관련 API# 인덱스 메타데이터, 매핑 정의index_settings = {    &#39;settings&#39;: {        &#39;number_of_shards&#39;: 2,        &#39;number_of_replicas&#39;: 1    },    &#39;mappings&#39;: {         &#39;properties&#39;: {            &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},            &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}        }            }}# 인덱스 생성es.indices.create(index=ES_INDEX, **index_settings)--------------------------------------------------------{&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;first_index&#39;}# 인덱스 메타 데이터 확인es.indices.get_settings()--------------------------------------------------------{&#39;first_index&#39;: {&#39;settings&#39;: {&#39;index&#39;: {&#39;routing&#39;: {&#39;allocation&#39;: {&#39;include&#39;: {&#39;_tier_preference&#39;: &#39;data_content&#39;}}},    &#39;number_of_shards&#39;: &#39;2&#39;,    &#39;provided_name&#39;: &#39;first_index&#39;,    &#39;creation_date&#39;: &#39;1641728644368&#39;,    &#39;number_of_replicas&#39;: &#39;1&#39;,    &#39;uuid&#39;: &#39;3QtIZXthRcGtCdV40WmUCg&#39;,    &#39;version&#39;: {&#39;created&#39;: &#39;7160299&#39;}}}}}# 인덱스 매핑 확인es.indices.get_mapping(index=ES_INDEX)--------------------------------------------------------{&#39;first_index&#39;: {&#39;mappings&#39;: {&#39;properties&#39;: {&#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},    &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}}}}}# 인덱스 삭제es.indices.delete(ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}# 인덱스 존재 유무es.indices.exists(ES_INDEX)--------------------------------------------------------True# 매핑 업데이트new_field = {  &quot;properties&quot;: {    &quot;school&quot; : {      &quot;type&quot;: &quot;text&quot;    }  }}es.indices.put_mapping(new_field, index=ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}문서 관련 API# 문서 삽입unit_document = {    &#39;name&#39;: &#39;Jay Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,    &#39;phone&#39;: &#39;010-1234-5678&#39;}es.index(index=ES_INDEX, doc_type=DOC_TYPE, id=1, document=unit_document)--------------------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 4, &#39;_primary_term&#39;: 1}# 문서 정보 확인es.get(index=ES_INDEX, doc_type=DOC_TYPE, id=1)-----------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;_seq_no&#39;: 0, &#39;_primary_term&#39;: 1, &#39;found&#39;: True, &#39;_source&#39;: {&#39;name&#39;: &#39;Jay Kim&#39;,  &#39;age&#39;: 28,  &#39;gender&#39;: &#39;male&#39;,  &#39;email&#39;: &#39;abc@gmail.com&#39;,  &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,  &#39;phone&#39;: &#39;010-1234-5678&#39;}}# 문서를 가지는 인덱스 생성 (이미 있으면 삽입)unit_document = {    &#39;name&#39;: &#39;Jae yeong Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;경북 구미 형곡동&#39;,    &#39;phone&#39;: &#39;010-3321-5668&#39;}es.create(index=ES_INDEX, id=2, body=unit_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 1, &#39;_primary_term&#39;: 1}# 문서 삭제es.delete(index=ES_INDEX, id=2)---------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;deleted&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 2, &#39;_primary_term&#39;: 1}# query로 삭제body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.delete_by_query(index=ES_INDEX, body=body)----------------------------------------------{&#39;took&#39;: 23, &#39;timed_out&#39;: False, &#39;total&#39;: 1, &#39;deleted&#39;: 1, &#39;batches&#39;: 1, &#39;version_conflicts&#39;: 0, &#39;noops&#39;: 0, &#39;retries&#39;: {&#39;bulk&#39;: 0, &#39;search&#39;: 0}, &#39;throttled_millis&#39;: 0, &#39;requests_per_second&#39;: -1.0, &#39;throttled_until_millis&#39;: 0, &#39;failures&#39;: []}# 문서 수정# &quot;&quot;doc&quot;&quot; is essentially Elasticsearch&#39;s &quot;&quot;_source&quot;&quot; fieldupdate_document = {&#39;doc&#39;: {         &#39;address&#39;: &#39;경북 구미 송정동&#39;,         &#39;age&#39;: 28,         &#39;email&#39;: &#39;ziont0510@gmail.com&#39;,    }}es.update(index=ES_INDEX, id=1, body=update_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;updated&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 5, &#39;_primary_term&#39;: 1}검색 API# 매칭되는 문서 개수body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.count(body=body, index=ES_INDEX)---------------------------------------------------------------{&#39;count&#39;: 2, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}}# 문서 검색body = {    &#39;size&#39;:10,    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}    es.search(body=body, index=ES_INDEX)-------------------------------------------------------{&#39;took&#39;: 6, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 0, &#39;relation&#39;: &#39;eq&#39;},  &#39;max_score&#39;: None,  &#39;hits&#39;: []}}집계 API",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-05T21:01:35+09:00'>05 Jan 2022</time><a class='article__image' href='/elasticsearch-element'> <img src='/images/elastic_4.png' alt='ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-element'>ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드</a> </h2><p class='article__excerpt'>Elasticsearch에서 데이터는 인덱스 안의 특정 타입에 문서로 저장되는데..</p></div></div></div>"
    } ,
  
    {
      "title"    : "MongoDB Series [Part1]: MongoDB Intro",
      "category" : "",
      "tags"     : "MongoDB",
      "url"      : "/mongodb-intro",
      "date"     : "Jan 4, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-04T21:01:35+09:00'>04 Jan 2022</time><a class='article__image' href='/mongodb-intro'> <img src='/images/mongodb_logo.png' alt='MongoDB Series [Part1]: MongoDB Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-intro'>MongoDB Series [Part1]: MongoDB Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part1]: ElasticSearch Installation",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-intro",
      "date"     : "Jan 3, 2022",
      "content"  : "Table of Contents  Elasticsearch 소개          Elasticsearch를 사용하는 이유                  Elasticsearch는 빠릅니다          Elasticsearch는 본질상 분산적입니다.          Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.          그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.                      Elasticsearch 설치          Docker를 이용한 설치      github + Docker를 이용한 설치      Linux에 직접 설치      마치며      Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64 OS architecture을 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-03T21:01:35+09:00'>03 Jan 2022</time><a class='article__image' href='/elasticsearch-intro'> <img src='/images/elastic_1.png' alt='ElasticSearch Series [Part1]: ElasticSearch Installation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-intro'>ElasticSearch Series [Part1]: ElasticSearch Installation</a> </h2><p class='article__excerpt'>Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진</p></div></div></div>"
    } ,
  
    {
      "title"    : "Pytorch Series [Part1]: torch",
      "category" : "",
      "tags"     : "Pytorch",
      "url"      : "/pytorch-torch",
      "date"     : "Jan 2, 2022",
      "content"  : "1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-02T21:01:35+09:00'>02 Jan 2022</time><a class='article__image' href='/pytorch-torch'> <img src='/images/pytorch_logo.webp' alt='Pytorch Series [Part1]: torch'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/pytorch-torch'>Pytorch Series [Part1]: torch</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part5]: 자바 조금 더 알아보기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series5",
      "date"     : "Jul 8, 2021",
      "content"  : "Table of Contents  1. 상속  2. 캐스팅과 제네릭          1) 캐스팅      2) 제네릭        3. 인터페이스와 추상 클래스          1) 인터페이스      2) 추상 클래스        참고1. 상속클래스간 공통된 속성과 기능이 많이 있을 경우 만약 이 공통된 부분을 클래스마다 다시 쓴다면 프로그래밍의 중요한 법칙 중 하나인 ‘DRY(Don’t Repeat Yourself; 중복 배체)’를 어기게 되는 것입니다. 자바의 ‘클래스 상속(Class Inheritance)’ 기능이 이 문제를 해결해줍니다.public class 자식클래스 extends 부모클래스 {    ...}접근 제어자가 private이 아니면 자식클래스에서도 변수와 메소드를 그대로 사용할 수 있습니다.자식 클래스가 부모 클래스가 가지고 있는 메소드를 덮어 쓰고 싶을 때는(기존 부모 클래스의 메소드와는 독립적인 메소드로 만들고 싶을 때) ‘메소드 오버라이딩(Method Overriding)’을 해줘야 합니다. 메소드 정의 위에 써져있는 @Override가 메소드 오버라이딩을 표시해줍니다. @Override와 같이 골뱅이(@)가 붙어있는 문법을 ‘어노테이션(Annotation)’이라고 합니다. 주석(Comment)과 어느정도 비슷하지만, 어노테이션은 자바에서 추가적인 기능을 제공합니다. 예를 들어서 @Override를 써줬는데 부모 클래스에 같은 이름의 메소드가 없는 경우, 오류가 나오게 됩니다.public class MinimumBalanceAccount extends BankAccount {    private int minimum;    @Override    public boolean withdraw(int amount) {        if (getBalance() - amount &amp;lt; minimum) {            System.out.println(&quot;적어도 &quot; + minimum + &quot;원은 남겨야 합니다.&quot;);            return false;        }        setBalance(getBalance() - amount);        return true;    }}이번에는 부모 클래스가 가지고 있는 메소드에서 몇 가지를 추가해서 쓰고 싶은 경우에는 super를 사용하면 됩니다.public class TransferLimitAccount extends BankAccount {    private int transferLimit;    @Override    boolean withdraw(int amount) {        if (amount &amp;gt; transferLimit) {            return false;        }        return super.withdraw(amount);    }}  2. 캐스팅과 제네릭1) 캐스팅ArrayList&amp;lt;BankAccount&amp;gt; accounts = new ArrayList&amp;lt;&amp;gt;();accounts.add(ba);accounts.add(mba);accounts.add(sa);for (BankAccount account : accounts) {    account.deposit(1000);}이렇게 하면 각 계좌가 BankAccount 타입으로 ‘캐스팅(Casting)’되고, 한꺼번에 묶어서 다룰 수 있습니다.sa에게는 이자를 붙여주고 싶은데, BankAccount 클래스에는 addInterest 메소드가 없습니다. 만약 여기서 SavingsAccount만 골라서 addInterest 메소드를 쓰고 싶으면 instanceof 키워드를 사용하면 됩니다.for (BankAccount account : accounts) {    account.deposit(1000);    if (account instanceof SavingsAccount) {        ((SavingsAccount) account).addInterest();    }}2) 제네릭아래 꺽쇠 기호(&amp;lt;&amp;gt;) 사이에 있는 T를 ‘타입 파라미터’라고 부릅니다. 그리고 이와 같이 타입 파라미터를 받는 클래스를 ‘제네릭 클래스(Generic Class)’라고 합니다.public class Box&amp;lt;T&amp;gt; {    private T something;    public void set(T object) {         this.something = object;    }    public T get() {        return something;    }}아래처럼 타입 파라미터로 String을 넘겨주면,Box&amp;lt;String&amp;gt; box = new Box&amp;lt;&amp;gt;();클래스에 있던 모든 T가 String으로 대체된다고 생각하면 됩니다.public class Box&amp;lt;String&amp;gt; {    private String object;    public void set(String object) {        this.object = object;    }    public String get() {        return object;    }}지금까지는 타입 파라미터로 아무 클래스나 넘길 수 있었는데요. extends 키워드를 이용하면 타입을 제한할 수도 있습니다.public class PhoneBox&amp;lt;T extends Phone&amp;gt; extends Box&amp;lt;T&amp;gt; {    public void handsFreeCall(String numberString) {        object.call(numberString);    }}3. 인터페이스와 추상 클래스1) 인터페이스클래스가 생성될 때, 특정 빈 메소드를 강제로 가지도록 하고 싶을 때 인터페이스를 이용합니다// 인터페이스public interface Shape {  // 빈 메소드  double getArea();  double getPerimeter();}// 특정 인터페이스를 따라야 하는 클래스public class Circle implements Shape {  ...  ...  public double getArea() {    return PI * radius * radius;  }  public double getPerimeter() {    return 2 * PI * radius;  }}2) 추상 클래스// 추상클래스public abstract class Shape {  // 변수  public double x, y;  // 메소드  public void move(doulbe x, double y) {    ...  }  // 빈 메소드 (추상 메소드)  public abstract double getArea();  public abstract double getPerimeter();}참고  도커 공식문서  클라우드 엔지니어 Won의 성장 블로그, 06. 도커 네트워크 포스트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-08T21:01:35+09:00'>08 Jul 2021</time><a class='article__image' href='/java-series5'> <img src='/images/java_logo.png' alt='Java Series [Part5]: 자바 조금 더 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series5'>Java Series [Part5]: 자바 조금 더 알아보기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part4]: 자바의 자료형 특징",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series4",
      "date"     : "Jul 7, 2021",
      "content"  : "Table of Contents  1. 기본형 참조형  2. null  3. final  4. 클래스 변수와 클래스 메소드          1) 클래스 변수      2) 클래스 메소드        5. Wrapper class  6. ArrayList  7. HashMap1. 기본형 참조형// 참조형의 경우 == 연산자는 같은 인스턴스를 가리키는지를 물어봄String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase() == &quot;abc&quot;);-----------------------------------------------------------------false// 인스턴스가 가지는 값이 같은지를 확인하고 싶으면 equals() 메소드를 사용해야함String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase().equals(&quot;abc&quot;));----------------------------------------------------------------true2. null어떤 언어들에서는 ‘비어있음’을 None으로 표현하고, 또 어떤 언어들에서는 nil로 표현합니다. 자바에서는 ‘비어있음’이 null이라는 값으로 표현됩니다. 단, null은 참조형 변수(Reference Type)만 가질 수 있는 값입니다.3. final변수를 정의할 때 final을 써주면, 그 변수는 ‘상수’가 됩니다. 즉, 한 번 정의하고 나서 다시 바꿀 수 없다는 것이죠.public final double pi = 3.141592;4. 클래스 변수와 클래스 메소드1) 클래스 변수클래스 변수는 인스턴스가 생성될 때마다 값이 초기화 되는 것이 아니라, 모든 인스턴스들이 함께 공유하는 변수입니다. 클래스 변수를 정의하기 위해서는 static이라는 키워드를 붙여주면 됩니다.자주 접하게 되는 클래스 변수는 바로 상수입니다. final을 공부할 때 상수를 보긴 했지만, 상수를 더 상수답게 쓰려면 static과 함께 쓰는 것이 좋습니다. 상수는 인스턴스에 해당되는 것이 아니며, 여러 복사본 대신 한 값만 저장해두는 것이 맞기 때문입니다. 상수 이름은 보통 모두 대문자로 쓰고, 단어가 여러 개인 경우 _로 구분 짓습니다.public class CodeitConstants {    public static final double PI = 3.141592653589793;    public static final double EULERS_NUMBER = 2.718281828459045;    public static final String THIS_IS_HOW_TO_NAME_CONSTANT_VARIABLE = &quot;Hello&quot;;    public static void main(String[] args) {        System.out.println(CodeitConstants.PI + CodeitConstants.EULERS_NUMBER);    }}2) 클래스 메소드마찬가지로, 클래스 메소드는 인스턴스가 아닌 클래스에 속한 메소드입니다. 클래스 메소드는 언제 사용할까요? 인스턴스 메소드는 인스턴스에 속한 것이기 때문에, 반드시 인스턴스를 생성해야 사용할 수 있습니다. 하지만 클래스 메소드는 클래스에 속한 것이기 때문에, 인스턴스를 생성하지 않고도 바로 실행할 수 있습니다.예를 들어,  Math.abs(), Math.max() 등을 사용하면, 자바에서 미리 만들어 둔 수학 관련 기능을 활용할 수 있습니다. 하지만 우리는 Math 클래스의 인스턴스를 생성하지는 않습니다. 필요하지 않기 때문이죠. 단지 Math 클래스의 기능(메소드)만 활용하면 됩니다.사실 우리가 가장 먼저 접한 ‘클래스 메소드’는 바로 main 메소드입니다. main은 자바 프로그램의 시작점이라고 했습니다. 첫 번째로 실행되는 코드이니, 어떤 인스턴스도 생성되어 있지 않습니다. 따라서 main 메소드 역시 인스턴스를 생성하지 않고 실행하는 ‘클래스 메소드’입니다. 클래스 메소드도 동일하게 static이라는 키워드로 정의할 수 있습니다.5. Wrapper class‘Wrapper 클래스’는 기본 자료형을 객체 형식로 감싸는 역할을 합니다. Integer 클래스는 int형을, Double 클래스는 double을, Long 클래스는 long을, Boolean 클래스는 boolean을 wrapping할 수 있습니다. 그런데 이런 Wrapper 클래스가 왜 필요할까요?기본형 자료형(Primitive Type)을 참조형(Reference Type)처럼 다루어야할 때 Wrapper 클래스를 사용하면 됩니다. 예를 들어서 ArrayList같은 컬렉션을 사용할 때는 꼭 참조형을 사용해야 합니다.// 생성자로 생성하는 방법Integer i = new Integer(123);// 리터럴로 생성하는 방법Integer i = 123;6. ArrayListimport java.util.ArrayList// ArrayList&amp;lt;안에 넣은 객체의 클래스&amp;gt;ArrayList&amp;lt;String&amp;gt; nameList = new ArrayList&amp;lt;&amp;gt;();nameList.add(&quot;Jay&quot;);nameList.add(&quot;Mike&quot;);nameList.remove(1);nameList.contains(&quot;Jay&quot;);for (String name : nameList) {  System.out.println(name)}7. HashMapHashMap&amp;lt;String, Integer&amp;gt; check = new HashMap&amp;lt;&amp;gt;();check.put(&quot;사과&quot;,new Integer(1))check.get(&quot;사과&quot;)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-07T21:01:35+09:00'>07 Jul 2021</time><a class='article__image' href='/java-series4'> <img src='/images/java_logo.png' alt='Java Series [Part4]: 자바의 자료형 특징'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series4'>Java Series [Part4]: 자바의 자료형 특징</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part3]: 자바와 객체지향 프로그래밍",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series3",
      "date"     : "Jul 5, 2021",
      "content"  : "Table of Contents  1. 객체 만들기  2. 객체 설계하기          1) 접근 제어자(Access Modifier)                  a) private          b) protected          c) public final                    2) 메소드 오버로딩(Method Overloading)      3) 생성자      1. 객체 만들기자바는 객체 단위로 동작하는 객체 지향 프로그래밍이기 때문에, Hello world 한 줄을 출력하더라도 클래스로 작성해야 합니다.클래스는 변수와 메소드를 갖는 객체(인스턴스)를 만드는 설계도라고 생각하시면 됩니다.public class BankAccount {    // 변수    private int balance;    private Person owner;        // 메소드    public void setBalance(int newBalance){        this.balance = newBalance;    }    public int getBalance(){        return balance;    }    public void setOwner(Person newOwner){        this.owner = newOwner;    }    public Person getOwner(){        return this.owner;    }        boolean deposit(int amount){        if (amount &amp;lt; 0 || owner.getCashAmount() &amp;lt; amount){            System.out.println(&quot;입금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance += amount;            owner.setCashAmount(owner.getCashAmount()-amount);            System.out.println(amount + &quot;원 입금하였습니다. 잔고: &quot; + balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;                    }    }    boolean withdraw(int amount){        if ( amount &amp;lt; 0 || getBalance() &amp;lt; amount) {            System.out.println(&quot;출금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance -= amount;            owner.setCashAmount(owner.getCashAmount()+amount);            System.out.println(amount + &quot;원 출금하였습니다. 잔고: &quot;+ balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;        }    }    }2. 객체 설계하기1) 접근 제어자(Access Modifier)a) privatepublic class Person {    private int age;    public void setAge(int newAge) {        if (newAge &amp;gt; 0) {            this.age = newAge;        }    }    public int getAge() {        return this.age;    }}age 변수를 public으로 하면 외부에서 Person 객체를 생성했을 때 age 변수에 직접 접근이 가능하다 (p1.age = -10 이런 식으로) 따라서 외부에서 무분별하게 접근하는 것을 막고자 접근 제어자를 public이 아닌 private으로 작성합니다.b) protected접근 제어자에는 public, private 그리고 protected가 있습니다. protected를 사용하면 자식 클래스에 한해서 변수에 직접적으로 접근이 가능합니다. 그렇게 함으로써 private을 사용했을 때 setter, getter 메소드를 작성해야하는 불편함을 해소해줍니다.public class Person {    protected int age;}public class Student extends Person {  public void olderAge() {    age = age + 1  }}c) public final자식 클래스 뿐 아니라 다른 곳에서도 사용되지만 수정은 안되도록 하는 ( 좀 더 일반적인) 방법은 public final입니다.public final double pi = 3.142) 메소드 오버로딩(Method Overloading)‘메소드 오버로딩(Method Overloading)’은 클래스 내에 같은 이름의 메소드를 2개 이상 정의할 수 있게 해주는 기능입니다.public class Calculator {    int add(int a, int b) {        return a + b;    }    int add(int a, int b, int c) {        return a + b + c;    }    double add(double a, double b) {        return a + b;    }}지금까지 써왔던 System.out.println()도 메소드오버로딩되어 있는 메소드입니다.3) 생성자‘생성자(Constructor)’는 크게 두 가지 역할이 있습니다  인스턴스를 만들고,  인스턴스의 속성(인스턴스 변수)들을 초기화시켜줍니다.생성자를 한 개도 정의 안 했을 경우에는 자바에서 자동으로 기본 생성자를 제공해줍니다.Person p1 = new Person();생성자를 하나라도 정의하면 위의 기본 생성자는 사용할 수 없습니다.public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public static void main(String[] args) {    Person p1 = new Person(&quot;Jay&quot;, 27);    }}  생성자 오버로딩도 가능합니다.// 생성자 오버로딩public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public Person(String pName) {    this.name = pName;    this.age = 12;    // 12살을 기본 나이로 설정    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-05T21:01:35+09:00'>05 Jul 2021</time><a class='article__image' href='/java-series3'> <img src='/images/java_logo.png' alt='Java Series [Part3]: 자바와 객체지향 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series3'>Java Series [Part3]: 자바와 객체지향 프로그래밍</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part2]: 자바 시작하기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series2",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  Hello World  변수와 연산  조건문  반복문  배열Hello Worldpublic class HelloWorld {    /*main: 프로그램을 실행하면 가장 먼저 실행되는 메소드    void: 리턴값 없음    String[] args: args라는 이름의 문자열 변수가 메소드에 전달*/    public static void main(String[] args) {        // System: 클래스, out: 클래스 변수, println: 메소드        System.out.println(&quot;Hello World&quot;);    }}변수와 연산public class Variables {    public static void main(String[] args) {        // 선언 방법        // 1)        int age;        age = 27;        // 2)        double num = 12.5;        // 자료형        // primitive type        int myInt = 123;        long myLong = 12345678910L;        double myDouble = 3.14; // double이 더 높은 정밀도, 소수형의 기본 타입        float f = 3.14f;        char a = &#39;a&#39;; // 쌍따옴표로 감싸면 String으로 인식함        char aPrime = 97; // 아스키 값 97 == &#39;a&#39;        char b = &#39;가&#39;;        boolean myBoolean = true;        // 객체형 type        String myString = &quot;jay kim&quot;;    }}조건문public class IfElse {    public static void main(String[] args) {        int temp = 15;        if (temp &amp;lt; 0) {            System.out.println(&quot;오늘의 날씨는 영하입니다: &quot; + temp +&quot;도&quot;);        } else if (temp &amp;lt; 5){            System.out.println(&quot;오늘의 날씨는 0도 이상 5도 미만입니다: &quot; + temp +&quot;도&quot;);        } else {            System.out.println(&quot;오늘의 날씨는 5도 이상입니다: &quot; + temp +&quot;도&quot;);        }    }}public class Switch {    public static void main(String[] args) {        int score = 80;        String grade;        switch (score / 10) {            case 10:                grade = &quot;A+&quot;;                break;            case 9:                grade = &quot;A&quot;;                break;            default:                grade = &quot;F&quot;;                break;        }        System.out.println(&quot;학점은 &quot; + grade + &quot;입니다.&quot;);    }}반복문public class For {    public static void main(String[] args) {        int sum = 0;        // i++는 실행 부분이 실행되고 나서 실행된다        for (int i = 1; i &amp;lt;= 5; i++) {            sum += i;            System.out.println(i);        }    }}public class While {    public static void main(String[] args ) {        int i = 1;        int sum = 0;        while (i &amp;lt;= 3) {            sum = sum + i;            i = i + 1;        }        System.out.println(sum);    }}배열public class Array {    public static void main(String[] args) {        // 배열 생성하는 첫 번째 방법        int[] intArray = new int[5];        intArray[0] = 2;        intArray[1] = 3;        intArray[2] = 5;        intArray[3] = 7;        intArray[4] = 11;        // 배열 생성하는 두 번째 방법        int[] arr1 = {1, 2, 3, 4, 5};        int[] arr2 = arr1;        int[] arr3 = arr1.clone();        arr1[0] = 100;        System.out.println(arr2[0]);        System.out.println(arr3[0]);        for (double i : intArray) {            System.out.println(i);        }        int[][] multiArray = new int[3][4];        int[][] multiArray2 = { {1 ,2, 3, 4},                            {5, 6, 7, 8},                            {9, 10, 11, 12}                            };        System.out.println(multiArray2[0][1]);    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series2'> <img src='/images/java_logo.png' alt='Java Series [Part2]: 자바 시작하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series2'>Java Series [Part2]: 자바 시작하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part1]: 자바와 가상머신",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series1",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  자바와 가상머신자바와 가상머신  한 번만 작성하면, 어디서든 동작한다. (Write Once, Run Anywhere.)어떤 언어는 운영체제에 따라 동작이 달라집니다.분명히 윈도우즈에서는 잘 동작했는데, 맥에서 동작하지 않는 일이 발생합니다.그래서 우리가 개발할 때는 항상 운영체제를 신경써야 합니다.중간 중간 테스트도 해주어야 하고요.만약 휴대폰 애플리케이션을 개발한다면 어떨까요?최악의 경우, 모든 휴대폰 기종을 모아서 매번 테스트를 해봐야겠네요.자바는 이런 ‘호환성’문제를 해결해 줍니다.‘자바 가상머신’이라는 것만 설치되면, 어느 운영체제이든, 어느 디바이스이든, 동일하게 동작합니다.(자바 가상머신은 영어로 Java Virtual Machine, 줄여서 JVM 이라고 부릅니다.)이러한 자바의 높은 호환성은 애플리케이션의 특징과도 잘 맞아떨어지기 때문에, 애플리케이션 개발에 활발히 사용되고 있죠.JVM을 사용해서 마음껏 개발할 수 있는 환경을 JRE (Java Runtime Environment) 라고 부르며, 내 컴퓨터에 이런 환경을 만들기 위해서는 JDK (Java Development Kit) 라는 것을 설치하면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series1'> <img src='/images/java_logo.png' alt='Java Series [Part1]: 자바와 가상머신'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series1'>Java Series [Part1]: 자바와 가상머신</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Intermediate Series [Part1]: Iterable, Iterator, Generator",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-mid-context_manager",
      "date"     : "Jun 1, 2021",
      "content"  : "Table of Contents  Python tricks we MUST all use",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-06-01T21:01:35+09:00'>01 Jun 2021</time><a class='article__image' href='/python-mid-context_manager'> <img src='/images/python_intermediate_logo.png' alt='Python Intermediate Series [Part1]: Iterable, Iterator, Generator'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-mid-context_manager'>Python Intermediate Series [Part1]: Iterable, Iterator, Generator</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part11]: Advanced RNN",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_rnn",
      "date"     : "Apr 6, 2021",
      "content"  : "Table of Contents  Advanced RNN          바닐라 RNN의 한계점      LSTM (Long Short Term Memory)      GRU  (Gated Recurrent Unit)                  참조                    Advanced RNN바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다.LSTM (Long Short Term Memory)바닐라 RNN의 한계를 극복하기 위해 다양한 RNN의 변형이 나왔습니다. LSTM과 GRU가 대표적인 예입니다.긴 시퀀스를 다룰 때 LSTM이 바닐라 RNN과 비교해 어떤 점에서 더 좋은지 알기위해 우선 LSTM의 구조에 대해 먼저 살펴보겠습니다.꽤나 복잡하게 생겨서 머리가 아플 수 있지만 하나하나 살펴보면 충분히 가능하기 때문에, 천천히 살펴보도록 하겠습니다.우선 위로 지나가는 Cell state 부분부터 한번 보겠습니다.Cell state의 역할은 중요한 정보는 그대로 넘겨주고, 중요하지 않은 정보는 약하게 함으로써 중요한 정보만 계속 흘러갈 수 있도록 해줍니다. 이걸 가능하게 하는 것이 바로 게이트입니다. i(t) 게이트를 통해 중요한 정보는 흘러가고, f(t) 게이트를 통해 중요하지 않은 정보를 약하게 만듭니다. 그러면 어떤 정보가 중요하고 중요하지 않은지는 어떤 기준으로 정해지고 어떻게 설정해야 할까요? 그 기준은 바로 이전 셀의 h(t-1)의 값과 현재 층의 입력 x(t) 으로 정해지며, 설정하는 것은 우리의 몫이 아닌 신경망의 역할입니다. 신경망은 학습을 통해 알아서 중요한 정보와 중요하지 않은 정보를 잘 선택할 수 있도록 학습됩니다.위 과정을 통해 C(t)를 만듭니다.정보의 중요도에 따라 크기가 달라진 C(t)의 값을 tanh에 넣어서 이 값을 다시 -1과 1사이의 값으로 만들어 준 후 o(t)에 곱해 줌으로써 h(t)를 계산합니다.C(t)와 h(t)를 다음 셀에 전달해줍니다.그래서 이러한 LSTM이 어떤 점에서 바닐라 RNN이 가지는 한계를 극복하게 된걸까요? 그것은 바로 C(t)가 셀에서 tanh함수를 거치지 않기 때문에 중요한 정보가 셀을 거듭하더라도 약해지지 않고  정보를 잘 전달할 수 있다는 것입니다.GRU  (Gated Recurrent Unit)GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수님이 집필한 논문에서 제안되었습니다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰습니다.LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했습니다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재합니다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있습니다.데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있습니다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문입니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-06T21:01:35+09:00'>06 Apr 2021</time><a class='article__image' href='/advanced_rnn'> <img src='/images/LSTM_2.png' alt='Deep Learning Series [Part11]: Advanced RNN'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_rnn'>Deep Learning Series [Part11]: Advanced RNN</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part10]: RNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_rnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  RNN          시퀀스 데이터 vs 시계열 데이터      시퀀스 데이터      바닐라 RNN      평가      깊은 RNN 모델      양방향 RNN 모델      바닐라 RNN의 한계점                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.          참조                    RNN시퀀스 데이터 vs 시계열 데이터  시퀀스 데이터는 순서만 중요한 데이터 (문장, 음성)  시계열 데이터는 순서뿐 아니라 데이터가 발생한 시간도 중요한 데이터 (주식, 센서 데이터)시퀀스 데이터  시퀀스 데이터는 IID가정을 대체로 위배하기 때문에, 순서를 바꾸면 데이터의 확률 분포도 바뀌게 됩니다.  이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률 분포를 다루기 위해 조건부 확률을 이용할 수 있습니다.  다음과 같은 문제를 해결하기 위한 모델을 만든다고 생각해 봅시다.이미지에 대한 설명문 달기주가 예측하기한국어 영어로 번역하기다음과 같은 문제는 입력과 출력이 시퀀스 형태를 가지고 있습니다. 이러한 시퀀스 데이터를 처리하기 위해 고안된 모델을 시퀀스 모델이라고 합니다. 그 중에서도 RNN은 딥러닝에서 가장 기본적인 시퀀스 모델입니다.one to one: 비 시퀀스 데이터를 다루는 경우one to many: 이미지 캡셔닝many to one: 주가 예측, 텍스트 분류 many to many: 번역바닐라 RNN그동안 신경망들은 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했습니다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 합니다. 그런데 그렇지 않은 신경망들도 있습니다. RNN(Recurrent Neural Network)이 그 중 하나입니다.RNN은 해당 층의 입력 데이터와 이전 층에서의 출력을 함께 입력으로 사용합니다.그리고 이전 층의 출력과 해당 층의 입력은 다음과 같이 결합되게 됩니다.(참고로 W_d와 W_h를 concatenation해서 쓸 수도 있습니다.)이를 모델에 적용해 다시 한 번 살펴보면 다음과 같습니다.이를 식으로 표현하면 다음과 같습니다.평가Loss함수 식을 보면 변수 t에 대해 theta값은 변하지 않는다 -&amp;gt; 펼쳐져 있지만 W_h와 W_d는 같은 레이어입니다.깊은 RNN 모델양방향 RNN 모델양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다.즉, RNN이 과거 시점(time step)의 데이터들을 참고해서, 찾고자하는 정답을 예측하지만 실제 문제에서는 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에 힌트가 있는 경우도 많습니다. 그래서 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다.바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다.만약 tanh가 아니라 relu를 쓴다면, 반대로 맨 앞에서 받았던 정보가 층을 거듭할수록 값이 너무 커져서 시퀀스 뒤에 위치한 데이터의 학습을 방해할 수 있습니다.이를 해결하기 위해 RNN의 advanced 버전인 LSTM과 GRU에 대해서는 다음 포스트에서 살펴보도록 하겠습니다.실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/basic_of_rnn'> <img src='/images/basic_of_rnn_5.png' alt='Deep Learning Series [Part10]: RNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_rnn'>Deep Learning Series [Part10]: RNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part9]: Advanced CNN  ",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_cnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  Advanced CNN          1. GoogLeNet                  1) GoogLeNet의 특징          2) Inception Module          3) GoogLeNet Network Structure                    2. ResNet                  1) Residual Learning          2) Residual Block          3) ResNet Architecture                    Advanced CNN1. GoogLeNet1) GoogLeNet의 특징  커널의 적절한 사이즈를 찾기 위해 고민하기 보다, 여러 가지 사이즈의 커널을 병렬로 이용함으로써 보다 풍부한 Feature Extraction 수행한다  1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시킨다.  1 X 1 이라서 Feature map dimension이 줄어든 것이 아니라, 1 X 1 필터의 채널 수를 작은 사이즈를 썼기 때문이다.  참고로 컨볼루션 커널의 사이즈, padding, striding이 Feature map의 사이즈를 결정한다.  컨볼루션 커널의 채널의 개수가 Feature map의 개수를 결정한다.2) Inception Module  위 그림과 같이 1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시켰다.  Inception모듈을 보면 여러 필터가 병렬적으로 연산되고 모듈 끝에서 결과들이 Concatenation됩니다.  따라서 Feature map의 개수는 달라도 괜찮지만, Feature map의 사이즈는 같아야 합니다.3) GoogLeNet Network Structure  다음과 같이 Inception Module은 총 9개로 구성되어 있다  Concatenation까지가 Inception Module에 포함되고 그 후에 보통 Pooling layer를 거친 뒤 다시 Inception Module로 들어가는 것으로 반복된다.  Inception Module을 2개 거친 후 Pooling하기도 하고, 5개 거치고 Pooling 하기도 한다.2. ResNet  VGG 모델이 나온 이후 깊은 Network가 좋은 성능을 낸다는 인식이 생겼다.  하지만 비슷한 방식으로 Network를 더 깊게 만들었을 때 오히려 성능이 저하되었다.  그 원인으로는 Gradient Vanishing과 파라미터 수 증가에 따른 학습 속도 저하가 있다.  파라미터 수 증가는 앞에서와 같이 1 X 1 컨볼루션으로 해결하였다.  Gradient Vanishing 문제는 Residual Learning을 통해 해결하였다.1) Residual Learning  처음 Residual Learning이 나오기 전에 시도되던 방법은 Identity mapping이다.  Identity mapping은 층은 더 깊게 만들되, Gradient vanishing은 생기지 않도록 하기 위해 이전 값을 그대로 다시 통과시키는 방법이다.  비선형성은 있어야 층을 깊게 쌓는 의미가 있으므로 Relu()정도가 있어야 하는데, 이렇게 되면 identity한 mapping이 되기 어려워진다.  그래서 좀 더 쉬운 방법으로 제안된 것이 Residual Learning이다.  H(x)가 x가 되도록 하는 것이 아니라, F(x)가 0이 되도록 학습하는 것이 쉽다.2) Residual Block  ResNet 50을 포함한 이보다 깊은 네트워크(50/101/152)에서는 1 X 1 Conv를 이용해 파라미터 갯수를 줄였다.  Residual Block 내에서는 Feature map 사이즈는 동일하고 Filter수만 변함3) ResNet Architecture",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/advanced_cnn'> <img src='/images/googlenet_1.png' alt='Deep Learning Series [Part9]: Advanced CNN  '> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_cnn'>Deep Learning Series [Part9]: Advanced CNN  </a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part8]: CNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_cnn",
      "date"     : "Apr 4, 2021",
      "content"  : "Table of Contents  CNN의 기초          CNN의 발단      CNN 구조                  1. 컨볼루션 연산          2. 커널(채널), 필터          3. 스트라이드, 패딩                          스트라이드(stride)              패딩(padding)                                4. 풀링 연산                    특성맵의 크기 구하기      CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.                    CNN의 기초CNN의 발단그동안 앞에서 설명한 신경망은 모두 기본 전제가 노드가 서로 완전 연결되어 있는 경우였습니다. 이를 완전 연결층(Fully connected layer)라고 합니다. 이는 목적한 바를 잘 이룰 수 있도록 특징 공간 갖는 모델을 만들 수 있지만, 가중치가 너무 많아 복잡도가 너무 높습니다. 이는 모델의 학습 속도를 더디게 하며 또한 과잉적합에 빠질 가능성을 높이게 됩니다. 컨볼루션 신경망(CNN)은 부분 연결 구조로 이러한 문제를 잘 해결하며 또한 특징을 잘 추출하도록 해줍니다. 이러한 이유로 CNN은 이미지, 비전 분야에서 매우 뛰어난 성능을 발휘했으며, 음성인식이나 자연어 처리같은 다른 작업에도 사용됩니다.실제로 1958년에 데이비드 허블의 연구에서는 인간의 시각 뉴런은 부분연결 구조를 가진다는 사실을 밝혀냈습니다. 이를 조금 더 자세히 설명하면, 눈의 시각 피질 안의 많은 뉴런은 작은 국부 수용장(Local receptive field)를 가지며 이는 시야의 영역에서 작은 특정 패턴에 뉴런이 반응한다는 것입니다. 예를 들어,눈이 처음 무언가를 봤을 때는 국부 수용장에 해당하는 작은 영역이 가지는 패턴에 먼저 반응을 하고, 시각 신호가 연속적으로 뇌의 뉴런들을 통과하며 다음 뉴런들은 점점 더 큰 수용장에 있는 복잡한 패턴에 반응을 합니다. 이러한 점에서 CNN은 사람의 눈과 비슷한 원리를 가지고 있다고 할 수 있습니다.CNN은 눈의 원리와 비슷하다.부분 연결 구조로 되어 있어 비교적 구해야 하는 가중치의 갯수가 적다.학습 속도가 훨씬 빠르며, 과잉적합에 빠질 가능성 또한 낮아진다.CNN 구조보통 이미지 분류를 위한 CNN의 구조는 다음과 같습니다.1. 컨볼루션 연산사실 CNN에서의 합성곱은 실제의 합성곱과는 다릅니다. 실제의 합성곱은 필터를 뒤집어야 하지만 CNN에서는 필터를 뒤집지 않습니다. 그 이유는 어차피 필터의 가중치 값은 처음에 보통 랜덤으로 초기화하게 됩니다. 따라서 가중치를 굳이 뒤집지 않아도 상관이 없습니다. 어쨋든 CNN에서의 컨볼루션 연산은 필터가 옆으로 움직이면서 데이터와 각각 원소별 곱셈을 진행하고 그 곱셈의 결과들을 하나의 값으로 합하면 됩니다.밑의 예시를 살펴보면 3×1 + 1×(-1) + 1×1 + 7×(-1) + 2×1 + 5×(-1) = -7 이 됩니다.다음의 컨볼루션 연산은 필터의 사이즈, 스트라이드의 크기, 패딩 여부 등에 따라 결과(특성 맵)이 달라집니다.2. 커널(채널), 필터여기서 CNN에서 정말 헷갈리지만 또 중요한 개념이 등장합니다. 바로 커널(채널)과 필터입니다. 느낌이 비슷해서 헷갈릴 수 있지만 엄연히 구분되어 사용되어야 하기 때문에 여기서 한 번 짚고 넘어가도록 하겠습니다.커널은 채널과 비슷한 의미로 데이터의 커널의 수(RGB채널의 경우 3)와 필터의 커널 수는 항상 같아야 합니다.필터는 카메라 필터와 비슷하게 shape을 위한 필터, curve를 위한 필터와 같은 필터를 의미합니다.예를 들어 채널이 1(Gray scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.만약 채널이 3(RGB scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.따라서 필터의 커널의 수는 항상 데이터의 커널의 수를 따르게 되고,결과(특성 맵)의 커널 수는 항상 필터의 수를 따르게 됩니다.CNN의 목표는 특징을 잘 추출해주는 필터의 가중치를 찾는 것입니다.3. 스트라이드, 패딩스트라이드(stride)  스트라이드는 필터의 미끄러지는 간격을 조절하는 것을 말합니다.  기본은 1이지만, 2를(2pixel 단위로 Sliding window 이동) 적용하면 입력 특성 맵 대비 출력 특성 맵의 크기를 대략 절반으로 줄여줍니다.  stride 를 키우면 공간적인 feature 특성을 손실할 가능성이 높아지지만, 이것이 중요 feature 들의 손실을 반드시 의미하지는 않습니다.  오히려 불필요한 특성을 제거하는 효과를 가져 올 수 있습니다. 또한 Convolution 연산 속도를 향상 시킵니다.패딩(padding)  패딩은 데이터 양 끝에 빈 원소를 추가하는 것을 말합니다.      패딩에는 밸리드(valid) 패딩, 풀(full) 패딩, 세임(same) 패딩이 있습니다. 패딩 각각의 역할은 다음과 같습니다.                            패딩          역할                                      밸리드          평범한 패딩으로 원소별 연산 참여도가 다르다                          풀          데이터 원소의 연산 참여도를 갖게 만든다                          세임          특성 맵의 사이즈가 기존 데이터의 사이즈와 같도록 만든다                      세임패딩을 적용하면 Conv 연산 수행 시 출력 특성 맵 이 입력 특성 맵 대비 계속적으로 작아지는 것을 막아줍니다.4. 풀링 연산  풀링층은 특성 맵의 사이즈를 줄여주는 역할을 합니다.  보통 최대 풀링 또는 평균 풀링을 많이 사용합니다.    보통은 Conv연산, ReLU activation함수를 적용한 후에 풀링을 적용합니다.    풀링은 비슷한 feature 들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화 시켜 줍니다.  일반적으로는 Sharp 한 feature 가 보다 Classification 에 유리하여 최대 풀링이 더 많이 사용됩니다.  풀링의 경우 특정 위치의 feature 값이 손실 되는 이슈로 인해 최근 Advanced CNN에서는 Stride만 이용하여 모델을 구성하는 경향입니다.특성맵의 크기 구하기  5×5 입력, 3×3 필터가 스트라이드가 1이고 패딩이 없는 경우, 특성 맵의 크기는 3×3이 됩니다.CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-04T21:01:35+09:00'>04 Apr 2021</time><a class='article__image' href='/basic_of_cnn'> <img src='/images/cnn_3.png' alt='Deep Learning Series [Part8]: CNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_cnn'>Deep Learning Series [Part8]: CNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-heapq",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/python-component-heapq'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-heapq'>Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/regulation",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝의 규제를 통한 일반화          1. 모델의 일반화      2. 규제의 필요성과 원리                  규제의 정의                    3. 규제 기법                  1) 데이터 증가(Data Augmentation)                          a) Salt &amp;amp; Pepper Noise              b) Rotation, Flipping Shifting              c) Dropping, Exchanging for Text augmentation              d) 새로운 데이터 생성                                2) 가중치 벌칙                          L2놈                                3) 드롭아웃(Dropout)          4) 조기 멈춤(Early stopping)                    딥러닝의 규제를 통한 일반화1. 모델의 일반화모델의 일반화란 무엇일까요? 모델이 특정 데이터에만 잘 맞는 것이 아니라 전례가 없는 데이터에 대해서도 높은 정확도를 유지하도록 하는 것을 모델의 일반화라고 합니다. 그러기 위해서는 우선  우리가 해결하고자 하는 문제가 가지고 있는 데이터의 특징 공간을 모델이 충분히 잘 수용할 수 있어야 합니다.모델의 용량은 데이터를 나타낼 수 있는 모델의 차원이라고 생각합니다. 따라서 모델이 데이터의 특징을 잘 찾아내기 위해서는, 모델의 차원이 데이터의 특징 공간의 차원 보다는 높아야 합니다. 그럼 무조건 모델의 용량이 크면 다 해결되는 걸까요? 그렇지는 않습니다. 왜냐하면 데이터에는 우리가 정말로 원하는 성분 말고도 우리가 원치 않는 노이즈가 함께 존재하는 경우가 대부분이기 때문입니다. 용량이 너무 크면 모델은 노이즈에 대한 잘못된 특징도 수용하기 때문에 성능이 떨어지게 됩니다. 이러한 현상을 모델이 데이터에 과잉적합되었다고 합니다. 따라서 현대 기계 학습은 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 노이즈에 민감하지 않도록 하기 위해 여러 가지 규제 기법을 적용하는 접근방법을 채택합니다.용량이 충분히 큰 모델에 여러 가지 규제 기법을 적용해 일반화 능력을 높일 수 있다.2. 규제의 필요성과 원리따라서 우리는 모델이 주어진 문제에 대해 최소한의 역할을 하기 위해 용량을 크게 하는 것이 좋으며(과소 적합 방지), 전례가 없는 데이터에 대해서도 높은 정확도를 가지는 성능이 좋은 모델을 만들기 위해 모델을 잘 일반화 시켜야 합니다(과잉 적합 방지).그리고 모델을 일반화 시키기 위한 방법을 우리는 규제라고 합니다.규제의 정의  일반화 오류를 줄이기 위해 학습 알고리즘을 수정하는 모든 방법3. 규제 기법현대 기계 학습은 아주 다양한 규제 기법을 사용합니다.  데이터를 통해          Data Augmentation -&amp;gt; Noise injection        Loss 함수를 통해          Weight Decay -&amp;gt; L1, L2 규제        Neural network layer를 통해          Dropout        학습 방식, 추론 방식을 통해          Early Stopping      Bagging &amp;amp; Ensemble      1) 데이터 증가(Data Augmentation)핵심 특징을 간직한 채, noise를 더하여 데이터를 확장하는 방법으로 보통 핵심 특징을 보존하기 위해 휴리스틱한 방법을 사용합니다.이를 통해 더욱 noise robust한 모델을 얻을 수 있습니다. 규칙을 통해 데이터를 증가시키려고 하면 모델이 그 규칙을 배우게 되기 때문에규칙이 아닌 Randomness를 통해 데이터를 증가시켜야 합니다.a) Salt &amp;amp; Pepper Noise  Adding RGB(255, 255, 255) noise  Adding RGB(0, 0, 0) noiseb) Rotation, Flipping Shiftingc) Dropping, Exchanging for Text augmentation  임의의 단어를 생략한다  임의로 특정 단어를 주변 단어와 위치를 바꾼다d) 새로운 데이터 생성  Autoencoder, GAN을 통해 데이터를 학습 후 새로운 데이터 생성2) 가중치 벌칙가중치 𝛉가 커지게 되면 R항이 커지게 되고 그러면 손실 함수 J가 증가하게 됩니다. 우리의 학습 알고리즘은 손실 함수가 작아지도록 하므로 R항은 가중치의 크기에 제약을 가하는 역할을 한다고 볼 수 있습니다. 규제 항 R은 가중치를 작은 값으로 유지하므로 모델의 용량을 제한하는 역할을 한다고 볼 수 있습니다. 𝜆는 층마다 다르게 할 수도 있고 같게 할 수도 있습니다.하지만 실제로 사용하게 되면 성능이 오히려 떨어져 잘 사용하진 않습니다.L2놈규제 항 R로 가장 널리 쓰이는 것은 L2놈이며 이를 가중치 감쇠 기법이라고 합니다.목적 함수가 달라졌으므로, 그래디언트와 가중치 또한 바뀌게 됩니다.3) 드롭아웃(Dropout)드롭아웃이란, 입력층과 은닉층의 모든 노드에 대해 일정 확률로 노드를 임의로 제거하는 것입니다. 해당되는 노드의 들어오고 나가는 엣지들을 모두 제거합니다.(0을 출력합니다) 보통 드롭아웃될 확률의 0.1~0.5로 합니다.여기서 이렇게 하는 것이 과연 어떤 의미가 있는지 궁금해 하시는 분들이 있을 것 같아 예를 한 가지 들어보도록 하겠습니다. 어떤 회사에서 직원들이 아침마다 일정 확률로 회사를 쉰다면 어떻게 될까요? 회사가 이런 식으로 운영된다면 어떠한 업무도 한 사람에게 전적으로 의지할 수 없게 되고, 전문성이 여러 사람에게 나뉘어져 있어야 합니다. 그렇기에 이 회사는 유연성이 훨씬 더 높아질 것입니다. 한 직원이 직장을 떠나도 크게 달라지는 것이 없을 것입니다.신경망 또한 마찬가지입니다. 노드들은 몇 개의 노드에만 지나치게 의존할 수 없습니다. 모든 노드에 주의를 기울여야 합니다. 그러므로 입력값의 작은 변화에 덜 민감해집니다. 결국 더 안정적인 네트워크가 되어 일반화 성능이 좋아집니다.테스트시에는 드롭아웃을 사용하지 않는 보통 신경망처럼 전방 계산을 수행하기 때문에 출력이 학습할 떄에 비해 1/p배 더 큽니다. 따라서 W에 p를 곱하여 이를 상쇄시켜 줘야합니다.일반적으로 (출력층을 제외한) 맨 위의 층부터 세 번째 층까지 있는 노드에만 드롭아웃을 적용합니다. 또한 많은 최신 신경망 구조는 마지막 은닉층 뒤에만 드롭아웃을 사용합니다.4) 조기 멈춤(Early stopping)보통 학습을 오래 시킬수록 더 최적점에 접근합니다. 하지만 어떤 시점을 넘어서면 모델이 훈련 데이터에만 너무 최적화가 되어 검증집합에 대해서는 오히려 성능이 떨어지기 시작합니다. 다시 말해, 일반화 능력이 하락하기 시작하는 것입니다. 따라서 일반화 능력이 최고인 지점, 즉 검증집합의 오류가 최저인 지점에서 학습을 멈추는 전략을 조기 멈춤이라고 합니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/regulation'> <img src='/images/regulation_1.png' alt='Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/regulation'>Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/performance",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝 성능 향상을 위한 요령          1. 딥러닝 성능 향상의 방향성      2. 데이터 전처리      3. 가중치 초기화(Initialization)      4. 배치 정규화(Batch Normalization)      5. 그래디언트 모멘텀(Gradient Momentum)      6. 적응적 학습률(Adaptive Learning-rate)      딥러닝 성능 향상을 위한 요령1. 딥러닝 성능 향상의 방향성딥러닝의 성능을 향상시킨다고 할 때는 보통 다음과 같은 두 가지를 말합니다.과잉 적합 방지를 통한 일반화 능력 극대화 (실전에 배치되었을 때의 성능을 극대화)학습 알고리즘의 수렴 속도 향상 (더 빠른 학습은 결국 더 좋은 성능을 가져다 준다)2. 데이터 전처리데이터 전처리는 데이터 정규화를 의미합니다.데이터가 양수, 음수 값을 골고루 갖도록 한다 =&amp;gt; 평균: 0특성 scale이 같도록 한다 =&amp;gt; 표준편차: 13. 가중치 초기화(Initialization)역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전파하면서 진행됩니다. 그런데 알고리즘이 하위층으로 진행될수록 그래디언트가 작아지는 경우가 많습니다. 이 문제를 그래디언트 소실이라고 합니다. 어떤 경우엔 반대로 그래디언트가 점점 커져 여러 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산합니다. 이 문제를 그래디언트 폭주라고 하며 순환 신경망에서 주로 나타납니다. 일반적으로 불안정한 그래디언트는 심층 신경망 훈련을 어렵게 만듭니다. 층마다 학습 속도가 달라질 수 있기 때문입니다.기계 학습 초기에는 가중치 초기화를 정규분포 형태(평균:0, 분산:1)를 갖도록 초기화하였습니다. 하지만 입력층의 노드 수가 많다면 출력층의 분포는 밑에 그림과 같이 값들이 대부분 0이나 1로 수렴하게 됩니다. 문제는 0과 1근처에서 활성화 함수의 그래디언트가 거의 0에 가깝다는 것 입니다. 그렇기 때문에 아래층까지 역전파가 진행되기도 전에 이미 그래디언트가 거의 소실됩니다.그렇다면 출력층의 분포가 어떤게 좋을까요? 출력층의 값이 고르게 분포해 시그모이드 함수의 비선형성과 적당한 그래디언트를 갖도록 하는 것이 좋을 것입니다.그렇다면 왜 이런 분포가 안되는 걸까요? 아마 그 이유는 입력층(각 층은 다음 층의 입력층이므로 결국 모든 층)의 노드 수가 많으면 가중치와 데이터가 정규분포를 갖는다고 하더라도 모두 더하게 되면 출력층에 sum(wx)의 값이 치우치게 되고 그러면 sigmoid(sum(wx))는 0또는 1로 주로 분포하게 될 것 입니다. 따라서 이를 완화시켜주기 위해서는 입력층의 노드 수가 많다면 그만큼 가중치의 분산을 작게 하여 최대한 작은 값을 갖도록 하면 sum(wx)의 값이 치우치게 되지 않도록 해줄 것입니다. 이와 관련한 몇 가지 초기화 방법을 살펴보겠습니다.🔔 가중치 초기화는 Gradient vanishing문제를 완화시켜줍니다.평균이 0인 정규 분포를 갖도록 한다표준편차 크면 그리고 노드의 갯수도 많으면 값이 특정 부분에 몰리게 된다.4. 배치 정규화(Batch Normalization)배치 정규화는 각 층에서 활성화 함수를 통과하기 전이나 후에 입력을 정규화한 다음, 두 개의 새로운 파라미터(𝛾, 𝛽)로 결과값의 스케일을 조정하고 이동시킵니다. 정규화 하기 위해서는 평균과 표준편차를 구해야 합니다. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가합니다. 테스트 시에는 어떻게 할까요? 간단한 문제는 아닙니다. 아마 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 합니다. 이 경우 입력의 평균과 표준편차를 계산할 방법이 없습니다. 샘플의 배치를 사용한다 하더라도 매우 작거나 독립 동일 분포(IID)조건을 만족하지 못할 수도 있습니다.케라스에서는 이를 층의 입력 평균과 표준편차의 이동 평균(moving average)을 사용해 훈련하는 동안 최종 통계를 추정함으로써 해결합니다. 케라스의 BatchNormalization층은 이를 자동으로 수행합니다.정리하면 배치 정규화 층마다 네 개의 파라미터 벡터가 학습됩니다.  𝛾(출력 스케일 벡터)와 𝛽(출력 이동 벡터)는 일반적인 역전파를 통해 학습됩니다. 𝜇(최종 입력 평균 벡터)와 𝜎(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정됩니다. 𝜇와 𝜎는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용됩니다.(배치 입력 평균과 표준편차를 대체하기 위해)다음과 같은 배치 정규화는Gradient vanishing문제를 완화시켜준다.Learning rate 높여도 학습이 잘된다.일반화 능력이 좋아진다.5. 그래디언트 모멘텀(Gradient Momentum)모멘텀은 학습을 좀 더 안정감 있게 하도록 해줍니다. 데이터에 의해 Gradient를 계산할 때 만약 Noisy한 데이터인 경우, Gradient가 잘못된 방향으로 갈 가능성이 큽니다. 그렇기 때문에 그 동안 누적된 Gradient를 감안하여 Gradient가 Noisy한 데이터에 의한 안 좋은 영향을 줄여준다. 영어로 잘 설명된 부분이 있어 가져와 보았습니다.  Momentum method can accelelerate gradient descent by taking accounts of previous gradients in the update rule equation in every iteration6. 적응적 학습률(Adaptive Learning-rate)가중치 업데이트의 척도가 되는 학습률을 각 가중치의 학습 진행 정도에 따라 다르게 바꿔주는 것을 적응적 학습률이라고 한다.적응적 학습률은가중치의 업데이트가 많이 이루어질수록 점점 학습률을 줄여나간다.특성마다 업데이트가 많이 된 특성은 학습률을 줄이고, 적게된 특성은 학습률을 늘린다.※ 적응적 학습률과 학습률 스케줄링의 차이가 뭘까? 둘 중 하나만 사용하면 되는걸까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/performance'> <img src='/images/performance_2.png' alt='Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/performance'>Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part5]: 다층 신경망 이론",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/multi_layer_perceptron",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  다층 신경망 이론          1. 인공신경망                  퍼셉트론                    2. 비선형 분류 문제                  다층 퍼셉트론(MLP)                    3. 딥러닝의 등장      4. 다층 신경망에서의 경사 하강법      다층 신경망 이론1. 인공신경망새가 비행기의 발명에 영감이 되었다면, 사람의 뇌는 인공신경망의 영감이 되었습니다. 인공 신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델입니다. 그러나 비행기가 새처럼 날개를 펄럭거릴 필요가 없듯이, 인공 신경망이 사람의 뇌와 똑같이 동작해야 할 이유는 없습니다. 최근 연구자들은 인공 신경망 연구의 창의성을 위해 이러한 비교 자체를 모두 버려야 한다고 주장합니다.인공 신경망은 딥러닝의 핵심입니다. 인공 신경망은 다재다능하고 강력하며 확장성 또한 좋아서 이미지 분류, 음성 인식, 비디오 추천 등 아주 복잡한 문제를 다루는 데 적합합니다.퍼셉트론퍼셉트론은 가장 간단한 인공 신경망 구조 중 하나로 1957년 프랑크 로젠블라트가 제안했습니다. 퍼셉트론은 TLU(Threshhold Logic Unit)이라고 불리는 인공 뉴런을 기반으로 합니다.그렇다면 퍼셉트론은 어떻게 훈련 될까요? 여기서 실제 생물학적 뉴런에 약간의 영감을 받았다고 할 수 있습니다. 도널드 헤브는 1949년에 ‘서로 활성화되는 세포가 서로 연결된다.’라는 아이디어를 제안합니다. 즉 두 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가한다는 것입니다. 여기서 퍼셉트론은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련됩니다. 퍼셉트론 학습규칙은 오차가 감소되도록 연결을 강화시킵니다.그러나 이러한 퍼셉트론은 XOR과 같은 비선형 분류 문제를 해결하지 못하는 약점이 지적되었습니다.2. 비선형 분류 문제XOR문제는 대표적인 비선형 분류 문제입니다. 이 문제는 1970년대 민스키의 『Perceptrons』에서 지적되었고, 이 후 한동안 신경망 연구가 정체기를 겪었습니다. 이 후 이를 해결하기 위한 방법이 몇 가지 발표되며 신경망 연구가 부활하는 계기가 되었습니다. 새로 도입한 기법을 요약하면,은닉층을 둔다.시그모이드 활성함수를 도입한다.오류 역전파 알고리즘을 사용한다.시그모이드 함수와 역전파 알고리즘은 살펴봤기 때문에 여기서는 은닉층의 필요성에 대해 알아보겠습니다. 다시 XOR문제로 돌아와 보면, XOR문제는 주어진 x1, x2 공간에서는 데이터를 분류하는 모델을 만들 수 없습니다. 따라서 분류가 가능하도록 해주는 특징공간으로 옮겨야 하는데, 이를 가능하게 해주는 것이 바로 은닉층의 역할입니다. 밑에 그림과 같이 두 개의 퍼셉트론을 이용해 새로운 특징공간 z1, z2로 옮기면 우리의 데이터를 분류할 수 있게 됩니다.다층 퍼셉트론(MLP)퍼셉트론을 여러 개 쌓아올린 인공 신경망을 다층 퍼셉트론이라 합니다. 다층 퍼셉트론은 하나의 입력층, 하나 이상의 은닉층 그리고 출력층으로 구성됩니다.3. 딥러닝의 등장은닉층을 여러 개 쌓아 올린 인공 신경망을 심층 신경망이라고 합니다. 딥러닝은 이러한 심층 신경망을 연구하는 분야입니다. 깊은 층을 통해 비선형 분류 문제를 해결하게 되며 이를 계기로 다양한 문제에 층을 깊이 쌓은 신경망 구조가 주목을 받기 시작했습니다. 우리의 생각으로는 뚜렷한 구별 방법이 떠오르지 않지만, 신경망을 깊게 쌓음으로써 기계가 여러 가지 특징 공간에서 데이터를 볼 수 있게 되었고 이 방법은 실제로 비정형 데이터(음성, 사진 등)를 다루는 데에 굉장한 성능을 보여주었습니다. 또한 깊은 층의 의미가 있기 위해 각 층마다 활성화 함수를 사용했는데, 미분 연산이 간단한 ReLU함수가 등장하게 되면서 층을 더 깊이 쌓는 것이 실제로 가능해지게 되었습니다. 이 때 부터 본격적으로 비약적인 발전을 하게 되었습니다.4. 다층 신경망에서의 경사 하강법다음과 같이 구한 W1, W2에 대한 각각의 그래디언트를 이용해 각각의 가중치를 업데이트 한다. 행렬로 표기된 이유는 배치 경사 하강법을 가정했기 때문이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/multi_layer_perceptron'> <img src='/images/multi_layer_3.png' alt='Deep Learning Series [Part5]: 다층 신경망 이론'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/multi_layer_perceptron'>Deep Learning Series [Part5]: 다층 신경망 이론</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part4]: 배치 경사 하강법",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/batch_gradient_descent",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  배치 경사 하강법          1. 확률적 경사 하강법(Stochastic Gradient Descent)      2. 배치 경사 하강법(Batch Gradient Descent)                  배치 경사 하강법 수식 과정                          (1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다              (2) Error를 구한다              (3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다              (4) 가중치를 업데이트 한다                                          배치 경사 하강법1. 확률적 경사 하강법(Stochastic Gradient Descent)확률적 경사 하강법은 데이터 세트에서 무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산합니다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용합니다. 그렇기 때문에 굉장히 빠른 속도로 가중치를 업데이트 할 수 있게 됩니다. 하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌이 납니다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것입니다. 그래서 느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법입니다.2. 배치 경사 하강법(Batch Gradient Descent)배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 64, 128개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용합니다. 다시 말해 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 64개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 겁니다.또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정을 겪지 않았습니다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능합니다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어입니다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 됩니다.확률적 경사 하강법과 배치 경사 하강법  배치 경사 하강법 수식 과정(1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다Forward propagation은 앞에서 했던 데이터와 가중치를 곱하고 합하는 과정들을 일컫는 말입니다.(2) Error를 구한다(3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다(4) 가중치를 업데이트 한다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/batch_gradient_descent'> <img src='/images/batch_1.png' alt='Deep Learning Series [Part4]: 배치 경사 하강법'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/batch_gradient_descent'>Deep Learning Series [Part4]: 배치 경사 하강법</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-itertools",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/python-component-itertools'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-itertools'>Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part3]: 분류를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/classification",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents  분류(Logistic regression)          1. 로지스틱 회귀(Logistic regression)      2. 활성화 함수: 시그모이드(Sigmoid)      3. 손실 함수: 크로스 엔트로피(Cross entropy)      4. 가중치 업데이트      분류(Logistic regression)1. 로지스틱 회귀(Logistic regression)앞에서 연속적인 값을 예측하는 모델에 대해 공부했습니다. 이번 포스트에서는 고양이, 개를 분류하는 이진 분류, 2가지 이상을 분류하는 다중 분류에 대해 공부해 보겠습니다. 그럼 회귀가 아니라 분류라고 해야 하지 않는가? 왜 로지스틱 회귀라고 할까? 여기서부터는 저의 생각이니 편하게 보고 그냥 넘기셔도 됩니다. 우선 로지스틱 회귀는 앞에서 봤던 회귀 모델과 같이 연속적인 값을 예측합니다. 다만 뒤에 시그모이드 함수와 합성함수를 취해주게 됩니다. 그럼 결과는 어떨까요? 시그모이드 함수의 결과는 0과 1사이의 실수값입니다. 그렇기 때문에 여전히 연속적인 값을 예측하는 것이라는 점에서 회귀라고 할 수 있는 것이지요. 다만 0과 1 사이의 값을 리턴하니까 예를 들면, 다음 사진이 고양이인지 아닌지에 대한 확률로 이용할 수 있을 것 같다는 생각이 듭니다. 그렇기 때문에 로지스틱 회귀라는 회귀 모델이지만 분류에 사용되는 것 같습니다.로지스틱 회귀는 0과 1사이의 실수값을 리턴해주므로 회귀다0과 1사이의 값을 분류를 위한 확률로 사용될 수 있기에 분류 모델에 사용된다.고양이, 개 사진 분류하기2. 활성화 함수: 시그모이드(Sigmoid)여기서부터 인공지능에서 중요한 개념 중 하나인 활성화 함수에 대해 알아보겠습니다. 활성화 함수의 종류는 다양합니다. 그 중 여기서는 Sigmoid 함수에 대해 알아보겠습니다. 그 밖에도 대표적으로 ReLU(Rectified Linear Unit) 함수가 있으면 ReLU는 최근 딥러닝에 많이 사용되는 대표적인 활성화 함수입니다.Sigmoid 함수는 0과 1사이의 연속적인 값을 리턴하기 때문에 확률로 사용하기 적합합니다. 그래서 이진 분류의 출력층에 활성화 함수로 많이 사용됩니다. ReLU 함수는 값이 0또는 입력값(x)이기 때문에 미분 계산 다른 활성화 함수보다 훨씬 간단합니다. 그러한 이유로 은닉층에 많이 이용되고 있습니다.보통 각 층에 있는 노드는 서로 같은 활성화 함수를 사용합니다. 밑에 그림은 입력층과 은닉층에는 ReLU함수를 사용했고, 출력층에는 이진 분류를 위해 Sigmoid함수를 사용하였습니다.은닉층에 활성화 함수가 없다면 층을 깊게 쌓아도 결국 가중치와 데이터의 곱과 합의 형태를 갖는 하나의 층에 불과하기 때문에 각 은닉층에는 ReLU와 같은 활성화 함수를 사용해야 층을 깊게 쌓는 의미가 있게 됩니다.활성화 함수는 각 층마다 활성화 함수를 가지고 있다.만약 각 층 간에 활성화 함수가 없다면 층이 깊어져도 선형 함수이기 때문에 층이 깊어져도 의미가 없다.보통 각 은닉층에는 ReLU함수가 사용되고, 이진 분류를 위해 출력층에는 Sigmoid 함수가 사용된다.다중 분류에는 출력층으로 Softmax 함수가 사용된다.3. 손실 함수: 크로스 엔트로피(Cross entropy)앞에서 최종적으로 모델의 예측 값을 얻었습니다. 이제 우리가 얻은 값과 실제 값 사이를 비교해 최적화를 하기 위해 손실 함수를 정의 해야 합니다.이 전 회귀 모델에서는 MSE를 사용했습니다. 분류를 위한 손실 함수로는 어떤 것을 선택하는 것이 좋을까요? 그대로 MSE를 쓴다면 어떨지 먼저 생각해봅시다.선형 회귀는 정답과 예상값의 오차 제곱이 최소가 되는 가중치를 찾는 것이 목표였습니다. 그렇다면 분류의 목표는 무엇일까요? 올바르게 분류된 데이터의 비율을 높이는 것이 분류의 목표입니다. 하지만 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용할 수가 없습니다. 대신 비슷한 역할을 하는 함수가 있습니다. 바로 그 함수가 로지스틱 손실 함수입니다.이진 분류를 위한 크로스 엔트로피 함수다중 분류를 위한 크로스 엔트로피 함수정답이 0인 경우 우리의 출력값이 1에 가까워지면 손실함수가 증가하게 됩니다. 반면에 정답이 1인 경우에는 출력값이 0에 가까워 질수록 손실함수가 증가합니다.4. 가중치 업데이트선형 회귀 모델에서는 가중치를 업데이트 할 때 손실함수를 바로 가중치에 대해 미분할 수 있었다. 하지만 실제로 은닉층이 생기고 활성화 함수가 추가되면 더 이상 바로 미분할 수가 없다. 그래디언트를 구하기 위해서는 우선 활성화 함수에 대해 편미분을 해야한다. 이렇게 합성함수를 순서대로 편미분해 곱한 것을 Chain rule이라고 한다.과정은 선형회귀보다 조금 복잡하지만 결과는 같게 나왔다.크로스 엔트로피 손실함수를 활성화 함수를 고려해 그래디언트를 구한 결과 여전히 err*x 이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/classification'> <img src='/images/dog_cat.png' alt='Deep Learning Series [Part3]: 분류를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/classification'>Deep Learning Series [Part3]: 분류를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-collections",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/python-component-collections'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-collections'>Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/linear_regression",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  선형 회귀          1. 선형 회귀(Linear regression)      2. 손실함수: MSE(Mean Squared Error)      3. 가중치 업데이트      선형 회귀1. 선형 회귀(Linear regression)딥러닝을 배울 때 출발점으로 좋은 모델이 바로 선형 회귀입니다. 왜냐하면 뒤에서 배우게 될 모델들은 보통 이러한 선형 회귀에서 약간 변형하고, 추가하는 방법을 통해 만들어 지기 때문입니다.선형 회귀에서선형: 모델이 가중치와 데이터의 곱과 합으로 구성되어 있다. 이러한 경우를 데이터의 선형 조합(linear combination)이라고 한다.회귀: 어떤 연속적인 값을 예측하는 것을 회귀 라고 한다.선형 회귀 모델2. 손실함수: MSE(Mean Squared Error)우리의 모델의 예측 값(y’)이 정답(y)에 가깝도록 하는 것이 목표일 때 손실함수를 어떻게 정의하면 좋을까? 바로 머릿 속에 떠오르는 방법은 둘 간의 오차로 정의하는 것입니다. 이것을 우리는 Error라고 한다. 근데 Error 값은 양수일 수도 있고, 음수일 수도 있습니다. 그 상태에서 Error들을 합하면 실제 우리가 생각하는 것보다 작을 것입니다.그렇기 때문에 항상 각각의 Error가 양수가 되도록 제곱을 취하도록 하겠습니다. 이것을 우리는 MSE라고 합니다. 아마 더 깊이 공부하다 보면 이런 MSE 손실함수로는 부족할 수 도 있다고 생각이 듭니다. 하지만 MSE는 처음에 인공지능을 시작할 때 사용하기 좋은 손실함수이기 때문에 여기서는 회귀 모델에서는 MSE를 손실함수로 사용한다 라고 까지만 하고 마치도록 하겠습니다.MSE를 통해 정의한 손실함수 L3. 가중치 업데이트가중치(w)를 업데이트해 우리의 모델을 최적화시켜보도록 하겠습니다. 앞에서 우리는 가중치를 업데이트 하는 방법으로 경사하강법(Gradient descent)를 사용한다고 했습니다. 그렇기 때문에 우선 각 가중치의 Gradient를 구해야합니다. 그리고 Gradient는 함수값을 가장 가파르게 증가하는 방향이므로 (-)를 취해 손실함수 값을 가장 빠르게 감소시키는 방향으로 가중치를 업데이트 하겠습니다.손실함수를 각각의 가중치에 대해 편미분한다.각각의 편미분에 (-)를 취해 해당 가중치 지점에서 가장 빨리 손실함수를 감소시키는 방향으로 가중치를 업데이트한다.(알파는 학습률이라는 파라미터인데 얼마나 크게/작게 가중치를 업데이트할 것인지 정하는 값이다.)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/linear_regression'> <img src='/images/mse.png' alt='Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linear_regression'>Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part1]: 딥러닝이 처음이라면",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/intro",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  딥러닝이 처음이라면          1. 미래를 바꿀 핵심 기술      2. 딥러닝이란 무엇인가?      3. 인공지능, 머신러닝, 딥러닝                  인공지능          머신러닝          딥러닝                    4. 딥러닝을 해보자                  학습          검증          결론                    딥러닝이 처음이라면1. 미래를 바꿀 핵심 기술그 동안 컴퓨터는 단순한 계산에서 복잡한 계산, 정보 전달을 가능하게 했고, 또 인간이 하던 단순한 작업 이를 테면 스팸 메일 분류와 같은 것들을 해왔습니다.그러나 최근 컴퓨터 하드웨어의 발달과, 축적된 데이터를 통해 사람들은 더 많은 시도를 해왔습니다. 그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다. 지금부터 이것들을 가능하게 한 딥러닝이 어떤 건지 하나씩 살펴 보도록 하겠습니다.2. 딥러닝이란 무엇인가?딥러닝이란 무엇일까요? 위키 백과에서는 다음과 같이 정의합니다.  딥 러닝은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계 학습 알고리즘의 집합으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다.좀 더 자세하고 전문적인 지식은 위키백과에서 찾아 보도록 하고 저는 이해하기 쉽도록 간단하게 정의해 보겠습니다.딥러닝은 특별한 규칙이나 문제를 푸는 방법을 컴퓨터에게 알려주지 않습니다.우리는 그저 복잡하고 설명하기 힘든 특성을 컴퓨터가 잘 잡아낼 수 있도록 신경망을 만듭니다. 그리고 그 신경망에 데이터를 제공함으로써 기계가 숨겨진 특성을 잘 찾아낼 수 있도록 잘 학습시킵니다. 이러한 과정이 바로 딥러닝입니다.3. 인공지능, 머신러닝, 딥러닝(출처: Samstory)인공지능인공지능은 인간이 뇌를 통해 하는 행동들을, 컴퓨터도 마치 생각하는 것처럼 유사하게 동작하는 것을 인공지능이라고 합니다. 머신러닝, 딥러닝도 결국 인공지능을 위한 하나의 방법인 겁니다.머신러닝머신러닝은 정해진 명령보다 데이터를 기반으로 예측이나 결정을 이끌어 내기 위해 특정한 알고리즘을 수행하는 모델을 구축하는 방식으로 모델을 구축함으로써 입력하지 않은 정보에 대해서다 판단이나 결정을 할 수 있게 됩니다.머신러닝 기법은 주로 정형 데이터를 다룹니다. 관계형 데이터베이스(Relational Database)나 엑셀 표로 정리되는 테이블 데이터를 생각하시면 되겠습니다. 의사결정에 필요한 데이터를 사람이 정리해 기계에 알려주면 기계는 이 정보를 토대로 판단이나 예측을 하는 경우입니다.딥러닝딥러닝은 머신러닝의 한 파트이지만 보다 조금 더 추상적인 모델입니다. 머신러닝과 같이 특정한 알고리즘을 수행하도록 모델을 만들기보다는, 사람이 잡아내기 힘든 추상적인 특성을 데이터에서 잘 추출하도록 단일 신경망을 그저 깊게 쌓는 방식으로 모델을 만듭니다.이러한 이유로 딥러닝은 주로 비정형 데이터를 다룹니다. 비정형 데이터란 지정된 방식으로 정리되지 않은 정보를 말합니다. 간단히 말하자면 이미지, 비디오, 텍스트 문장이나 문서, 음성 데이터 등을 말합니다. 비정형 데이터는 인간이 그 특성을 잡아내기 매우 힘든 데이터이기 때문에 딥러닝이 큰 힘을 발휘하게 됩니다.4. 딥러닝을 해보자이번 챕터에서 딥러닝의 전체적인 과정을 정말 간략히만 훑어보도록 하겠습니다. 딥러닝을 포함해 인공지능의 목표는 어떻게 보면 새로운 데이터에 대한 예측이라고 생각하면 될 것 같습니다. 예를 들어 부동산 가격 예측, 주가 예측, 승패 예측, 날씨 예측과 같은 것들이 있겠죠. 앞의 두 가지는 연속적인 값을 예측하므로 회귀(regression)라고 하고, 뒤의 두 가지는 분류(classification)라고 합니다. 그럼 인공지능 모델은 크게 다음과 같이 두 가지 목적에 따라 분류된다고 할 수 있습니다.  회귀(regression)모델: 데이터의 일반적인 경향을 가장 잘 나타내는 모델을 만드는 것이 목표  분류(classification)모델: 데이터를 일반적으로 가장 잘 분류하는 decision boundary를 찾는 것이 목표학습그럼 이제 우리의 목표는 데이터를 이용해 다음과 같은 모델을 만드는 것입니다. 어떻게 만들 수 있을까요? 이것은 마치 어렸을 때 수학 시험을 위해 문제가 가득한 문제집을 무수히 많이 푸는 과정과 비슷하다고 할 수 있습니다. 컴퓨터는 우리의 데이터를 기반으로 최소한의 오차를 내기 위해 계속 학습하게 됩니다. 드디어 우리의 모델이 무엇을 해야할지 목표가 생겼습니다.목표: 오차 함수를 최솟값으로 만드는 것입니다.목표가 생겼으니 이제 목표를 향해 어떻게 나아갈 지를 생각해봐야 합니다. 어떻게 나아가야 할까요? 우리의 컴퓨터가 목표(최솟값)가 어디 있는지 한 번에 알면 좋겠지만 그렇지는 않습니다. 마치 다음과 같습니다.  앞이 보이지 않습니다. (어디가 최솟값인지 알 수 없습니다)      힌트는 최솟값이 우리의 목표라는 것입니다.      깜깜한 상태에서 가장 밑으로 내려가기 위해서는 발을 더듬으며 내리막길 중 어디가 가장 가파른지를 찾을 것입니다. 그쪽으로 가야 가장 빨리 내려갈 수 있겠죠. 우리의 모델도 이와 비슷한 방법으로 학습을 시킬 수 있습니다. 이 방법을 경사 하강법(Gradient descent)이라고 합니다.방법: 경사하강법을 통해 학습할 것입니다.결론: 학습이란 경사하강법을 통해 오차가 최소가 되도록 하는 것입니다.💡 경사하강법이란 함수의 특정 지점에서의 그래디언트(가장 가파르게 증가하는 방향) 반대 방향으로 우리의 모델을 조금씩 수정해 나가는 것으로, 그래디언트 반대 방향인 이유는 그래디언트 가장 가파르게 증가하는 방향이기 때문에 부호(-)를 취해줌으로써 정확히 반대 방향으로 가면 오차함수를 가장 빨리 감소시키는 방향으로 모델을 수정할 수 있다.검증우리는 데이터를 이용해 경사하강법을 사용함으로써 오차함수를 최소로 하도록 모델을 수정하면 된다고 배웠습니다. 하지만 우리는 수능을 앞두고 있는 상황에서 항상 문제집만 풀지는 않습니다. 자칫 잘못하면 문제집에 나오는 문제들은 너무 완벽하게 공부했지만 수능에 출제될 문제와는 전혀 다를 수도 있습니다. 우리는 언제까지 문제집을 푸는게 도움이 되는지 판단을 할 수 있어야 합니다. 매번 문제집을 풀며 중간 중간 모의고사를 통해 모의고사 성적을 확인합니다. 그러다가 어느 순간 모의고사 성적이 오히려 나빠진다면 문제집을 더이상 풀면 안됩니다. (이를 과대적합(overfitting)이라고 합니다.)훈련마다 검증 데이터를 통해 언제 훈련을 멈출 지 정한다.검증 데이터에서 가장 좋은 성능을 갖는 모델을 우리의 모델로 선택한다.결론지금까지 살펴본 과정들이 딥러닝의 간략한 모습이라고 할 수 있습니다.1. 목적에 맞게 회귀 또는 분류 모델을 선택한다. 2. 그에 맞는 목적함수를 설정한다.  3. 경사하강법을 통해 학습한다.  4. 검증 데이터를 통해 언제 학습을 종료할지 결정한다.  5. 검증 데이터에서 가장 좋은 성능을 낸 모델을 선택한다.다음 포스트에서 부터 하나씩 자세하게 다뤄보도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/intro'> <img src='/images/deep_learning.png' alt='Deep Learning Series [Part1]: 딥러닝이 처음이라면'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/intro'>Deep Learning Series [Part1]: 딥러닝이 처음이라면</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series6",
      "date"     : "Mar 25, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-25T21:01:35+09:00'>25 Mar 2021</time><a class='article__image' href='/mysql-series6'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series6'>MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series5",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series5'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series5'>MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series4",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  테이블 다루기          테이블의 컬럼 구조 확인하기      컬럼 추가, 이름 변경      컬럼 삭제      컬럼 타입 변경      컬럼 속성 변경      테이블에 제약 사항 걸기      테이블의 제약 사항 삭제      컬럼 순서 앞으로 당기기      컬럼 순서 정하기      컬럼명 속성 동시에 바꾸기      테이블 복제하기      테이블 뼈대만 복제하기        외래키 설정하기          외래키 설정      외래키 정책      외래키 삭제      외래키 파악      이번 포스트에서는 테이블을 처음 구축할 때 필요한 설정을 하기 위한 SQL문에 대해 배워보겠습니다.테이블 다루기테이블의 컬럼 구조 확인하기DESCRIBE [테이블 이름]컬럼 추가, 이름 변경ALTER TABLE [테이블 이름] ADD [추가할 컬럼] CHAR(10) NULL;ALTER TABLE [테이블 이름]RENAME COLUMN [원래 컬럼명] TO [바꿀 컬럼명];컬럼 삭제ALTER TABLE [테이블 이름]DROP COLUMN [삭제할 컬럼명];컬럼 타입 변경ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT;컬럼 속성 변경# NOT NULL 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL;# DEFAULT 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL DEFAULT [주고 싶은 default값];# DATETIME, TIMESTAMP 타입에 줄 수 있는 특별한 속성# DEFAULT CURRENT_TIMESTAMP: 값 입력 안되면 default로 현재 시간 입력ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP;# 처음 default로 현재 시간 넣어주고, 데이터 갱신될 때 마다 갱신된 시간 넣어줌  ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;# UNIQUE 속성# UNIQUE는 PRIMARY KEY와 다르게 NULL 허용ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT UNIQUE;테이블에 제약 사항 걸기ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍] CHECK [제약 사항(ex. age &amp;lt; 100)];테이블의 제약 사항 삭제ALTER TABLE [테이블 이름]DROP CONSTRAINT [제약 사항 이름];컬럼 순서 앞으로 당기기ALTER TABLE [테이블 이름]MODIFY [컬럼명] INT FIRST;컬럼 순서 정하기ALTER TABLE [테이블 이름]MODIFY [뒤에 올 컬럼명] INT AFTER [앞에 있는 컬럼명];컬럼명 속성 동시에 바꾸기ALTER TALBE [테이블 이름]CHANGE [원래 컬럼명] [바꿀 컬럼명] VARCHAR(10) NOT NULL;테이블 복제하기CREATE TABLE [복제한 테이블의 이름]AS SELECT * FROM [원본 테이블의 이름]테이블 뼈대만 복제하기CREATE TABLE [복제한 테이블의 이름]LIKE [원본 테이블의 이름]외래키 설정하기외래키(Foreign Key)란 한 테이블의 컬럼 중에서 다른 테이블의 특정 컬럼을 식별할 수 있는 컬럼을 말합니다. 그리고 외래키에 의해 참조당하는 테이블을 부모 테이블(parent table), 참조당하는 테이블(referenced table)이라고 합니다. 외래키를 이용하면 테이블간의 참조 무결성을 지킬 수 있습니다. 참조 무결성이란 아래 그림과 같이 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미합니다.예를 들어, 강의 평가인 review 테이블에는 ‘컴퓨터 개론’에 관한 평가 데이터가 남아있지만, 강의 목록을 나타내는 course 테이블에는 ‘컴퓨터 개론’ 과목이 삭제된다면 이상한 상황이 벌어질 것입니다. 이 때 외래키를 통해 지정해 놓으면 이런 상황을 해결할 수 있습니다.이렇게 외래키는 두 개 이상의 테이블에서 중요한 역할을 하기 때문에 외래키 속성을 어떻게 설정하는지는 굉장히 중요한 문제입니다.외래키 설정ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍]    FOREIGN KEY (자식테이블의 컬럼)    REFERENCES 부모테이블 (부모테이블의 컬럼)    ON DELETE [DELETE정책]    ON UPDATE [UPDATE정책];외래키 정책  RESTRICT: 자식 테이블에서 삭제/갱신해야만 부모 테이블에서도 삭제/갱신 가능  CASCADE: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터도 같이 삭제/갱신  SET NULL: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터의 컬럼에 NULL 지정외래키 삭제ALTER TABLE [테이블 이름]DROP FOREIGN KEY [제약 사항이 걸린 테이블];외래키 파악SELECT    i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME,    k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAMEFROM information_schema.TABLE_CONSTRAINTS iLEFT JOIN information_schema.KEY_COLUMN_USAGE kUSING(CONSTRAINT_NAME)WHERE i.CONSTRAINT_TYPE = &#39;FOREIGN KEY&#39;;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series4'> <img src='/images/sql_2.png' alt='MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series4'>MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 파이썬의 네임스페이스",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-namespace",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents파이썬 식별자, 스코프, 네임스페이스",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-namespace'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 파이썬의 네임스페이스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-namespace'>Python Basic Series [Part6]: 파이썬의 네임스페이스</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-file_directory",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-component-file_directory'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-file_directory'>Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series3",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents  테이블 생성 및 삭제          SQL문 데이터 타입        데이터 CRUD          데이터 추가      데이터 갱신      데이터 삭제      앞에서 저희가 배웠던 SQL문들은 모두 이미 테이블이 주어졌고 그 테이블에 데이터가 쌓여있는 상태에서 원하는 데이터를 조회하는 방법에 관한 것들이었습니다.하지만 저희가 직접 테이블을 생성하고 데이터를 쌓아야 하는 순간도 있을 것입니다. 이번 포스트에서는 이러한 순간에 필요한 SQL문에 대해 배워 보겠습니다.테이블 생성 및 삭제# 데이터베이스 생성CREATE DATABASE [생성할 데이터베이스 이름]CREATE DATABASE IF NOT EXISTS [생성할 데이터베이스 이름]# 데이터베이스 지정USE [생성한 데이터베이스 이름]# 테이블 생성CREATE TABLE [데이터베이스 이름].[생성할 테이블 이름] (    [컬럼1] INT NOT NULL AUTO_INCREMENT PRIMARY KEY,    [컬럼2] VARCHAR(20) NULL,    [컬럼3] VARCHAR(15) NULL,    또는 PRIMARY KEY ([&#39;컬럼1&#39;]));# 테이블 삭제DROP TABLE [테이블 이름]SQL문 데이터 타입            종류      타입              정수형      TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT              실수형      DECIMAL, FLOAT, DOUBLE              날짜 및 시간      DATE, TIME, DATETIME, TIMESTAMP              문자열      CHAR, VARCHAR, TEXT            TINYINTsigned: -128 ~ 127unsigned: 0 ~ 255        INTsigned: -2147483648 ~ 2147483647unsigned: 0 ~ 4294967295        DECIMALDECIMAL(M, D): M은 전체 숫자의 최대 자리수, D는 소수점 자리 숫자의 최대 자리수DECIMAL(5, 2): -999.99 ~ 999.99M은 최대 65까지 가능, D는 최대 30까지 가능        FLOAT-3.4 * 10^38 ~ 3.4 * 10^38        DOUBLE-1.7 * 10^308 ~ 1.7 * 10^308FLOAT와 비교해 범위도 더 넓고, 정밀도 또한 더 높음(더 많은 소수점 자리 수 지원)        DATE날짜를 저장하는 데이터 타입’2021-03-21’ 이런 형식의 연, 월, 일 순        TIME시간을 저장하는 데이터 타입’09:27:31’ 이런 형식의 시, 분, 초        DATETIME날짜와 시간을 저장하는 데이터 타입’2021-03-21 09:30:27’ 이런 식으로 연, 월, 일, 시, 분, 초        TIMESTAMPDATETIME과 같다차이점은 TIMESTAMP는 타임 존 정보도 포함        CHARCHAR(30): 최대 30자의 문자열을 저장 (0~255까지 가능)차지하는 용량이 항상 숫자값에 고정됨데이터의 길이가 크게 변하지 않는 상황에 적합        VARCHARVARCHAR(30): 최대 30자의 문자열을 저장 (0~65536까지 가능)차지하는 용량이 가변적. 30이어도 그 이하의 길이면 용량도 적게 차지함해당 값의 사이즈를 나타내는 부분(1byte 또는 2byte)이 저장 용량에 추가데이터 길이가 크게 들쑥날쑥해지는 경우에 적합        TEXT문자열이 아주 긴 상황에 적합  데이터 CRUD데이터 추가# 데이터 추가INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼2, 컬럼3, ...)VALUES (컬럼1의 데이터, 컬럼2의 데이터, 컬럼3의 데이터, ...);# 특정 컬럼에만 데이터 넣을 수도 있다INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼3)VALUES (컬럼1의 데이터, 컬럼3의 데이터);데이터 갱신# 데이터 갱신UPDATE [사용할 테이블 이름]    SET 컬럼1 = [갱신 데이터] WHERE [조건]; # 기존 값을 기준으로 갱신UPDATE [사용할 테이블 이름]SET 컬럼1 = [컬럼1 + 3] WHERE [조건]; 데이터 삭제DELETE FROM [사용할 테이블 이름]WHERE [조건]",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/mysql-series3'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series3'>MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series2",
      "date"     : "Mar 14, 2021",
      "content"  : "Table of Contents  조인          서로 구조가 다른 테이블간의 조인                  LEFT OUTER JOIN          RIGHT OUTER JOIN          INNER JOIN                    구조가 같은 테이블간의 조인                  UNION          UNION ALL          INTERSECT          MINUS                      서브쿼리          뷰        마치며조인여러 테이블을 합쳐서 하나의 테이블인 것처럼 보는 행위를 ‘조인(join)’이라고 합니다. 실무에서는 이 조인을 잘해야 제대로된 데이터 분석을 할 수 있습니다. 조인은 SQL을 얼마나 잘 쓰는지 판단하는 척도 중 하나일만큼 정말 중요한 개념입니다.서로 구조가 다른 테이블간의 조인  서로 구조가 다른 테이블을 특정 컬럼을 기준으로 조인SELECT     p.name    p.team    r.team    r.regionFROM player AS p LEFT OUTER JOIN region AS rON p.team = r.team # ON 대신 USING(team) 이렇게 할 수도 있음LEFT OUTER JOINRIGHT OUTER JOININNER JOIN구조가 같은 테이블간의 조인UNION  중복을 허용하지 않는 합집합SELECT * FROM old_playerUNIONSELECT * FROM new_player;UNION ALL  중복을 허용하는 합집합INTERSECT  교집합  MySQL에서는 지원 하지 않음  INNER JOIN으로 해결MINUS  차집합  MySQL에서는 지원 하지 않음  LEFT/RIGHT OUTER JOIN으로 해결서브쿼리  SELECT문의 결과로 나온 값/열/테이블을 적재적소에 맞게 다른 SELECT문의 입력으로 사용할 수 있습니다뷰  때에 따라 서브쿼리가 이중 중첩, 삼중 중첩되는 경우도 있습니다.  이 때 생기는 SELECT문의 복잡성을 줄이고자 뷰를 사용할 수 있습니다.  특정 역할을 하는 SELECT문들을 뷰로 저장해둡니다.  코드 스니펫처럼 필요할 때마다 가져와서 사용할 수 있습니다.  백엔드 개발자들의 자산과도 같습니다마치며지쳐서 너무 대충해버렸다…생각날 때마다 조금씩 보충해야겠다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-14T21:01:35+09:00'>14 Mar 2021</time><a class='article__image' href='/mysql-series2'> <img src='/images/sql_5.png' alt='MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series2'>MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series1",
      "date"     : "Mar 7, 2021",
      "content"  : "Table of Contents  DBMS          Database      DBMS      SQL      MySQL      DBMS의 종류      DBMS의 구조        SELECT문          SELECT      FROM      WHERE      ORDER BY      LIMIT      GROUP BY      HAVING      SELECT문의 작성순서와 실행순서      SQL에서 제공하는 함수      NULL 데이터 다루는 방법        마치며DBMS빅 데이터 시대에서 데이터 저장은 가장 중요한 부분 중 하나입니다. 힘들게 얻은 데이터를 저장하지 않는다면 큰 자원 낭비겠죠. 하지만 중요한 것은 단순히 저장에 그치는 것이 아니라, 어떤 식으로 저장해 그 후 데이터를 추가, 갱신, 삭제 할 때 문제(NULL, 중복 등)가 생기지 않도록 할 것인지에 대한 고민도 이루어져야 한다는 것 입니다. 이번 MySQL 시리즈에서 이런 문제들을 어떻게 해결할 것인지에 대해 공부해보도록 하겠습니다.Database데이터베이스는 데이터의 집합 또는 데이터 저장소라고 정의할 수 있습니다.DBMS데이터베이스를 보통 직접적으로 접근하지는 않습니다. 사용자들이 데이터베이스를 그냥 접근한다면 데이터의 일관성도 떨어질 것이고, 관리도 쉽지 않을 것 입니다. 이러한 이유로 데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS(DataBase Management System)이라고 합니다.DBMS는 데이터베이스를 구축하는 틀을 제공하고, 효율적으로 데이터를 검색하고 저장하는 기능을 제공합니다. 또한 응용 프로그램들이 데이터베이스에 접근할 수 있는 인터페이스를 제공하고, 장애에 대한 복구 기능, 사용자 권한에 따른 보안성 유지 기능 등을 제공합니다.SQLDBMS를 이용해 데이터베이스를 사용하게 됩니다. 그렇다면 저희는 DBMS와 소통하는 방법을 알아야 합니다. 여기서 DBMS와 소통하기 위한 언어를 SQL(Structured Query Language)라고 합니다. SQL을 이용하면 데이터베이스 조작에 필요한 모든 명령어를 DBMS에 전달함으로써 수행할 수 있습니다.MySQL처음 SQL이라는 언어는 IBM이라고 하는 회사에서 System/R이라는 DBMS와, 이것을 사용하기 위해 필요한 언어인 SEQUEL을 만들면서 처음 등장했습니다. 그런데 SEQUEL(Structured English Query Language)은 그 단어가 이미 다른 곳에서 사용되고 있다는 문제(상표권 문제) 때문에 그 이름이 SQL(Structured Query Language)로 변경되었습니다. 그러다가 1987년, 국제 표준화 기구(ISO)에서 SQL에 관한 국제 표준(ISO 9075:1987)이 제정되었습니다.하지만 우리가 실제로 사용하는 SQL은 이 국제 표준에 완벽히 부합하지는 않습니다. Oracle, Microsoft SQL Server, MySQL 등의 DBMS에서 지원되는 SQL이 표준을 완벽히 준수하지는 않는다는 뜻입니다. 그 이유는 다양하지만 일단 많은 DBMS 회사들이 성능 향상과 더 다양한 기능 제공을 위해서, 차라리 표준을 일부 벗어나는 것을 택하는 경우가 많기 때문입니다.MySQL은 가장 처음 MySQL AB라고 하는 스웨덴 회사에서 개발되었습니다. 현재는 인수 과정을 거쳐 Oracle의 소유입니다. 이로 인해 지금 Oracle에는 Oracle(회사명과 같은 DBMS)과 MySQL이라는 서비스를 함께 제공하고 있습니다.두 DBMS의 시장에서의 쓰임새를 보면 약간의 차이가 있습니다. 은행, 거래소 등과 같이 데이터 처리의 정확성, 운영의 안정성 등이 엄격하게 요구되는 분야에서는 오라클이 주로 사용되고 있고, 우리가 흔히 쓰는 앱, 웹 사이트 같은 서비스를 만들 때는 MySQL을 쓰는 경우가 많습니다.DBMS의 종류위와 같이 많은 회사에서 성능 향상과 목적에 맞게 SQL이라는 언어를 조금씩 변형, 개선하여 새로운 DBMS로 개발해왔습니다. 이러한 이유로 MySQL과 같이 ~SQL이라는 용어도 사실상은 그 언어를 지원하는 DBMS 자체를 의미하게 되었습니다. 그래서 약간 헷갈리지만 관계형 데이터를 위한 DBMS의 경우 RDBMS, 비 관계형 데이터를 위한 DBMS의 경우 NoSQL이라고 하게 되었습니다.RDBMS: MySQL, Oracle, MariaDB(MySQL 개발자들이 만든 오픈소스), SQLite 등NoSQL: MongoDB, ElasticSearch, Cassandra 등DBMS의 구조  client(클라이언트 프로그램): 유저의 데이터베이스 관련 작업을 위해, SQL을 입력할 수 있는 화면 등을 제공하는 프로그램  server(서버 프로그램): client로부터 SQL 문 등을 전달받아 데이터베이스 관련 작업을 직접 처리하는 프로그램MySQL에서 서버 프로그램의 이름은 mysqld, 클라이언트 프로그램 이름은 mysql입니다. mysql은 보통 CLI 환경에서 사용하는 프로그램입니다. CLI 환경이 아니라 GUI 환경에서 mysql을 사용하려면 mysql을 GUI 환경에서 사용할 수 있도록 해주는 프로그램을 사용하면 됩니다. 대표적으로 Oracle이 공식적으로 제공하는 MySQL Workbench라는 프로그램이 있습니다.SELECT문MySQL에서 데이터를 조회하거나 분석할 때 필요한 SELECT문에 대해서 간단히 정리해 보겠습니다.SELECT  특정 컬럼이나 컬럼의 연산 결과를 지정SELECT *SELECT addressSELECT height / weightSELECT MAX(age)SELECT MAX(age) AS max_ageSELECT     (CASE         WHEN age IS NOT NULL THEN age         ELSE &quot;N/A&quot;     END) AS ageFROM  기준이 되는 테이블 지정FROM customersFROM orders# 예시SELECT name FROM customers;WHERE  컬럼에 조건을 지정WHERE age = 20WHERE gender != &#39;m&#39;WHERE age &amp;gt;= 27WHERE age NOT BETWEEN 20 AND 30 # 20~30WHERE age IN (20, 30) # 20 or 30WHERE address LIKE &#39;서울%&#39;WHERE address LIKE &#39;%고양시%&#39;WHERE address LIKE BINARY &#39;%Kim%&#39; # Kim 매칭, kim 매칭 xWHERE email LIKE &#39;__@%&#39; # _는 임의의 문자 1개# 예시SELECT * FROM customers WHERE age &amp;gt; 25;ORDER BY  정렬 기준을 지정ORDER BY height ASCORDER BY height DESC# 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASC;LIMIT  보고자 하는 결과의 개수를 지정LIMIT 5 # 5개LIMIT 10, 5 # 10번째부터 5개# 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASCLIMIT 3;GROUP BY  특정 컬럼의 값을 기준으로 그루핑  그루핑 하고나면 모든 함수연산 또한 그룹 단위로 실행GROUP BY genderGROUP BY countryGROUP BY country, genderGROUP BY SUBSTRING(address, 1, 2)# 예시SELECT genderFROM customersGROUP BY gender;SELECT gender, MAX(age)FROM customersGROUP BY gender;GROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUP# 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;;SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*)FROM memberGROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUPHAVING region IS NOT NULLORDER BY region ASC, gender DESC;HAVING  그루핑된 결과에 조건을 지정HAVING region = &#39;서울&#39;# 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;🦊 WHERE과 HAVING의 차이점WHERE: 주어진 테이블의 전체 row에서 필터링을 하는 용도HAVING: GROUP BY 되고 난 후 row에서 필터링 하는 용도SELECT문의 작성순서와 실행순서작성 순서SELECT FROMWHEREGROUP BYHAVING ORDER BYLIMIT 실행 순서FROMWHERE GROUP BYHAVING SELECTORDER BYLIMIT SQL에서 제공하는 함수# 모든 데이터 타입COUNT(*)DISTINCT(gender)# 문자열 데이터 타입SUBSTRING(address, 1, 2) # address의 첫번째 문자에서 2개LENGTH(address)UPPER(address)LOWER(address)LPAD(address)RPAD(address)# 숫자 데이터 타입# 집계(aggregation) 함수MAX(height)MIN(weight)AVG(weight)# 산술(mathematical) 함수ABS(balance)CEIL(height)FLOOR(height)ROUND(height)# 날짜 및 시간 데이터 타입YEAR(birthday)MONTH(birthday)DAYOFMONTH(birthday)DATEDIFF(birthday, &#39;2002-01-01&#39;)# 예시SELECT * FROM customers WHERE MONTH(birthday) IN (4, 5, 6);NULL 데이터 다루는 방법WHERE address IS NULLWHERE address IS NOT NULL# COALESCE(a, b, c) 함수는 a, b, c 중 가장 먼저 NULL아닌 값 리턴COALESCE(height, &quot;키 정보 없음&quot;)COALESCE(height, weight * 2.5, &quot;키 정보 없음&quot;)# IFNULL(a, b) 함수는 a가 NULL 아니면 a, NULL이면 b 리턴IFNULL(height, &quot;키 정보 없음&quot;)# IF(condition, a, b) 함수는 condition이 True이면 a, False이면 b리턴IF(address IS NOT NULL, address, &quot;N/A&quot;)# CASE 함수CASE    WHEN address IS NOT NULL THEN address    ELSE N/AEND# 예시SELECT addressFROM customersWHERE address IS NOT NULL;SELECT COALESCE(height, &quot;키 정보 없음&quot;), COALESCE(gender, &quot;성별 정보 없음&quot;)FROM customers;SELECT IF(address IS NOT NULL, address, &quot;N/A&quot;)FROM customers;SELECT    CASE        WHEN address IS NOT NULL THEN address        ELSE N/A    ENDFROM customers;마치며여기까지 테이블 한 개에 대해서 데이터를 조회하고 분석하는 방법에 대해 살펴보았습니다. 다음 포스트에서는 테이블이 여러 개인 경우에 대해 데이터를 조회하고 분석할 때 필요한 문법에 대해 알아보겠습니다.MySQL 실습 제공 사이트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-07T21:01:35+09:00'>07 Mar 2021</time><a class='article__image' href='/mysql-series1'> <img src='/images/mysql_1.png' alt='MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series1'>MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리</a> </h2><p class='article__excerpt'>데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS(DataBase Management System)이라고 합니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-dictionary",
      "date"     : "Apr 23, 2020",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-dictionary'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-dictionary'>Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part3]: 파이썬 리스트 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-list",
      "date"     : "Apr 23, 2020",
      "content"  : "Table of Contents  1. 리스트 자료형의 특징  2. 리스트 생성  3. 인덱싱, 슬라이싱  4. 리스트 메소드          4-1 .append(), .extend(), .insert(), .copy()      4-2 .pop(), .remove(), .clear()      4-3 .sort(), .reverse()      4-4 .count(), .index()        5. 리스트에서 주목할 만한 것들          5-1 List &amp;amp; Range      5-2 리스트 표현식 (List comprehension)      5-3 리스트와 문자열 넘나들기      5-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)      1. 리스트 자료형의 특징문자열 자료형은 순서가 있는 시퀀스형 자료형입니다. 그렇기 때문에 인덱싱, 슬라이싱을 통해 데이터의 일부분을 추출할 수 있습니다.데이터 관련 분야에서 일을 하다 보면 리스트 자료형 데이터를 목적에 맞게 다듬거나 또는 다른 자료형으로 변환해 사용하는 일이 많기 때문에 자료형 중에서도 비중이 높은 편입니다.2. 리스트 생성리스트는 여러 가지 자료형을 가질 수 있는 시퀀스형 자료형입니다.또한 값을 변경할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = [3.1, 2.5, 7]&amp;gt;&amp;gt;&amp;gt; c = [&quot;Hello&quot;, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; d = [1, 4.5, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; a[0] = 100&amp;gt;&amp;gt;&amp;gt;a[100, 2, 3, 4]🔔 리스트를 곱하거나 더하면 값이 반복되거나 추가됩니다&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a + [5][1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a + 5 -&amp;gt; Error&amp;gt;&amp;gt;&amp;gt; a * 2[1, 2, 3, 4, 1, 2, 3, 4]3. 인덱싱, 슬라이싱이번에는 위에서 만들어진 리스트 데이터를 가지고 원하는 부분만 가져올 수 있도록 해주는 인덱싱, 슬라이싱에 대해 알아보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&quot;&quot;&quot;인덱싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0]1&amp;gt;&amp;gt;&amp;gt; a[2]3&amp;gt;&amp;gt;&amp;gt; a[4] = 10 -&amp;gt; Error  (a[50] = 10 이런식으로 하면 그 사이의 인덱스에 값을 표시할 수 없어서 무조건 차례대로 값을 채워넣어야 함 -&amp;gt; 더하기 또는 append 메소드)&quot;&quot;&quot;슬라이싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0:3] # 0에서 부터 5앞까지 -&amp;gt; 인덱스 0~4[1, 2, 3]&amp;gt;&amp;gt;&amp;gt; a[:][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::-1] #처음부터 끝까지 거꾸로 슬라이싱 (중요)[4, 3, 2, 1]4. 리스트 메소드리스트 데이터는 프로그래밍을 하다보면 정말 자주 만나게 되는 자료형 중에 하나입니다.그렇기 때문에 문자열 객체의 메소드를 잘 활용할 줄 아는 것이 굉장히 중요합니다.먼저 어떤 메소드가 있는지 확인해 보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__add__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;,  &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__imul__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;,   &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;,    &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__rmul__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;,    &#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;]실제로 코딩을 하실 때는 기억이 안나면 그 때마다 dir() 함수를 사용해 어떤게 있는지 살펴보면 됩니다.4-1 .append(), .extend(), .insert(), .copy().append()리스트 맨 끝에 인자로 넣어준 값 하나를 추가해준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.append(100)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100]여러 개를 추가하고 싶어서 인자로 값을 여러 개 준다면? -&amp;gt; 에러가 난다그래서 [100, 101, 102] 이런 식으로 추가하면? -&amp;gt; 에러는 안나지만 리스트가 추가되어 원하는 모습과는 다르다..extend()iterable한 객체를 인자로 넣어주면 그 안의 원소들이 모두 차례대로 리스트에 추가된다.&amp;gt;&amp;gt;&amp;gt; a.extend([101, 102, 103]) # 리스트와 같은 iterable한 객체를 인자로 주어야 한다.&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100, 101, 102, 103]이제는 맨 뒤가 아니라 원하는 인덱스에 값을 추가(교체)하고 싶다..insert()인자로 인덱스와 값을 넣어주면 인덱스에 값을 넣어준다.인덱스에 이미 값이 있으면 바꿔주고 리스트 길이보다 인덱스 값이 크거나 같으면 리스트 맨 뒤에 값을 넣어준다.-&amp;gt; 길이 신경쓰지 않고 해줘도 오류는 안난다. (내가 원하는 인덱스에 값이 들어가지 않을 수도 있지만)&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1, 10)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1000, 7)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4, 7].copy()객체와 똑같은 값을 가지는 리스트를 복사한다. 변수를 지정해주면 새로운 메모리에 저장된다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = a.copy()&amp;gt;&amp;gt;&amp;gt; b.append(5)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b[1, 2, 3, 4, 5]4-2 .pop(), .remove(), .clear().pop()리스트의 가장 끝에 있는 원소를 뽑아 리턴해준다.&amp;gt;&amp;gt;&amp;gt; a = [&#39;banana&#39;, &#39;lemon&#39;, &#39;apple&#39;]&amp;gt;&amp;gt;&amp;gt; a.pop()&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;banana&#39;, &#39;lemon&#39;].remove()인자로 받은 값은 값을 제거해준다.&amp;gt;&amp;gt;&amp;gt; a.remove(2)&amp;gt;&amp;gt;&amp;gt; a[1, 3, 4].clear()리스트를 싹 비운다.&amp;gt;&amp;gt;&amp;gt; a.clear()&amp;gt;&amp;gt;&amp;gt; a[]4-3 .sort(), .reverse().sort()  리스트를 작은 값부터 순서대로 정렬해준다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a = [&#39;안녕&#39;, &#39;Hello&#39;, &#39;Hi&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a.sort(key=len)&amp;gt;&amp;gt;&amp;gt; a[&#39;안녕&#39;, &#39;Hi&#39;, &#39;Hello&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: x**2)&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]# 같은 제곱값에 대해 양수가 먼저 나오게 하려면 양수가 논리연산 시 False가 되면 되므로 기준을 0보다 작은지로 하면 된다 &amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: (x**2, x&amp;lt;=0))&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]&amp;gt;&amp;gt;&amp;gt; a =[False, True, False, True, True, False]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[False, False, False, True, True, True]🔔 sorted() 함수sorted() 함수는 정렬된 값을 리턴해줄 뿐 인자로 받은 리스트를 정렬하지는 않는다.또 한가지 중요한 특징은 sorted()함수는 리스트 뿐 아니라 모든 iterable한 값들을 정렬시켜 준다는 것입니다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted(a)[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a[3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted({1: &#39;D&#39;, 2: &#39;B&#39;, 3: &#39;B&#39;, 4: &#39;E&#39;, 5: &#39;A&#39;})[1, 2, 3, 4, 5]  🔔 .sort()와 sorted() 모두 key, reverse 인자를 갖는다  key: 정렬을 목적으로 하는 함수를 값으로 넣는다. lambda를 이용할 수 있다. key 매개 변수의 값은 단일 인자를 취하고 정렬 목적으로 사용할 키를 반환하는 함수(또는 다른 콜러블)여야 합니다.  reverse: bool값을 넣는다. 기본값은 reverse=False(오름차순)이다..reverse()리스트의 원소의 순서를 뒤집어준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.reverse()&amp;gt;&amp;gt;&amp;gt; a[4, 3, 2, 1]🔔 reversed() 함수뒤집은 값을 리턴해줄 뿐 인자로 받은 리스트는 그대로다.🔔 문자열을 뒤집는 방법a.reverse()a = list(reversed(a))a = a[::-1]4-4 .count(), .index().count()인자로 받은 값이 등장하는 횟수를 리턴해준다. (코드 생략).index()인자로 받은 값의 인덱스를 리턴해준다. (코드 생략)5. 리스트에서 주목할 만한 것들5-1 List &amp;amp; Range1부터 1000까지 값을 하나씩 출력하는 코드를 짠다고 할 때for i in [1, 2, 3, 4, 5, 6, 7, 8, ..., 1000]:  print(i)로 하게 되면 위의 코드를 실행하기 위해 1000개의 요소를 적어서 리스트를 만드는 것은 너무 비효율적입니다.이를 개선시키는 방법으로for i in range(1000):  print(i)이렇게 해주면 훨씬 짧고 간결한 코드를 작성할 수 있습니다.range(start, end, step)range(1000) =&amp;gt; 0, 1, 2, 3, ..., 999range(1, 1000) =&amp;gt; 1, 2, 3, ..., 999range(1, 1000, 2) =&amp;gt; 1, 3, 5, 7, ..., 9995-2 리스트 표현식 (List comprehension)&amp;gt;&amp;gt;&amp;gt; a = []&amp;gt;&amp;gt;&amp;gt; for i in range(100):        if i % 3 == 0 and i % 5 == 0:          a.append(i)&amp;gt;&amp;gt;&amp;gt; [i for i in range(100) if i % 3 == 0 and i % 5 == 0]5-3 리스트와 문자열 넘나들기문자열을 리스트로 바꿔야 하는 경우문자열은 값을 바꿀 수가 없기 때문에 예를 들어 스펠링을 고치기 위해서는리스트로 바꿔서 고친 후 다시 문자열로 변환해줘야 한다.&amp;gt;&amp;gt;&amp;gt; name = &#39;kinziont&#39;&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39; -&amp;gt; 에러&amp;gt;&amp;gt;&amp;gt; name = list(name)&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39;&amp;gt;&amp;gt;&amp;gt; name[&#39;k&#39;, &#39;i&#39;, &#39;m&#39;, &#39;z&#39;, &#39;i&#39;, &#39;o&#39;, &#39;n&#39;, &#39;t&#39;]&amp;gt;&amp;gt;&amp;gt; name = str(name)&amp;gt;&amp;gt;&amp;gt; name&#39;kimziont&#39;문자열 데이터를 단어 단위 또는 문장 단위로 토크나이징하기 위해 문자열 메소드인 .split()을 쓰면자동으로 리스트로 변환된다.5-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)a = [1, 2, 3, 4] # 1*4 vectorb = [[1, 2], [3, 4]] # 2*2 matrixc = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] # 2*2*2 tensora[0] -&amp;gt; 1b[0] -&amp;gt; [1, 2]c[0] -&amp;gt; [[1, 2], [3, 4]]c[0][1] -&amp;gt; [3, 4]c[0][1][0] -&amp;gt; 3",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-list'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part3]: 파이썬 리스트 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-list'>Python Basic Series [Part3]: 파이썬 리스트 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part2]: 파이썬 숫자 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-number",
      "date"     : "Apr 23, 2020",
      "content"  : "Table of Contents  1. 숫자 자료형의 종류  2. 파이썬의 특별한 점  3. 2진법, 8진법, 16진법  4. 부동 소수점 연산 오류  5. 숫자 자료형 관련 메소드1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-number'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part2]: 파이썬 숫자 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-number'>Python Basic Series [Part2]: 파이썬 숫자 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part1]: 파이썬에서 데이터의 특성",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-intro",
      "date"     : "Apr 23, 2020",
      "content"  : "Table of Contents  파이썬 데이터는 객체다  타입  가변성  참조  복사          변수의 대입을 통한 복사      얕은 복사      깊은 복사      파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.타입            이름      타입      가변                  불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)참조변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사변수의 대입을 통한 복사immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.얕은 복사또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.깊은 복사따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-intro'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part1]: 파이썬에서 데이터의 특성'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-intro'>Python Basic Series [Part1]: 파이썬에서 데이터의 특성</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Korean-to-French Translation",
      "category" : "",
      "tags"     : "",
      "url"      : "/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say",
      "date"     : "Nov 7, 2018",
      "content"  : "아직 글을 작성 중입니다.Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Sam Bark diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Sam Bark on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Sam Bark potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Sam Bark on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-11-07T00:00:00+09:00'>07 Nov 2018</time><a class='article__image' href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'> <img src='/images/08.jpg' alt='Korean-to-French Translation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'>Korean-to-French Translation</a> </h2><p class='article__excerpt'>한국어를 프랑스어로 번역해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Real Dubbing using Tacotron",
      "category" : "",
      "tags"     : "",
      "url"      : "/the-way-to-get-started-is-to-quit-talking-and-begin-doing",
      "date"     : "Apr 23, 2018",
      "content"  : "아직 글을 작성 중입니다.Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Jeroen Bendeler diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Jeroen Bendeler on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Jairph potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Jairph on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-04-23T00:00:00+09:00'>23 Apr 2018</time><a class='article__image' href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'> <img src='/images/grafana_1.webp' alt='Real Dubbing using Tacotron'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'>Real Dubbing using Tacotron</a> </h2><p class='article__excerpt'>실제 배역 주인공의 목소리를 이용해 더빙을 합니다.</p></div></div></div>"
    } 
  
]

[
  
    {
      "title"    : "[Javascript]: 비동기 프로그래밍",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-asynchronous",
      "date"     : "Feb 10, 2024",
      "content"  : "Table of Contents  비동기 함수  프로미스          프로미스의 생성      프로미스의 후속 처리 메서드                  Promise.prototype.then          Promise.prototype.catch          Promise.prototype.finally                    프로미스의 에러 처리      fetch        async/await          async      await      에러 처리      비동기 함수  함수의 실행 순서는 실행 컨텍스트 스택(콜 스택)으로 관리한다  자바스크립트 엔진은 하나의 실행 컨텍스트 스택만을 갖는다  이는 함수를 실행할 수 있는 창구가 단 하나이며, 동시에 2개 이상의 함수를 동시에 실행할 수 없다는 것을 의미한다  이처럼 자바스크립트 엔진은 한 번에 하나의 태스크만 실행할 수 있는 싱글 스레드 방식으로 동작한다  싱글 스레드 방식은 한 번에 하나의 태스크만 실행할 수 있기 때문에 처리에 시간이 걸리는 태스크를 실행할 경우 블로킹이 발생한다  이처럼 현재 실행 중인 태스크가 종료할 때까지 다음 실행될 태스크가 대기하는 방식을 동기(synchronous) 처리라고 한다  동기 처리 방식은 실행 순서가 보장된다는 장점이 있지만, 앞선 태스크가 종료될 때까지 이후 태스크들이 블로킹되는 단점이 있다  반면에 현재 실행 중인 태스크가 종료되지 않은 상태라 해도 다음 태스크를 곧바로 실행하는 방식을 비동기(asynchronous) 처리라고 한다  비동기 처리를 수행하는 비동기 함수는 전통적으로 콜백 패턴을 사용한다. 하지만 콜백 패턴은 콜백 헬을 발생시켜 가독성을 나쁘게 하고, 비동기 처리 중 발생한 에러의 예외 처리가 곤란하며, 여러 개의 비동기 처리를 한 번에 처리하는 데도 한계가 있다  setTimeout, setInterval, HTTP 요청, 이벤트 핸들러는 비동기 처리 방식으로 동작한다  자바스크립트는 이벤트 루프를 통해 동시성을 지원한다  비동기 함수의 콜백 함수 또는 이벤트 핸들러는 태스크 큐에 일시적으로 보관된다  이벤트 루프는 콜 스택에 현재 실행 중인 실행 컨텍스트가 있는지, 그리고 태스크 큐에 대기 중인 함수(콜백 함수, 이벤트 핸들러 등)가 있는지 반복해서 확인한다  만약 콜 스택이 비어있고, 태스크 큐에 대기 중인 함수가 있다면 이벤트 루프는 순차적으로 태스크 큐에 대기 중인 함수를 콜 스택으로 이동시킨다      이때 콜 스택으로 이동한 함수는 실행된다. 즉, 태스크 큐에 일시 보관된 함수들은 비동기 처리 방식으로 동작한다    자바스크립트 엔진은 싱글 스레드로 동작하지만, 브라우저는 멀티 스레드로 동작한다  브라우저에 있는 태스크 큐와 이벤트 루프가 비동기 처리를 돕는 것이다프로미스  ES6에서 비동기 처리를 위한 또 다른 패턴으로 프로미스(Promise)를 도입했다  프로미스는 전통적인 콜백 패턴이 가진 단점을 보완하며 비동기 처리 시점을 명확하게 표현할 수 있다는 장점이 있다  비동기 함수를 호출하면 함수 내부의 비동기로 동작하는 코드가 완료되지 않았다 해도 기다리지 않고 즉시 종료된다  즉, 비동기 함수 내부의 비동기로 동작하는 코드는 비동기 함수가 종료된 후, 나중에 완료된다  따라서 비동기 함수 내부의 비동기로 동작하는 코드에서 처리 결과를 외부로 반환하거나 상위 스코프의 변수에 할당하면 기대한 대로 동작하지 않는다  예를 들어, 비동기 함수인 setTimeout 함수의 콜백 함수는 setTimeout 함수가 종료된 이후에 호출된다  비동기 함수는 비동기 처리 결과를 외부에 반환할 수 없고, 상위 스코프의 변수에 할당할 수도 없다  따라서 비동기 함수의 처리 결과(ex. 서버의 응답)에 대한 후속 처리는 비동기 함수 내부에서 수행해야 한다  이때 비동기 처리 결과에 대한 후속 처리를 수행하는 콜백 함수를 전달하는 것이 일반적이다  하지만 콜백 함수는 콜백 헬 문제와 에러 처리를 못한다는 문제가 있다  이를 극복하기 위해 ES6에서 프로미스가 도입되었다프로미스의 생성  Promise 생성자 함수는 비동기 처리를 수행할 콜백 함수를 인수로 전달 받는데 이 콜백 함수는 resolve와 reject 함수를 인수로 전달받는다  비동기 처리는 Promise 생성자 함수가 인수로 전달받는 콜백 함수 내부에서 수행한다. 만약 비동기 처리가 성공하면 비동기 처리 결과를 resolve 함수에 인수로 전달하면서 호출하고, 실패하면 에러를 reject 함수에 인수로 전달하면서 호출한다            프로미스의 상태 정보      의미      상태 변경 조건              pending      비동기 처리가 아직 수행되지 않은 상태      프로미스가 생성된 직후 기본 상태              fulfilled      비동기 처리가 수행된 상태 (성공)      resolve 함수 호출              rejected      비동기 처리가 수행된 상태 (실패)      reject 함수 호출        생성된 직후의 프로미스는 기본적으로 pending 상태다  이후 비동기 처리가 수행되면 처리 결과에 따라 다음과 같이 프로미스의 상태가 변경된다          성공: resolve 함수를 호출해 프로미스를 fulfilled 상태로 변경한다      실패: reject 함수를 호출해 프로미스를 rejected 상태로 변경한다        이처럼 프로미스의 상태는 resolve 또는 reject 함수를 호출하는 것으로 결정된다  프로미스는 pending 상태에서 fulfilled 또는 rejected 상태가 되면 더는 다른 상태로 변화할 수 없다  프로미스는 비동기 처리 상태와 더불어 비동기 처리 결과도 갖는다  성공하면, resolve() 함수의 첫 번째 인자로 넘겨준 값을 처리 결과 값으로 한다  실패하면, reject() 함수의 첫 번째 인자로 넘겨준 에러 객체를 결과 값으로 한다  즉, 프로미스는 비동기 처리 상태와 처리 결과를 관리하는 객체다프로미스의 후속 처리 메서드  프로미스가 fulfilled 또는 rejected가 되면 처리 결과를 가지고 무언가를 해야한다  이를 위해 프로미스는 후속 메서드 then, catch, finally를 제공한다  프로미스의 비동기 처리 상태가 변화하면 후속 처리 메서드에 인수로 전달한 콜백 함수가 선택적으로 호출된다  모든 후속 처리 메서드는 프로미스를 반환하며, 비동기로 동작한다Promise.prototype.then  then 메서드는 두 개의 콜백 함수를 인수로 전달받는다  첫 번째 콜백 함수는 프로미스가 fulfilled 상태가 되면 호출된다. 이때 콜백 함수는 프로미스의 비동기 처리 결과를 인수로 전달받는다  두 번째 콜백 함수는 프로미스가 rejected 상태가 되면 호출된다. 이때 콜백 함수는 프로미스의 에러를 인수로 전달받는다  then 메서드는 언제나 프로미스를 반환한다. 만약 then 메서드의 콜백 함수가 프로미스를 반환하면 그 프로미스를 그대로 반환하고, 값을 반환하면 그 값을 암묵적으로 resolve 또는 reject 하여 프로미스를 생성해 반환한다Promise.prototype.catch  catch 메서드는 한 개의 콜백 함수를 인수로 전달받는다  catch 메서드의 콜백 함수는 프로미스가 rejected 상태인 경우에만 호출된다  catch 메서드는 then(undefined, onRejected)과 동일하게 동작한다  마찬가지로 언제나 프로미스를 반환한다Promise.prototype.finally  finally 메서드는 한 개의 콜백 함수를 인수로 전달받는다  finally 메서드의 콜백 함수는 프로미스의 성공, 실패와 상관없이 무조건 한 번 호출된다  프로미스의 상태와 상관없이 수행해야 할 처리 내용이 있을 때 유용하다  마찬가지로 언제나 프로미스를 반환한다프로미스의 에러 처리  then, catch를 이용해 에러를 처리할 수 있다  단, then 메서드의 두 번째 콜백 함수가 첫 번째 콜백 함수에서 발생한 에러를 캐치하지는 않는다  then이 단 한번만 나온다면 두 번째 콜백함수를 사용해 그 안에서 에러 처리를 해도 괜찮다  하지만 then이 여러 번에 걸쳐 체이닝 되는 경우에는 두 번째 콜백 함수를 쓰지 않고, catch 로 에러를 처리하는게 가독성이 좋다fetch  fetch 함수는 HTTP 요청 전송 기능을 제공하는 클라이언트 사이드 Web API다  fetch 함수는 프로미스를 지원하기 때문에 비동기 처리가 훨씬 자유롭다  fetch 함수는 HTTP 응답을 나타내는 Response 객체를 래핑한 Promise 객체를 반환한다  fetch 함수는 Response 객체를 래핑한 프로미스를 반환하므로, then을 통해 프로미스가 resolve한 Response 객체를 전달 받을 수 있다  fetch 함수가 반환하는 프로미스는 404와 같은 HTTP 에러가 발생해도 에러를 reject 하지 않고 불리언 타입의 ok 상태를 false로 설정한 Response 객체를 resolve 한다  오프라인 등의 네트워크 장애나 CORS 에러에 의해 요청이 완료되지 못한 경우에만 프로미스를 reject 한다  따라서 HTTP 에러는 프로미스가 resolve한 불리언 타입의 ok 상태를 확인해 명시적으로 처리해야 한다async/await  ES8 에서 비동기 처리를 동기 처리처럼 동작하도록 구현할 수 있는 async/await이 도입되었다  async/await은 프로미스를 기반으로 동작한다  async/await을 사용하면 프로미스의 then/catch/finally과 같은 후속 처리 메서드를 사용하지 않고도 마치 동기 처리처럼 프로미스를 사용할 수 있다async  async는 함수 정의문 앞에 붙는 키워드로, 함수가 비동기적으로 실행되도록 한다. 즉 비동기 함수를 정의할 때 사용하는 키워드이다  async 함수는 언제나 프로미스를 반환한다. async 함수는 암묵적으로 반환값을 resolve하는 프로미스를 반환한다await  await 키워드는 프로미스가 settled 상태(fulfilled 또는 rejected)가 될 때까지 대기하다가 settled 상태가 되면 프로미스가 resolve한 처리 결과를 반환한다  await 키워드는 반드시 프로미스 앞에서 사용해야 한다  await 키워드는 다음 실행을 일시 중지시켰다가 프로미스가 settled상태가 되면 다시 재개한다에러 처리  async/await 에서 에러 처리는 try...catch 문을 사용할 수 있다  콜백 함수를 인자로 전달받는 비동기 함수와는 달리 프로미스를 반환하는 비동기 함수는 명시적으로 호출할 수 있기 때문에 호출자가 명확하다const foo = async () =&amp;gt; {  try {    const wrongUrl = &#39;https://wrong.url&#39;;    const response = await fetch(wrongUrl);    const data = await response.json();    console.log(data)  } catch (err) {    console.error(err); // TypeError: Failed to fetch  }}foo();  catch 문은 HTTP 통신에서 발생한 네트워크 에러뿐 아니라 try 코드 블록 내의 모든 문에서 발생한 에러까지 모두 캐치할 수 있다  async 함수 내에서 catch 문을 사용해서 에러 처리를 하지 않으면 async 함수는 발생한 에러를 reject 하는 프로미스를 반환한다  따라서 async 함수를 호출하고 Promise.prototype.catch 후속 처리 메서드를 사용해 에러를 캐치할 수도 있다const foo = async () =&amp;gt; {  const wrongUrl = &#39;https://wrong.url&#39;;  const response = await fetch(wrongUrl);  const data = await response.json();  return data}foo()  .then(console.log)  .catch(console.error) // TypeError: Failed to fetch",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-10T21:01:35+09:00'>10 Feb 2024</time><a class='article__image' href='/js-asynchronous'> <img src='/images/js_logo.png' alt='[Javascript]: 비동기 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-asynchronous'>[Javascript]: 비동기 프로그래밍</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker]: 진화하는 컨테이너 표준",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/container-series2",
      "date"     : "Feb 10, 2024",
      "content"  : "Table of Contents  도커와 컨테이너 기술  도커 엔진          초기 도커 엔진의 모습      새롭게 바뀐 도커 엔진의 모습                  Client          dockerd          containerd          shim          runc                      OCI  CRI  참고도커와 컨테이너 기술Docker는 컨테이너 기술을 사용하여 애플리케이션에 필요한 환경을 신속하게 구축하고 테스트 및 배포를 할 수 있게 해주는 플랫폼으로 접근 장벽이 비교적 높았던 컨테이너 기술의 상용화를 앞당겼습니다.컨테이너 기술은 하드웨어와 호스트 OS는 그대로 둔 채 애플리케이션 영역만 캡슐화하여 독립적인 환경을 제공해주는 가상화 방식입니다. 이 덕분에 이전의 가상화 방식에서는 애플리케이션마다 완전한 OS가 필요했지만 컨테이너 기술에서는 이러한 완전한 OS의 불필요한 중복(redundancy)을 제거하였습니다. 이러한 컨테이너 기술 덕분에 CPU, RAM 및 스토리지와 같은 시스템 리소스를 더 많이 확보할 수 있게 되었으며, 라이센스 비용 절감, OS 패치 및 유지보수에 들어가는 오버헤드를 제거할 수 있게 되었습니다.도커 엔진이러한 컨테이너 기술을 실현하기 위해서는 컨테이너를 실행시킬 수 있는 환경을 구축해야 하는데 이를 컨테이너 엔진이라고 하고, 도커에서는 도커 엔진에 해당됩니다.초기 도커 엔진의 모습초기 도커 엔진은 크게 dockerd와 LXC로 이루어져 있었습니다.      dockerd초기 도커 엔진의 dockerd 지금의 dockerd보다 훨씬 많은 역할을 하고 있었습니다. 그 이유는 도커가 처음 등장할 당시 도커 개발자들의 목표는 컨테이너 기술의 대중화였습니다. 그렇기 때문에 최대한 사용성을 간편하게 하고 싶었고 이러한 목적으로 도커 개발자들은 컨테이너 기술을 사용하는데 필요한 많은 기능들을 dockerd에 담아두었었습니다.    이 당시 dockerd에는 현재의 Docker Client, Docker API, Docker Runtime, Image Build와 같은 역할들을 모두 담당하고 있었습니다.        LXCLXC는 단일 호스트 상에서 여러개의 고립된 리눅스 시스템(컨테이너)들을 실행하기 위한 운영 시스템 레벨 가상화 방법입니다. LXC는 dockerd에게 Linux kernel에 존재하는 컨테이너의 기본 building block의 namespaces나 cgroups(control groups)에 대한 접근을 제공했습니다.    namespaces: 운영 시스템을 논리적으로 나누어 고립된 환경을 제공하는 역할cgroups: 고립된 환경에서 사용할 자원을 제한하는 역할  새롭게 바뀐 도커 엔진의 모습도커는 2016년 12월 14일 쿠버네티스, AWS Fargate, Rancher와 같은 컨테이너 기술 기반의 소프트웨어에 dockerd안에 포함되어 있던 containerd라는 컨테이너 런타임을 제공해주기 위해 컨테이너를 모듈화하였습니다. (도커 공식문서 참고)Client도커 클라이언트는 개발자들이 도커를 사용할 때 Docker CLI로 도커 서버에 명령어를 전달하는 역할을 합니다. 흔히 저희가 사용하는 docker run과 같은 명령어가 REST API로 형태로 dockerd에게 전달됩니다.dockerd도커 데몬(dockerd)은 Docker API 요청을 수신하며 이미지 관리, 이미지 빌드, REST API, 인증, 보안, 코어 네트워킹, 오케스트레이션 등과 같은 역할을 담당합니다.containerdcontainerd는 Container의 생명주기를 관리합니다 (= container lifecycle operations).containerd는 원래 작고, 가벼운 Container lifecycle operations으로 설계되었는데, 시간이 지나면서 image pulls, volumes and networks와 같은 기능들이 확장되었습니다.shim앞에서 containerd가 새로운 컨테이너를 만들기 위해 runc를 사용한다고 했는데요. 생성되는 모든 container 당 runc의 새로운 인스턴스를 fork 합니다. 그러나 각 컨테이너가 생성되면, 상위 runc 프로세스가 종료됩니다.수백 개의 runc 인스턴스를 실행하지 않고도 수백 개의 container를 실행할 수 있습니다.컨테이너의 할당된 부모 runc 프로세스가 종료되면, 연결된 containerd-shim 프로세스가 컨테이너의 부모프로세스가 됩니다.이는 containerd에게 컨테이너의 file descriptor(e.g. stdin/out)와 종료 상태를 관리하는 데 필요한 최소한의 코드를 메모리에 남깁니다.runcrunc는 libcontainer용 CLI Wrapper로, 독립된 container runtime입니다.docker가 container 관련된 기능들을 쉽게 사용할 수 있도록 해주는 가볍고 이식가능한 툴입니다.다시 말해, container 동작 환경이 갖추어진 가볍고 이식 가능한 툴입니다.Docker에서 runc는 목적은 단 하나인데요, 바로 Container 생성입니다.OCICRI참고  Johan Fischer, Comparing Container Runtimes: containerd vs. Docker  tutorialworks: The differences between Docker, containerd, CRI-O and runc  LinkedIn: containerd는 무엇이고 왜 중요할까?  cloud native wiki: 3 Types of Container Runtime and the Kubernetes Connection  pageseo: Docker Engine, 제대로 이해하기 (1)  Devin Jeon, Kubernetes의 Docker container runtime 지원 중단에 대하여  Selecting a container runtime for use with Kubernetes  A Comprehensive Container Runtime Comparison  Don’t Panic: Kubernetes and Docker",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-10T21:01:35+09:00'>10 Feb 2024</time><a class='article__image' href='/container-series2'> <img src='/images/container_7.png' alt='[Docker]: 진화하는 컨테이너 표준'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series2'>[Docker]: 진화하는 컨테이너 표준</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] 네트워크 이해하기",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-series9",
      "date"     : "Feb 9, 2024",
      "content"  : "Table of Contents  Bridge Network Driver  Overlay Network Driver  도커 네트워크 실습          도커 네트워크의 몇 가지 특징      Bridge 드라이버 사용해보기      Overlay 드라이버 사용해보기        참고도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다.이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.당연히 다음 질문은 어떤 네트워크 드라이버를 사용해야 하는가 하는 것입니다. 각 드라이버는 트레이드오프를 제공하며 사용 사례에 따라 다른 장점이 있습니다. 도커 엔진과 함께 제공되는 내장 네트워크 드라이버가 있으며 네트워킹 벤더와 커뮤니티에서 제공하는 플러그인 네트워크 드라이버도 있습니다. 가장 일반적으로 사용되는 내장 네트워크 드라이버는 bridge, overlay, macvlan입니다. 이번 포스트에서는 비교적 간단한 드라이버인 bridge와 overlay에 대해서만 살펴보겠습니다.Bridge Network Driverbridge 네트워크 드라이버가 우리 목록의 첫 번째 드라이버입니다. 이해하기 쉽고, 사용하기 쉽고, 문제 해결이 간단하기 때문에 개발자와 Docker를 처음 접하는 사람들에게 좋은 네트워킹 선택이 됩니다. bridge 드라이버는 private 네트워크를 호스트 내부에 생성해 컨테이너들이 생성한 네트워크 안에서 통신할 수 있도록 합니다. 컨테이너에 포트를 노출함으로써 외부 액세스가 허용됩니다. 도커는 서로 다른 도커 네트워크 간의 연결을 차단하는 규칙을 관리하여 네트워크를 보호합니다.내부적으로 도커 엔진은 리눅스 브리지, 내부 인터페이스, iptables 규칙 및 호스트 경로를 만들어 컨테이너 간의 연결을 가능하게 합니다. 아래 강조 표시된 예에서는 도커 브리지 네트워크가 생성되고 두 개의 컨테이너가 이 네트워크에 연결됩니다. 도커 엔진은 별도의 설정 없이 필요한 연결을 수행하고 컨테이너에 대한 서비스 디스커버리를 제공하며 다른 네트워크와의 통신을 차단하도록 보안 규칙을 구성합니다.우리의 애플리케이션은 현재 호스트 8000번 포트에서 서비스되고 있습니다. 도커 브리지는 컨테이너 이름으로 web이 db와 통신할 수 있도록 하고 있습니다. 브릿지 드라이버는 같은 네트워크에 있기 때문에 자동으로 우리를 위해 서비스 디스커버리를 합니다.브리지 드라이버는 로컬 범위 드라이버이므로 단일 호스트에서 서비스 디스커버리, IPAM 및 연결만 제공합니다. 다중 호스트 서비스 검색을 수행하려면 컨테이너를 호스트 위치에 매핑할 수 있는 외부 솔루션이 필요합니다. 이 때 필요한 것이 바로 overlay 드라이버입니다.Overlay Network Driveroverlay 네트워크 드라이버는 multi-host 네트워킹의 많은 복잡성을 획기적으로 단순화합니다. Swarm 스코프 드라이버로, 개별 호스트가 아닌 전체 Swarm 또는 UCP 클러스터에서 작동합니다.overlay 드라이버는 컨테이너 네트워크를 물리적 네트워크와 분리해주는 VXLAN data plane을 사용합니다. 덕분에 다양한 클라우드, 온-프레미스 네트워크 환경 속에서 최고의 이식성을 제공해줍니다.도커 네트워크 실습(라우드 엔지니어 Won의 성장 블로그 참고)도커 네트워크의 몇 가지 특징  도커는 컨테이너에 내부 IP(eth0)를 순차적으로 할당  컨테이너 외부에 노출시킬 엔드포인트로 veth(Virtual Ethernet) 생성  컨테이너마다 veth 네트워크 인터페이스 자동 생성  docker0는 기본 생성되는 디폴트 브리지로 각 veth 인터페이스와 호스트의 기본 네트워크인 eth0와 연결Bridge 드라이버 사용해보기Overlay 드라이버 사용해보기참고  도커 공식문서  MARK CHURCH, Understanding Docker Networking Drivers and their use cases  클라우드 엔지니어 Won의 성장 블로그, 06. 도커 네트워크 포스트  DaleSeo: Docker 네트워크 사용법  Julie의 Tech블로그, 도커 - 네트워킹 / bridge와 overlay  도커 공식문서, Networking with overlay networks  도커 공식문서, Use overlay networks  How To Communicate Between Docker Containers  Using placement constraints with Docker Swarm  Install Docker Engine on Ubuntu",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-09T21:01:35+09:00'>09 Feb 2024</time><a class='article__image' href='/docker-series9'> <img src='/images/docker_logo.png' alt='[Docker] 네트워크 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series9'>[Docker] 네트워크 이해하기</a> </h2><p class='article__excerpt'>도커에서 외부와의 통신, 컨테이너 간 통신 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] 퍼시스턴시한 어플리케이션 만들기",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-volume",
      "date"     : "Feb 8, 2024",
      "content"  : "Table of Contents  도커에서 데이터 관리하기  Volume          실습                  볼륨 생성                          Dockerfile VOLUME 인스트럭션              docker volume 명령어              docker compose                                볼륨 삭제          볼륨 확인          볼륨 마운트                      Bind Mount          실습        참고도커에서 데이터 관리하기  기본적으로 컨테이너 안에서 생성된 모든 파일은 컨테이너 레이어에 저장된다  그래서, 해당 컨테이너가 삭제되면 데이터도 함께 사라진다 (컨테이너 종료는 데이터를 삭제하지 않는다)  따라서 컨테이너의 생명 주기와 관계없이 데이터를 영구적으로 저장하기 위한 방법이 필요하다  또한 여러 컨테이너가 데이터를 공유할 수 있으면 데이터를 컨테이너별로 중복 저장할 필요가 없어 컨테이너를 더욱 효율적으로 관리할 수 있게 될 것이다  이러한 이유로 도커는 영구적인 요소와의 마운트를 다음의 두 가지 방법으로 제공한다          볼륨(volume)      바인드(bind)      Volume  도커 볼륨은 도커에서 스토리지를 다루는 단위다  데이터베이스처럼 영구성이 필요한 애플리케이션을 컨테이너로 실행하려면 볼륨을 사용해야 한다  볼륨을 사용하면 컨테이너의 데이터가 호스트 컴퓨터 내에 있는 도커에 의해 관리되는 파일 시스템(Linux기준 /var/lib/docker/volumes/)에 데이터가 저장된다  볼륨은 완전히 도커에 의해서만 관리되어 호스트 머신의 디렉토리 구조나 OS에 독립적인, 도커에서 데이터를 유지하기 위한 권장되는 메커니즘이다  볼륨 드라이버를 사용해 클라우드 또는 리모트 호스트에 데이터를 저장할 수도 있다                볼륨을 사용하기 좋은 경우                        여러 컨테이너에 마운트하고 싶은 경우 (명시적으로 표현한 볼륨이 없으면 자동으로 생성하고 마운트 해준다)        도커 호스트의 파일 구조를 모르는 경우 (bind mount와 달리 Volume은 볼륨 명으로 관리)        백업, 데이터 통합이 필요한 경우          실습볼륨 생성  볼륨은 도커에서 이미지나 컨테이너와 동급인 요소다Dockerfile VOLUME 인스트럭션  Dockerfile 의 VOLUME 인스트럭션으로 만들 수 있다  하지만 이 방법은 볼륨명(source)을 지정할 수 없고, 컨테이너의 마운트 지점(target)만 지정할 수 있다  Dockerfile에서 VOLUME의 용도는 이미지가 데이터베이스 같은 유상태 애플리케이션의 경우 사용자가 볼륨을 지정하지 않더라도 데이터를 유실하지 않기 위한 안전장치 용도이다  컨테이너를 실행할 때 볼륨을 새로 정의하면 VOLUME 은 무시된다docker volume 명령어  docker volume 명령어를 사용해 볼륨을 만들고 확인하고 삭제할 수 있다  볼륨은 docker volume create 명령어를 이용해 명시적으로 볼륨을 생성할 수 있다# 볼륨 생성docker volume create my-volume# 볼륨 마운트docker container run -v my-volume:/data &amp;lt;이미지명&amp;gt;docker composeversion: &quot;3.9&quot;services:  frontend:    image: node:lts    volumes:      - my-volume:/data # 볼륨 마운트volumes:  my-volume: # 볼륨 생성볼륨 삭제  특정 볼륨을 삭제할 때는 docker volume rm &amp;lt;볼륨명&amp;gt; 명령어를 사용한다  만약 사용하지 않는 불특정 볼륨들을 제거하고 싶다면 docker volume prune 명령어를 사용하면 된다볼륨 확인  특정 볼륨을 확인할 때는 docker volume inspect &amp;lt;볼륨명&amp;gt; 명렁어를 사용한다  볼륨 목록을 확인할 때는 docker volume ls 명령어를 사용한다볼륨 마운트  볼륨(바인드 포함)을 마운트 할 때는 -v 옵션과 --mount 옵션 두 가지가 있다  -v는 기존에 있던 옵션이고, --mount는 이 후에 나온 좀 더 명시적인 옵션이다 (도커에서 --mount 옵션 권장)  차이점은 -v는 콜론(:)으로 값들을 구분하고, --mount는 키-밸류 페어로 구분해 조금 더 명시적이다  바인드 마운트의 경우 -v는 명시한 디렉터리가 존재하지 않으면 생성하고, --mount는 에러를 발생한다-v &amp;lt;볼륨명&amp;gt;:&amp;lt;컨테이너 마운트 지점&amp;gt;:&amp;lt;옵션 ex. ro&amp;gt; # 볼륨 마운트-v &amp;lt;호스트 디렉터리&amp;gt;:&amp;lt;컨테이너 마운트 지점&amp;gt;:&amp;lt;옵션 ex. ro&amp;gt; # 바인드 마운트--mount type=volume, source=&amp;lt;볼륨명&amp;gt;, target=&amp;lt;컨테이너 마운트 지점&amp;gt;, readonly # 볼륨 마운트--mount type=bind, source=&amp;lt;호스트 디렉터리&amp;gt;, target=&amp;lt;컨테이너 마운트 지점&amp;gt;, readonly # 바인드 마운트docker volume inspect my-vol[    {        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: {},        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;,        &quot;Name&quot;: &quot;my-vol&quot;,        &quot;Options&quot;: {},        &quot;Scope&quot;: &quot;local&quot;    }]Bind Mount  바인드 마운트는 호스트 컴퓨터 파일 시스템의 디렉터리를 컨테이너 파일 시스템의 디렉터리로 만든다  (볼륨은 볼륨명이라는 가상의 식별자를 사용하고, 바인드는 호스트 컴퓨터 파일 시스템의 절대 경로를 사용한다)  바인드 마운트는 양방향으로 동작한다. 컨테이너에서 만든 파일을 호스트 컴퓨터에서 수정할 수도 있고, 반대로 호스트에서 만든 파일도 컨테이너에서 수정할 수 있다  만약 컨테이너의 마운트 경로에 이미지에서 만들어뒀던 파일이 이미 존재하는 경우, 이 파일들은 완전히 대체된다 (볼륨, 바인드 모두)  컨테이너에서 호스트 파일 시스템에 접근할 수 있다는 점 때문에 보안에 안좋은 영향을 끼칠 수 있다                바인드를 사용하기 좋은 경우                        컨테이너의 애플리케이션에 필요한 설정 파일을 호스트 컴퓨터에서 관리할 때 유용하다          실습  바인드는 말그대로 호스트 컴퓨터의 경로와 바인드만 하면된다. 볼륨처럼 따로 생성할 필요 없다-v &amp;lt;호스트 디렉터리&amp;gt;:&amp;lt;컨테이너 마운트 지점&amp;gt;:&amp;lt;옵션 ex. ro&amp;gt; # -v 옵션--mount type=bind, source=&amp;lt;호스트 디렉터리&amp;gt;, target=&amp;lt;컨테이너 마운트 지점&amp;gt;, readonly # --mount 옵션 (권장방식)참고  도커 공식문서: Manage data in Docker  도커 공식문서: Docker-compose volume configuration  DaleSeo: Docker 컨테이너에 데이터 저장 (볼륨/바인드 마운트)  stack overflow: Understanding “VOLUME” instruction in DockerFile",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-08T21:01:35+09:00'>08 Feb 2024</time><a class='article__image' href='/docker-volume'> <img src='/images/docker_logo.png' alt='[Docker] 퍼시스턴시한 어플리케이션 만들기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-volume'>[Docker] 퍼시스턴시한 어플리케이션 만들기</a> </h2><p class='article__excerpt'>도커에서 데이터를 영구적으로 저장하는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] 컨테이너에 저장된 데이터는 어떻게 될까?",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-series6",
      "date"     : "Feb 7, 2024",
      "content"  : "Table of Contents  Container Layer  UFS  CoW  Storage Driver  정리  참고도커를 공부하면서 궁금했던 것 중에 하나가 컨테이너에서 생성된 파일은 어디에 저장되어 있는걸까? 였습니다. 그동안 저는 도커에 다른 저장소를 마운트하면 컨테이너에서 생성된 데이터를 저장할 수 있고 그렇지 않다면 컨테이너가 삭제되면서 같이 사라진다라고 알고 있었는데 그러면 컨테이너가 사라지기 전까지는 어디에 저장되어 있는지 궁금해졌습니다.그러던 중 좋은 글을 공유해 놓은 블로그를 알게되어 이와 관련해 정리해보았습니다. (참고: Rain.i 블로그)Container Layer도커 컨테이너는 도커 이미지로부터 만들어진 인스턴스입니다. 도커 이미지를 토대로 여러 개의 컨테이너를 만들 수 있습니다. 예를 들어 우분투 운영체제를 제공하는 이미지를 이용해 어떤 컨테이너에는 파이썬을 설치하고, 어떤 곳에는 nginx를 설치해 웹 서버로 사용할 수도 있습니다. 이렇게 새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다. 이걸 보면 도커는 각각의 서비스를 컨테이너화 했을 뿐 아니라 컨테이너도 또 컨테이너화 한 것 같은 느낌이 드네요.도커가 컨테이너를 이런식으로 구현한 이유는 이미지의 상태를 최대한 그대로 보존하여 컨테이너를 계속 생성하더라도 토대가 변하지 않아 예상치 못한 오류를 예방할 수 있고 관리하기도 편합니다. 사용하는 입장에서도 어차피 컨테이너를 삭제하면 원래 기본 이미지 상태로 돌아가니까 걱정없이 컨테이너를 조작할 수 있을 것 입니다.우선 컨테이너를 생성하고 새로운 데이터를 생성하면 도커 상에서는 Container Layer에 저장된다는 것을 알았습니다. 그런데 Container Layer도 결국 도커를 띄운 호스트의 자원을 이용하기 때문에 제 컴퓨터(로컬이라면 데스크탑이나 노트북, 리모트라면 AWS의 EC2 정도) 어딘가에 저장이 되어 있을 것입니다. 이렇게 컨테이너들이 사용하는 이미지나 변경사항들은 모두 호스트 File system 의 /var/lib/docker 디렉토리 내에 저장된다. 이 영역을 Docker area 또는 Backing Filesystem 이라고 부르기도 한다.만약 컨테이너에서 생성된 파일을 버리지 않고 저장하고 싶다면 다음의 두 가지 방법을 사용할 수 있습니다.  Commit: 컨테이너 상에서 변경을 수행한 후 새로운 이미지로 만들어둔다.  Volume: 변경사항을 로컬 또는 외부 볼륨에 저장하도록 한다.UFS위의 내용을 읽다보면 이러한 의문이 생길 수 있습니다. ubuntu 이미지가 가지고 있던 Filesystem이 아닌 별도의 Filesystem에 Container Layer의 데이터가 저장이 되는데 왜 우리는 컨테이너를 사용할 때 이러한 사실을 몰랐을까? 그 이유는 바로 도커에서는 UFS(Union File System)라는 방식을 이용해 Image Layer와 Container Layer의 Filesystem을 하나로 통합해서 저희에게 제공해줍니다.이러한 UFS 방식의 장점은 무엇일까요? 가장 큰 장점은 Image Layer의 데이터를 여러 컨테이너가 공유할 수 있다는 점입니다. 공유한다는 것은 여러 개의 컨테이너를 띄우더라도 Image Layer의 데이터 용량은 단 1개만큼만 저장된다는 말입니다.CoW위의 그림과 같이 Image Layer의 a라는 파일을 a&#39;으로 수정할 때 Image Layer에서 파일이 수정되지 않고 Container Layer 위에서 새로 파일을 복사한 후 수정하는 것을 CoW(Copy on Write)라고 합니다. 이러한 기법을 통해 기존의 이미지에 대한 변경을 막을 수 있습니다. 하지만 Copy-on-Write 기법은 그 동작 구조 상 다음의 단점이 있습니다.  Performance Overhead: data 를 먼저 복제(Copy)한 후 변경을 수행해야함  Capacity Overhead: 원본 데이터 뿐 아니라, 변경된 데이터도 저장해야함따라서 되도록이면 중복 사용되고 수정되지 않을만한 데이터들을 이미지 레이어로 구성하는 것이 좋습니다.Storage Driver위에서 그동안 배운 UFS와 CoW 방식을 도커에서 쉽게 이용할 수 있는 것은 도커의 Storage Driver 덕분입니다. Storage Driver는 컨테이너 내에서의 파일 I/O 처리를 담당하는 드라이버입니다. Storage Driver는 Pluggable한 구조로 되어 있고 특성도 다릅니다. 또한 리눅스 배포판마다 지원하는 드라이버도 다르므로 자신의 workload에 맞는 Storage Driver를 선택해아 합니다.Storage Driver의 종류(참고: 도커 공식문서)리눅스 배포판별 지원하는 Storage Driver(참고: 도커 공식문서)Storage Driver와 Backing File SystemStorage Driver는 Container Layer의 데이터를 Backing filesystem(/var/lib/docker)으로 저장하고 사용자에게 layered filesystem으로 제공해 줍니다.   (참고로 볼륨 마운트는 이러한 Storage Driver의 도움없이 직접 Host의 Filesystem에 접근 가능합니다.)참고로 Storage Driver와 Backing filesystem 간에도 종속성이 있습니다.(참고: 도커 공식문서)Storage Driver와 graphDBStorage Driver는 사용자에게 최적의 통합된 파일 시스템을 제공하기 위해서는 layer 별 관계를 조회하고 key를 통해 특정 image를 검색하는 등, 이러한 일련의 정보 검색 및 관리하는 데이터베이스가 필요합니다. 이런 정보를 저장하고 있는 데이터베이스를 graphDB라고 합니다. (graphDB는 Storage Driver의 뇌와 같은 역할?)정리  UFS: Container Layer와 Image Layer의 파일이 통합되어 보인다  CoW: Image Layer 내의 파일을 원본은 유지하는 방향으로 파일을 수정할 수 있다  Storage Driver: 위의 기능들을 실제로 수행하는 드라이버  graphDB: Storage Driver가 최적의 실행을 하는데 필요한 정보를 저장하고 있는 SQLite기반 DB참고  도커 공식문서 About Storage Driver  Rain.i님의 도커 컨테이너 까보기(2) – Container Size, UFS 포스트  Davaom’s Tech Blog, [Docker] 컨테이너의 구조 포스트",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-07T21:01:35+09:00'>07 Feb 2024</time><a class='article__image' href='/docker-series6'> <img src='/images/docker_logo.png' alt='[Docker] 컨테이너에 저장된 데이터는 어떻게 될까?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series6'>[Docker] 컨테이너에 저장된 데이터는 어떻게 될까?</a> </h2><p class='article__excerpt'>도커 컨테이너의 파일 시스템 생성 원리에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 타입 심화편(2): 객체",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-type-object",
      "date"     : "Feb 7, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-07T07:01:35+09:00'>07 Feb 2024</time><a class='article__image' href='/typescript-type-object'> <img src='/images/typescript_logo.png' alt='[Typescript] 타입 심화편(2): 객체'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-type-object'>[Typescript] 타입 심화편(2): 객체</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 타입 심화편(1): 함수",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-type-function",
      "date"     : "Feb 7, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-07T07:01:35+09:00'>07 Feb 2024</time><a class='article__image' href='/typescript-type-function'> <img src='/images/typescript_logo.png' alt='[Typescript] 타입 심화편(1): 함수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-type-function'>[Typescript] 타입 심화편(1): 함수</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 데코레이터",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-decorator",
      "date"     : "Feb 7, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-07T07:01:35+09:00'>07 Feb 2024</time><a class='article__image' href='/typescript-decorator'> <img src='/images/typescript_logo.png' alt='[Typescript] 데코레이터'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-decorator'>[Typescript] 데코레이터</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 유틸리티 타입",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-utility-types",
      "date"     : "Feb 6, 2024",
      "content"  : "Table of Contents  Utility Type  Partial  Pick  Omit  Record  AwaitUtility Type  유틸리티 타입은 기존 타입을 변환하여 새로운 타입을 만드는데 유용한 기능을 제공한다Partial  파셜(Partial) 타입은 기존 타입의 모든 가능한 부분집합을 의미하는 타입이다  기존 타입의 모든 프로퍼티를 옵셔널(optional)로 만든다type User = {    name: string    age: number    address: string}type UpdateUser = Partial&amp;lt;User&amp;gt;let updateBody: UpdateUserupdateBody = {} // OKupdateBody = { name: &#39;Kim&#39; } // OKupdateBody = { name: &#39;Lee&#39;, age: 20, address: &#39;Seoul&#39; } // OKupdateBody = { foo: &#39;bar&#39; } // ErrorPick  픽(Pick) 타입은 기존 타입의 특정 속성만 명시적으로 골라 정의한 새로운 타입이다type User = {    name: string    age: number    address: string}type UpdateUser = Pick&amp;lt;User, &#39;age&#39; | &#39;address&#39;&amp;gt;let updateBody: UpdateUserupdateBody = {} // ErrorupdateBody = { name: &#39;Kim&#39; } // ErrorupdateBody = { age: 20, address: &#39;Seoul&#39; } // OKOmit  오밋(Omit) 타입은 기존 타입의 특정 속성만 명시적으로 제외한 타입이다type User = {    name: string    age: number    address: string}type UpdateUser = Omit&amp;lt;User, &#39;name&#39;&amp;gt;let updateBody: UpdateUserupdateBody = {} // ErrorupdateBody = { name: &#39;Kim&#39; } // ErrorupdateBody = { age: 20, address: &#39;Seoul&#39; } // OKRecord  키와 값의 타입을 정의한 객체를 의미하는 타입이다const user: Record&amp;lt;string, string&amp;gt; = { name: &#39;Kim&#39;, address: &#39;Seoul&#39; }type UserKey = &#39;name&#39; | &#39;age&#39; | &#39;address&#39;type UserValue = string | numberconst user: Record&amp;lt;UserKey, UserValue&amp;gt; = { name: &#39;Kim&#39;, age: 19, address: &#39;Seoul&#39; }Await",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-06T21:01:35+09:00'>06 Feb 2024</time><a class='article__image' href='/typescript-utility-types'> <img src='/images/typescript_logo.png' alt='[Typescript] 유틸리티 타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-utility-types'>[Typescript] 유틸리티 타입</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] Index signature",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-index-signature",
      "date"     : "Feb 6, 2024",
      "content"  : "Table of Contents  인덱스 시그니처란  인덱스 시그니처 사용하기  인덱스 시그니처 사용 예시  참고인덱스 시그니처란  객체타입의 프로퍼티에 가능한 타입을 나타내는것을 의미한다  정확한 프로퍼티의 이름을 미리 알 수 없는 경우 유용하다인덱스 시그니처 사용하기  다음과 같이 UserType을 인덱스 시그니처를 이용해 정의하면, 키가 문자열, 값도 문자열로 하는 어떤 프로퍼티도 올 수 있다는 의미다type UserType = {    [key: string]: string}const user: UserType = {    name: &#39;Mike&#39;,    address: &#39;Seoul&#39;}const userTwo: UserType = {    name: &#39;Carl&#39;,    address: &#39;LA&#39;,    job: &#39;engineer&#39;}  값의 타입은 자유롭게 쓸 수 있다type UserType = {    [key: string]: string | number}const userThree: UserType = {    name: &#39;Jack&#39;,    address: &#39;New York&#39;,    job: &#39;engineer&#39;,    age: 25}  인덱스 시그니처의 키 타입은 제한적이다  string, number, symbol, template literal 그리고 이들의 유니언 타입만 가능하다  template literal 쓸 때는 mapped types 방식을 써야 한다 (in 사용)[key: string]: string // O[key: string | number]: string // O[key: &#39;name&#39;]: string // X[key: in &#39;name&#39;]: string // O[key: in &#39;name&#39; | &#39;address&#39;]: string // O인덱스 시그니처 사용 예시const obj = {    foo: &quot;hello&quot;,}  위의 obj 객체에서 foo 키의 타입은 string이 아니고 문자열 리터럴 &#39;foo&#39;이다  타입스크립트에서는 string 타입의 키를 이용한 객체 접근을 허용하지 않는다const literalFoo = &quot;foo&quot;const stringFoo: string = &quot;foo&quot;console.log(obj[literalFoo]) // OKconsole.log(obj[stringFoo]) // Error  다음과 같은 for...of문을 이용한 객체접근도 같은 이유로 컴파일 에러가 난다for (let key of Object.keys(obj)) {    console.log(obj[key]) // Error}  이럴 때 인덱스 시그니처를 사용할 수 있다  인덱스 시그니처의 이름은 마음대로 표기해도 된다 (ex. index, idx, key 등)type ObjType = {    [idx: string]: string    foo: string    bar: string}  const obj: ObjType = {    foo: &quot;hello&quot;,    bar: &quot;world&quot;,}for (let key of Object.keys(obj)) {    console.log(obj[key]) // OK}참고  Typescript Handbook  Harry Park’s Blog",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-06T21:01:35+09:00'>06 Feb 2024</time><a class='article__image' href='/typescript-index-signature'> <img src='/images/typescript_logo.png' alt='[Typescript] Index signature'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-index-signature'>[Typescript] Index signature</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] Docker Compose",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-docker-compose",
      "date"     : "Feb 6, 2024",
      "content"  : "Table of Contents#",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-06T21:01:35+09:00'>06 Feb 2024</time><a class='article__image' href='/docker-docker-compose'> <img src='/images/docker_logo.png' alt='[Docker] Docker Compose'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-docker-compose'>[Docker] Docker Compose</a> </h2><p class='article__excerpt'>여러 개의 도커 컨테이너를 실행하고 관리하는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 컨테이너(1) ECS",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/container-ecs",
      "date"     : "Feb 6, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-06T21:01:35+09:00'>06 Feb 2024</time><a class='article__image' href='/container-ecs'> <img src='/images/aws_logo.png' alt='[AWS] 컨테이너(1) ECS'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-ecs'>[AWS] 컨테이너(1) ECS</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] keyof / typeof / indexed access type / mapped types",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-keyof-typeof",
      "date"     : "Feb 5, 2024",
      "content"  : "Table of Contents  keyof  typeof  Indexed Access Type  Mapped Typeskeyof  객체타입의 키(key)값으로 이루어진 유니언 타입을 반환한다interface Car {    brand: string    color: string}type CarPropertiesType = keyof Car // &#39;brand&#39; | &#39;color&#39;const brand: CarPropertiesType = &#39;brand&#39;const color: CarPropertiesType = &#39;color&#39;typeof  피연산자의 타입을 반환한다const car: Car = { brand: &#39;Tesla&#39;, color: &#39;red&#39; }typeof car // &#39;object&#39;Indexed Access Type  다른 객체 타입의 프로퍼티의 타입을 타입화하는 것을 말한다type Person = { age: number; name: string; alive: boolean };type Age = Person[&quot;age&quot;]; // numbertype AgeOrName = Person[&quot;age&quot;| &quot;name&quot;]; // number | stringtype KeyofPerson = Person[keyof Person]; // number | string | booleanMapped Types  다른 타입의 속성을 순회하여 새로운 타입을 만드는 것을 말한다      맵드 타입에서 in은 마치 자바스크립트의 for in 문법과 유사하게 동작한다    아래의 예제를 보면, key에는 제네릭 타입 T의 키를 순회하며 name, age, address가 하나씩 들어간다type User = {    name: string    age: number    address: string}type UpdateUser&amp;lt;T&amp;gt; = {    [key in keyof T]?: T[key]}let updateBody: UpdateUser&amp;lt;User&amp;gt;updateBody = {}updateBody = { name: &#39;Park&#39; }  위에서는 옵셔널 연산자(?)를 붙여줌으로써 새로운 UpdateUser 라는 새로운 타입을 정의했다  반대로 옵셔널 연산자를 제거할 수도 있다. -를 붙여주면 해당 연산자를 제거한다는 의미가 된다type RequiredUser&amp;lt;T&amp;gt; = {    [key in keyof T]-?: T[key]}let requiredUser: RequiredUser&amp;lt;UpdateUser&amp;lt;User&amp;gt;&amp;gt;requiredUser = { name: &#39;Lee&#39;, age: 24, address: &#39;LA&#39; }",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-05T21:01:35+09:00'>05 Feb 2024</time><a class='article__image' href='/typescript-keyof-typeof'> <img src='/images/typescript_logo.png' alt='[Typescript] keyof / typeof / indexed access type / mapped types'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-keyof-typeof'>[Typescript] keyof / typeof / indexed access type / mapped types</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "주식 매매 프로젝트",
      "category" : "project",
      "tags"     : "",
      "url"      : "/project-stock-trading",
      "date"     : "Feb 5, 2024",
      "content"  : "Table of Contents  프론트엔드 영역의 등장          HTML      자바스크립트      CSS      Ajax      SPA        리액트의 등장  모던 프론트엔드          Virtual DOM      선언형 UI      컴포넌트 기반      전략적인 렌더링        참고프론트엔드 영역의 등장HTML  HyperText Markup Language  텍스트를 h1, div 와 같은 태그로 구조화 하였다  텍스트로 된 문서에 링크를 입혀 웹 공간에서 문서간 이동을 가능하게 했다자바스크립트  웹 브라우저에 동적인 요소를 구현하기 위해 프로그래밍 언어를 개발했다CSS  텍스트의 서식을 따로 분리하기 위해 만들어졌다Ajax  Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 비동기적으로 받아와 변경된 부분만 재렌더링할 수 있게 되었다SPA출처: Scalable Path  Single Page Application  하나의 HTML 파일(index.html)만 이용해 서비스를 제공하는 어플리케이션을 말한다  하나의 HTML 파일에 변경되는 요소만 재렌더링 하는 방식으로 훨씬 부드러운 사용자 경험을 제공한다  프론트엔드의 랜더링 방식을 페이지 단위의 렌더링에서 컴포넌트 단위의 렌더링으로 변화시켰다  Ajax가 SPA를 촉발시켰으며, SPA의 등장으로 서버는 이제 더이상 전체 HTML 파일을 제공할 필요 없이, 필요한 데이터만 JSON 형식으로 보내면 되게 되었다  이로 인해 서버사이드 진영에서는 자연스럽게 화면을 담당하는 코드와 데이터를 담당하는 코드를 분리하는 MVC 패턴 형태로 코드를 작성하게 되었다  그리고 웹 애플리케이션의 규모가 점점 커지게 되면서, 프론트엔드라는 영역이 따로 분리되어 생겨났다                SPA의 장단점                      장점                      페이지 이동에 있어 유저에게 더 높은 UX 를 제공한다            JSON API 를 통해 느슨한 결합 형태로 설계할 수 있다            필요한 데이터만 요청하면 되므로 서버의 부하가 감소한다                          단점                      유저가 처음 접속시 이후 요소를 만드는데 필요한 모든 자바스크립트 코드를 불러오기 때문에 오래 걸린다            HTML 파일이 데이터로 모두 채워져 있지 않기 때문에 SEO의 성능이 낮다            이후에 알아볼 리액트 또한 SPA 기반 프레임워크인데, 이러한 단점을 서버사이드 렌더링(SSR)으로 보완했다                            리액트의 등장  UI 만을 담당하는 프론트엔드라는 영역이 따로 분리된 데에는 그만큼 프론트엔드 영역의 규모와 복잡성이 커졌기 때문이다  페이스북은 프론트엔드 영역에 프레임워크의 필요성을 느끼고 개발 끝에 2013년 리액트를 세상에 공개했다  리액트는 SPA 방식의 프론트엔드 라이브러리이다  리액트는 모던 프론트엔드 철학이 잘 반영되어있고 이에 필요한 기능들을 제공한다  (리액트에 관한 더 자세한 내용은 이 후 포스트들을 참고한다)                리액트가 라이브러리인 이유                      리액트는 UI에 꼭 필요한 기능들만 가지고 있고, 라우팅, 테스트, 빌드와 같은 부가 요소는 서드파티를 임포트하는 방식으로 사용하도록 설계되었다.        또한 파일 구조, 코드 등을 프레임워크 처럼 강제하지 않는다. 이러한 이유로 리액트를 프레임워크가 아닌 라이브러리라고 한다          모던 프론트엔드  모던 프론트엔드는 인터랙티브하고 사용자 친화적, 미학적인 웹 인터페이스를 만드는데 사용되는 프론트엔드 기술의 현재 경향을 말한다  모던 프론트엔드의 특징은 다음과 같다          리액트에서 제공: Virtual DOM, 선언형 UI, 컴포넌트 기반      next.js에서 제공: 전략적인 렌더링      Virtual DOM  메모리에 가상의 DOM을 구현해놓고, 변경이 발생했을 때 변경된 부분을 실제 DOM에 반영한다  가상 DOM에서의 변화는 렌더링을 유발하지 않아 연산 비용이 낮고, 다수의 변경을 그룹화하여 한 번에 처리할 수도 있어 효율적이다  또한 리액트에서는 개발자가 직접하기 부담스러운 명령적인 DOM 조작, 관리를 프레임워크에 위임한다  Virtual DOM은 렌더링 성능을 개선해준다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다전략적인 렌더링  렌더링은 크게 서버 사이드 렌더링(SSR)과 클라이언트 사이드 렌더링(CSR)으로 나뉜다  SSR          서버에서 완성된 HTML을 클라이언트에게 서빙하는 방식      사용자가 요청한 페이지에서 필요한 모든 데이터를, 서버에서 HTML 파일에 담아 완성된 HTML 파일을 반환한다      초기 랜딩 페이지, 레이아웃, 정적 페이지와 같이 변경이 자주 일어나지 않는 경우에 적합하다        CSR          클라이언트에서 자바스크립트 코드를 실행해 동적으로 HTML 파일을 생성하는 방식      사용자가 요청한 데이터만 서버에서 받아온 후, 변경된 컴포넌트만 클라이언트에서 재렌더링 한다      인터랙티브하며 데이터 변경이 자주 발생하는 경우에 적합하다        next.js 프레임워크는 상황에 적합한 렌더링 방식을 취하는 전략적인 렌더링 기능을 제공한다  전략적인 렌더링은 두 렌더링 방식의 장점을 모두 이용하도록 해준다                SSR과 CSR의 장단점                  SSR                  장점                          초기 사이트 접속시 로딩 시간이 짧다              HTML 파일에 모든 데이터가 담겨있기 때문에 SEO 성능이 좋다                                단점                          사용자가 페이지 이동시마다 전체 렌더링이 일어나기 때문에 낮은 사용자 경험을 제공하며, 서버에 부하가 높아진다                                          CSR                  장점                          전체 렌더링이 아닌 일부만 렌더링 되기 때문에 높은 사용자 경험을 제공하며, 서버 부하를 낮춘다                                단점                          사용자가 처음 사이트 접속시 필요한 모든 자바스크립트 코드를 로드하고, 랜딩 페이지에 해당하는 자바스크립트 코드를 실행해 렌더링해야 하기 때문에, 처음 접속시 페이지 로딩이 느리다              데이터가 포함된 HTML 파일이 클라이언트에 의해 완성되기 때문에, SEO 성능이 낮아진다                                            참고  모던 웹 프론트엔드의 이해, sejinkim  What Is a Single-Page Application (SPA)? Pros &amp;amp; Cons With Examples, Scalable path",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-05T21:01:35+09:00'>05 Feb 2024</time><a class='article__image' href='/project-stock-trading'> <img src='/images/stock_logo_2.jpeg' alt='주식 매매 프로젝트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-stock-trading'>주식 매매 프로젝트</a> </h2><p class='article__excerpt'>이커머스 웹 서비스의 프론트엔드, 백엔드, CI/CD, 모니터링이 포함된 프로젝트</p></div></div></div>"
    } ,
  
    {
      "title"    : "SNS 프로젝트",
      "category" : "project",
      "tags"     : "",
      "url"      : "/project-sns",
      "date"     : "Feb 5, 2024",
      "content"  : "Table of Contents  프론트엔드 영역의 등장          HTML      자바스크립트      CSS      Ajax      SPA        리액트의 등장  모던 프론트엔드          Virtual DOM      선언형 UI      컴포넌트 기반      전략적인 렌더링        참고프론트엔드 영역의 등장HTML  HyperText Markup Language  텍스트를 h1, div 와 같은 태그로 구조화 하였다  텍스트로 된 문서에 링크를 입혀 웹 공간에서 문서간 이동을 가능하게 했다자바스크립트  웹 브라우저에 동적인 요소를 구현하기 위해 프로그래밍 언어를 개발했다CSS  텍스트의 서식을 따로 분리하기 위해 만들어졌다Ajax  Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 비동기적으로 받아와 변경된 부분만 재렌더링할 수 있게 되었다SPA출처: Scalable Path  Single Page Application  하나의 HTML 파일(index.html)만 이용해 서비스를 제공하는 어플리케이션을 말한다  하나의 HTML 파일에 변경되는 요소만 재렌더링 하는 방식으로 훨씬 부드러운 사용자 경험을 제공한다  프론트엔드의 랜더링 방식을 페이지 단위의 렌더링에서 컴포넌트 단위의 렌더링으로 변화시켰다  Ajax가 SPA를 촉발시켰으며, SPA의 등장으로 서버는 이제 더이상 전체 HTML 파일을 제공할 필요 없이, 필요한 데이터만 JSON 형식으로 보내면 되게 되었다  이로 인해 서버사이드 진영에서는 자연스럽게 화면을 담당하는 코드와 데이터를 담당하는 코드를 분리하는 MVC 패턴 형태로 코드를 작성하게 되었다  그리고 웹 애플리케이션의 규모가 점점 커지게 되면서, 프론트엔드라는 영역이 따로 분리되어 생겨났다                SPA의 장단점                      장점                      페이지 이동에 있어 유저에게 더 높은 UX 를 제공한다            JSON API 를 통해 느슨한 결합 형태로 설계할 수 있다            필요한 데이터만 요청하면 되므로 서버의 부하가 감소한다                          단점                      유저가 처음 접속시 이후 요소를 만드는데 필요한 모든 자바스크립트 코드를 불러오기 때문에 오래 걸린다            HTML 파일이 데이터로 모두 채워져 있지 않기 때문에 SEO의 성능이 낮다            이후에 알아볼 리액트 또한 SPA 기반 프레임워크인데, 이러한 단점을 서버사이드 렌더링(SSR)으로 보완했다                            리액트의 등장  UI 만을 담당하는 프론트엔드라는 영역이 따로 분리된 데에는 그만큼 프론트엔드 영역의 규모와 복잡성이 커졌기 때문이다  페이스북은 프론트엔드 영역에 프레임워크의 필요성을 느끼고 개발 끝에 2013년 리액트를 세상에 공개했다  리액트는 SPA 방식의 프론트엔드 라이브러리이다  리액트는 모던 프론트엔드 철학이 잘 반영되어있고 이에 필요한 기능들을 제공한다  (리액트에 관한 더 자세한 내용은 이 후 포스트들을 참고한다)                리액트가 라이브러리인 이유                      리액트는 UI에 꼭 필요한 기능들만 가지고 있고, 라우팅, 테스트, 빌드와 같은 부가 요소는 서드파티를 임포트하는 방식으로 사용하도록 설계되었다.        또한 파일 구조, 코드 등을 프레임워크 처럼 강제하지 않는다. 이러한 이유로 리액트를 프레임워크가 아닌 라이브러리라고 한다          모던 프론트엔드  모던 프론트엔드는 인터랙티브하고 사용자 친화적, 미학적인 웹 인터페이스를 만드는데 사용되는 프론트엔드 기술의 현재 경향을 말한다  모던 프론트엔드의 특징은 다음과 같다          리액트에서 제공: Virtual DOM, 선언형 UI, 컴포넌트 기반      next.js에서 제공: 전략적인 렌더링      Virtual DOM  메모리에 가상의 DOM을 구현해놓고, 변경이 발생했을 때 변경된 부분을 실제 DOM에 반영한다  가상 DOM에서의 변화는 렌더링을 유발하지 않아 연산 비용이 낮고, 다수의 변경을 그룹화하여 한 번에 처리할 수도 있어 효율적이다  또한 리액트에서는 개발자가 직접하기 부담스러운 명령적인 DOM 조작, 관리를 프레임워크에 위임한다  Virtual DOM은 렌더링 성능을 개선해준다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다전략적인 렌더링  렌더링은 크게 서버 사이드 렌더링(SSR)과 클라이언트 사이드 렌더링(CSR)으로 나뉜다  SSR          서버에서 완성된 HTML을 클라이언트에게 서빙하는 방식      사용자가 요청한 페이지에서 필요한 모든 데이터를, 서버에서 HTML 파일에 담아 완성된 HTML 파일을 반환한다      초기 랜딩 페이지, 레이아웃, 정적 페이지와 같이 변경이 자주 일어나지 않는 경우에 적합하다        CSR          클라이언트에서 자바스크립트 코드를 실행해 동적으로 HTML 파일을 생성하는 방식      사용자가 요청한 데이터만 서버에서 받아온 후, 변경된 컴포넌트만 클라이언트에서 재렌더링 한다      인터랙티브하며 데이터 변경이 자주 발생하는 경우에 적합하다        next.js 프레임워크는 상황에 적합한 렌더링 방식을 취하는 전략적인 렌더링 기능을 제공한다  전략적인 렌더링은 두 렌더링 방식의 장점을 모두 이용하도록 해준다                SSR과 CSR의 장단점                  SSR                  장점                          초기 사이트 접속시 로딩 시간이 짧다              HTML 파일에 모든 데이터가 담겨있기 때문에 SEO 성능이 좋다                                단점                          사용자가 페이지 이동시마다 전체 렌더링이 일어나기 때문에 낮은 사용자 경험을 제공하며, 서버에 부하가 높아진다                                          CSR                  장점                          전체 렌더링이 아닌 일부만 렌더링 되기 때문에 높은 사용자 경험을 제공하며, 서버 부하를 낮춘다                                단점                          사용자가 처음 사이트 접속시 필요한 모든 자바스크립트 코드를 로드하고, 랜딩 페이지에 해당하는 자바스크립트 코드를 실행해 렌더링해야 하기 때문에, 처음 접속시 페이지 로딩이 느리다              데이터가 포함된 HTML 파일이 클라이언트에 의해 완성되기 때문에, SEO 성능이 낮아진다                                            참고  모던 웹 프론트엔드의 이해, sejinkim  What Is a Single-Page Application (SPA)? Pros &amp;amp; Cons With Examples, Scalable path",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-05T21:01:35+09:00'>05 Feb 2024</time><a class='article__image' href='/project-sns'> <img src='/images/sns_project_logo.png' alt='SNS 프로젝트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-sns'>SNS 프로젝트</a> </h2><p class='article__excerpt'>이커머스 웹 서비스의 프론트엔드, 백엔드, CI/CD, 모니터링이 포함된 프로젝트</p></div></div></div>"
    } ,
  
    {
      "title"    : "AI 툴 프로젝트",
      "category" : "project",
      "tags"     : "",
      "url"      : "/project-ai-tool",
      "date"     : "Feb 5, 2024",
      "content"  : "Table of Contents  프론트엔드 영역의 등장          HTML      자바스크립트      CSS      Ajax      SPA        리액트의 등장  모던 프론트엔드          Virtual DOM      선언형 UI      컴포넌트 기반      전략적인 렌더링        참고프론트엔드 영역의 등장HTML  HyperText Markup Language  텍스트를 h1, div 와 같은 태그로 구조화 하였다  텍스트로 된 문서에 링크를 입혀 웹 공간에서 문서간 이동을 가능하게 했다자바스크립트  웹 브라우저에 동적인 요소를 구현하기 위해 프로그래밍 언어를 개발했다CSS  텍스트의 서식을 따로 분리하기 위해 만들어졌다Ajax  Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 비동기적으로 받아와 변경된 부분만 재렌더링할 수 있게 되었다SPA출처: Scalable Path  Single Page Application  하나의 HTML 파일(index.html)만 이용해 서비스를 제공하는 어플리케이션을 말한다  하나의 HTML 파일에 변경되는 요소만 재렌더링 하는 방식으로 훨씬 부드러운 사용자 경험을 제공한다  프론트엔드의 랜더링 방식을 페이지 단위의 렌더링에서 컴포넌트 단위의 렌더링으로 변화시켰다  Ajax가 SPA를 촉발시켰으며, SPA의 등장으로 서버는 이제 더이상 전체 HTML 파일을 제공할 필요 없이, 필요한 데이터만 JSON 형식으로 보내면 되게 되었다  이로 인해 서버사이드 진영에서는 자연스럽게 화면을 담당하는 코드와 데이터를 담당하는 코드를 분리하는 MVC 패턴 형태로 코드를 작성하게 되었다  그리고 웹 애플리케이션의 규모가 점점 커지게 되면서, 프론트엔드라는 영역이 따로 분리되어 생겨났다                SPA의 장단점                      장점                      페이지 이동에 있어 유저에게 더 높은 UX 를 제공한다            JSON API 를 통해 느슨한 결합 형태로 설계할 수 있다            필요한 데이터만 요청하면 되므로 서버의 부하가 감소한다                          단점                      유저가 처음 접속시 이후 요소를 만드는데 필요한 모든 자바스크립트 코드를 불러오기 때문에 오래 걸린다            HTML 파일이 데이터로 모두 채워져 있지 않기 때문에 SEO의 성능이 낮다            이후에 알아볼 리액트 또한 SPA 기반 프레임워크인데, 이러한 단점을 서버사이드 렌더링(SSR)으로 보완했다                            리액트의 등장  UI 만을 담당하는 프론트엔드라는 영역이 따로 분리된 데에는 그만큼 프론트엔드 영역의 규모와 복잡성이 커졌기 때문이다  페이스북은 프론트엔드 영역에 프레임워크의 필요성을 느끼고 개발 끝에 2013년 리액트를 세상에 공개했다  리액트는 SPA 방식의 프론트엔드 라이브러리이다  리액트는 모던 프론트엔드 철학이 잘 반영되어있고 이에 필요한 기능들을 제공한다  (리액트에 관한 더 자세한 내용은 이 후 포스트들을 참고한다)                리액트가 라이브러리인 이유                      리액트는 UI에 꼭 필요한 기능들만 가지고 있고, 라우팅, 테스트, 빌드와 같은 부가 요소는 서드파티를 임포트하는 방식으로 사용하도록 설계되었다.        또한 파일 구조, 코드 등을 프레임워크 처럼 강제하지 않는다. 이러한 이유로 리액트를 프레임워크가 아닌 라이브러리라고 한다          모던 프론트엔드  모던 프론트엔드는 인터랙티브하고 사용자 친화적, 미학적인 웹 인터페이스를 만드는데 사용되는 프론트엔드 기술의 현재 경향을 말한다  모던 프론트엔드의 특징은 다음과 같다          리액트에서 제공: Virtual DOM, 선언형 UI, 컴포넌트 기반      next.js에서 제공: 전략적인 렌더링      Virtual DOM  메모리에 가상의 DOM을 구현해놓고, 변경이 발생했을 때 변경된 부분을 실제 DOM에 반영한다  가상 DOM에서의 변화는 렌더링을 유발하지 않아 연산 비용이 낮고, 다수의 변경을 그룹화하여 한 번에 처리할 수도 있어 효율적이다  또한 리액트에서는 개발자가 직접하기 부담스러운 명령적인 DOM 조작, 관리를 프레임워크에 위임한다  Virtual DOM은 렌더링 성능을 개선해준다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다전략적인 렌더링  렌더링은 크게 서버 사이드 렌더링(SSR)과 클라이언트 사이드 렌더링(CSR)으로 나뉜다  SSR          서버에서 완성된 HTML을 클라이언트에게 서빙하는 방식      사용자가 요청한 페이지에서 필요한 모든 데이터를, 서버에서 HTML 파일에 담아 완성된 HTML 파일을 반환한다      초기 랜딩 페이지, 레이아웃, 정적 페이지와 같이 변경이 자주 일어나지 않는 경우에 적합하다        CSR          클라이언트에서 자바스크립트 코드를 실행해 동적으로 HTML 파일을 생성하는 방식      사용자가 요청한 데이터만 서버에서 받아온 후, 변경된 컴포넌트만 클라이언트에서 재렌더링 한다      인터랙티브하며 데이터 변경이 자주 발생하는 경우에 적합하다        next.js 프레임워크는 상황에 적합한 렌더링 방식을 취하는 전략적인 렌더링 기능을 제공한다  전략적인 렌더링은 두 렌더링 방식의 장점을 모두 이용하도록 해준다                SSR과 CSR의 장단점                  SSR                  장점                          초기 사이트 접속시 로딩 시간이 짧다              HTML 파일에 모든 데이터가 담겨있기 때문에 SEO 성능이 좋다                                단점                          사용자가 페이지 이동시마다 전체 렌더링이 일어나기 때문에 낮은 사용자 경험을 제공하며, 서버에 부하가 높아진다                                          CSR                  장점                          전체 렌더링이 아닌 일부만 렌더링 되기 때문에 높은 사용자 경험을 제공하며, 서버 부하를 낮춘다                                단점                          사용자가 처음 사이트 접속시 필요한 모든 자바스크립트 코드를 로드하고, 랜딩 페이지에 해당하는 자바스크립트 코드를 실행해 렌더링해야 하기 때문에, 처음 접속시 페이지 로딩이 느리다              데이터가 포함된 HTML 파일이 클라이언트에 의해 완성되기 때문에, SEO 성능이 낮아진다                                            참고  모던 웹 프론트엔드의 이해, sejinkim  What Is a Single-Page Application (SPA)? Pros &amp;amp; Cons With Examples, Scalable path",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-05T21:01:35+09:00'>05 Feb 2024</time><a class='article__image' href='/project-ai-tool'> <img src='/images/ai_project_logo.png' alt='AI 툴 프로젝트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-ai-tool'>AI 툴 프로젝트</a> </h2><p class='article__excerpt'>이커머스 웹 서비스의 프론트엔드, 백엔드, CI/CD, 모니터링이 포함된 프로젝트</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 타이머 함수",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-timer",
      "date"     : "Feb 5, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-05T21:01:35+09:00'>05 Feb 2024</time><a class='article__image' href='/js-timer'> <img src='/images/js_logo.png' alt='[Javascript]: 타이머 함수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-timer'>[Javascript]: 타이머 함수</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 제네릭 타입",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-generic",
      "date"     : "Feb 4, 2024",
      "content"  : "Table of Contents  제네릭  간단한 제네릭 만들어보기  제네릭 제약  제네릭 타입의 특징제네릭  타입을 변수처럼 동적으로 정하는 방법을 의미한다  타입을 변수화하면 어떤 컴포넌트가 하나의 타입에만 잘 동작하는 것이 아니라, 타입이 동적으로 변할 때 그에 맞게 동작하는 재사용성 높은 컴포넌트를 만들 수 있다간단한 제네릭 만들어보기  타입을 변수화 한다  타입을 T로 변수화 해보자 (T는 x처럼 흔한 표기법중 하나일 뿐 꼭 T로 써야할 필요는 없다)  하나의 값을 받아 배열로 만들어 리턴하는 함수를 만든다고 해보자function arraify&amp;lt;T&amp;gt; (x: T): T[] {    return [x]}arraify&amp;lt;number&amp;gt;(5) // [5]arraify&amp;lt;string&amp;gt;(&#39;apple&#39;) // [&#39;apple&#39;]제네릭 제약  위에서 우리는 T에 number 타입도 할당 가능했고, string 타입도 할당 가능했다  하지만 함수 안에서 x.length와 같이 특정 타입일 때만 가지는 속성을 이용한다면, T에 할당할 수 있는 타입도 그에 맞게 제약을 해줘야 한다function returnLengthFn&amp;lt;T&amp;gt; (x: T): number {    return x.length // &#39;T&#39; 형식에 &#39;length&#39; 속성이 없습니다}  제네릭 타입 T를 length 속성을 가지는 타입으로 제약해보자interface Lengthwise {    length: number;  }function returnLengthFn&amp;lt;T extends Lengthwise&amp;gt; (x: T): number {    return x.length}returnLengthFn&amp;lt;string&amp;gt;(&#39;apple&#39;)returnLengthFn&amp;lt;number&amp;gt;(5) // &#39;number&#39; 형식이 &#39;Lengthwise&#39; 제약 조건을 만족하지 않습니다제네릭 타입의 특징  콤마(,)로 구분해 여러 개의 제네릭 타입을 사용할 수도 있다  keyof와 같은 연산자를 이용할 수도 있다  사용할 때 꼭 타입을 명시하지 않아도 되며, 이 때는 컴파일러가 타입을 추론해 준다  사용할 때 타입을 명시하지 않은 경우, 디폴트를 할당하도록 해줄 수도 있다interface SmartPhonePicker {    a: &#39;Apple&#39;;    s: &#39;Samsung&#39;;    m: &#39;Shaomi&#39;;    n: &#39;Sony&#39;;  }function pickSmartPhone&amp;lt;T extends SmartPhonePicker, K extends keyof T&amp;gt;(picker: T, key: K) {    return picker[key]}const picker: SmartPhonePicker = { a: &#39;Apple&#39;, s: &#39;Samsung&#39;, m: &#39;Shaomi&#39;, n: &#39;Sony&#39; };pickSmartPhone(picker, &#39;a&#39;)pickSmartPhone(picker, &#39;x&#39;) // &#39;&quot;x&quot;&#39; 형식의 인수는 &#39;keyof SmartPhonePicker&#39; 형식의 매개 변수에 할당될 수 없습니다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-04T21:01:35+09:00'>04 Feb 2024</time><a class='article__image' href='/typescript-generic'> <img src='/images/typescript_logo.png' alt='[Typescript] 제네릭 타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-generic'>[Typescript] 제네릭 타입</a> </h2><p class='article__excerpt'>제네릭 타입을 배우고 타입에 따른 동적인 컴포넌트를 만들 수 있다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 클래스 기반의 객체지향 프로그래밍",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-class",
      "date"     : "Feb 4, 2024",
      "content"  : "Table of Contents  클래스  클래스 생성  클래스 상속          extends      implements      abstract        접근 제어자          public      private      protected      readonly      인터페이스와 접근제어자      클래스  자바스크립트는 클래스가 필요없는 프로토타입 기반 객체지향 언어다  ES6부터 자바나 C#과 같은 클래스 기반 객체지향 프로그래밍 언어와 매우 유사한 객체 생성 메커니즘을 제공해줬다  그리고 타입스크립트에서는 여기에 public, private과 같은 접근 제어자와 추상 클래스, 인터페이스를 추가로 도입했다클래스 생성  클래스를 하나 만들어보자  클래스는 타입(type), 인터페이스(interface) 처럼 객체 타입을 정의하는 것을 넘어, 인스턴스를 생성하는 생성자 역할을 할 수 있다는 점이 클래스만의 특징이다class Car {    color: string    velocity: number    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }}const car: Car = new Car(&#39;red&#39;, 50);  주의할 점은, 타입 어노테이션에 Car를 붙이면 이는 클래스 Car가 아니라, 객체 new Car()를 의미한다는 것이다const getInstanceFunc = (car: Car) =&amp;gt; {    console.log(car)}const getClassFunc = (car: typeof Car) =&amp;gt; {    console.log(car)}getInstanceFunc(car)getClassFunc(Car)클래스 상속  타입스크립트에서는 자바스크립트에서 제공하는 extends와, 타입스크립트에서 제공하는 implements 방식으로 상속 관계를 정의할 수 있다  extends는 자바스크립트로 변환된 후에도 남아있지만, implements는 자바스크립트로 변환된 후에는 사라진다  클래스 기반 객체지향 프로그래밍에서 인터페이스는 의존 관계를 나타내는 역할, 클래스는 실제 객체를 생성하는 구현체 역할을 한다  (추상적 형태인 인터페이스로 관계를 나타냄으로써 최대한 의존 관계를 느슨하게 만들어준다)extends  extends는 클래스와 클래스간의 상속 관계 또는 인터페이스와 인터페이스간의 상속 관계를 정의할 때 사용한다class HyundaiCar extends Car {    maxVelocity: number        constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }}const hCar: HyundaiCar = new HyundaiCar(&#39;red&#39;, 50, 120);  HyundaiCar는 Car를 상속받기 때문에, Car로서 갖춰야 할 것들은 모두 갖추고 있다. 그래서 타입 어노테이션을 Car로 해도 괜찮다const hCar: Car = new HyundaiCar(&#39;black&#39;, 50, 130);  인터페이스간의 상속 관계도 나타낼 수 있다interface Car {    color: string    velocity: number}interface HyundaiCar extends Car {    color: string    velocity: number    maxVelocity: number}implements  Car를 인터페이스로 만든 경우 implements를 사용해야 한다  인터페이스는 구현체가 자신의 속성을 모두 구현(implement)하도록 유도한다  implements는 타입스크립트에서 제공하는 기능이기 때문에 자바스크립트로 변환시 사라진다interface Car {    color: string    velocity: number}class HyundaiCar implements Car {    color: string    velocity: number    maxVelocity: number        constructor(color: string, velocity: number, maxVelocity: number) {        this.color = color        this.velocity = velocity        this.maxVelocity = maxVelocity    }}  타입 어노테이션을 Car, HyundaiCar 어떤 것으로 해도 가능하다const hCar: HyundaiCar = new HyundaiCar(&#39;black&#39;, 50, 130);const hCarTwo: Car = new HyundaiCar(&#39;red&#39;, 50, 160);abstract  추상클래스는 클래스지만 인터페이스와 유사하다          인스턴스 생성 역할을 못한다      어떤 클래스가 구현할 때 만족했으면 하는 최소한의 부분을 정의한 클래스        추상 클래스는 특정 부분만 구현하도록 요구할 수 있고, 인터페이스는 모든 부분을 구현하도록 요구한다  구체적으로 구현해줬으면 하는 요소(프로퍼티, 메서드) 앞에 abstract를 붙여준다abstract class Car {    color: string    velocity: number    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }    abstract printMaxVelocity(): void}class HyundaiCar extends Car {    readonly maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }    printMaxVelocity(): void {        console.log(this.maxVelocity)    }}const car: HyundaiCar = new HyundaiCar(&#39;red&#39;, 50, 120)접근 제어자  특정 프로퍼티나 메서드의 접근 가능한 정도를 제어할 수 있다 (접근 가능하다는 의미는 get, set 가능하다는 의미)  public, private, protected, readonly 는 타입스크립트에만 있다  (자바스크립트에서는 private은 #으로 문법적 지원, protected는 _로 암묵적 약속으로 사용하고 있었다)public  자기 자신 클래스 뿐만 아니라 어디서든 접근 가능한 프로퍼티 혹은 메서드  앞에 아무런 접근 제어자 안붙이면 기본적으로 public이다class Car {    color: string    velocity: number    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }}const car: Car = new Car(&#39;red&#39;, 50)  외부에서 읽기/쓰기 가능하다console.log(car.color)hCar.color = &#39;black&#39;private  자기 자신 클래스에서만 접근 가능하다  외부에서는 접근 불가능하다. 즉 값을 읽을수도 쓸 수도 없다class Car {    color: string    private velocity: number    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }}const car: Car = new Car(&#39;red&#39;, 50)  velocity는 private 때문에 외부에서 접근 불가능하다console.log(car.velocity) // &#39;velocity&#39; 속성은 private이며 &#39;Car&#39; 클래스 내에서만 액세스할 수 있습니다  만약 Car를 상속받는 HyundaiCar 클래스가 있으면, HyundaiCar의 velocity 프로퍼티도 private이다class HyundaiCar extends Car {    maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }}const car: HyundaiCar = new HyundaiCar(&#39;red&#39;, 50, 120)car.velocity // &#39;velocity&#39; 속성은 private이며 &#39;Car&#39; 클래스 내에서만 액세스할 수 있습니다.protected  자신을 상속받은 클래스에서만 접근 가능하다  자식 클래스에게만 쓰고 읽을 권한을 준다class Car {    color: string    private velocity: number // private한 경우    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }}class HyundaiCar extends Car {    maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }    printVelocity() {        console.log(this.velocity) // &#39;velocity&#39; 속성은 private이며 &#39;Car&#39; 클래스 내에서만 액세스할 수 있습니다.    }}class Car {    color: string    protected velocity: number // protected한 경우    constructor(c: string, v: number) {        this.color = c        this.velocity = v    }}class HyundaiCar extends Car {    maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }    printVelocity() {        console.log(this.velocity) // 자식 클래스에서 접근 가능하다    }}readonly  내부, 외부에서 모두 쓰기는 불가능하고, 읽기만 가능하다  readonly가 붙은 프로퍼티는 정의와 동시에 할당하거나, 생성자 함수 (constructor) 안에서만 값을 할당할 수 있다class HyundaiCar extends Car {    readonly maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        super(color, velocity)        this.maxVelocity = maxVelocity    }    upgradeMaxVelocity(inc: number) {        console.log(this.maxVelocity) // 이렇게 읽는것만 하는 것은 가능        this.maxVelocity += inc // 쓰기는 불가능 (읽기 전용 속성이므로 &#39;maxVelocity&#39;에 할당할 수 없습니다.)    }}const car: HyundaiCar = new HyundaiCar(&#39;red&#39;, 50, 120)인터페이스와 접근제어자  인터페이스에서는 private, protected 같은 것들을 쓸 수 없다  인터페이스를 구현한 클래스가 있다고 해보자  만약 클래스의 특정 요소에 private을 붙이고 싶다면?  클래스에서만 private을 붙이니까, 똑바로 구현을 안했다고 한다. 그래서 인터페이스에도 private을 붙이니까 인터페이스에는 private을 못 쓴다고 한다interface Car {    color: string    private velocity: number // &#39;private&#39; 한정자는 형식 멤버에 나타날 수 없습니다}// HyundaiCar&#39; 클래스가 &#39;Car&#39; 인터페이스를 잘못 구현합니다. &#39;velocity&#39; 속성은 &#39;HyundaiCar&#39; 형식에서 private이지만 &#39;Car&#39; 형식에서는 그렇지 않습니다.class HyundaiCar implements Car {    color: string    private velocity: number     private maxVelocity: number    constructor(color: string, velocity: number, maxVelocity: number) {        this.color = color        this.velocity = velocity        this.maxVelocity = maxVelocity    }}  private 같은 접근 제어자 쓰고 싶은 경우에는, 인터페이스가 아닌 클래스에 따로 정의 (ex. maxVelocity)  아니면 추상클래스 사용",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-04T21:01:35+09:00'>04 Feb 2024</time><a class='article__image' href='/typescript-class'> <img src='/images/typescript_logo.png' alt='[Typescript] 클래스 기반의 객체지향 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-class'>[Typescript] 클래스 기반의 객체지향 프로그래밍</a> </h2><p class='article__excerpt'>타입스크립트에서 제공하는 클래스 기반 객체지향 프로그래밍에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] Dockerfile(3) COPY ADD VOLUME WORKDIR",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-dockerfile-volume",
      "date"     : "Feb 4, 2024",
      "content"  : "Table of Contents  WORKDIR  VOLUME  COPY  ADD  참고WORKDIR  WORKDIR 인스트럭션은 Docker 파일에서 이어지는 모든 RUN, CMD, ENTRIPOINT, COPY 및 ADD 인스트럭션에 대한 작업 디렉토리를 설정한다  WORKDIR을 명시하지 않은 경우, 베이스 이미지의 WORKDIR을 이어받는다. 베이스 이미지에도 없으면 디폴트로 루트 경로(/)가 된다      명시한 경로가 없으면 경로에 필요한 디렉터리를 만든다    WORKDIR 명령은 Docker 파일에서 여러 번 사용할 수 있다  상대 경로가 제공되는 경우 이전 WORKDIR 명령의 경로에 상대적이다. 예를 들어 다음 RUN 인스트럭션의 작업 디렉토리는 /a/b/c이다WORKDIR /aWORKDIR bWORKDIR cRUN pwd  또한 ENV를 이용해 Dockerfile에서 명시한 환경 변수의 경우 WORKDIR 명령어에서 해석할 수 있다  아래의 $DIRPATH/$DIRNAME은 /path/$DIRNAME으로 해석된다ENV DIRPATH=/pathWORKDIR $DIRPATH/$DIRNAMERUN pwdVOLUME  컨테이너의 마운트 지점을 생성한다  데이터베이스 같은 유상태 애플리케이션의 경우 사용자가 볼륨을 지정하지 않더라도 데이터를 유실하지 않기 위한 안전장치 용도이다VOLUME /dataCOPY  COPY 인스트럭션은 호스트 파일 시스템의 파일을 컨테이너에 복사한다  COPY &amp;lt;source&amp;gt; &amp;lt;target&amp;gt;          source에는 파일 하나가 올 수도 있고, 와일드카드로 여러 파일을 매치할 수도 있다      target에는 절대경로가 올 수도 있고, 상대경로가 올 수도 있다. 상대경로가 올 경우 WORKDIR에 대한 상대경로가 된다      # t로 시작하는 모든 txt파일을 &amp;lt;WORKDIR&amp;gt;/relativeDir/ 로 복사한다COPY t*.txt relativeDir/# test.txt, teso.txt, tesi.txt과 같은 파일을 /absoluteDir/ 로 복사한다COPY tes?.txt /absoluteDir/ADD  COPY 인스트럭션과 비슷하다  ADD는 추가로 URL을 통해 파일을 다운로드할 수 있고, 압축된 파일을 자동으로 추출할 수 있는 기능도 가지고 있다      COPY가 더욱 명료하고 예측 가능하다는 점 때문에 도커에서 권장한다. 특별한 경우에만 ADD를 사용하는 것이 좋다    아래 예시는 big.tar.xz 파일을 http://example.com에서 다운로드하고, /container_directory에 압축 해제하여 추가한다ADD http://example.com/big.tar.xz /container_directory참고  Docker 공식문서  [Docker] Dockerfile의 COPY와 ADD 명령어 비교, 김징어의 Devlog",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-04T21:01:35+09:00'>04 Feb 2024</time><a class='article__image' href='/docker-dockerfile-volume'> <img src='/images/docker_logo.png' alt='[Docker] Dockerfile(3) COPY ADD VOLUME WORKDIR'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-dockerfile-volume'>[Docker] Dockerfile(3) COPY ADD VOLUME WORKDIR</a> </h2><p class='article__excerpt'>도커 이미지를 만드는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 타입 가드",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-type-guard",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  타입 가드          typeof      instanceof      in      동등 연산자      사용자 정의 타입 가드        참고타입 가드  올 수 있는 타입이 여러 가지가 될 수 있는 상황에서 타입을 제한시키는 것을 의미한다  (정확히 한 가지로 제한할 수도 있고, 예를 들어 ‘flying 속성을 가지는 객체 타입’처럼 몇가지로 축약 제한할 수도 있다)typeof  타입스크립트에서 typeof 연산자는 다음중 하나로 타입을 제한할 수 있다  string, number, bigint, boolean, symbol, undefined, object, function// 배열 string[] 을 typeof 연산자 결과는 object 이다 (배열도 객체이므로)const myFunc = (x: string | string[]) =&amp;gt; {    if (typeof x === &#39;string&#39;) {        console.log(x.length)    } else if (typeof x === &#39;object&#39;) {        console.log(x.join(&#39;, &#39;))    }}  typeof 연산자를 이용한 타입 가드에는 한계점이 있다. 객체는 모두 object 타입으로 간주하기 때문에 객체간의 구분이 불가능하다class MyObject {    message: string    constructor(message: string) {        this.message = message    }}// MyObject, string[] 둘 다 typeof 연산자의 결과가 object이기 때문에 구분이 안된다const myFunc = (x: MyObject | string[]) =&amp;gt; {    if (typeof x === &#39;object&#39;) {        console.log(x.message)    }}  이럴 때는 instanceof 를 사용하면 된다instanceof  instanceof 연산자는 좌변의 타입을 좌변 객체의 프로토타입 체인이 우변의 프로토타입을 포함하는 타입으로 제한할 수 있다  object 타입간에도 구분이 가능해진다  instanceof 체크는 런타임에 일어난다. 그래서 우변의 값이 자바스크립트로 변환된 후에도 살아있어야 한다 (type 연산자로 정의하면 런타임에 사라진다)class MyObject {    message: string    constructor(message: string) {        this.message = message    }}// 이렇게 정의하면 에러남. instanceof 연산자는 런타임에 체크하기 때문에// type MyObject = {//     message: string// }const myFunc = (x: MyObject | string[]) =&amp;gt; {    if (x instanceof MyObject) {        console.log(x.message)    } else {        console.log(x.length)    }}  클래스(class) 말고 타입(type)을 쓰고 싶다면 in 연산자를 사용하면 된다type MyObject = {    message: string}const myFunc = (x: MyObject | string[]) =&amp;gt; {    if (&#39;message&#39; in x) {        console.log(x.message)    }}in  어떤 타입의 객체 또는 그 프로토타입 체인이 특정 속성을 가지는 타입으로 제한한다type MyObject = {    message: string}const myFunc = (x: MyObject | string[]) =&amp;gt; {    if (&#39;message&#39; in x) {        console.log(x.message)    }}동등 연산자  ===, !==, ==, != 연산자를 이용해 타입을 제한한다  자바스크립트에서 typeof null은 &#39;object&#39;이다. 그래서 null을 구분할 때도 유용하다type Hyundai = {    brand: &#39;Hyundai&#39;    hyundaiVelocity: number}type Tesla = {    brand: &#39;Tesla&#39;    teslaVelocity: number}const myFunc = (car: Hyundai | Tesla) =&amp;gt; {    if (car.brand == &#39;Hyundai&#39;) {        console.log(car.hyundaiVelocity)    } else if (car.brand == &#39;Tesla&#39;) {        console.log(car.teslaVelocity)    }}const myCar: Hyundai = { brand: &#39;Hyundai&#39;, hyundaiVelocity: 100 }myFunc(myCar)사용자 정의 타입 가드  커스텀 타입, 인터페이스 등의 복잡한 타입은 typeof, instanceof 등을 활용하기 어렵다  이때 사용자 정의 타입 가드를 사용하면 좋다// 사용자 정의 타입 가드const isTesla = (car: Hyundai | Tesla): car is Tesla =&amp;gt; {    return car.brand == &#39;Tesla&#39;}const myFunc = (car: Hyundai | Tesla) =&amp;gt; {    if ( isTesla(car) ) {        console.log(car.teslaVelocity)    } else {        console.log(car.hyundaiVelocity)    }}참고  Typescript Deep Dive  [TypeScript] 타입 가드 (Type Guard), SO’s CODE",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/typescript-type-guard'> <img src='/images/typescript_logo.png' alt='[Typescript] 타입 가드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-type-guard'>[Typescript] 타입 가드</a> </h2><p class='article__excerpt'>타입 가드를 이용해 타입을 제한하는 방법을 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "영어 교육 프로젝트",
      "category" : "project",
      "tags"     : "",
      "url"      : "/project-english-edu",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  프론트엔드 영역의 등장          HTML      자바스크립트      CSS      Ajax      SPA        리액트의 등장  모던 프론트엔드          Virtual DOM      선언형 UI      컴포넌트 기반      전략적인 렌더링        참고프론트엔드 영역의 등장HTML  HyperText Markup Language  텍스트를 h1, div 와 같은 태그로 구조화 하였다  텍스트로 된 문서에 링크를 입혀 웹 공간에서 문서간 이동을 가능하게 했다자바스크립트  웹 브라우저에 동적인 요소를 구현하기 위해 프로그래밍 언어를 개발했다CSS  텍스트의 서식을 따로 분리하기 위해 만들어졌다Ajax  Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 비동기적으로 받아와 변경된 부분만 재렌더링할 수 있게 되었다SPA출처: Scalable Path  Single Page Application  하나의 HTML 파일(index.html)만 이용해 서비스를 제공하는 어플리케이션을 말한다  하나의 HTML 파일에 변경되는 요소만 재렌더링 하는 방식으로 훨씬 부드러운 사용자 경험을 제공한다  프론트엔드의 랜더링 방식을 페이지 단위의 렌더링에서 컴포넌트 단위의 렌더링으로 변화시켰다  Ajax가 SPA를 촉발시켰으며, SPA의 등장으로 서버는 이제 더이상 전체 HTML 파일을 제공할 필요 없이, 필요한 데이터만 JSON 형식으로 보내면 되게 되었다  이로 인해 서버사이드 진영에서는 자연스럽게 화면을 담당하는 코드와 데이터를 담당하는 코드를 분리하는 MVC 패턴 형태로 코드를 작성하게 되었다  그리고 웹 애플리케이션의 규모가 점점 커지게 되면서, 프론트엔드라는 영역이 따로 분리되어 생겨났다                SPA의 장단점                      장점                      페이지 이동에 있어 유저에게 더 높은 UX 를 제공한다            JSON API 를 통해 느슨한 결합 형태로 설계할 수 있다            필요한 데이터만 요청하면 되므로 서버의 부하가 감소한다                          단점                      유저가 처음 접속시 이후 요소를 만드는데 필요한 모든 자바스크립트 코드를 불러오기 때문에 오래 걸린다            HTML 파일이 데이터로 모두 채워져 있지 않기 때문에 SEO의 성능이 낮다            이후에 알아볼 리액트 또한 SPA 기반 프레임워크인데, 이러한 단점을 서버사이드 렌더링(SSR)으로 보완했다                            리액트의 등장  UI 만을 담당하는 프론트엔드라는 영역이 따로 분리된 데에는 그만큼 프론트엔드 영역의 규모와 복잡성이 커졌기 때문이다  페이스북은 프론트엔드 영역에 프레임워크의 필요성을 느끼고 개발 끝에 2013년 리액트를 세상에 공개했다  리액트는 SPA 방식의 프론트엔드 라이브러리이다  리액트는 모던 프론트엔드 철학이 잘 반영되어있고 이에 필요한 기능들을 제공한다  (리액트에 관한 더 자세한 내용은 이 후 포스트들을 참고한다)                리액트가 라이브러리인 이유                      리액트는 UI에 꼭 필요한 기능들만 가지고 있고, 라우팅, 테스트, 빌드와 같은 부가 요소는 서드파티를 임포트하는 방식으로 사용하도록 설계되었다.        또한 파일 구조, 코드 등을 프레임워크 처럼 강제하지 않는다. 이러한 이유로 리액트를 프레임워크가 아닌 라이브러리라고 한다          모던 프론트엔드  모던 프론트엔드는 인터랙티브하고 사용자 친화적, 미학적인 웹 인터페이스를 만드는데 사용되는 프론트엔드 기술의 현재 경향을 말한다  모던 프론트엔드의 특징은 다음과 같다          리액트에서 제공: Virtual DOM, 선언형 UI, 컴포넌트 기반      next.js에서 제공: 전략적인 렌더링      Virtual DOM  메모리에 가상의 DOM을 구현해놓고, 변경이 발생했을 때 변경된 부분을 실제 DOM에 반영한다  가상 DOM에서의 변화는 렌더링을 유발하지 않아 연산 비용이 낮고, 다수의 변경을 그룹화하여 한 번에 처리할 수도 있어 효율적이다  또한 리액트에서는 개발자가 직접하기 부담스러운 명령적인 DOM 조작, 관리를 프레임워크에 위임한다  Virtual DOM은 렌더링 성능을 개선해준다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다전략적인 렌더링  렌더링은 크게 서버 사이드 렌더링(SSR)과 클라이언트 사이드 렌더링(CSR)으로 나뉜다  SSR          서버에서 완성된 HTML을 클라이언트에게 서빙하는 방식      사용자가 요청한 페이지에서 필요한 모든 데이터를, 서버에서 HTML 파일에 담아 완성된 HTML 파일을 반환한다      초기 랜딩 페이지, 레이아웃, 정적 페이지와 같이 변경이 자주 일어나지 않는 경우에 적합하다        CSR          클라이언트에서 자바스크립트 코드를 실행해 동적으로 HTML 파일을 생성하는 방식      사용자가 요청한 데이터만 서버에서 받아온 후, 변경된 컴포넌트만 클라이언트에서 재렌더링 한다      인터랙티브하며 데이터 변경이 자주 발생하는 경우에 적합하다        next.js 프레임워크는 상황에 적합한 렌더링 방식을 취하는 전략적인 렌더링 기능을 제공한다  전략적인 렌더링은 두 렌더링 방식의 장점을 모두 이용하도록 해준다                SSR과 CSR의 장단점                  SSR                  장점                          초기 사이트 접속시 로딩 시간이 짧다              HTML 파일에 모든 데이터가 담겨있기 때문에 SEO 성능이 좋다                                단점                          사용자가 페이지 이동시마다 전체 렌더링이 일어나기 때문에 낮은 사용자 경험을 제공하며, 서버에 부하가 높아진다                                          CSR                  장점                          전체 렌더링이 아닌 일부만 렌더링 되기 때문에 높은 사용자 경험을 제공하며, 서버 부하를 낮춘다                                단점                          사용자가 처음 사이트 접속시 필요한 모든 자바스크립트 코드를 로드하고, 랜딩 페이지에 해당하는 자바스크립트 코드를 실행해 렌더링해야 하기 때문에, 처음 접속시 페이지 로딩이 느리다              데이터가 포함된 HTML 파일이 클라이언트에 의해 완성되기 때문에, SEO 성능이 낮아진다                                            참고  모던 웹 프론트엔드의 이해, sejinkim  What Is a Single-Page Application (SPA)? Pros &amp;amp; Cons With Examples, Scalable path",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/project-english-edu'> <img src='/images/english_project_logo.png' alt='영어 교육 프로젝트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-english-edu'>영어 교육 프로젝트</a> </h2><p class='article__excerpt'>이커머스 웹 서비스의 프론트엔드, 백엔드, CI/CD, 모니터링이 포함된 프로젝트</p></div></div></div>"
    } ,
  
    {
      "title"    : "이커머스 프로젝트",
      "category" : "project",
      "tags"     : "",
      "url"      : "/project-ecommerce",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  프로젝트 소개  기획 배경  개발 환경  프로젝트 아키텍처  IA  UI 예시          메인 화면      로그인/회원가입 화면        ERD  시연 영상  URL프로젝트 소개  제목: 의류 이커머스 프로젝트  기간: 2024 3월 12일 ~  기획 배경  웹서비스의 대표적인 분야인 이커머스를 주제로 프로젝트를 만들었다  사용자 검색어 기반으로 스마트한 검색 결과를 제공한다 (ex. 바지 기장이 100cm인 검은색 긴 바지)  옵션 선택에 따라 다른 이미지를 제공한다  후불 결제개발 환경  프론트엔드(FE)          Next.js        백엔드(BE):          NestJS        CI/CD:          Docker      AWS ECR      AWS EKS      Github      Github Action      프로젝트 아키텍처IAUI 예시메인 화면로그인/회원가입 화면ERD시연 영상URL  깃허브 링크:  노션 링크:",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/project-ecommerce'> <img src='/images/ecommerce_logo.jpeg' alt='이커머스 프로젝트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-ecommerce'>이커머스 프로젝트</a> </h2><p class='article__excerpt'>이커머스 웹 서비스의 프론트엔드, 백엔드, CI/CD, 모니터링이 포함된 프로젝트</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Next.js]: 렌더링",
      "category" : "frontend",
      "tags"     : "nextjs",
      "url"      : "/nextjs-rendering",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  DTO          DTO 사용에 필요한 라이브러리      DTO 만들기      DTO  Data Transfer Object의 약자  들어오고 나가는 데이터의 형태를 애플리케이션에서 규격화한 클래스(DTO)에 맞는지 검증하는 데이터 검증 방식  (DTO는 Nest에서 정해진 형태로 제공하는 구성요소는 아니지만, 여러 백엔드 애플리케이션에서 데이터 검증 목적으로 많이 사용)DTO 사용에 필요한 라이브러리  Nest에서 DTO를 적용하려면 아래의 라이브러리를 설치해야함  class-validator: DTO 클래스에서 프로퍼티를 검증할 때 사용할 유용한 데코레이터를 제공해줌  class-transformer: plain json과 class object 간의 직렬화/역직렬화에 유용한 기능을 제공? Nest에서 제공하는 빌트인 파이프인 ValidationPipe가 직렬화/역직렬화 기능을 함. 그래서 class-transformer가 꼭 필요한 건 아닌 것 같음. (하지만 여전히 다른 유용한 기능도 많이 제공하기 때문에 설치해두면 좋음)npm i class-validator class-transformerDTO 만들기// create-cat.dto.tsimport { IsString, IsInt } from &#39;class-validator&#39;;export class CreateCatDto {  @IsString()  name: string;  @IsInt()  age: number;  @IsString()  breed: string;}// main.tsconst app = await NestFactory.create(AppModule);app.useGlobalPipes(new ValidationPipe())// controller.ts@Post()create(@Body() createCatDto: CreateCatDto) {  ...}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/nextjs-rendering'> <img src='/images/next_logo.png' alt='[Next.js]: 렌더링'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nextjs-rendering'>[Next.js]: 렌더링</a> </h2><p class='article__excerpt'>Next.js의 렌더링 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] Dockerfile(2) RUN CMD ENTRYPOINT",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-dockerfile-command",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  shell form exec form          exec form      shell form        RUN  CMD  ENTRYPOINT  정리shell form exec formRUN, CMD, 그리고 ENTRYPOINT 인스트럭션은 두 가지 작성법이 있다  exec form: INSTRUCTION [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]      shell form: INSTRUCTION command param1 param2    The shell form automatically uses a command shell, whereas the exec form does not.exec form  안의 값들은 반드시 쌍따옴표로 감싸줘야 한다  shell processing이 안 일어나기 때문에 $HOME 과 같은 변수의 치환이 발생하지 않는다. 필요하면 쉘을 직접 표기해야 한다 ([ &quot;sh&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ])shell form  shell form은 자동으로 커맨드 쉘(/bin/sh -c)을 적용한다  백슬래쉬(╲)로 하나의 인스트럭션을 여러 줄에 걸쳐 쓸 수 있다RUN source $HOME/.bashrc &amp;amp;&amp;amp; ╲echo $HOME# They&#39;re equivalent to the following line:RUN source $HOME/.bashrc &amp;amp;&amp;amp; echo $HOME  SHELL 인스트럭션으로 기본 쉘을 바꿀 수 있다SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]RUN echo helloRUNRUN apt-get updateRUN apt-get install -y curl  RUN 인스트럭션은 빌드 단계에 실행할 명령어를 정의한다  RUN 인스트럭션은 새로운 레이어를 만든다  shell form이 주로 사용된다  주로 패키지 설치, 빌드 명령어 등에 사용된다  Dockerfile에서 처음 빌드될 때 실행된 RUN 인스트럭션은 캐시되었다가 다음 빌드 때 사용된다. 캐시를 무효화 시키려면 docker build --no-cache 이런 식으로 --no-cache 플래그를 써줘야 한다CMD  CMD 인스트럭션은 컨테이너가 실행될 때 실행할 명령어를 정의한다  CMD 인스트럭션은 Dockerfile에 하나만 있을 수 있다  CMD의 주목적은 실행중인 컨테이너에 디폴트를 제공하기 위함이다  컨테이너가 생성될 때 실행된다 (stop되었다가 start될 때는 실행되지 않는다)  디폴트에는 실행 가능한 것도 포함될 수도 있고, 생략한다면 ENTRYPOINT 인스트럭션에 반드시 포함시켜야 한다  CMD의 디폴트는 docker run에 의해 덮어씌어질 수도 있다  CMD가 ENTRYPOINT에 디폴트를 제공하기 위한 용도로 사용된다면 둘 다 exec form으로 작성해야 한다ENTRYPOINT  컨테이너가 실행될 때마다 실행된다 (stop되었다가 start 될 때마다 실행된다)  exec form이 선호된다 (shell form 쓰면 ₩ 인스트럭션이 사용되지 않는다)  ENTRYPOINT 인스트럭션은 CMD나 docker run과 다르게 덮어씌어지지 않는다정리  빌드 타임에만 필요한 명령어는 RUN 인스트럭션에 shell form으로 정의한다  메인 프로세스와 같은 컨테이너가 실행될 때마다 같이 실행되어야할 명령어는 ENTRYPOINT에 exec form으로 정의한다  디폴트로 전달하고 싶은 인자는 CMD에 exec form으로 정의한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/docker-dockerfile-command'> <img src='/images/docker_logo.png' alt='[Docker] Dockerfile(2) RUN CMD ENTRYPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-dockerfile-command'>[Docker] Dockerfile(2) RUN CMD ENTRYPOINT</a> </h2><p class='article__excerpt'>도커 이미지를 만드는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] Dockerfile(1) FROM LABEL ARG ENV",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-dockerfile-arg",
      "date"     : "Feb 3, 2024",
      "content"  : "Table of Contents  도커 이미지 만들기  Dockerfile  FROM  LABEL  ARG  ENV도커 이미지 만들기  도커 이미지는 우리가 원하는 애플리케이션을 실행하는데 필요한 변수, 명령어, 파일 등이 정의되어 있는 프로그램과 같다  도커 컨테이너는 도커 이미지가 실행된 프로세스와 같다Dockerfile  도커 이미지는 Dockerfile 이라는 파일로 만들 수 있다  Dockerfile에 도커 이미지를 만들기 위한 인스트럭션을 작성한 후, docker build 명령어를 이용하면 도커 이미지가 만들어진다  도커 인스트럭션 목록은 아래와 같다            Instruction      Description              ADD      Add local or remote files and directories.              ARG      Use build-time variables.              CMD      Specify default commands.              COPY      Copy files and directories.              ENTRYPOINT      Specify default executable.              ENV      Set environment variables.              EXPOSE      Describe which ports your application is listening on.              FROM      Create a new build stage from a base image.              HEALTHCHECK      Check a container’s health on startup.              LABEL      Add metadata to an image.              ONBUILD      Specify instructions for when the image is used in a build.              RUN      Execute build commands.              SHELL      Set the default shell of an image.              STOPSIGNAL      Specify the system call signal for exiting a container.              USER      Set user and group ID.              VOLUME      Create volume mounts.              WORKDIR      Change working directory.      FROMFROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt;[:&amp;lt;tag&amp;gt;] [AS &amp;lt;name&amp;gt;]  FROM은 빌드를 위한 stage를 초기화하고 이후의 인스트럭션을 위한 기본 이미지를 설정한다  그렇기 때문에 유효한 Dockerfile은 반드시 FROM 명령어로부터 시작해야 한다  AS 뒤에 이름을 설정함으로써 이후 나오는 FROM 인스트럭션이 만드는 stage에 COPY --from=&amp;lt;name&amp;gt; 인스트럭션을 통해 데이터를 전달할 수 있다  이 때 각각의 FROM은 이전 인스트럭션이 만든 상태를 없앤다FROM &amp;lt;image&amp;gt; AS apple...FROM &amp;lt;image2&amp;gt;...# &amp;lt;image&amp;gt;의 빌드 결과로 생성된 파일 중 원하는 파일만 복사COPY --from=apple /dir/you/want/from/apple /dir/of/image2  FROM 앞에 올 수 있는 유일한 인스트럭션은 ARG로, ARG는 이미지 빌드 시간동안 사용될 임시 변수를 저장할 수 있다ARG  CODE_VERSION=latestFROM base:${CODE_VERSION}CMD  /code/run-appLABELLABEL &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; ...  LABEL 인스트럭션은 이미지에 메타데이터를 추가하기 위해 사용된다LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; ╲com.example.label-with-value=&quot;foo&quot; ╲version=&quot;1.0&quot; ARGARG &amp;lt;name&amp;gt;[=&amp;lt;default value&amp;gt;]  이미지 빌드 단계에서 사용될 변수를 정의한다ARG author=kim  docker build --build-arg author=lee 이런식으로 빌드 명령어에서 덮어쓸 수도 있다  ENV 인스트럭션에서 같은 이름의 변수를 정의하면, ENV가 덮어쓰게 된다ENVENV &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt; ...  환경변수를 정의한다  지정한 환경변수는 빌드 단계와 컨테이너 실행 단계에서 사용 가능한 변수다ENV MY_NAME=&quot;John Doe&quot;ENV MY_CAT=fluffy",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-03T21:01:35+09:00'>03 Feb 2024</time><a class='article__image' href='/docker-dockerfile-arg'> <img src='/images/docker_logo.png' alt='[Docker] Dockerfile(1) FROM LABEL ARG ENV'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-dockerfile-arg'>[Docker] Dockerfile(1) FROM LABEL ARG ENV</a> </h2><p class='article__excerpt'>도커 이미지를 만드는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] 타입 기초편",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-type",
      "date"     : "Feb 2, 2024",
      "content"  : "Table of Contents  타입  기초 타입          숫자      문자열      불린      배열      튜플        부가적인 타입          any      unknown      never      void        집합 연산자를 통해 확장된 새로운 타입          union      intersection        null과 undefined          null      undefined        타입 지정할 때의 팁  참고타입  타입의 범위는 좁힐수록 좋다  타입스크립트 컴파일러는 확실하게 타입을 알 수 있는 경우에는 자동으로 타입을 추론(Type Inference)한다  타입은 변수, 함수의 매개변수, 함수의 반환 값에 지정한다  :을 이용해 코드에 타입을 정의하는 방식을 타입 표기(Type Annotation)라고 한다기초 타입  숫자는 number, 문자열은 string, 불린은 boolean 타입이다  String, Number, Boolean와 같은 (대문자로 시작하는) 타입은 유효한 타입이지만, 코드상에서 이러한 특수 내장 타입을 사용하는 경우는 극히 드뭅니다. 항상 string, number, boolean 타입을 사용하세요.숫자  숫자 타입은 number로 표기한다      잘 쓰진 않지만 더 구체적으로 표기하기 위해 3, 5 처럼 리터럴로 표기할 수도 있다.    아래와 같이 변수 x에 3을 할당한다고 해보자const x = 3;  여기에 타입을 지정할 필요가 없다. 컴파일러가 x의 타입은 number이며, 더 정확히는 리터럴 3이라는 것을 추론할 수 있기 때문이다// 이렇게 표기해도 되지만, 어차피 컴파일러가 타입 추론을 하기 때문에 굳이 이렇게 적을 필요는 없다const x: 3 = 3;// 타입은 좁힐수록 좋다는 사실을 생각해보면, 이 방식은 잘못된 방식이다. 리터럴 3으로 추론할 수 있는데 굳이 범위를 number로 넓히는 것은 바람직 하지 않다const x: number = 3;  let 이라는 키워드로 재할당 가능한 변수를 만든 경우에는 타입을 어떻게 지정해야 할까?  여기서 자바스크립트에서는 오류가 안나지만, 타입스크립트에서는 오류가 발생하는 부분이 있다let count = 0; // number로 추론// 자바스크립트에서는 이렇게 해도 오류가 생기지 않지만 타입스크립트는 number에 string을 할당했기 때문에 에러가 난다count = &#39;1&#39;  반면 아래는 자바스크립트, 타입스크립트에서 모두 오류가 안난다let count; // any로 추론count = 0;count = &#39;1&#39;;  하지만 이러한 코드는 문맥상으로도 결코 바람직하지 않다. 그래서 최종적으로 아래와 같은 방법으로 코드를 작성하는게 좋다let count = 0;count = 1;// 또는let count: number;count = 0;count = 1;문자열  문자열 타입은 string으로 표기한다const menu = &#39;chicken&#39;; // 문자열 리터럴 &#39;chicken&#39;으로 타입 추론let menu = &#39;chicken&#39;; // 재할당 가능하기 때문에 string으로 타입 추론let menu; // any로 타입 추론let menu: string; // string으로 타입 추론불린  불린 타입은 boolean으로 표기한다const isGood = true; // 불린 리터럴 true로 타입 추론let isGood = true; // 재할당 가능하기 때문에 boolean으로 타입 추론let isGood; // any로 타입 추론let isGood: boolean; // boolean으로 타입 추론배열  Array&amp;lt;원소 타입&amp;gt; (ex. Array&amp;lt;number&amp;gt;) 또는 원소 타입[] (ex. number[])로 표기한다  (&amp;lt;&amp;gt;는 제네릭 표기법이다)  (자바스크립트에서 배열을 typeof 해도 object가 나오지만, 타입스크립트에서는 object가 아니다)let arr: number[] = [];arr = [1, 2, 3];arr.push(4);arr = [&#39;1&#39;, &#39;2&#39;, &#39;3&#39;]; // 안됨arr.push(&#39;4&#39;); // 안됨튜플  튜플은 길이가 고정되고 각 요소의 타입이 지정되어 있는 배열이다  [첫 번째 요소의 타입, 두 번째 요소의 타입, ...](ex. [string, number]) 이런식으로 표기한다let arr: [string, number];arr = [&#39;hello&#39;, 3];arr = [&#39;hello&#39;]; // 안됨arr = [&#39;hello&#39;, 3, &#39;foo&#39;]; // 안됨부가적인 타입any  any는 타입 검사를 원하지 않는 경우에 사용한다  어떠한 타입의 값을 할당하더라도 에러가 발생하지 않는다  코드를 작성하면서 지금 당장 타입을 모르겠으나 당장의 컴파일러 에러를 면하고 싶은 경우, 아니면 정말 어떠한 타입이 와도 상관없는 경우에 사용한다  처음에 코드를 작성할 때 any로 모두 작성하고, 점진적으로 타입을 지정하는 식으로 개발하기도 한다let myVariable: any = 1;myVariable = &#39;chicken&#39;;myVariable = truemyVariable = [1, 2, 3]myVariable.foo() // 이런 잘못된 코드를 사전에 잡아내지 못한다unknown  지금은 타입을 알 수 없지만, 나중에 사용할 때는 반드시 타입을 지정하도록 하고 싶은 경우에 쓴다let unknownVar: unknown;unknownVar = 1; // 이렇게 1을 할당해도 여전히 컴파일러는 타입을 unknown으로 추론한다unknownVar.toString() // unknown 타입이기 때문에 에러가 난다  아래와 같은 방법으로 타입을 명확하게 할 수 있다let unknownVar: unknown;unknownVar = &#39;abc&#39; // 여전히 unknown 타입이다// 이렇게 타입 가드 방식으로 타입을 명확하게 할 수 있다if (typeof unknownVar === &#39;number&#39;) {    unknownVar.toString()} else if (typeof unknownVar === &#39;string&#39;) {    unknownVar.length}// 또는 as 를 써서 타입을 명확하게 할 수도 있다 (이를 Type Assertion 이라고 함)(unknownVar as string).lengthnever  never는 어떠한 값도 올 수 없는 영역으로 집합으로 치면 공집합과 같다  &#39;apple&#39;은 string 이라는 영역에 속하고, 15는 number 라는 영역에 속한다. 어떤 영역에도 속할 수 없으면 그 값은 논리적으로 never 영역에 속한다// foo 함수는 인자로 string 또는 number만 받을 수 있다// 근데 if 문으로 string을 받고, else if 문으로 number를 받고나면, else 문으로는 갈 수 있는 값이 없다// 이러한 영역에 존재하는 값의 타입을 never라고 한다function foo(param:string | number){    if (typeof param===&#39;string&#39;) {        console.log(&quot;문자열입니다.&quot;);    }    else if (typeof param===&#39;number&#39;) {        console.log(&quot;숫자입니다.&quot;);    }    else {        ...    }}      never가 필요한 이유는 무엇일까?        어떤 값도 올 수 없게 막아놓는 역할을 할 수 있다  type Car = {    runningVelocity: number    flyingVelocity?: never // flyingVelocity 라는 값을 못 가지도록 막아놓는다}type Airplane = {    flyingVelocity: number    runningVelocity?: never // runningVelocity 라는 값을 못 가지도록 막아놓는다}let vehicle: Car | Airplanevehicle = {    runningVelocity: 250,    // flyingVelocity: 300}vehicle = {    // runningVelocity: 250,    flyingVelocity: 300}  어떤 값도 절대 반환할 수 없음을 의미하는 역할을 할 수 있다// 이 함수는 뭔가를 반환하기도 전에 에러를 던진다// --&amp;gt; 뭔가를 절대 반환할 수 없다// --&amp;gt; 이럴 때 리턴 타입으로 never를 쓸 수 있다// (아래와 같이 표현식으로 나타내면 컴파일러가 알아서 never를 추론한다)// (선언문으로 나타내면 컴파일러가 void로 추론한다 이럴 때는 명시적으로 never를 표기해주는게 좋다)const returnErrorFunc = () =&amp;gt; {    throw new Error(&#39;error!&#39;)}// 이 함수는 뭔가를 반환하지 못하고 평생 반복문을 돈다// 이런 함수의 리턴 타입도 never에 해당한다const infiniteLoopFunc = () =&amp;gt; {    while (true) {        const x = 1;    }}void  함수가 반환하는 값이 없음을 의미한다// 아래의 두 함수는 모두 컴파일러가 리턴 타입으로 void를 추론한다// 자바스크립트는 리턴 타입을 undefined로 간주한다const returnVoidFunc = () =&amp;gt; {    return}const returnVoidFunc = () =&amp;gt; {}집합 연산자를 통해 확장된 새로운 타입union  제시한 타입중 하나 이상의 타입을 만족하는 타입을 나타낸다  파이프(|) 기호를 이용해 표시한다 (ex. const myFavoriteCar = Hyundai | BMW | Tesla)type Hyundai = {    hyundaiThing: &#39;hyundai&#39;}type Tesla = {    teslaThing: &#39;tesla&#39;}type BMW = {    BMWThing: &#39;bmw&#39;}type MyFavoriteCarType = Hyundai | Tesla | BMW;let myFavoriteCar: MyFavoriteCarTypemyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;} // OKmyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;, teslaThing: &#39;tesla&#39;} // OKmyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;, teslaThing: &#39;tesla&#39;, BMWThing: &#39;bmw&#39; } // OKintersection  제시한 모든 타입을 만족하는 타입을 나타낸다  &amp;amp; 기호를 이용한다 (ex. const myFavoriteCar = Hyundai &amp;amp; BMW &amp;amp; Tesla)type MyFavoriteCarType = Hyundai &amp;amp; Tesla &amp;amp; BMW;let myFavoriteCar: MyFavoriteCarTypemyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;} // XmyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;, teslaThing: &#39;tesla&#39;} // XmyFavoriteCar = { hyundaiThing: &#39;hyundai&#39;, teslaThing: &#39;tesla&#39;, BMWThing: &#39;bmw&#39; } // OKnull과 undefinednullundefined타입 지정할 때의 팁  우선 타입 추론할 수 있는지 확인하기 위해 그냥 자바스크립트로 코드를 쓴다  컴파일러가 알아서 추론했다면 마우스를 식별자 위에 올려 컴파일러가 어떻게 추론했는지 확인한다  더 좁히고 싶은 경우에는 별도로 타입을 지정해 타입을 좁힌다  컴파일러가 추론에 실패했으면 우리가 직접 타입을 지정해주면 된다참고  타입스크립트의 Never 타입 완벽 가이드, TOAST UI  TS 탐구생활 - TS의 never 타입, Witch-Work",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-02T21:01:35+09:00'>02 Feb 2024</time><a class='article__image' href='/typescript-type'> <img src='/images/typescript_logo.png' alt='[Typescript] 타입 기초편'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-type'>[Typescript] 타입 기초편</a> </h2><p class='article__excerpt'>타입스크립트에서 타입을 지정하는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Typescript] Intro",
      "category" : "language",
      "tags"     : "typescript",
      "url"      : "/typescript-intro",
      "date"     : "Feb 2, 2024",
      "content"  : "Table of Contents  타입스크립트란  타입스크립트의 특징  타입스크립트 설정타입스크립트란  타입스크립트는 자바스크립트의 동적 타입 언어라는 특징 때문에 생겨나는 단점들을 보완하기 위해 등장한 언어이다  타입스크립트는 자바스크립트에 타입을 부여한 자바스크립트의 확장편이라고 할 수 있다  타입스크립트를 사용함으로 얻게되는 장점들은 다음과 같다          정적 타입 검사: 코드를 실행하기 전 개발 단계에서 오류를 발견할 수 있다      코드 자동 완성: VScode, WebStorm 등과 같은 에디터를 사용하면 특정 부분에 어떤 코드를 쓸 수 있는지 알 수 있다      가이드: 코드를 읽는 사람의 입장에서 코드에서 사용되는 값들이 어떤 타입이고 어떤 특징을 가지는지 쉽게 이해할 수 있다      타입스크립트의 특징  타입스크립트는 자바스크립트 코드를 작성하기 위한 수단일 뿐, 실제로 브라우저 또는 node.js에서 동작하는 코드는 자바스크립트이다. 그래서 타입스크립트는 결국 자바스크립트로 변환되어야 한다. 이 때 사용하는 것이 타입스크립트 컴파일러(tsc)이다  변환된 자바스크립트 코드를 보면, 타입스크립트에서 작성한 코드들이 일부 남아있는 것도 있고, 사라지는 것들도 있다.타입스크립트 설정  타입스크립트에는 설정 가능한 여러가지 값들이 있는데, 이를 위해 타입스크립트는 tsconfig.json 파일을 사용한다 (tsc --init 명령어로 생성)  tsconfig.json 파일에는, 정적 타입을 얼마나 엄격하게 검사할지, 자바스크립트 어떤 버전으로 변환할지, 변환한 파일은 어디 저장할지 등을 지정할 수 있다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-02T21:01:35+09:00'>02 Feb 2024</time><a class='article__image' href='/typescript-intro'> <img src='/images/typescript_logo.png' alt='[Typescript] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/typescript-intro'>[Typescript] Intro</a> </h2><p class='article__excerpt'>타입스크립트란 무엇이고 타입스크립트의 특징, 타입스크립트를 사용함으로써 얻게되는 이점에 대해 공부한다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Next.js]: 라우팅",
      "category" : "frontend",
      "tags"     : "nextjs",
      "url"      : "/nextjs-routing",
      "date"     : "Feb 2, 2024",
      "content"  : "Table of Contents  DTO          DTO 사용에 필요한 라이브러리      DTO 만들기      DTO  Data Transfer Object의 약자  들어오고 나가는 데이터의 형태를 애플리케이션에서 규격화한 클래스(DTO)에 맞는지 검증하는 데이터 검증 방식  (DTO는 Nest에서 정해진 형태로 제공하는 구성요소는 아니지만, 여러 백엔드 애플리케이션에서 데이터 검증 목적으로 많이 사용)DTO 사용에 필요한 라이브러리  Nest에서 DTO를 적용하려면 아래의 라이브러리를 설치해야함  class-validator: DTO 클래스에서 프로퍼티를 검증할 때 사용할 유용한 데코레이터를 제공해줌  class-transformer: plain json과 class object 간의 직렬화/역직렬화에 유용한 기능을 제공? Nest에서 제공하는 빌트인 파이프인 ValidationPipe가 직렬화/역직렬화 기능을 함. 그래서 class-transformer가 꼭 필요한 건 아닌 것 같음. (하지만 여전히 다른 유용한 기능도 많이 제공하기 때문에 설치해두면 좋음)npm i class-validator class-transformerDTO 만들기// create-cat.dto.tsimport { IsString, IsInt } from &#39;class-validator&#39;;export class CreateCatDto {  @IsString()  name: string;  @IsInt()  age: number;  @IsString()  breed: string;}// main.tsconst app = await NestFactory.create(AppModule);app.useGlobalPipes(new ValidationPipe())// controller.ts@Post()create(@Body() createCatDto: CreateCatDto) {  ...}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-02T21:01:35+09:00'>02 Feb 2024</time><a class='article__image' href='/nextjs-routing'> <img src='/images/next_logo.png' alt='[Next.js]: 라우팅'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nextjs-routing'>[Next.js]: 라우팅</a> </h2><p class='article__excerpt'>Next.js의 라우팅 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] 도커 간단하게 시작해보기",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-docker-run",
      "date"     : "Feb 2, 2024",
      "content"  : "Table of Contents#",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-02T21:01:35+09:00'>02 Feb 2024</time><a class='article__image' href='/docker-docker-run'> <img src='/images/docker_logo.png' alt='[Docker] 도커 간단하게 시작해보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-docker-run'>[Docker] 도커 간단하게 시작해보기</a> </h2><p class='article__excerpt'>도커를 이용해 컨테이너를 실행할 때 자주 사용하는 명령어를 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Next.js] Intro",
      "category" : "frontend",
      "tags"     : "nextjs",
      "url"      : "/nextjs-intro",
      "date"     : "Feb 1, 2024",
      "content"  : "Table of Contents  네트워크네트워크네트워크는 서로를 연결시켜 놓은 망이라고 할 수 있습니다. 그렇다고 연결만 해놓고 끝내면 되는 것은 아니고, 연결된 장비들끼리 대화(통신)를 할 수 있어야 합니다. 왜 이런 개념이 등장했을까요? 그 이유는 바로 자원을 공유하고 싶어서 였습니다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-01T21:01:35+09:00'>01 Feb 2024</time><a class='article__image' href='/nextjs-intro'> <img src='/images/next_logo.png' alt='[Next.js] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nextjs-intro'>[Next.js] Intro</a> </h2><p class='article__excerpt'>Next.js의 특징에 대해 간단히 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] Intro",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/docker-intro",
      "date"     : "Feb 1, 2024",
      "content"  : "Table of Contents  도커  도커의 장점  도커의 구성요소          도커 클라이언트      도커 데몬      도커 레지스트리        이미지와 컨테이너          이미지      컨테이너      도커  컨테이너 기술은 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해준다  컨테이너 기술은 운영체제를 제외한 애플리케이션 실행에 필요한 모든 파일(라이브러리, 바이너리, 구성 파일 등)을 패키지로 묶어 배포하는 것이다  도커 이전에도 이러한 컨테이너 기술은 존재했었지만 기술적으로 높은 진입 장벽 때문에 대중화되지 못하고 있었다  도커의 등장으로 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되면서 컨테이너 기술이 대중화 되었다도커의 장점  도커는 애플리케이션을 컨테이너화 하도록 도와주고, 컨테이너를 실행시킬 런타임을 제공해준다  도커 컨테이너는 별도의 게스트 OS 없이, 호스트 머신 커널의 namespace와 cgroups를 이용해 독립적인 OS와 같은 환경을 제공하기 때문에 훨씬 빠르고 가볍다도커의 구성요소  도커의 아키텍처는 클라이언트-서버 아키텍처이다도커 클라이언트  도커 클라이언트(docker)는 REST API를 사용해 도커 데몬(dockerd)에게 도커 관련 명령어를 전달할 수 있다  사용자가 docker run과 같은 명령을 사용하면 도커 클라이언트는 해당 명령을 도커 데몬에게 전송하고 도커 데몬은 명령을 수행하게 된다도커 데몬  도커 데몬(dockerd)은 도커 클라이언트로부터 요청을 받으면 이미지, 컨테이너, 네트워크, 볼륨과 같은 도커 오브젝트를 생성하고 관리한다도커 레지스트리  도커 레지스트리는 도커 이미지 오브젝트를 저장, 관리하는 곳이다  public한 곳(docker hub)도 있고, private(AWS의 ECR)한 곳도 있다이미지와 컨테이너이미지  도커 이미지는 도커 컨테이너를 생성하기 위한 읽기 전용 템플릿이다  도커는 애플리케이션을 컨테이너화하기 위해 이미지로 만든다  기본 이미지 위에 원하는 커스터마이징을 통해 새로운 이미지를 만들 수도 있고, 이렇게 만들어진 이미지는 도커 레지스트리에 공유할 수 있다  이미지를 만들 때에는 Dockerfile에 필요한 명령어를 정의하여 만들 수 있다  Dockerfile에 정의된 각각의 명령어들은 이미지의 레이어를 생성하며, 이러한 레이어들이 모여 이미지를 구성한다컨테이너  컨테이너는 도커 이미지를 기반으로 실행되는 프로세스를 말한다  컨테이너는 Docker API 사용하여 생성, 시작, 중지, 이동 또는 삭제 할 수 있는 이미지의 실행 가능한 인스턴스를 나타낸다  컨테이너가 제거될 때는 영구 저장소에 저장되지 않은 변경 사항은 모두 해당 컨테이너와 같이 사라진다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-02-01T21:01:35+09:00'>01 Feb 2024</time><a class='article__image' href='/docker-intro'> <img src='/images/docker_logo.png' alt='[Docker] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-intro'>[Docker] Intro</a> </h2><p class='article__excerpt'>도커의 등장배경과 도커의 구성요소에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 실행 컨텍스트와 클로저",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-context-closure",
      "date"     : "Jan 27, 2024",
      "content"  : "Table of Contents  실행 컨텍스트의 역할  실행컨텍스트를 바르게 이해하면 자바스크립트가 스코프를 기반으로 식별자와 식별자에 바인딩된 값을 관리하는 방식과, 호이스팅이 발생하는 이유, 클로저의 동작 방식, 그리고 태스크 큐와 함께 동작하는 이벤트 핸들러와 비동기 처리의 동작방식을 이해할 수 있다  모든 소스코드는 실행에 앞서 평가 과정을 거치며 코드를 실행하기 위한 준비를 한다. 다시 말해, 자바스크립트 엔진은 소스코드를 소스코드의 평가와 소스코드의 실행 과정으로 나누어 처리한다  소스코드 평가 과정이 끝나면 비로소 선언문을 제외한 소스코드가 순차적으로 실행되기 시작한다. 즉 런타임이 시작된다.  이때 소스코드 실행에 필요한 정보, 즉 변수나 함수의 참조를 실행 컨텍스트가 관리하는 스코프에서 검색해서 취득한다실행 컨텍스트의 역할  실행 컨텍스트는 소스코드를 실행하는데 필요한 환경을 제공하고 코드의 실행 결과를 실제로 관리하는 영역이다  좀 더 구체적으로 말해, 실행 컨텍스트는 식별자를 등록하고 관리하는 스코프와 코드 실행 순서 관리를 구현한 내부 메커니즘으로, 모든 코드는 실행 컨텍스트를 통해 실행되고 관리된다  식별자와 스코프는 실행 컨텍스트의 렉시컬 환경으로 관리하고 코드 실행 순서는 실행 컨텍스트 스택으로 관리한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-27T21:01:35+09:00'>27 Jan 2024</time><a class='article__image' href='/js-context-closure'> <img src='/images/js_logo.png' alt='[Javascript]: 실행 컨텍스트와 클로저'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-context-closure'>[Javascript]: 실행 컨텍스트와 클로저</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: this",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-this",
      "date"     : "Jan 26, 2024",
      "content"  : "Table of Contents  this  일반 함수 호출  메서드 호출  생성자 함수 호출  Function.prototype.apply/call/bind 메서드에 의한 간접 호출  결론this  객체에서 동작을 나타내는 메서드는 자신의 상태를 참조하고 변경할 수 있어야 한다. 이 때 메서드가 자신이 속한 객체의 프로퍼티를 참조하려면 먼저 자신이 속한 객체를 가리키는 식별자를 참조할 수 있어야 한다  자바스크립트는 자신이 생성할 인스턴스를 가리키는 this라는 특수한 식별자를 제공한다  this는 자신이 속한 객체 또는 자신이 생성할 인스턴스를 가리키는 자기 참조 변수다      단 자바스크립트에서 this 바인딩은 함수 호출 방식에 의해 동적으로 결정된다    동일한 함수도 다양한 방식으로 호출할 수 있다. 함수를 호출하는 방식은 다음과 같다          일반 함수 호출      메서드 호출      생성자 함수 호출      Function.prototype.apply/call/bind 메서드에 의한 간접 호출      일반 함수 호출  일반함수로 호출된 모든 함수(중첩함수, 콜백 함수 포함) 내부의 this에는 전역 객체가 바인딩된다  하지만 메서드 내에서 정의한 중첩 함수 또는 메서드에게 전달한 콜백 함수가  일반 함수로 호출될 때 this가 전역 객체를 바인딩하는 것은 문제가 있다  이 문제를 해결하는 가장 깔끔한 방법은 화살표 함수를 사용하는 것이다  화살표 함수 내부의 this는 상위 스코프의 this를 가리킨다const obj = {    value: 100,    foo() {        setTimeout(() =&amp;gt; console.log(this.value), 100)    }}메서드 호출  메서드 내부의 this는 메서드를 호출한 객체가 바인딩 된다생성자 함수 호출  생성자 함수 내부의 this에는 생성자 함수가 미래에 생성할 인스턴스가 바인딩 된다Function.prototype.apply/call/bind 메서드에 의한 간접 호출  apply, call, bind 메서드는 Function.prototype의 메서드다. 즉 이들 메서드는 모든 함수가 상속받아 사용할 수 있다  apply와 call 메서드는 함수를 호출하면서 첫 번째 인수로 전달한 특정 객체를 호출한 함수의 this에 바인딩한다  (apply는 전달할 인수를 배열 형식으로 전달, call은 쉼표로 구분하여 전달하는 차이다)  bind 메서드는 함수를 호출하지 않는다  bind 메서드는 첫 번째 인수로 전달한 객체에 바인딩된 새로 생성된 함수를 반환한다  위에서 일반 함수의 this가 전역 객체에 바인딩되는 문제를 화살표 함수로 해결했었는데, 이 문제를 apply/call/bind로 해결할 수도 있었다  콜백함수 안에 사용된 함수가 일반 함수로 정의된 함수가 전달될 수 있기 때문에 bind 메서드를 사용해 이 문제를 사전에 방지할 수 있다const person = {    name: &#39;Lee&#39;,    foo(callback) {        setTimeout(callback.bind(this), 100)    }}// 이렇게 일반 함수가 전달되어도 사전에 bind 메서드를 사용해서 문제를 사전에 방지했기 때문에 괜찮다person.foo(function() { console.log(`Hi my name is ${this.name}`) })결론  메서드나 생성자 함수는 크게 걱정할 문제가 없다  만약 콜백함수로 사용될 가능성이 있는 함수는 일반 함수로 정의하는 것보다는, 화살표 함수로 정의하는게 낫다  콜백함수가 일반함수로 정의되었다면, 콜백함수를 참조하는 곳에서 bind 메서드를 사용해 바인딩 해야 한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-26T21:01:35+09:00'>26 Jan 2024</time><a class='article__image' href='/js-this'> <img src='/images/js_logo.png' alt='[Javascript]: this'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-this'>[Javascript]: this</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 프로토타입",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-prototype",
      "date"     : "Jan 26, 2024",
      "content"  : "Table of Contents  자바스크립트는 프로토타입 기반의 객체지향 프로그래밍 언어다      객체지향 프로그래밍의 핵심은 상속인데, 자바스크립트는 프로토타입을 기반으로 상속을 구현하여 불필요한 중복을 제거한다    아래 그림과 같이 getArea 라는 메서드를 프로토타입 객체로 보내면 중복을 제거할 수 있다  생성자 함수가 생성한 모든 인스턴스는 생성자 함수와 쌍을 이루는 프로토타입 객체의 모든 프로퍼티와 메서드를 상속받는다  프로토타입은 어떤 객체의 상위 객체 역할을 하는 객체로서 다른 객체에 공유 프로퍼티를 제공한다  모든 객체는 [[Prototype]] 이라는 내부 슬롯을 가지며, 이 내부 슬롯의 값은 프로토타입의 참조다. [[Prototype]]에 저장되는 프로토타입은 객체 생성 방식에 의해 결정된다. 즉 객체가 생성될 때 객체 생성 방식에 따라 프로토타입이 결정되고 [[Prototype]]에 저장된다  [[Prototype]] 내부 슬롯에는 직접 접근할 수 없지만, 위 그림처럼 __proto__ 접근자 프로퍼티를 통해 자신의 프로토타입, 즉 자신의 [[Prototype]] 내부 슬롯이 가리키는 프로토타입에 간접적으로 접근할 수 있다  생성자 함수는 prototype 프로퍼티로 프로토타입 객체에 접근 가능하고, 객체는 __proto__ 프로퍼티로 객체에 접근 가능하고, 프로토타입은 constructor 프로퍼티로 생성자 함수에 접근 가능하다  모든 객체는 __proto__ 프로퍼티를 통해 자신의 프로토타입, 즉 [[Prototype]] 내부 슬롯에 간접적으로 접근할 수 있다  __proto__ 은 객체가 소유하고 있는 프로퍼티가 아니고, Object.prototype의 프로퍼티를 상속받은 것이다  모든 객체는 프로토타입의 계층 구조인 프로토타입 체인에 묶여있다. 자바스크립트 엔진은 객체의 프로퍼티 또는 메서드에 접근하려고 할 때 해당 객체에 접근하려는 프로퍼티가 없다면 __proto__ 프로퍼티가 가리키는 참조를 따라 자신의 부모 역할을 하는 프로토타입의 프로퍼티를 순차적으로 검색한다. 프로토타입 체인의 종점, 즉 프로토타입 체인의 최상위 객체는 Object.prototype이며, 이 객체의 프로퍼티와 메서드는 모든 객체에 상속된다  코드 내에서 __proto__ 프로퍼티를 직접 사용하는 것보다는 프로토타입을 참조하고 싶은 경우에는 Object.getPrototypeOf 메서드를 사용하고, 프로토타입을 교체하고 싶은 경우에는 Object.setPrototypeOf 메서드를 사용할 것을 권장한다객체 instanceof 생성자 함수  instanceof 연산자는 우변의 생성자 함수의 prototype 프로퍼티에 바인딩된 프로토타입 체인안에 좌변의 객체의 프로토타입이 속하는지를 확인한다프로퍼티 in 객체  in 연산자는 객체 내에 특정 프로퍼티가 존재하는지 여부를 확인한다  in 연산자는 객체의 프로퍼티뿐 아니라, 객체가 상속받은 모든 프로토타입의 프로퍼티를 확인하기 때문에, ES6에서 도입된 Reflect.has 메서드가 더 정확하다고 할 수 있다for (변수 선언문 in 객체) {}  객체의 모든 프로퍼티를 순회하며 열거하려면 for...in 문을 사용한다  for...in 문은 객체의 프로토타입 체인 상에 존재하는 모든 프로토타입의 프로퍼티중에서 프로퍼티 어트리뷰트 [[Enumerable]]의 값이 true인 프로퍼티를 순회하며 열거한다  for...in 문은 자신의 고유 프로퍼티뿐 아니라 상속받은 프로퍼티도 열거한다. 그래서 자신의 프로퍼티만 열거하기 위해서는 Object.keys/values/entries 메서드를 사용하는 것이 낫다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-26T21:01:35+09:00'>26 Jan 2024</time><a class='article__image' href='/js-prototype'> <img src='/images/js_logo.png' alt='[Javascript]: 프로토타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-prototype'>[Javascript]: 프로토타입</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Node.js] Express 사용하기",
      "category" : "backend",
      "tags"     : "nodejs",
      "url"      : "/nodejs-express",
      "date"     : "Jan 25, 2024",
      "content"  : "Table of Contents  Express에서 제공하는 유용한 미들웨어          JSON parser      Static        미들웨어 확장Express에서 제공하는 유용한 미들웨어JSON parserStatic미들웨어 확장",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-25T21:01:35+09:00'>25 Jan 2024</time><a class='article__image' href='/nodejs-express'> <img src='/images/node_logo.png' alt='[Node.js] Express 사용하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nodejs-express'>[Node.js] Express 사용하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: var, let, const 키워드",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-var-let-const",
      "date"     : "Jan 25, 2024",
      "content"  : "Table of Contents  var  let  const  결론var  var은 중복 선언을 허용한다  함수 레벨 스코프이다  변수 호이스팅이 일어난다  (var은 런타임 이전에 무조건 undefined로 초기화되어서 런타임에 선언문을 만나기 전에 참조 에러가 안생기고 undefined를 참조한다)let  중복 선언은 금지하지만, 재할당은 허용한다 (재할당은 새로운 메모리 주소에 새로운 값을 저장하고 식별자가 새로운 메모리 주소와 바인딩되는 것을 말한다)  블록 레벨 스코프이다  변수 호이스팅이 발생하지만, 초기화가 런타임 때 선언문을 만났을 때 일어난다  (변수가 런타임 이전에 등록은 되지만, 초기화는 런타임 때 선언문을 만났을 때 일어나기 때문에 그 전에 참조하면 참조 에러가 난다)const  중복 선언도 금지하고, 재할당도 금지한다  블록 레벨 스코프이다  let과 마찬가지로 변수 호이스팅이 발생하지만, 호이스팅 발생하지 않는 것처럼 선언문 이 후에 참조할 수 있다결론  ES6 이후로는 var은 사용하지 않아도 된다  일단 const를 사용하자  재할당이 필요한 경우는 let을 사용하자",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-25T21:01:35+09:00'>25 Jan 2024</time><a class='article__image' href='/js-var-let-const'> <img src='/images/js_logo.png' alt='[Javascript]: var, let, const 키워드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-var-let-const'>[Javascript]: var, let, const 키워드</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Node.js] 노드에서 제공하는 기본 기능",
      "category" : "backend",
      "tags"     : "nodejs",
      "url"      : "/nodejs-web-os-path",
      "date"     : "Jan 24, 2024",
      "content"  : "Table of Contents  process  fs  os  path  url  util  worker_threads  child_process  동기 비동기  버퍼와 스트림  쿠키  세션process  현재 실행중인 노드 프로세스에 대한 정보를 담고 있는 노드 내장 객체// 설치되 노드의 버전process.version// 프로세스 아이디process.pid// 프로세스 경과 시간process.uptime()// 현재 프로세스가 실행되는 위치process.cwd()// 시스템 환경 변수들이 들어있는 객체// 비밀키(데이터베이스 비밀번호, 서드파티 앱 키 등)를 보관하는 용도로도 쓰임process.envfsos  운영체제의 정보를 담고 있는 내장 모듈// 홈 디렉터리 경로os.homedir()// 컴퓨터의 코어 정보os.cpus()// 사용 가능한 메모리(RAM)os.freemem()// 전체 메모리 용량os.totalmem()path  폴더와 파일의 경로를 쉽게 조작하도록 도와주는 내장 모듈// 파일이 위치한 폴더 경로path.dirname(경로)// 파일의 확장자path.extname(경로)// 파일의 이름(확장자 포함)path.basename(경로)// 여러 경로를 하나로 합쳐준다path.join(경로1, 경로2, ..)url  URL 주소를 파싱해서 쉽게 조작하도록 도와주는 내장 모듈import * as url from &#39;url&#39;const myUrl = new url.URL(&#39;https://www.google.com/search?q=apple&#39;)console.log(myUrl)URL {  href: &#39;https://www.google.com/search?q=apple&#39;,  origin: &#39;https://www.google.com&#39;,  protocol: &#39;https:&#39;,  username: &#39;&#39;,  password: &#39;&#39;,  host: &#39;www.google.com&#39;,  hostname: &#39;www.google.com&#39;,  port: &#39;&#39;,  pathname: &#39;/search&#39;,  search: &#39;?q=apple&#39;,  searchParams: URLSearchParams { &#39;q&#39; =&amp;gt; &#39;apple&#39; },  hash: &#39;&#39;}util  util.deprecate()import * as util from &#39;util&#39;const addFn = (a, b) =&amp;gt; {    return a + b}const deprecatedAddFn = util.deprecate(addFn, &#39;addFn을 쓰지 마세요&#39;)console.log(addFn(3, 5))console.log(deprecatedAddFn(2, 3))  util.promisify()  프로미스의 등장 이전에는 비동기 함수의 실행 결과에 대한 후속 처리를 콜백함수로 했다  콜백함수의 콜백 헬 문제, 에러 처리 불가 문제 등을 해결하기 위해 프로미스가 등장했다  util.promisify()가 콜백 패턴을 프로미스 패턴으로 바꿔준다  단 콜백이 (err, data) =&amp;gt; {} 형태여야 한다const promisifiedRandomBytes = util.promisify(crypto.randomBytes)promisifiedRandomBytes(64).then().catch()...worker_threadschild_process동기 비동기버퍼와 스트림쿠키세션",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-24T21:01:35+09:00'>24 Jan 2024</time><a class='article__image' href='/nodejs-web-os-path'> <img src='/images/node_logo.png' alt='[Node.js] 노드에서 제공하는 기본 기능'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nodejs-web-os-path'>[Node.js] 노드에서 제공하는 기본 기능</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 지역 변수와 전역 변수",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-global-variable",
      "date"     : "Jan 24, 2024",
      "content"  : "Table of Contents  전역 변수          전역 변수의 문제점      전역 변수 사용 억제하는 방법        지역 변수전역 변수  전역 변수는 런타임 이전에 선언문이 실행되어 생성되고, 런타임에 할당문이 실행되어 값을 가지게 된다  전역 변수는 전역 객체(브라우저에서는 window, 노드에서는 global)와 생명 주기가 같다  그래서 window는 웹페이지를 닫기 전까지, global은 노드에서 프로세스를 종료하기 전까지 살아있기 때문에 전역 변수도 그 때 까지 살아서 메모리를 점유한다전역 변수의 문제점  긴 생명 주기: 생명주기가 길어서 메모리 리소스를 오랜 기간 점유한다  스코프 체인 상에서 종점에 존재: 검색 속도가 가장 느리다전역 변수 사용 억제하는 방법  즉시 실행 함수  모듈화지역 변수  지역 변수는 함수가 호출되어 함수 안의 코드가 실행될 때, 함수 안에서 정의된 변수들의 선언문이 실행되어 생성되고, 이 후 해당 할당문 위치에 왔을 때 실제로 값이 할당된다  그래서 함수 안에서 정의된 지역 변수는 그 함수가 호출되지 않으면 함수 내부의 변수 선언문도 실행되지 않는다  (전역 변수는 함수 호출과 같은 진입점이 없어 바로 선언문이 실행되는 것과 대비되는 점이다)  이처럼 호이스팅은 스코프를 단위로 동작한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-24T21:01:35+09:00'>24 Jan 2024</time><a class='article__image' href='/js-global-variable'> <img src='/images/js_logo.png' alt='[Javascript]: 지역 변수와 전역 변수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-global-variable'>[Javascript]: 지역 변수와 전역 변수</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Node.js] 웹서버",
      "category" : "backend",
      "tags"     : "nodejs",
      "url"      : "/nodejs-web-server",
      "date"     : "Jan 23, 2024",
      "content"  : "Table of Contents  http 모듈 불러오기  서버 생성하기  서버 포트 열기  클라이언트로부터 요청 받기  전체 코드  Express 프레임워크  Node.js는 백엔드 서버용 코드를 작성하는데 유용한 http 모듈을 기본 제공한다http 모듈 불러오기  CommonJS 방식과, ESModule 방식이 있다// CommonJS 방식const http = require(&#39;http&#39;)// ESModule 방식import * as http from &#39;http&#39;서버 생성하기  createServer([options][, requestListener])  requestListener는 요청(request) 이벤트가 발생했을 때 실행될 콜백 함수를 말한다  여기서 작성해도 되지만 나는 역할 구분을 명확히 하기 위해 뒤에서 server.on(&#39;request&#39;, 콜백함수) 방식으로 따로 분리해서 작성했다import * as http from &#39;http&#39;const server = htttp.createServer()서버 포트 열기server.listen(8000)클라이언트로부터 요청 받기  Node.js는 이벤트 방식으로 동작한다  ‘요청(request)’이라는 이벤트가 들어왔을 때 실행할 콜백함수를 등록하면 된다  요청은 크게 경로(path)와 메서드(method)로 분기한다  요청에 대한 응답은 스트림 형식 또는 파일 형식으로 반환할 수 있다import * as url from &#39;url&#39;server.on(&#39;request&#39;, (req, res) =&amp;gt; {        const pathname = url.parse(req.url).pathname    if (pathname === &#39;/&#39;) {        res.write(&#39;&amp;lt;h1&amp;gt;Hello Node !&amp;lt;/h1&amp;gt;&#39;)    } else if (pathname === &#39;/items&#39;) {        if (req.method === &#39;GET&#39;) {            res.write(&#39;&amp;lt;h1&amp;gt;All Items&amp;lt;/h1&amp;gt;&#39;)        } else if (req.method === &#39;POST&#39;) {            res.write(&#39;&amp;lt;h1&amp;gt;Create Item&amp;lt;/h1&amp;gt;&#39;)        }    }    res.end()})전체 코드import * as http from &#39;http&#39;import * as url from &#39;url&#39;const server = http.createServer()server.listen(8000)server.on(&#39;request&#39;, (req, res) =&amp;gt; {    // http://localhost:8000/foo/bar?a=z 에 대하여,    // req.url -&amp;gt; &#39;/foo/bar?a=z&#39;    // url.parse(req.url).pathname -&amp;gt; &#39;/foo/bar&#39;    // url.parse(req.url).path -&amp;gt; &#39;/foo/bar?a=z&#39;    // url.parse(req.url).href -&amp;gt; &#39;/foo/bar?a=z&#39;    const pathname = url.parse(req.url).pathname        if (pathname === &#39;/&#39;) {        res.write(&#39;&amp;lt;h1&amp;gt;Hello Node !&amp;lt;/h1&amp;gt;&#39;)    } else if (pathname === &#39;/items&#39;) {        if (req.method === &#39;GET&#39;) {            res.write(&#39;&amp;lt;h1&amp;gt;All Items&amp;lt;/h1&amp;gt;&#39;)        } else if (req.method === &#39;POST&#39;) {            res.write(&#39;&amp;lt;h1&amp;gt;Create Item&amp;lt;/h1&amp;gt;&#39;)        }    }    res.end()    })Express 프레임워크  위에서 http 모듈로도 충분히 백엔드 서버를 만들 수 있음을 확인했다  하지만 요청 분기, 응답 반환 방식이 코드 규모가 커지면 점점 가독성이 떨어진다  이러한 단점을 해결하기 위해 Express 프레임워크를 사용할 예정이다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-23T21:01:35+09:00'>23 Jan 2024</time><a class='article__image' href='/nodejs-web-server'> <img src='/images/node_logo.png' alt='[Node.js] 웹서버'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nodejs-web-server'>[Node.js] 웹서버</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Node.js] 모듈",
      "category" : "backend",
      "tags"     : "nodejs",
      "url"      : "/nodejs-module",
      "date"     : "Jan 23, 2024",
      "content"  : "Table of Contents  CommonJS 모듈  ES 모듈  모듈: 특정한 기능을 하는 함수나 변수들의 집합  모듈로 만들면 여러 프로그램에서 재사용 가능CommonJS 모듈module.exports = {    odd,    even}// module.exports = odd// module.exports = [odd, even]// module.exports 는 파일에서 한 번만 쓸 수 있음const { odd, even } = require(&#39;./var.js&#39;)// const { myOdd: odd, myEven: even } = require(&#39;./var.js&#39;)// const mynumbers = require(&#39;./var.js&#39;)// mynumbers.odd// mynumbers.even  module.exports 외에도 exports로 모듈을 만들 수 있음exports.odd = oddexports.even = even// module.exports와 exports를 같이 쓸 수는 없음  node.js에서 최상위 스코프의 this는 module.exports를 가리킴console.log(this === module.exports) // truefunction a() {    console.log(this === global)}a() // true  require(&#39;./var.js&#39;) 실행만 하고 변수에 할당 안하면, var.js 파일 실행은 하지만, 모듈은 가져와서 사용하지 않음ES 모듈  자바스크립트 진영에서 표준으로 채택한 모듈화 방법  mjs 확장자를 사용하거나 package.json에 type: “module”을 추가해야 함export const odd = &#39;홀수&#39;export const even = &#39;짝수&#39;  export default 는 모듈에서 한 번만 사용할 수 있다export default const odd = &#39;홀수&#39;// export default const even = &#39;짝수&#39; Xexport default const mynumbers = { odd: &#39;홀수&#39;, even: &#39;짝수&#39; }  모듈을 임포트할 때 export 사용한 모듈은 구조 분해 할당 형태로 불러온다  export default 사용한 모듈은 구조 분해 할당이 필요없고, 다른 이름으로 불러올 수도 있다  확장자(ex. .js, .mjs 등)를 생략하면 안된다import { odd } from &#39;./var.js&#39;import mynumbers from &#39;./var.js&#39;import mymymy from &#39;./var.js&#39; // export default는 임포트할 요소가 유일하기 때문에 불러올 때 이름이 달라도 된다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-23T21:01:35+09:00'>23 Jan 2024</time><a class='article__image' href='/nodejs-module'> <img src='/images/node_logo.png' alt='[Node.js] 모듈'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nodejs-module'>[Node.js] 모듈</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 스코프",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-scope",
      "date"     : "Jan 23, 2024",
      "content"  : "Table of Contents  스코프          스코프 결정 방식      스코프 종류      스코프 체인      렉시컬 스코프      스코프  스코프는 식별자의 유효범위를 의미한다  자바스크립트 엔진이 식별자를 검색할 때 스코프 체인을 통해 식별자를 참조하고 있는 코드의 스코프에서 시작해 상위 스코프 방향으로 이동하며 선언된 식별자를 검색한다  상위 스코프에서 유효한 식별자는 하위 스코프에서도 자유롭게 참조할 수 있지만, 하위 스코프에서 유효한 식별자는 상위 스코프에서 참조할 수 없다  식별자중에서 변수는 정의할 수 있는 키워드 종류가 var, let, const 이렇게 세 가지가 있는데, var로 정의된 변수는 함수 레벨 스코프를 가지고, let, const로 정의된 변수를 블록 레벨 스코프를 가진다스코프 결정 방식  식별자가 선언된 위치에 의해 스코프가 결정된다let x = &#39;global&#39;; // 글로벌 스코프를 가진다function foo() {  let x = &#39;local&#39;; // 지역 스코프를 가진다  console.log(x); // x를 참조하고 있는 이 코드는 자기 자신의 스코프에서 먼저 x를 찾고, 없으면 점점 상위 스코프로 확장한다. 여기서는 자신의 스코프에 &#39;local&#39;이 있다}foo();console.log(x); // x를 참조하고 있는 이 코드는 자기 자신의 스코프가 글로벌이다. 그래서 글로벌에 정의된 &#39;global&#39;을 참조한다스코프 종류  스코프는 크게 글로벌 스코프와 지역 스코프가 있다  글로벌 스코프에서 정의된 변수는 어디서든 참조할 수 있다  지역 스코프는 자신 스코프와 하위 스코프에서만 참조할 수 있다          var로 정의된 변수는 함수 안에서 정의되었을 때만 지역 스코프로 여겨지고, 그 외의 경우에는 글로벌 스코프로 여겨진다      ES6에서 부터 블록 레벨 스코프를 지원하기 위해 let, const 라는 키워드를 만들었다      덕분에 if문, for문, while문, try/catch문에서 let, const로 정의한 변수가 자신 스코프를 지역 스코프로 다룰 수 있게 되었다      스코프 체인  코드에서 식별자를 참조하게 되면, 자바스크립트 엔진이 해당 식별자의 값을 찾게된다  이 때 식별자의 찾는 순서는, 식별자를 참조한 코드의 자신 스코프에서 점점 상위 스코프, 마지막으로 없으면 글로벌 스코프까지 순차적으로 탐색하게 된다  이렇게 자신 스코프 -&amp;gt; 상위 스코프들 -&amp;gt; 글로벌 스코프 순으로 찾는 모습이 마치 체인처럼 보여 스코프 체인이라 한다렉시컬 스코프  함수는 다른 식별자들과 다르게 정의된 곳의 코드와 참조(호출)된 곳의 코드의 스코프가 다를 수 있다  정의된 곳을 상위 스코프로 결정하는 방식을 렉시컬 스코프 또는 정적 스코프라 한다  참조된 곳을 상위 스코프로 결정하는 방식을 동적 스코프라 한다  자바스크립트를 비롯한 대부분의 프로그래밍 언어는 렉시컬 스코프 방식을 따른다const x = 1;function foo() {  const x = 10;  bar();}function bar() {  console.log(x);}foo();// foo 함수의 경우 정의된 곳과 참조된 곳 모두 글로벌 스코프다// bar 함수의 경우 정의된 곳은 글로벌 스코프, 참조된 곳은 foo 함수 스코프다// 그래서 bar 함수 안에서 x를 찾기 위해 상위 스코프로 이동할 때,// bar 함수가 정의된 글로벌 스코프가 상위 스코프다// 그래서 글로벌 스코프에서 x를 찾는다 -&amp;gt; 1을 출력",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-23T21:01:35+09:00'>23 Jan 2024</time><a class='article__image' href='/js-scope'> <img src='/images/js_logo.png' alt='[Javascript]: 스코프'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-scope'>[Javascript]: 스코프</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Node.js] Intro",
      "category" : "backend",
      "tags"     : "nodejs",
      "url"      : "/nodejs-intro",
      "date"     : "Jan 22, 2024",
      "content"  : "Table of Contents  노드란  노드의 장단점  노드 구조  노드의 특징          이벤트 기반      싱글 스레드 논블로킹I/O      노드란      2009년, 라이언 달이 만든 V8 엔진으로 빌드된 자바스크립트 런타임 환경    Node.js가 없던 시절에는 자바스크립트를 실행하려면, HTML 안에 &amp;lt;script&amp;gt;태그로 전달해줘야만 브라우저의 자바스크립트 엔진에서 실행 가능했다  Node.js의 등장으로 브라우저 이외의 환경에서도 자바스크립트를 실행할 수 있게 되었다  Node.js는 브라우저의 런타임과 다르게, 하드웨어의 파일 시스템에 접근할 수 있는 런타임 제공한다노드의 장단점  장점          프론트엔드에서 사용하는 javascript 언어를 이용해 백엔드 서버 개발이 가능해졌다      훌륭한 패키지 매니저(npm), 커뮤니티, 프레임워크 성숙도      비동기 이벤트 기반 I/O를 사용해 동시에 여러 요청을 다룰 수 있다      V8 엔진의 JIT 컴파일러 특성 때문에 서버 기동이 빠르다        단점          싱글 스레드이기 때문에 CPU 코어를 하나만 사용한다      그래서 CPU 작업이 많은 경우에는 사용하기 적절하지 않을 수 있다      노드 구조노드의 특징이벤트 기반  이벤트가 발생할 때 미리 지정해둔 작업을 수행하는 방식을 말한다  이벤트의 예: 클릭, 네트워크 요청, 타이머 등  이벤트 리스너: 이벤트를 등록하는 함수  콜백 함수: 이벤트가 발생했을 때 실행될 함수Node.js 교과서 책, 조현영 저자 참조싱글 스레드 논블로킹I/O  노드는 멀티 스레드 지원하지만 기본적으로 싱글 스레드 방식이다          (멀티 스레드 코드는 프로그래밍 난이도도 높고, 제대로 작성하지 않으면 컴퓨팅 자원이 낭비된다. 그래서 멀티 스레드에 숙련된 개발자들끼리 일할 때 유용하다)        시간이 오래 걸리면서, 다른 곳에서 할 수 있는 작업들은 논 블로킹으로 백그라운드에서 병렬로 실행시킨다          (ex. I/O 작업(파일 시스템 접근, 네트워크 요청), 압축, 암호화 등 )      (나머지 코드는 블로킹 방식으로 실행된다)        결론은 Node.js는 싱글 스레드로 프로그래밍 난이도는 낮추고, 논블로킹 I/O로 처리 성능을 높였다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-22T21:01:35+09:00'>22 Jan 2024</time><a class='article__image' href='/nodejs-intro'> <img src='/images/node_logo.png' alt='[Node.js] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nodejs-intro'>[Node.js] Intro</a> </h2><p class='article__excerpt'>노드의 구조, 장단점, 특징에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 함수",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-function",
      "date"     : "Jan 22, 2024",
      "content"  : "Table of Contents  함수          함수 정의                  함수 리터럴          함수 선언문          함수 표현식          함수 생성 시점과 함수 호이스팅          화살표 함수                    참조에 의한 전달과 외부 상태의 변경      다양한 함수의 형태                  즉시 실행 함수          중첩 함수          콜백 함수                    함수  프로그래밍에서 함수는 일련의 과정을 문(statement)으로 구현하고 코드 블록으로 감싸서 하나의 실행 단위로 정의한 것이다  함수를 이용하면 코드의 재사용성을 높이고 가독성을 향상시킨다  자바스크립트에서 함수는 호출할 수 있는 객체다함수 정의  자바스크립트에서 함수를 정의하는 방법에는 함수 선언문, 함수 표현식, Function 생성자 함수 그리고 화살표 함수가 있다          함수 선언문과 함수 표현식은 함수 리터럴을 통해 정의한다      Function 생성자 함수는 거의 사용하지 않는다 (권장하는 방식이 아니다)      화살표 함수는 ES6에서 도입된 것으로 위의 방식들을 간략화한 방법이다      함수 리터럴  함수라는 객체를 표현하는 방식을 의미한다  숫자 리터럴은 숫자 값으로 표기해야 하고, 문자열 리터럴도 따옴표로 묶어야 되듯이, 함수 리터럴도 함수 리터럴로 인식되려면 규칙이 필요하다  함수 리터럴은 function 키워드, 함수 이름, 매개변수 목록, 함수 몸체로 구성된다// 기본 방식// function 키워드, 함수 이름, 매개변수 목록, 함수 몸체function add(x, y) {    return x + y}// 매개변수 없어도 된다function sayHello() {    return &#39;Hello&#39;}// 함수 이름 없어도 된다// 하지만, 식별자에 할당해야 한다const add = function(x, y) {    return x + y}  함수 리터럴은 함수 선언문 또는 함수 표현식, 이 두 가지 함수 정의에 사용된다  만약 함수 이름이 없다면, 무조건 함수 표현식으로 정의하는데만 사용되야 한다    function () { // 식별자가 필요합니다  return &#39;hello&#39;}const sayHello = function () { // 함수 표현식  return &#39;hello&#39;}        함수 이름이 있다면,          값으로 평가되는 문맥인 경우, 예를 들어 변수에 할당하거나 피연산자로 사용하면 이는 함수 표현식으로 정의한 것이다      단독으로 사용하면 이는 함수 선언문으로 정의한 것이다      function add(x, y) { // 함수 선언문    return x + y}const add = function add(x, y) { // 함수 표현식    return x + y}함수 선언문  함수 선언문은 함수 리터럴의 이름을 생략할 수 없다  (변수에 할당도 못하는데 함수 이름까지 없으면 함수를 식별할 수가 없으니까)  함수 호출은 함수 이름에 의해서가 아니라 함수 객체를 가리키는 식별자로 호출한다  그래서 함수 선언문도 암묵적으로는 함수 이름과 똑같은 식별자를 자바스크립트 엔진이 생성하고, 거기에 함수 객체를 할당한다  이렇게 자바스크립트 엔진이 식별자를 생성해 함수 객체를 할당한 후부터 함수를 값처럼 사용할 수 있다 (일급 객체)function add(x, y) { // 함수 선언문    return x + y}// 자바스크립트 엔진이 암묵적으로 아래와 같이 변경한다const add = function add(x, y) {    return x + y}함수 표현식  함수 표현식으로 함수를 정의하면, 자바스크립트 엔진이 별다른 과정을 거치지 않아도 값으로 사용할 수 있다  함수 표현식의 함수 리터럴은 함수 이름을 생략하는 것이 일반적이다 (재귀 함수의 경우에는 함수 이름 필요)함수 생성 시점과 함수 호이스팅  함수 선언문으로 함수를 정의하면 런타임 이전에 함수 객체가 먼저 생성된다. 그리고 자바스크립트 엔진은 함수 이름과 동일한 이름으로 식별자를 암묵적으로 생성하고 생성된 함수 객체를 할당한다  이렇게 런타임 이전에 이미 함수 객체가 생성되어 있기 때문에, 런타임에 함수 선언문 이전부터 함수를 참조하고 호출할 수 있다      이를 함수 호이스팅이라고 한다    반면 함수 표현식은 런타임 이전에는 함수를 할당한 변수의 선언문 까지만 실행되고, 해당 변수에는 undefined가 할당되어 있다. 그리고 런타임에 비로소 함수 객체가 생성되어 변수에 할당된다  함수 표현식은 런타임에서 함수 표현식을 만났을 때 비로소 함수 객체가 할당되기 때문에 표현식 이전에 함수를 참조하고 호출할 수 없다      이렇게 변수 선언문만 런타임 전에 실행되고, 할당문은 런타임에 실행되는 방식을 변수 호이스팅이라 한다    결론적으로 함수 호이스팅은 함수를 호출하기 전에 함수를 선언해야 한다는 당연한 규칙을 위배하기 때문에 권장되지 않고, 변수 호이스팅 되는 함수 표현식으로 함수를 정의할 것을 권장한다화살표 함수  화살표 함수는 ES6에서 도입되었으며 함수를 정의할 때 function 키워드 대신 화살표 =&amp;gt;를 사용한다  화살표 함수는 항상 익명 함수로 정의한다      화살표 함수는 단순히 위의 정의 방법들을 단순화하기 위한 용도가 아니라, 화살표 함수만의 특징과 용도가 있다    화살표 함수는 생성자 함수로 사용할 수 없다  기존 함수와 this 바인딩 방식이 다르다  prototype 프로퍼티가 없다      arguments 객체를 생성하지 않는다    (중요한 부분이기 때문에 나중에 26장 공부한 후 보충한다)참조에 의한 전달과 외부 상태의 변경  함수의 매개변수로 전달한 인자를 함수 안에서 조작하면 어떻게 될까?          원시값은 원본이 훼손되지 않는다      객체는 원본이 훼손된다        var, let 으로 정의한 원시값을 변경하면, 값을 새로운 메모리에 할당하고 해당 메모리에 저장된 값을 수정한다. 그래서 원본 값은 보존된다  객체는 변경 가능한 타입이기 때문에, 원시값을 변경하면, 해당 메모리에 있는 원시값이 그대로 수정된다. 그래서 원본 값이 훼손된다  이렇게 함수가 외부 상태를 변경하는 것은 변화의 추적을 어렵게 하기 때문에 지양되는 방식이다. 그래서 깊은 복사를 통해 매개변수에 전달하면 이러한 부수 효과를 피할 수 있다const changeVal = function(prim, obj) { // 외부 상태를 건드리는 비순수 함수    prim += 100;    obj.name = &#39;Kim&#39;;}// 외부 상태let num = 100;let person = { name: &#39;Lee&#39; };console.log(num) // 100console.log(person) // { name: &#39;Lee&#39; }changeVal(num, person);console.log(num) // 100 --&amp;gt; 원시값은 원본이 보존console.log(person) // { name: &#39;Kim&#39; } --&amp;gt; 객체는 원본이 훼손다양한 함수의 형태즉시 실행 함수  함수 정의와 동시에 즉시 호출되는 함수를 즉시 실행 함수라고 한다  즉시 실행 함수는 단 한 번만 호출되며 다시 호출할 수 없다  즉시 실행 함수는 그룹 연산자 (...)로 먼저 함수 리터럴 + 호출 연산자를 감싸줘야 한다    (function () {}())        그룹 연산자가 없으면 함수 선언문으로 여겨지고 함수 선언문은 함수 객체가 아니기 때문에 뒤의 함수 호출 연산자를 붙일 수 없다  그룹 연산자로 감싸줘야 함수 리터럴, 즉 함수 객체로 평가되고 뒤에 호출 연산자를 붙일 수 있다  즉시 실행 함수도 일반 함수처럼 인수를 전달할 수 있고 값을 반환할 수도 있다    const res = ( function (x, y) { return x + y }(3, 5) ) // 8      중첩 함수  함수 내부에 정의된 함수를 중첩 함수 또는 내부 함수라 한다  그리고 중첩 함수를 포함하는 함수를 외부 함수라 한다  중첩 함수는 자신을 포함하는 외부 함수를 돕는 헬퍼 함수의 역할을 한다function outer() {    let x = 1;    function inner() {        let y = 2;        console.log(x + y);    }    inner();}outer();콜백 함수  반복되는 부분을 함수로 만들어 다른 함수 매개변수에 인자로 전달되는 함수를 콜백 함수라 한다  콜백 함수를 인자로 받는 함수를 고차 함수라 한다const callBackFunc = function (x) {    console.log(x)}const higherOrderFunc = function (n, cb) {    for (let i = 0; i &amp;lt; n; i++) {        cb(i)    }}higherOrderFunc(5, callBackFunc);",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-22T21:01:35+09:00'>22 Jan 2024</time><a class='article__image' href='/js-function'> <img src='/images/js_logo.png' alt='[Javascript]: 함수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-function'>[Javascript]: 함수</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[FastAPI]",
      "category" : "backend",
      "tags"     : "FastAPI",
      "url"      : "/fastapi-intro",
      "date"     : "Jan 22, 2024",
      "content"  : "Table of Contents  참고참고  Guru99, 43+ Docker Interview Questions and Answers (2022)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-22T21:01:35+09:00'>22 Jan 2024</time><a class='article__image' href='/fastapi-intro'> <img src='/images/fastapi_logo.png' alt='[FastAPI]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/fastapi-intro'>[FastAPI]</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 보안(1) IAM",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-security-iam",
      "date"     : "Jan 22, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-22T21:01:35+09:00'>22 Jan 2024</time><a class='article__image' href='/aws-security-iam'> <img src='/images/aws_logo.png' alt='[AWS] 보안(1) IAM'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-security-iam'>[AWS] 보안(1) IAM</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 원시 값과 객체의 비교",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-primitive-and-object",
      "date"     : "Jan 21, 2024",
      "content"  : "Table of Contents  원시 값과 객체의 비교          원시 값      객체      원시 값과 객체의 비교  원시 타입의 값, 즉 원시 값은 변경 불가능한 값이다  원시 값을 변수에 할당하면 메모리에는 실제 값이 저장된다  원시 값을 갖는 변수를 다른 변수에 할당하면 원본의 원시 값이 복사되어 전달된다. 이를 값에 의한 전달이라 한다      원시 값을 let, var로 선언하면 재할당 할 수 있는데, 재할당은 원본 값을 변경하는 것이 아니고 새로운 메모리 공간에 값을 저장한 후, 변수가 참조하던 메모리 공간의 주소를 변경한다    객체는 변경 가능한 값이다.  객체를 변수에 할당하면 메모리에는 참조 값이 저장된다  객체를 가리키는 변수를 다른 변수에 할당하면 원본의 참조 값이 복사되어 전달된다. 이를 참조에 의한 전달이라 한다원시 값  원시 값을 저장하려면 먼저 확보해야 하는 메모리 공간의 크기를 결정해야 한다.  단 ECMAScript 사양에 문자열(2바이트)과 숫자(8바이트) 이외의 원시 타입은 크기를 명확히 규정하지 않아, 브라우저에 따라 다를 수 있다  (문자열은 문자 1개당 2바이트. 숫자는 어떤 크기의 숫자든 8바이트이다)const score = 80const copy = score// copy에는 score에 할당된 원시 값 80이 새로운 메모리 공간에 복사되어 새로운 메모리 공간의 주소와 바인딩 된다// 이를 값에 의한 전달이라 한다// score 변수와 copy 변수의 값 80은 다른 메모리 공간에 저장된 별개의 값이다객체  객체를 할당한 변수에는 실제 객체를 저장하고 있는 메모리의 메모리 주소를 저장하고 있다  참조값은 이렇게 객체가 저장된 메모리 공간의 주소를 말한다  (이렇게 객체를 메모리 주소를 저장하는 방식으로 설계한 이유는, 복사할 때 객체 전체를 복사하면 메모리 낭비가 커질 수 있어서이다)  이러한 방식의 단점은 원시값과는 다르게 여러 개의 식별자가 하나의 객체를 공유하게 되어서 예상치 못한 문제가 발생할 수 있다  객체 값 자체를 복사하고 싶으면 깊은 복사를 사용해야 한다.  깊은 복사를 지원하는 lodash 와 같은 라이브러리를 사용하면 된다const person = {    name: &#39;Lee&#39;}const copy = person",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-21T21:01:35+09:00'>21 Jan 2024</time><a class='article__image' href='/js-primitive-and-object'> <img src='/images/js_logo.png' alt='[Javascript]: 원시 값과 객체의 비교'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-primitive-and-object'>[Javascript]: 원시 값과 객체의 비교</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 태스크(5): 기계 번역 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-task-machine-translation",
      "date"     : "Jan 21, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-21T21:01:35+09:00'>21 Jan 2024</time><a class='article__image' href='/ai-nlp-task-machine-translation'> <img src='/images/nlp_logo.png' alt='[NLP] 태스크(5): 기계 번역 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-task-machine-translation'>[NLP] 태스크(5): 기계 번역 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 객체 리터럴",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-object-literal",
      "date"     : "Jan 20, 2024",
      "content"  : "Table of Contents  객체 리터럴          프로퍼티      메서드      객체 리터럴  객체 타입은 다양한 타입의 값을 하나의 단위로 구성한 복합적인 자료구조이다  객체는 프로퍼티와 메서드로 구성된 집합체이다  객체는 변경 가능한 자료구조이다const person = {    name: &#39;Lee&#39;,    sayHello: function() {        console.log(&#39;Hi&#39;)    }};프로퍼티  프로퍼티 키가 네이밍 규칙을 따르지 않으면 따옴표로 감싸줘야 한다  프로퍼티 키로 숫자 리터럴을 사용하면 따옴표는 붙지 않지만 내부적으로 문자열로 변환된다  존재하지 않는 프로퍼티에 접근하면 undefined를 반환한다메서드  메서드 내부에서 사용한 this 키워드는 객체 자신을 가리키는 참조변수다  ES6에서는 메서드를 정의할 때, 메서드 축약표현을 사용할 수 있다    const person = {  name: &#39;Lee&#39;,  sayHello() {      console.log(&#39;Hi&#39;)  }};      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-20T21:01:35+09:00'>20 Jan 2024</time><a class='article__image' href='/js-object-literal'> <img src='/images/js_logo.png' alt='[Javascript]: 객체 리터럴'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-object-literal'>[Javascript]: 객체 리터럴</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 태스크(4): 자연어 생성 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-task-nlg",
      "date"     : "Jan 20, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-20T21:01:35+09:00'>20 Jan 2024</time><a class='article__image' href='/ai-nlp-task-nlg'> <img src='/images/nlp_logo.png' alt='[NLP] 태스크(4): 자연어 생성 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-task-nlg'>[NLP] 태스크(4): 자연어 생성 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 제어문",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-control-statement",
      "date"     : "Jan 19, 2024",
      "content"  : "Table of Contents  제어문          조건문      반복문      제어문  제어문은 조건문이나 반복문을 일컫는다  지나친 제어문은 코드의 흐름을 혼란스럽게 한다  forEach, map, , filter, reduce 와 같은 고차함수를 사용한 함수형 프로그래밍은 이러한 문제를 해결하려고 노력한다조건문  특정 조건을 만족할 때 코드를 실행시키는 역할을 한다  if문, 삼항연산자, switch문을 통해 구현할 수 있다if (조건식) { 실행할 문 } // 기본적인 조건문 형태if (조건식) 실행할 문 // 실행할 문이 1개면 중괄호 생략 가능if (조건식1) {  실행할 문 // 조건식1을 만족하는 경우} else if (조건식2) {  실행할 문 2 // 조건식1은 만족하지 않지만 조건식2는 만족하는 경우} else {  실행할 문 3 // 조건식1과 조건식2를 모두 만족하지 않는 경우}(조건식) ? truthy일 경우 실행할 문 : falsy일 경우 실행할 문 // 삼항 연산자const score = 70;score &amp;gt; 50 ? console.log(&#39;합격&#39;) : console.log(&#39;탈락&#39;);// if문과 다르게 값으로 평가된다 -&amp;gt; 조건에 따라 동적으로 변수에 값을 할당하는 코드를 간결하게 구현할 수 있다const dinner = score &amp;gt; 50 : &#39;고기반찬&#39; : &#39;삼각김밥&#39;; switch (표현식) { // switch문은 표현식이 문자열이나 숫자값인 경우가 많다  case 표현식1:      실행할 문      break // break 없으면 아래 문이 실행된다 -&amp;gt; break 걸어줘야 한다  case 표현식2:      실행할 문      break  default:      일치하는 case문 없을 때 실행할 문}반복문  코드를 반복 실행시키는 역할을 한다  for문, while문 do..while문, forEach, map과 같은 고차함수를 이용해 구현한다  break는 실행을 중단하고 반복문을 빠져나오는 역할, continue는 실행을 중단하고 반복문의 증감식으로 실행 흐름을 이동하는 역할을 한다for (변수 할당문; 조건식; 증감식) {  실행할 문}for (let i = 0; i &amp;lt; 10; i++) {  console.log(i)}// 이터러블의 원소들을 순회할 때const arr = [1, 2, 3, 4, 5]for (let num of arr) {  console.log(num) // 1, 2, 3, 4, 5}// 객체의 프로퍼티들을 순회할 때const person = {  name: &#39;Kim&#39;  address: &#39;Seoul&#39;}for (let k in person) {  console.log(k) // name, address}while (조건식) {  실행할 문}// do..while은 먼저 실행할 문을 한 번 실행한 후에 조건에 따라 반복 실행할지를 결정한다do {  실행할 문} while (조건식)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-19T21:01:35+09:00'>19 Jan 2024</time><a class='article__image' href='/js-control-statement'> <img src='/images/js_logo.png' alt='[Javascript]: 제어문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-control-statement'>[Javascript]: 제어문</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 데이터베이스(1) RDS (준비중)",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-database-rds",
      "date"     : "Jan 19, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-19T21:01:35+09:00'>19 Jan 2024</time><a class='article__image' href='/aws-database-rds'> <img src='/images/aws_logo.png' alt='[AWS] 데이터베이스(1) RDS (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-database-rds'>[AWS] 데이터베이스(1) RDS (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 태스크(3): Question Answering (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-task-question-answering",
      "date"     : "Jan 19, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-19T21:01:35+09:00'>19 Jan 2024</time><a class='article__image' href='/ai-nlp-task-question-answering'> <img src='/images/nlp_logo.png' alt='[NLP] 태스크(3): Question Answering (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-task-question-answering'>[NLP] 태스크(3): Question Answering (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 연산자",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-operator",
      "date"     : "Jan 18, 2024",
      "content"  : "Table of Contents  연산자          산술 연산자      비교 연산자      삼항 연산자      논리 연산자      typeof 연산자      그 외의 연산자      연산자산술 연산자  +(덧셈), -(뺼셈), *(곱하기), /(몫), %(나머지), ++(증가), --(감소)  숫자 타입이 아닌 다른 타입 앞에 +를 붙이면 숫자 타입으로 변환하여 반환한다          원래 값은 바꾸지 않는다      정확한 값을 반환하기 어려운 경우는 NaN을 반환한다        const x = &#39;1&#39;;const numberX = +x; // 문자열 타입 &#39;1&#39;은 + 연산자에 의해 숫자 타입 1로 변환된다const x = &#39;true&#39;;const numberX = +x; // 불린 타입도 +연산자에 의해 숫자 타입으로 변환된다const x = &#39;Hello&#39;;const numberX = +x; // 문자열 타입 &#39;Hello&#39;는 + 연산자에 의해 숫자 타입으로 변환되지만, 특수한 경우인 NaN을 반환한다        +연산자를 쓸 때, 두 개의 피연산자중에 문자열이 있으면 +연산자는 문자열 연결 연산자로 동작한다비교 연산자  &amp;gt;, &amp;lt;, &amp;lt;=, &amp;gt;=, ==, ===, !=, !==  (==와 !=는 먼저 암묵적 타입 변환을 통해 타입을 일치시킨 후 값이 같은지 비교한다)  (===와 !==는 값뿐만 아니라 타입도 같은지 비교한다)삼항 연산자  if ...else문과 비슷한 역할을 한다  &amp;lt;조건식&amp;gt; ? &amp;lt;조건식이 truthy할 경우 실행할 문&amp;gt; : &amp;lt;조건식이 falsy일 경우 실행할 문&amp;gt;  조건에 따라 변수에 다른 값을 할당하고 싶을 때 많이 사용한다  if ...else문과 다르게 값으로 평가된다const x = 90x &amp;gt; 50 ? console.log(&#39;통과입니다&#39;) : console.log(&#39;탈락입니다&#39;)-&amp;gt; &#39;통과입니다&#39;const number = 1const gender = number === 1 ? &#39;male&#39; : &#39;female&#39; // gender에 &#39;male&#39; 할당됨논리 연산자  AND 연산자(&amp;amp;&amp;amp;), OR 연산자(||) 표현식의 결과는 2개의 피연산자중 한쪽으로 평가된다1 &amp;amp;&amp;amp; false // false1 || false // 1&#39;Cat&#39; &amp;amp;&amp;amp; &#39;Dog&#39; // &#39;Dog&#39;typeof 연산자  typeof 연산자는 피연산자의 데이터 타입을 문자열로 반환한다  다음 7가지 문자열중 하나를 반환한다: &#39;string&#39;, &#39;number&#39;, &#39;boolean&#39;, &#39;undefined&#39;, &#39;symbol&#39;, &#39;object&#39;, &#39;function&#39;  (null, [], {}, new Date()는 &#39;object&#39;)  null이 &#39;object&#39;를 반환하는 것은 자바스크립트 초기 버전의 버그다  따라서 null 여부는 타입이 아니라 값 자체를 비교해야 한다그 외의 연산자  ?.: 옵셔널 체이닝 연산자 (앞의 객체가 null 또는 undefined인지 확인한다. 아니면 뒤의 프로퍼티를 읽는다)  ??: null 병합 연산자 (앞의 값이 null 또는 undefined인지 확인한다. 아니면 앞의 값 반환 맞으면 뒤의 값을 반환)  delete: 프로퍼티 삭제  new: 생성자 함수를 호출할 때 사용하여 인스턴스를 생성  instanceof: 좌변의 객체가 우변의 생성자 함수와 연결된 인스턴스인지 판별  in: 프로퍼티 존재 확인const person = nullperson.name // TypeError: Cannot read properties of null (reading &#39;name&#39;)person?.name // undefinedconst x = nullconst y = 0x ?? 100 // 100y ?? 100 // 0",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-18T21:01:35+09:00'>18 Jan 2024</time><a class='article__image' href='/js-operator'> <img src='/images/js_logo.png' alt='[Javascript]: 연산자'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-operator'>[Javascript]: 연산자</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 태스크(2): 텍스트 유사도 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-task-text-similarity",
      "date"     : "Jan 18, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-18T21:01:35+09:00'>18 Jan 2024</time><a class='article__image' href='/ai-nlp-task-text-similarity'> <img src='/images/nlp_logo.png' alt='[NLP] 태스크(2): 텍스트 유사도 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-task-text-similarity'>[NLP] 태스크(2): 텍스트 유사도 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] Transformer",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-transformer",
      "date"     : "Jan 18, 2024",
      "content"  : "Table of Contents  GOOD EXPRESSION related to Transformer  트랜스포머 주요 파라미터  트랜스포머(Transformer)의 구조          1. 포지셔널 인코딩(Positional Encoding)      2. 어텐션(Attention)      3. 인코더(Encoder)                  1) 인코더의 셀프 어텐션                          a) 셀프 어텐션의 의미와 이점              b) 셀프 어텐션의 동작 메커니즘: Q, K, V벡터 얻기              c) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)              d) 행렬 연산으로 일괄 처리하기              e) 멀티 헤드 어텐션(Multi-head Attention)              f) 패딩 마스크(Padding Mask)                                2) 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)          3) 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)                          a) 잔차 연결(Residual connection)              b) 층 정규화(Layer Normalization)                                          4. 인코더에서 디코더(Decoder)로                  1) 디코더의 첫번째 서브층 : 셀프 어텐션          2) 디코더의 두번째 서브층 : 인코더-디코더 어텐션                    5. Inference      6. 참조      7. Shape by shape                  Masking 보충                    논문이번 포스트는 [딥러닝을 이용한 자연어 처리 입문 참조]의 자료를 발췌한 것으로 해당 링크를 통해 더 자세한 내용을 확인하실 수 있습니다.GOOD EXPRESSION related to Transformer트랜스포머(Transformer)는 2017년 구글이 발표한 논문인 “Attention is all you need”에서 나온 모델로 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도, 논문의 이름처럼 어텐션(Attention)만으로 구현한 모델입니다.트랜스포머 주요 파라미터아래에서 정의하는 하이퍼파라미터는 트랜스포머를 제안한 논문에서 사용한 값으로 목적에 따라 값은 달라질 수 있습니다.[딥러닝을 이용한 자연어 처리 입문 참조]트랜스포머(Transformer)의 구조트랜스포머는 RNN을 사용하지 않지만 기존의 seq2seq처럼 인코더에서 입력 시퀀스를 입력받고, 디코더에서 출력 시퀀스를 출력하는 인코더-디코더 구조를 유지하고 있습니다. 다만 다른 점은 인코더와 디코더라는 단위가 N개가 존재할 수 있다는 점입니다.이전 seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점(time-step)을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조입니다. 트랜스포머를 제안한 논문에서는 인코더와 디코더의 개수를 각각 6개를 사용하였습니다.1. 포지셔널 인코딩(Positional Encoding)우선 트랜스포머의 입력에 대해서 알아보겠습니다. RNN이 자연어 처리에서 유용했던 이유는 단어의 위치에 따라 단어를 순차적으로 입력받아서 처리하는 RNN의 특성으로 인해 각 단어의 위치 정보(position information)를 가질 수 있다는 점에 있었습니다.하지만 트랜스포머는 단어 입력을 순차적으로 받는 방식이 아니고 한 번에 문장이 가지고 있는 모든 단어를 받기 떄문에, 단어의 위치 정보를 통해 단어간의 sequential한 정보를 줄 필요가 있습니다. 따라서 트랜스포머는 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩(positional encoding)이라고 합니다.임베딩 벡터가 인코더의 입력으로 사용되기 전에 포지셔널 인코딩값이 더해지는 과정을 시각화하면 아래와 같습니다.트랜스포머는 위치 정보를 가진 값을 만들기 위해서 아래의 두 개의 함수를 사용합니다.좀 더 엄밀히 얘기하면, 위에서 본 임베딩 벡터와 포지셔널 인코딩의 덧셈은 사실 임베딩 벡터가 모여 만들어진 문장 벡터 행렬과 포지셔널 인코딩 행렬의 덧셈 연산을 통해 이루어진다는 점을 이해해야 합니다.pos는 입력 문장에서의 임베딩 벡터의 위치를 나타내며, i는 임베딩 벡터 내의 차원의 인덱스를 의미합니다. 위의 식에 따르면 임베딩 벡터 내의 각 차원의 인덱스가 짝수인 경우에는 사인 함수의 값을 사용하고 홀수인 경우에는 코사인 함수의 값을 사용합니다. 위의 수식에서 (pos, 2i)일 때는 사인 함수를 사용하고, (pos, 2i+1)일 때는 코사인 함수를 사용하고 있음을 주목합시다.위와 같은 포지셔널 인코딩 방법을 사용하면 순서 정보가 보존되는데, 예를 들어 각 임베딩 벡터에 포지셔널 인코딩값을 더하면 같은 단어라고 하더라도 문장 내의 위치에 따라서 트랜스포머의 입력으로 들어가는 임베딩 벡터의 값이 달라집니다. 결국 트랜스포머의 입력은 순서 정보가 고려된 임베딩 벡터라고 보면 되겠습니다. 이를 코드로 구현하면 아래와 같습니다.2. 어텐션(Attention)트랜스포머에는 총 3가지의 다른 어텐션이 사용됩니다.셀프 어텐션은 본질적으로 Query, Key, Value가 동일한 경우를 말합니다. 반면, 세번째 그림 인코더-디코더 어텐션에서는 Query가 디코더의 벡터인 반면에 Key와 Value가 인코더의 벡터이므로 셀프 어텐션이라고 부르지 않습니다. 주의할 점은 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미입니다.인코더의 셀프 어텐션 : Query = Key = Value디코더의 마스크드 셀프 어텐션 : Query = Key = Value디코더의 인코더-디코더 어텐션 : Query : 디코더 벡터 / Key = Value : 인코더 벡터위 그림은 트랜스포머의 아키텍처에서 세 가지 어텐션이 각각 어디에서 이루어지는지를 보여줍니다. 세 개의 어텐션에 추가적으로 ‘멀티 헤드’라는 이름이 붙어있습니다. 뒤에서 설명하겠지만, 이는 트랜스포머가 어텐션을 병렬적으로 수행하는 방법을 의미합니다.3. 인코더(Encoder)트랜스포머는 하이퍼파라미터인  num_layers 개수의 인코더 층을 쌓습니다. 논문에서는 총 6개의 인코더 층을 사용하였습니다. 인코더를 하나의 층이라는 개념으로 생각한다면, 하나의 인코더 층은 크게 총 2개의 서브층(sublayer)으로 나뉘어집니다. 바로 셀프 어텐션과 피드 포워드 신경망입니다. 위의 그림에서는 멀티 헤드 셀프 어텐션과 포지션 와이즈 피드 포워드 신경망이라고 적혀있지만, 멀티 헤드 셀프 어텐션은 셀프 어텐션을 병렬적으로 사용하였다는 의미고, 포지션 와이즈 피드 포워드 신경망은 우리가 알고있는 일반적인 피드 포워드 신경망입니다. 우선 셀프 어텐션에 대해서 알아봅시다.1) 인코더의 셀프 어텐션a) 셀프 어텐션의 의미와 이점어텐션 중에서는 셀프 어텐션(self-attention)이라는 것이 있습니다. 단지 어텐션을 자기 자신에게 수행한다는 의미입니다. 이처럼 기존에는 디코더 셀의 은닉 상태가 Q이고 인코더 셀의 은닉 상태가 K라는 점에서 Q와 K가 서로 다른 값을 가지고 있었습니다. 그런데 셀프 어텐션에서는 Q, K, V가 전부 동일합니다. 트랜스포머의 셀프 어텐션에서의 Q, K, V는 아래와 같습니다.Q : 입력 문장의 모든 단어 벡터들K : 입력 문장의 모든 단어 벡터들V : 입력 문장의 모든 단어 벡터들주의할 점은 여기서 Query, Key 등이 같다는 것은 벡터의 값이 같다는 것이 아니라 벡터의 출처가 같다는 의미입니다.셀프 어텐션을 통해 얻을 수 있는 이점은 무엇일까요?셀프 어텐션은 입력 문장 내의 단어들끼리 유사도를 구하므로서 그것(it)이 동물(animal)과 연관되었을 확률이 높다는 것을 찾아냅니다.b) 셀프 어텐션의 동작 메커니즘: Q, K, V벡터 얻기셀프 어텐션은 인코더의 초기 입력인 d_(model)의 차원을 가지는 단어 벡터들을 이용해 Q벡터, K벡터, V벡터를 얻는 작업을 거칩니다. 이때 이 Q벡터, K벡터, V벡터들은  d_(model)보다 더 작은 차원을 가지는데, 논문에서는 d_(model)=512의 차원을 가졌던 각 단어 벡터들을 64의 차원을 가지는 Q벡터, K벡터, V벡터로 변환하였습니다.64라는 값은 트랜스포머의 또 다른 하이퍼파라미터인 num_heads로 인해 결정되는데, 트랜스포머는 d_(model)을 num_heads로 나눈 값을 각 Q벡터, K벡터, V벡터의 차원으로 결정합니다. 논문에서는 num_heads를 8로하였습니다. 이제 그림을 통해 이해해봅시다. 예를 들어 여기서 사용하고 있는 예문 중 student라는 단어 벡터를 Q, K, V의 벡터로 변환하는 과정을 보겠습니다.기존의 벡터로부터 더 작은 벡터는 가중치 행렬을 곱하므로서 완성됩니다. 이 가중치 행렬은 훈련 과정에서 학습됩니다.c) 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)각 Q 벡터는 모든 K 벡터에 대해서 어텐션 스코어를 구하고, 어텐션 스코어를 구한 뒤에 이를 사용하여 모든 V 벡터를 가중합하여 어텐션 값 또는 컨텍스트 벡터를 구하게 됩니다. 그리고 이를 모든 Q 벡터에 대해서 반복합니다.여기서는 닷-프로덕트 어텐션(dot-product attention)에서 값을 스케일링하는 것을 추가하였다고 하여 스케일드 닷-프로덕트 어텐션(Scaled dot-product Attention)이라고 합니다. 이제 그림을 통해 이해해봅시다.그런데 한 가지 의문이 남습니다. 굳이 이렇게 각 Q 벡터마다 일일히 따로 연산할 필요가 있을까요?d) 행렬 연산으로 일괄 처리하기케일드 닷-프로덕트 어텐션을 수행하였던 위의 과정들을 벡터 연산이 아니라 행렬 연산을 사용하면 일괄 계산이 가능합니다. 우선, 각 단어 벡터마다 일일히 가중치 행렬을 곱하는 것이 아니라 문장 행렬에 가중치 행렬을 곱하여 Q 행렬, K 행렬, V 행렬을 구합니다. (Q, K, V가 벡터가 아니라 행렬이 되었다.)이제 행렬 연산을 통해 어텐션 스코어는 어떻게 구할 수 있을까요?어텐션 스코어 행렬을 구하였다면 남은 것은 어텐션 분포를 구하고, 이를 사용하여 모든 단어에 대한 어텐션 값을 구하는 일입니다. 이는 간단하게 어텐션 스코어 행렬에 소프트맥스 함수를 사용하고, V 행렬을 곱하는 것으로 해결됩니다. 이렇게 되면 각 단어의 어텐션 값을 모두 가지는 어텐션 값 행렬이 결과로 나옵니다.조금 더 구체적인 예시를 보면 다음과 같습니다.e) 멀티 헤드 어텐션(Multi-head Attention)이제 num_heads의 의미와 왜 차원을 축소시킨 벡터로 어텐션을 수행하였는지 이해해 보겠습니다.트랜스포머 연구진은 한 번의 어텐션을 하는 것보다 여러번의 어텐션을 병렬로 사용하는 것이 더 효과적이라고 판단하였습니다. 그래서 d_(model)의 차원을 num_heads개로 나누어 d_(model)/num_heads의 차원을 가지는 Q, K, V에 대해서 num_heads개의 병렬 어텐션을 수행합니다. 논문에서는 하이퍼파라미터인 num_heads의 값을 8로 지정하였고, 8개의 병렬 어텐션이 이루어지게 됩니다. 다시 말해 위에서 설명한 어텐션이 8개로 병렬로 이루어지게 되는데, 이때 각각의 어텐션 값 행렬을 어텐션 헤드라고 부릅니다. 이때 가중치 행렬 W_Q, W_K, W_V의 값은 8개의 어텐션 헤드마다 전부 다릅니다.병렬 어텐션으로 얻을 수 있는 효과는 무엇일까요? 그리스로마신화에는 머리가 여러 개인 괴물 히드라나 케로베로스가 나옵니다. 이 괴물들의 특징은 머리가 여러 개이기 때문에 여러 시점에서 상대방을 볼 수 있다는 겁니다. 이렇게 되면 시각에서 놓치는 게 별로 없을테니까 이런 괴물들에게 기습을 하는 것이 굉장히 힘이 들겁니다. 멀티 헤드 어텐션도 똑같습니다. 어텐션을 병렬로 수행하여 다른 시각으로 정보들을 수집하겠다는 겁니다.예를 들어보겠습니다. 앞서 사용한 예문 ‘그 동물은 길을 건너지 않았다. 왜냐하면 그것은 너무 피곤하였기 때문이다.’를 상기해봅시다. 단어 그것(it)이 쿼리였다고 해봅시다. 즉, it에 대한 Q벡터로부터 다른 단어와의 연관도를 구하였을 때 첫번째 어텐션 헤드는 ‘그것(it)’과 ‘동물(animal)’의 연관도를 높게 본다면, 두번째 어텐션 헤드는 ‘그것(it)’과 ‘피곤하였기 때문이다(tired)’의 연관도를 높게 볼 수 있습니다. 각 어텐션 헤드는 전부 다른 시각에서 보고있기 때문입니다.병렬 어텐션을 모두 수행하였다면 모든 어텐션 헤드를 연결(concatenate)합니다. 모두 연결된 어텐션 헤드 행렬의 크기는 (seq_len, d_(model))이 됩니다. (결국엔 차원이 유지된다)어텐션 헤드를 모두 연결한 행렬은 또 다른 가중치 행렬 W_O를 곱하게 되는데, 이렇게 나온 결과 행렬이 멀티-헤드 어텐션의 최종 결과물입니다. 때 결과물인 멀티-헤드 어텐션 행렬은 인코더의 입력이었던 문장 행렬의 (seq_len, d_(model))크기와 동일합니다. 트랜스포머는 다수의 인코더를 쌓기 때문에 행렬의 크기는 계속 유지되어야 합니다.f) 패딩 마스크(Padding Mask)아직 설명하지 않은 내용이 있습니다. 앞서 구현한 스케일드 닷 프로덕트 어텐션 함수 내부를 보면 mask라는 값을 인자로 받아서, 이 mask값에다가 -1e9라는 굉장히 큰 음수값을 곱한 후 어텐션 스코어 행렬에 더해주고 있습니다. 이 연산의 정체는 무엇일까요? 이는 입력 문장에 &amp;lt;PAD&amp;gt; 토큰이 있을 경우 어텐션에서 사실상 제외하기 위한 연산입니다. 트랜스포머에서는 Key의 경우에 &amp;lt;PAD&amp;gt; 토큰이 존재한다면 이에 대해서는 유사도를 구하지 않도록 마스킹(Masking)을 해주기로 했습니다. 여기서 마스킹이란 어텐션에서 제외하기 위해 값을 가린다는 의미입니다. 어텐션 스코어 행렬에서 행에 해당하는 문장은 Query이고, 열에 해당하는 문장은 Key입니다. 그리고 Key에 &amp;lt;PAD&amp;gt;가 있는 경우에는 해당 열 전체를 마스킹을 해줍니다. 어텐션 스코어 행렬이 소프트맥스 함수를 지난 후에는 해당 위치의 값은 0에 굉장히 가까운 값이 되어 단어 간 유사도를 구하는 일에 &amp;lt;PAD&amp;gt; 토큰이 반영되지 않게 됩니다.이제 두번째 서브층인 포지션-와이즈 피드 포워드 신경망에 대해서 알아보겠습니다.2) 포지션-와이즈 피드 포워드 신경망(Position-wise FFNN)지금은 인코더를 설명하고 있지만, 포지션 와이즈 FFNN은 인코더와 디코더에서 공통적으로 가지고 있는 서브층입니다. 포지션-와이즈 FFNN는 쉽게 말하면 완전 연결 FFNN(Fully-connected FFNN)이라고 해석할 수 있습니다. 앞서 인공 신경망은 결국 벡터와 행렬 연산으로 표현될 수 있음을 배웠습니다. 아래는 포지션 와이즈 FFNN의 수식을 보여줍니다.여기서 x는 앞서 멀티 헤드 어텐션의 결과로 나온 (seq_len, d_(model))의 크기를 가지는 행렬을 말합니다. 가중치 행렬 W_1은 (d_(model), d_ff)의 크기를 가지고, 가중치 행렬 W_2은 (d_ff, d_(model))의 크기를 가집니다. 논문에서 은닉층의 크기인 d_ff는 앞서 하이퍼파라미터를 정의할 때 언급했듯이 2,048의 크기를 가집니다.여기서 매개변수 W_1, b_1, W_2, b_2는 하나의 인코더 층 내에서는 다른 문장, 다른 단어들마다 정확하게 동일하게 사용됩니다. 하지만 인코더 층마다는 다른 값을 가집니다.3) 잔차 연결(Residual connection)과 층 정규화(Layer Normalization)이제 인코더에 대한 설명은 거의 다왔습니다! 트랜스포머에서는 인코더에서 추가적으로 사용하는 기법이 있는데, 바로 Add &amp;amp; Norm입니다. 더 정확히는 잔차 연결(residual connection)과 층 정규화(layer normalization)를 의미합니다.위의 그림은 앞서 Position-wise FFNN를 설명할 때 사용한 앞선 그림에서 화살표와 Add &amp;amp; Norm(잔차 연결과 정규화 과정)을 추가한 그림입니다. 추가된 화살표들은 서브층 이전의 입력에서 시작되어 서브층의 출력 부분을 향하고 있는 것에 주목합시다. 추가된 화살표가 어떤 의미를 갖고 있는지는 잔차 연결과 층 정규화를 배우고 나면 이해할 수 있습니다.a) 잔차 연결(Residual connection)이를 식으로 표현하면 x + Sublayer(x)라고 할 수 있습니다.가령, 서브층이 멀티 헤드 어텐션이었다면 잔차 연결 연산은 다음과 같습니다.b) 층 정규화(Layer Normalization)잔차 연결 후 층 정규화 연산을 수식으로 표현하자면 다음과 같습니다.LN = LayerNorm(x + Sublayer(x))이제 층 정규화를 하는 과정에 대해서 이해해봅시다. 층 정규화는 텐서의 마지막 차원(=d_(model))에 대해서 평균과 분산을 구하고, 이를 가지고 어떤 수식을 통해 값을 정규화하여 학습을 돕습니다. 여기서 텐서의 마지막 차원이란 것은 트랜스포머에서는 d_(model) 차원을 의미합니다. 아래 그림은 d_(model) 차원의 방향을 화살표로 표현하였습니다.이제 감마와 베타라는 벡터를 준비합니다. 단, 이들의 초기값은 각각 1과 0입니다.정규화의 최종 수식은 다음과 같으며, 감마와 베타는 학습 가능한 파라미터입니다.4. 인코더에서 디코더(Decoder)로지금까지 인코더에 대해서 정리해보았습니다. 이렇게 구현된 인코더는 총 num_layers만큼의 층 연산을 순차적으로 한 후에 마지막 층의 인코더의 출력을 디코더에게 전달합니다. 인코더 연산이 끝났으므로 이제 디코더 연산이 시작되어 디코더 또한 총 num_layers만큼의 연산을 하는데, 이때 매번 인코더가 보낸 출력을 각 디코더 층 연산에 사용합니다. 이제 본격적으로 디코더에 대해서 이해해봅시다.1) 디코더의 첫번째 서브층 : 셀프 어텐션위 그림과 같이 디코더도 인코더와 동일하게 임베딩 층과 포지셔널 인코딩을 거친 후의 문장 행렬이 입력됩니다. 트랜스포머 또한 seq2seq와 마찬가지로 Teacher Forcing을 사용하여 훈련되므로 학습 과정에서 디코더는 번역할 문장에 해당되는 &amp;lt;sos&amp;gt; je suis étudiant의 문장 행렬을 한 번에 입력받습니다. 그리고 디코더는 이 문장 행렬로부터 각 시점의 단어를 예측하도록 훈련됩니다.여기서 문제가 있습니다. seq2seq의 디코더에 사용되는 RNN 계열의 신경망은 입력 단어를 매 시점마다 순차적으로 받으므로 다음 단어 예측에 현재 시점 이전 이전에 입력된 단어들만 참고할 수 있습니다. 반면, 트랜스포머는 문장 행렬로 입력을 한 번에 받으므로 현재 시점의 단어를 예측하고자 할 때, 입력 문장 행렬로부터 미래 시점의 단어까지도 참고할 수 있는 현상이 발생합니다. 가령, suis를 예측해야 하는 시점이라고 해봅시다. seq2seq의 디코더라면 현재까지 디코더에 입력된 단어는 &amp;lt;sos&amp;gt;와 je뿐일 것입니다. 반면, 트랜스포머는 이미 문장 행렬로 &amp;lt;sos&amp;gt; je suis étudiant를 입력받았습니다.이를 위해 트랜스포머의 디코더에서는 현재 시점의 예측에서 현재 시점보다 미래에 있는 단어들을 참고하지 못하도록 룩-어헤드 마스크(look-ahead mask)를 도입했습니다. 직역하면 ‘미리보기에 대한 마스크’라고 할 수 있습니다.이제 자기 자신보다 미래에 있는 단어들은 참고하지 못하도록 다음과 같이 마스킹합니다.트랜스포머에는 총 세 가지 어텐션이 존재하며, 각 어텐션 시 함수에 전달하는 마스킹은 다음과 같습니다.인코더의 셀프 어텐션: 패딩 마스크디코더의 셀프 어텐션: 룩-어헤드 마스크디코더의 인코더-디코더 어텐션: 패딩 마스크2) 디코더의 두번째 서브층 : 인코더-디코더 어텐션디코더의 두번째 서브층에 대해서 이해해봅시다. 인코더-디코더 어텐션은 Query가 디코더에서 만들어진 행렬인 반면, Key와 Value는 인코더에서 온 행렬입니다.디코더의 두번째 서브층을 확대해보면, 다음과 같이 인코더로부터 두 개의 화살표가 그려져 있습니다.두 개의 화살표는 각각 Key와 Value를 의미하며, 이는 인코더의 마지막 층에서 온 행렬로부터 얻습니다. 반면, Query는 디코더의 첫번째 서브층의 결과 행렬로부터 얻는다는 점이 다릅니다. Query가 디코더 행렬, Key가 인코더 행렬일 때, 어텐션 스코어 행렬을 구하는 과정은 다음과 같습니다.인코더와 마찬가지로 디코더도 num_layers개만큼 쌓아주면 트랜스포머 모델이 완성됩니다.5. Inference6. 참조[딥러닝을 이용한 자연어 처리 입문 참조][DSBA연구실 김동화님의 Transformer &amp;amp; BERT 유튜브 강의]7. Shape by shapeMasking 보충",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-18T21:01:35+09:00'>18 Jan 2024</time><a class='article__image' href='/ai-basic-transformer'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] Transformer'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-transformer'>[AI Basic] Transformer</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] Attention",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-attention",
      "date"     : "Jan 18, 2024",
      "content"  : "Table of Contents  GOOD EXPRESSION related to Attention  Attention value 계산: Query, Key, Value  Attention에서 유의해야 할 점  Bahdanau Attention논문GOOD EXPRESSION related to Attentionseq2seq 모델은 인코더에서 입력 시퀀스를 context vector라는 하나의 고정된 크기의 벡터 표현으로 압축하고, 디코더는 이 context vector에 기반해 출력 시퀀스를 만들어냈습니다.하지만 마지막 context vector만을 이용해 출력 시퀀스를 만들어 내는 것으로는 좋은 성능을 갖는 machine translation 모델을 만들기 힘듭니다. (정보 손실, 기울기 소실 문제)그래서 이를 위한 대안으로 Attention이 등장하게 되었습니다.Attention의 기본 아이디어는 디코더에서 출력 단어를 예측하는 매 시점마다, 인코더에서의 전체 입력 문장을 참고한다는 점입니다. 여기서 중요한 점은 전체 임베딩 벡터를 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야 할 단어와 연관이 있는 부분을 집중적으로(attention) 본다는 점입니다.모델이 단어 각각을 번역하는 과정에서, 목표 문장의 단어를 생성할 때마다 원 문장에서 가장 관련된 정보들에 대한 검색을 실시합니다.해당 모델은 디코딩할 때 디코더의 hidden state와 어텐션을 통해 인코더로부터 얻은 벡터 중 적절한 부분집합을 선택하여 사용합니다.Attention value 계산: Query, Key, ValueQ: 특정 시점의 디코더 셀에서의 hidden state (내가 찾기 위해 던진 질문)K: 입력으로 이용된 모든 임베딩 벡터의 인코더에서의 hidden states (질문과의 비교대상)V: 입력으로 이용된 모든 임베딩 벡터의 인코더에서의 hidden states (Q, K로 구해진 유사도에 관한 가중치가 가중합 되는 대상)예측해야 하는 시점에 Q를 인코더에 보냅니다. 그럼 인코더에서는 이 Q(Q: Query vector가 인코더에서 Query를 위한 가중치 매트릭스와 곱해진 값)를 바탕으로 각각의 K값들과 (K: Key가 Key를 위한 매트릭스와 곱해진 값) dot product를 수행하고 그 값을 소프트맥스 함수에 넣으면 각 V에 줘야할 가중치(Attention score)를 얻게 됩니다. 이 결과를 Attention value라고 합니다.참고로 s_t는 3개의 입력을 이용해 만들어진다 -&amp;gt; 1. 해당 셀의 입력(x_t), 2. 이전 셀의 hidden state(s_(t-1)), 3.  어텐션Attention에서 유의해야 할 점Attention은 최근 딥러닝에서 가장 주목받고 있는 메커니즘 중에 하나입니다. 여기서 주의할 점은 Attention을 사용하고 있는 모든 모델들이 위와 똑같은 방법으로 Attention을 사용하고 있지는 않다는 점입니다. 나중에 배울 Vanilla Transformer에만 해도 Self-Attention, Masked Decoder Self-Attention, Encoder-Decoder Attention과 같이 여러 Attention 메커니즘이 이용됩니다. 그래서 여기서는 그냥 포인트만 잡고 뒤에서 배울 모델들에 적용되는 Attention은 그때 그때 이해를 하면 좋을 것 같습니다.Bahdanau AttentionBahdanau Attention이 Luong Attention과 가장 큰 다른점은 Bahdanau Attention의 Query는 s_t가 아니라, s_(t-1)이라는 점이다.[Hyungcheol Noh’s Blog 참조]",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-18T21:01:35+09:00'>18 Jan 2024</time><a class='article__image' href='/ai-basic-attention'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] Attention'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-attention'>[AI Basic] Attention</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 데이터 타입",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-data-type",
      "date"     : "Jan 17, 2024",
      "content"  : "Table of Contents  데이터 타입          숫자 타입      문자열 타입      데이터 타입  원시 타입(Primitive): 숫자, 문자열, 불리언, undefined, null, Symbol  객체 타입(Object): 객체, 함수, 배열 등 원시 타입이 아닌 모든 타입숫자 타입  ECMAScript 사양에 따르면 숫자 타입의 값은 64비트 부동소수점 형식을 따른다  즉 모든 수를 실수로 처리하며, 정수만 표현하기 위한 데이터 타입이 별도로 존재하지 않는다  숫자 타입은 추가적으로 세 가지 특별한 값도 표현할 수 있다 (Infinity, -Infinity, NaN)문자열 타입  일반 문자열: 쌍따옴표(“) 또는 작은 따옴표(‘) 사용      템플릿 문자열: 백틱(`) 사용    템플릿 문자열을 사용하면, 여러 줄에 걸쳐서 나오는 문자열을 있는 그대로 저장할 수 있다 (탭, 줄바꿈 등 모두 반영된다)  또 템플릿 문자열을 사용하면 표현식을 삽입할 수 있다  일반 문자열은 탭, 줄바꿈 등을 표현하려면 이스케이프 시퀀스를 사용해야 한다  (백슬래쉬 하나인데, 하나만 하면 Jekyll Search가 안되서 두 개로 표기)          \\0: Null      \\b: 백스페이스      \\t: 탭      \\&#39;: 작은 따옴표      …      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-17T21:01:35+09:00'>17 Jan 2024</time><a class='article__image' href='/js-data-type'> <img src='/images/js_logo.png' alt='[Javascript]: 데이터 타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-data-type'>[Javascript]: 데이터 타입</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 저장소(1) S3",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-storage-s3",
      "date"     : "Jan 17, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-17T21:01:35+09:00'>17 Jan 2024</time><a class='article__image' href='/aws-storage-s3'> <img src='/images/aws_logo.png' alt='[AWS] 저장소(1) S3'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-storage-s3'>[AWS] 저장소(1) S3</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 태스크(1): 감정 분류 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-task-sentiment-classification",
      "date"     : "Jan 17, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-17T21:01:35+09:00'>17 Jan 2024</time><a class='article__image' href='/ai-nlp-task-sentiment-classification'> <img src='/images/nlp_logo.png' alt='[NLP] 태스크(1): 감정 분류 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-task-sentiment-classification'>[NLP] 태스크(1): 감정 분류 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] RNN",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-rnn",
      "date"     : "Jan 17, 2024",
      "content"  : "Table of Contents  RNN          시퀀스 데이터 vs 시계열 데이터      시퀀스 데이터      바닐라 RNN      평가      깊은 RNN 모델      양방향 RNN 모델      바닐라 RNN의 한계점      RNN시퀀스 데이터 vs 시계열 데이터  시퀀스 데이터는 순서만 중요한 데이터 (문장, 음성)  시계열 데이터는 순서뿐 아니라 데이터가 발생한 시간도 중요한 데이터 (주식, 센서 데이터)시퀀스 데이터  시퀀스 데이터는 IID가정을 대체로 위배하기 때문에, 순서를 바꾸면 데이터의 확률 분포도 바뀌게 됩니다.  이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률 분포를 다루기 위해 조건부 확률을 이용할 수 있습니다.  다음과 같은 문제를 해결하기 위한 모델을 만든다고 생각해 봅시다.이미지에 대한 설명문 달기주가 예측하기한국어 영어로 번역하기다음과 같은 문제는 입력과 출력이 시퀀스 형태를 가지고 있습니다. 이러한 시퀀스 데이터를 처리하기 위해 고안된 모델을 시퀀스 모델이라고 합니다. 그 중에서도 RNN은 딥러닝에서 가장 기본적인 시퀀스 모델입니다.one to one: 비 시퀀스 데이터를 다루는 경우one to many: 이미지 캡셔닝many to one: 주가 예측, 텍스트 분류 many to many: 번역바닐라 RNN그동안 신경망들은 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했습니다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 합니다. 그런데 그렇지 않은 신경망들도 있습니다. RNN(Recurrent Neural Network)이 그 중 하나입니다.RNN은 해당 층의 입력 데이터와 이전 층에서의 출력을 함께 입력으로 사용합니다.그리고 이전 층의 출력과 해당 층의 입력은 다음과 같이 결합되게 됩니다.(참고로 W_d와 W_h를 concatenation해서 쓸 수도 있습니다.)이를 모델에 적용해 다시 한 번 살펴보면 다음과 같습니다.이를 식으로 표현하면 다음과 같습니다.평가Loss함수 식을 보면 변수 t에 대해 theta값은 변하지 않는다 -&amp;gt; 펼쳐져 있지만 W_h와 W_d는 같은 레이어입니다.깊은 RNN 모델양방향 RNN 모델양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다.즉, RNN이 과거 시점(time step)의 데이터들을 참고해서, 찾고자하는 정답을 예측하지만 실제 문제에서는 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에 힌트가 있는 경우도 많습니다. 그래서 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다.바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다.만약 tanh가 아니라 relu를 쓴다면, 반대로 맨 앞에서 받았던 정보가 층을 거듭할수록 값이 너무 커져서 시퀀스 뒤에 위치한 데이터의 학습을 방해할 수 있습니다.이를 해결하기 위해 RNN의 advanced 버전인 LSTM과 GRU에 대해서는 다음 포스트에서 살펴보도록 하겠습니다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-17T21:01:35+09:00'>17 Jan 2024</time><a class='article__image' href='/ai-basic-rnn'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] RNN'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-rnn'>[AI Basic] RNN</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javscript] 표현식과 문",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-expression-and-statement",
      "date"     : "Jan 16, 2024",
      "content"  : "Table of Contents  표현식과 문          문      표현식      값                  리터럴          표현식이 평가되어 생성된 결과                    표현식과 문문  문(statement)은 프로그램을 구성하는 기본 단위이자 최소 실행 단위(묶음)다  문은 선언문, 할당문, 조건문, 반복문 등으로 구분된다표현식  표현식(expression)은 값으로 평가될 수 있는 문(statement)이다  문의 실행 결과가 값이면 표현식이라고 할 수 있다  표현식에는 리터럴, 식별자(변수, 함수 등의 이름), 연산자, 함수 호출 등으로 이루어질 수 있다값  자바스크립트에는 간단히 값이라 생각이 바로 드는 것들도 있지만, 값처럼 평가되지 않을 것 같지만 의외로 값으로 평가되는 것들도 있다리터럴  정수 리터럴: ex. 10  소수점 리터럴 ex. 10.5  문자열 리터럴 ex. &#39;apple&#39;  불리언 리터럴 ex. true  null 리터럴 ex. null  undefined 리터럴 ex. undefined  객체 리터럴: ex. { name: &#39;Lee&#39;, address: &#39;Seoul&#39; }  배열 리터럴: ex. [1, 2, 3]  함수 리터럴: ex. function() {}  정규 표현식 리터럴: ex. /[A-Z]+/g표현식이 평가되어 생성된 결과  리터럴 표현식: ex. 10  식별자 표현식: ex. result, person.name, arr[1]  연산자 표현식: ex. 10 + 20, sum = 10, sum !== 10      함수/메서드 호출 표현식: ex. square(), perseon.getName()    변수 선언문은 표현식이 아니다. 그래서 값으로 평가되지 않는다    var foo = var x;-&amp;gt; Syntax Error        할당문은 표현식이다. 그래서 값으로 평가할 수 있다var foo = x = 100;-&amp;gt; foo는 100으로 평가된다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-16T21:01:35+09:00'>16 Jan 2024</time><a class='article__image' href='/js-expression-and-statement'> <img src='/images/js_logo.png' alt='[Javscript] 표현식과 문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-expression-and-statement'>[Javscript] 표현식과 문</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] CNN",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-cnn",
      "date"     : "Jan 16, 2024",
      "content"  : "Table of Contents  CNN의 기초          CNN의 발단      CNN 구조                  1. 컨볼루션 연산          2. 커널(채널), 필터          3. 스트라이드, 패딩                          스트라이드(stride)              패딩(padding)                                4. 풀링 연산                    특성맵의 크기 구하기      CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기      CNN의 기초CNN의 발단그동안 앞에서 설명한 신경망은 모두 기본 전제가 노드가 서로 완전 연결되어 있는 경우였습니다. 이를 완전 연결층(Fully connected layer)라고 합니다. 이는 목적한 바를 잘 이룰 수 있도록 특징 공간 갖는 모델을 만들 수 있지만, 가중치가 너무 많아 복잡도가 너무 높습니다. 이는 모델의 학습 속도를 더디게 하며 또한 과잉적합에 빠질 가능성을 높이게 됩니다. 컨볼루션 신경망(CNN)은 부분 연결 구조로 이러한 문제를 잘 해결하며 또한 특징을 잘 추출하도록 해줍니다. 이러한 이유로 CNN은 이미지, 비전 분야에서 매우 뛰어난 성능을 발휘했으며, 음성인식이나 자연어 처리같은 다른 작업에도 사용됩니다.실제로 1958년에 데이비드 허블의 연구에서는 인간의 시각 뉴런은 부분연결 구조를 가진다는 사실을 밝혀냈습니다. 이를 조금 더 자세히 설명하면, 눈의 시각 피질 안의 많은 뉴런은 작은 국부 수용장(Local receptive field)를 가지며 이는 시야의 영역에서 작은 특정 패턴에 뉴런이 반응한다는 것입니다. 예를 들어,눈이 처음 무언가를 봤을 때는 국부 수용장에 해당하는 작은 영역이 가지는 패턴에 먼저 반응을 하고, 시각 신호가 연속적으로 뇌의 뉴런들을 통과하며 다음 뉴런들은 점점 더 큰 수용장에 있는 복잡한 패턴에 반응을 합니다. 이러한 점에서 CNN은 사람의 눈과 비슷한 원리를 가지고 있다고 할 수 있습니다.CNN은 눈의 원리와 비슷하다.부분 연결 구조로 되어 있어 비교적 구해야 하는 가중치의 갯수가 적다.학습 속도가 훨씬 빠르며, 과잉적합에 빠질 가능성 또한 낮아진다.CNN 구조보통 이미지 분류를 위한 CNN의 구조는 다음과 같습니다.1. 컨볼루션 연산사실 CNN에서의 합성곱은 실제의 합성곱과는 다릅니다. 실제의 합성곱은 필터를 뒤집어야 하지만 CNN에서는 필터를 뒤집지 않습니다. 그 이유는 어차피 필터의 가중치 값은 처음에 보통 랜덤으로 초기화하게 됩니다. 따라서 가중치를 굳이 뒤집지 않아도 상관이 없습니다. 어쨋든 CNN에서의 컨볼루션 연산은 필터가 옆으로 움직이면서 데이터와 각각 원소별 곱셈을 진행하고 그 곱셈의 결과들을 하나의 값으로 합하면 됩니다.밑의 예시를 살펴보면 3×1 + 1×(-1) + 1×1 + 7×(-1) + 2×1 + 5×(-1) = -7 이 됩니다.다음의 컨볼루션 연산은 필터의 사이즈, 스트라이드의 크기, 패딩 여부 등에 따라 결과(특성 맵)이 달라집니다.2. 커널(채널), 필터여기서 CNN에서 정말 헷갈리지만 또 중요한 개념이 등장합니다. 바로 커널(채널)과 필터입니다. 느낌이 비슷해서 헷갈릴 수 있지만 엄연히 구분되어 사용되어야 하기 때문에 여기서 한 번 짚고 넘어가도록 하겠습니다.커널은 채널과 비슷한 의미로 데이터의 커널의 수(RGB채널의 경우 3)와 필터의 커널 수는 항상 같아야 합니다.필터는 카메라 필터와 비슷하게 shape을 위한 필터, curve를 위한 필터와 같은 필터를 의미합니다.예를 들어 채널이 1(Gray scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.만약 채널이 3(RGB scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.따라서 필터의 커널의 수는 항상 데이터의 커널의 수를 따르게 되고,결과(특성 맵)의 커널 수는 항상 필터의 수를 따르게 됩니다.CNN의 목표는 특징을 잘 추출해주는 필터의 가중치를 찾는 것입니다.3. 스트라이드, 패딩스트라이드(stride)  스트라이드는 필터의 미끄러지는 간격을 조절하는 것을 말합니다.  기본은 1이지만, 2를(2pixel 단위로 Sliding window 이동) 적용하면 입력 특성 맵 대비 출력 특성 맵의 크기를 대략 절반으로 줄여줍니다.  stride 를 키우면 공간적인 feature 특성을 손실할 가능성이 높아지지만, 이것이 중요 feature 들의 손실을 반드시 의미하지는 않습니다.  오히려 불필요한 특성을 제거하는 효과를 가져 올 수 있습니다. 또한 Convolution 연산 속도를 향상 시킵니다.패딩(padding)  패딩은 데이터 양 끝에 빈 원소를 추가하는 것을 말합니다.      패딩에는 밸리드(valid) 패딩, 풀(full) 패딩, 세임(same) 패딩이 있습니다. 패딩 각각의 역할은 다음과 같습니다.                            패딩          역할                                      밸리드          평범한 패딩으로 원소별 연산 참여도가 다르다                          풀          데이터 원소의 연산 참여도를 갖게 만든다                          세임          특성 맵의 사이즈가 기존 데이터의 사이즈와 같도록 만든다                      세임패딩을 적용하면 Conv 연산 수행 시 출력 특성 맵 이 입력 특성 맵 대비 계속적으로 작아지는 것을 막아줍니다.4. 풀링 연산  풀링층은 특성 맵의 사이즈를 줄여주는 역할을 합니다.  보통 최대 풀링 또는 평균 풀링을 많이 사용합니다.    보통은 Conv연산, ReLU activation함수를 적용한 후에 풀링을 적용합니다.    풀링은 비슷한 feature 들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화 시켜 줍니다.  일반적으로는 Sharp 한 feature 가 보다 Classification 에 유리하여 최대 풀링이 더 많이 사용됩니다.  풀링의 경우 특정 위치의 feature 값이 손실 되는 이슈로 인해 최근 Advanced CNN에서는 Stride만 이용하여 모델을 구성하는 경향입니다.특성맵의 크기 구하기  5×5 입력, 3×3 필터가 스트라이드가 1이고 패딩이 없는 경우, 특성 맵의 크기는 3×3이 됩니다.CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-16T21:01:35+09:00'>16 Jan 2024</time><a class='article__image' href='/ai-basic-cnn'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] CNN'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-cnn'>[AI Basic] CNN</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: 변수",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-variable",
      "date"     : "Jan 15, 2024",
      "content"  : "Table of Contents  변수          연산과 저장      식별자      변수 선언      변수명은 어디에 등록 되는가      변수 선언은 언제 실행되는가      변수 값의 할당      변수연산과 저장  사람은 계산과 기억을 모두 두뇌에서 하지만, 컴퓨터는 연산과 기억을 수행하는 부품이 나눠져 있다  컴퓨터는 CPU를 사용해 연산하고, 메모리를 사용해 데이터를 기억한다  메모리는 데이터를 저장할 수 있는 메모리 셀의 집합체다  메모리 셀 하나의 크기는 1바이트(8비트)이며, 컴퓨터는 메모리 셀의 크기(1바이트) 단위로 데이터를 저장하거나 읽어들인다  각 셀은 고유의 메모리 주소를 갖는다. 메모리 주소는 메모리 공간의 위치를 나타내며, 0부터 시작해 메모리 크기만큼 정수로 표현된다      메모리에 저장되는 모든 값은 이진수로 저장된다    프로그래밍 언어는 기억하고 싶은 값을 메모리에 저장하고, 저장된 값을 읽어 들여 재사용하기 위해 변수라는 메커니즘을 사용한다  변수는 하나의 값을 저장하기 위해 확보한 메모리 공간 또는 그 메모리 공간을 식별하기 위해 붙인 이름을 말한다  간단히 말하자면 변수는 값의 위치(메모리 주소)를 가리키는 상징적인 이름이다  (직접 메모리 주소를 쓰지 않는 이유는, 컴퓨터는 그 때 그때 상황에 맞게 유동적으로 메모리를 사용하기 때문에, 사용자가 직접 제어하면 위험)식별자  식별자는 값이 저장되어 있는 메모리 주소를 기억(저장)해야 한다. (식별자 x는 값 30이 저장되어 있는 0x066F913을 기억해야 한다)  식별자는 값이 저장되어 있는 메모리 주소와 매핑관계를 맺으며, 이 매핑 정보도 메모리에 저장되어야 한다  이처럼 식별자는 값이 아니라 메모리 주소를 기억하고 있다  식별자로 매핑 관계에 있는 메모리 주소를 읽어온 후, 해당 메모리 주소를 가지는 메모리에 접근해 값을 참조한다  식별자라는 용어는 변수 이름에만 국한해서 사용되지 않는다. 예를 들어 변수, 함수, 클래스 등의 이름은 모두 식별자다  식별자는 네이밍 규칙을 준수해야 하며, 선언에 의해 자바스크립트 엔진에 식별자의 존재를 알린다변수 선언  변수 선언이란, 값을 저장하기 위한 메모리 공간을 확보하고, 변수명과 메모리 주소를 바인딩하는 것  변수를 선언할 때는 var, let, const 키워드를 사용한다  변수를 선언한 직후에는 확보된 메모리 공간에 undefined라는 값이 암묵적으로 할당되어 초기화 된다  (이렇게 초기화를 하는 이유는 혹시라도 이전에 다른 애플리케이션이 사용했던 값이 남아 있을 수 있는 문제를 해결하려고)변수명은 어디에 등록 되는가  변수명을 비롯한 모든 식별자는 실행 컨텍스트에 등록된다  실행 컨텍스트는 자바스크립트 엔진이 소스코드를 평가하고 실행하기 위해 필요한 환경을 제공하고,  코드의 실행 결과를 실제로 관리하는 영역이다  변수명과 변수 값은 실행 컨텍스트 내에 키-값 형태의 객체로 등록되어 관리된다변수 선언은 언제 실행되는가  변수 선언은 소스코드가 한 줄씩 실행되는 시점, 즉 런타임이 아니라 소스 코드의 평가 과정 단계에서 먼저 실행된다  (변수 뿐만 아니라, var, let, const, function, class 키워드를 사용해서 선언하는 모든 식별자)  이렇게 선언문이 마치 코드의 맨 위에 있는 것과 같은 자바스크립트의 특징을 호이스팅이라 한다변수 값의 할당  변수에 값이 실제로 할당되는 코드는 런타임에 실행된다  설령 var score = 80; 이라고 선언과 동시에 할당하더라도, 따로 분리되어 각각 평가 시점와 런타임에 실행된다  심지어 선언된 시점의 초기화된 undefined의 메모리 주소와, 할당된 값의 메모리 주소는 다르다      (근데 이러면 선언 시점에 메모리 공간을 확보하는게 무슨 의미지? 어차피 다른 빈 메모리 공간 또 찾아서 거기에 실제 값을 저장하는데)    재할당은 현재 변수명이 가리키는 변수값을 버리고, 새로운 값을 가리키도록 하는것이다  (기존의 변수값은 더이상 참조되지 않아, 가비지 컬렉터에 의해 메모리에서 자동 해제된다)  변수 선언 키워드중에 var과 let은 재할당이 되고, const는 재할당이 안된다  (그러면 const score = 80; 은 어떻게 undefined로 초기화 되고, 80이라는 값으로 재할당 되는거지)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-15T21:01:35+09:00'>15 Jan 2024</time><a class='article__image' href='/js-variable'> <img src='/images/js_logo.png' alt='[Javascript]: 변수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-variable'>[Javascript]: 변수</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] PLM(3): GPT (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-plm-gpt",
      "date"     : "Jan 15, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-15T21:01:35+09:00'>15 Jan 2024</time><a class='article__image' href='/ai-nlp-plm-gpt'> <img src='/images/nlp_logo.png' alt='[NLP] PLM(3): GPT (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-plm-gpt'>[NLP] PLM(3): GPT (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 최적화",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-optimization",
      "date"     : "Jan 15, 2024",
      "content"  : "Table of Contents  딥러닝 성능 향상을 위한 요령          딥러닝 성능 향상의 방향성      데이터 정규화      가중치 초기화(Initialization)      배치 정규화(Batch Normalization)      그래디언트 모멘텀(Gradient Momentum)      적응적 학습률(Adaptive Learning-rate)      딥러닝 성능 향상을 위한 요령딥러닝 성능 향상의 방향성  과잉 적합 방지를 통한 일반화 능력 극대화 (실전에 배치되었을 때의 성능을 극대화)  학습 알고리즘의 속도 향상 (더 빠른 학습은 결국 더 좋은 성능을 가져다 준다)데이터 정규화  데이터가 양수, 음수 값을 골고루 갖도록 한다 =&amp;gt; 평균: 0  특성 scale이 같도록 한다 =&amp;gt; 표준편차: 1가중치 초기화(Initialization)역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전파하면서 진행된다. 그런데 알고리즘이 하위층으로 진행될수록 그래디언트가 작아지는 경우가 많다. 이 문제를 그래디언트 소실이라고 한다. 어떤 경우엔 반대로 그래디언트가 점점 커져 여러 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산한다. 이 문제를 그래디언트 폭주라고 하며 순환 신경망에서 주로 나타난다. 일반적으로 불안정한 그래디언트는 심층 신경망 훈련을 어렵게 만든다. 층마다 학습 속도가 달라질 수 있기 때문이다.기계 학습 초기에는 가중치 초기화를 정규분포 형태(평균:0, 분산:1)를 갖도록 초기화하였다. 하지만 입력층의 노드 수가 많다면 출력층의 분포는 밑에 그림과 같이 값들이 대부분 0이나 1로 수렴하게 된다. 문제는 0과 1근처에서 활성화 함수의 그래디언트가 거의 0에 가깝다는 것이다. 그렇기 때문에 아래층까지 역전파가 진행되기도 전에 이미 그래디언트가 거의 소실된다.그렇다면 출력층의 분포가 어떤게 좋을까? 출력층의 값이 고르게 분포해 시그모이드 함수의 비선형성과 적당한 그래디언트를 갖도록 하는 것이 좋을 것이다.그렇다면 왜 이런 분포가 안되는 걸까? 아마 그 이유는 입력층(각 층은 다음 층의 입력층이므로 결국 모든 층)의 노드 수가 많으면 가중치와 데이터가 정규분포를 갖는다고 하더라도 모두 더하게 되면 출력층에 sum(wx)의 값이 치우치게 되고 그러면 sigmoid(sum(wx))는 0또는 1로 주로 분포하게 될 것 이다. 따라서 이를 완화시켜주기 위해서는 입력층의 노드 수가 많다면 그만큼 가중치의 분산을 작게 하여 최대한 작은 값을 갖도록 하면 sum(wx)의 값이 치우치게 되지 않도록 해줄 것이다. 이와 관련한 몇 가지 초기화 방법을 살펴보자.🔔 가중치 초기화는 Gradient vanishing문제를 완화시켜준다.  평균이 0인 정규 분포를 갖도록 한다  표준편차 크면 그리고 노드의 갯수도 많으면 값이 특정 부분에 몰리게 된다.배치 정규화(Batch Normalization)배치 정규화는 각 층에서 활성화 함수를 통과하기 전이나 후에 입력을 정규화한 다음, 두 개의 새로운 파라미터(𝛾, 𝛽)로 결과값의 스케일을 조정하고 이동시킨다. 정규화 하기 위해서는 평균과 표준편차를 구해야 한다. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가한다. 테스트 시에는 어떻게 할까? 간단한 문제는 아니다. 아마 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 한다. 이 경우 입력의 평균과 표준편차를 계산할 방법이 없다. 샘플의 배치를 사용한다 하더라도 매우 작거나 독립 동일 분포(IID)조건을 만족하지 못할 수도 있다.딥러닝에서는 이를 층의 입력 평균과 표준편차의 이동 평균(moving average)을 사용해 훈련하는 동안 최종 통계를 추정함으로써 해결한다.정리하면 배치 정규화 층마다 네 개의 파라미터 벡터가 학습된다.  𝛾(출력 스케일 벡터)와 𝛽(출력 이동 벡터)는 일반적인 역전파를 통해 학습된다. 𝜇(최종 입력 평균 벡터)와 𝜎(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정된다. 𝜇와 𝜎는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용된다.(배치 입력 평균과 표준편차를 대체하기 위해)배치 정규화는 다음과 같은 이점이 있다  Gradient vanishing문제를 완화시켜준다  Learning rate 높여도 학습이 잘된다  일반화 능력이 좋아진다그래디언트 모멘텀(Gradient Momentum)모멘텀은 학습을 좀 더 안정감 있게 하도록 해준다. 데이터에 의해 Gradient를 계산할 때 만약 Noisy한 데이터인 경우, Gradient가 잘못된 방향으로 갈 가능성이 크다. 그렇기 때문에 그 동안 누적된 Gradient를 감안하여 Gradient가 Noisy한 데이터에 의한 안 좋은 영향을 줄여준다.  Momentum method can accelelerate gradient descent by taking accounts of previous gradients in the update rule equation in every iteration적응적 학습률(Adaptive Learning-rate)가중치 업데이트의 척도가 되는 학습률을 각 가중치의 학습 진행 정도에 따라 다르게 바꿔주는 것을 적응적 학습률이라고 한다. 예를 들어, 가중치의 업데이트가 많이 이루어질수록 점점 학습률을 줄여나간다. 또 특성마다 업데이트가 많이 된 특성은 학습률을 줄이고, 적게된 특성은 학습률을 늘린다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-15T21:01:35+09:00'>15 Jan 2024</time><a class='article__image' href='/ai-basic-optimization'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 최적화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-optimization'>[AI Basic] 최적화</a> </h2><p class='article__excerpt'>딥러닝 학습의 최적화는 주로 학습 속도를 빠르게 하고, 과잉 적합을 방지하는데 주 목적이 있다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Linux]: 프로세스",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/linux_one",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/linux_one'> <img src='/images/linux_logo.png' alt='[Linux]: 프로세스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux_one'>[Linux]: 프로세스</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 보안: 인증/권한/암호화 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-security",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/mongodb-security'> <img src='/images/mongo_logo.png' alt='[MongoDB] 보안: 인증/권한/암호화 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-security'>[MongoDB] 보안: 인증/권한/암호화 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 샤딩 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-sharding",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents  샤딩 알고리즘샤딩 알고리즘",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/mongodb-sharding'> <img src='/images/mongo_logo.png' alt='[MongoDB] 샤딩 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-sharding'>[MongoDB] 샤딩 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Javascript]: Intro",
      "category" : "language",
      "tags"     : "javascript",
      "url"      : "/js-intro",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents  프로그래밍          프로그래밍이란?      프로그래밍 언어        자바스크립트          자바스크립트의 역사      자바스크립트의 특징      자바스크립트 성장의 촉매제      프로그래밍프로그래밍이란?  프로그래밍은 컴퓨터를 이용해 프로그램, 서비스, 플랫폼, 소프트웨어와 같은 것들을 만드는 것을 의미한다  프로그래밍을 하면 항상 특정 기능을 사용자에게 제공하기 위해 요구사항이 생기고, 그러한 요구사항을 해결하기 위한 문제 해결 능력이 필요하다  프로그래밍에서 문제 해결 능력을 갖기 위해서는 컴퓨팅 사고, 즉 컴퓨터 입장에서 문제를 바라볼 수 있어야 한다프로그래밍 언어  우리가 고안해낸 해결 방안을 수행하는 주체는 컴퓨터이다. 그러기 위해서는 컴퓨터가 이해할 수 있는 언어로 명령을 전달해야 한다. 컴퓨터가 이해할 수 있는 언어를 기계어라고 한다. 기계어는 이진수로 이루어져 있기 때문에, 사람이 이해하는데 어려움이 있다  그래서 먼저 인간에게 친숙한 언어로 코드를 작성하고, 일정 단계를 거쳐 기계가 이해할 수 있는 언어로 변환해 컴퓨터에게 넘겨주는 것이 좋다. 이런 변환을 위해 보통 컴파일러 또는 인터프리터를 사용한다  정리하면 프로그래밍 언어는 인간 친화적이며 컴퓨터에게 명령을 전달할 수 있는 언어를 의미한다. 프로그래밍 언어에는 C언어, Java, Python, Javascript 등이 있다자바스크립트자바스크립트의 역사  1995년, 넷스케이프 커뮤니케이션즈(이하 넷스케이프)가 웹 브라우저 시장을 장악하고 있었다  넷스케이프 웹페이지의 보조적인 기능을 제공하기 위해, 브라우저에서 동작하는 프로그래밍 언어를 개발했고, 그게 바로 자바스크립트이다  이 후 자바스크립트와 비슷한 여러 언어들이 등장했고, 브라우저마다 조금씩 다른 프로그래밍 언어를 사용하게 되어 크로스 브라우징 이슈가 심각한 문제가 되었다  넷스케이프는 컴퓨터 시스템의 표준을 관리하는 ECMA 인터네셔널에 자바스크립트의 표준화를 요청했고, 1997년 ECMA-262라 불리는 자바스크립트 표준의 초판 사양이 완성되었다  상표권 문제 때문에 자바스크립트의 정식 명칭은 ECMAScript가 되었고, 2015년 ECMAScript6 (ES6) 부터 범용 프로그래밍 언어로서 갖춰야 할 다양한 기능을 갖추게 되었다자바스크립트의 특징  자바스크립트는 웹 브라우저에서 유일하게 동작하는 프로그래밍 언어이다  자바스크립트는 개발자가 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어이다. 그래서 별도의 실행 파일이 생기지 않는다  인터프리터 언어이지만, 런타임에 일부 코드는 컴파일러가 빠르게 동작되는 머신 코드로 변환하고, 인터프리터는 소스 코드를 런타임에 한 줄씩 즉시 바이트 코드로 변환한 후 실행함으로써 인터프리터 언어의 느린 처리 속도 문제를 많이 개선했다  명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어이다자바스크립트 성장의 촉매제  Ajax: 비동기 방식으로 브라우저와 서버간의 데이터 교환을 가능하도록 한 기능으로, Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 요청하고, 변경된 부분만 재렌더링할 수 있게 되었다.  V8 엔진: 구글에서 개발한 자바스크립트를 해석하고 실행하는 엔진이다  Node.js: V8 엔진으로 빌드된 자바스크립트 런타임 환경으로, Node.js의 등장으로 자바스크립트를 브라우저 밖에서도 실행할 수 있게 되었다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/js-intro'> <img src='/images/js_logo.png' alt='[Javascript]: Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/js-intro'>[Javascript]: Intro</a> </h2><p class='article__excerpt'>이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] PLM(2): BERT (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-plm-bert",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/ai-nlp-plm-bert'> <img src='/images/nlp_logo.png' alt='[NLP] PLM(2): BERT (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-plm-bert'>[NLP] PLM(2): BERT (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 분류",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-classification",
      "date"     : "Jan 14, 2024",
      "content"  : "Table of Contents  분류(Classification)          손실 함수      분류(Classification)  분류는 강아지와 고양이와 같은 이중 분류(Binary classification) 또는 자동차, 비행기, 배와 같은 다중 분류(Multi class classification)로 구분된다  회귀 모델과 거의 비슷하다. 다른 점은 출력층에 이중 분류는 시그모이드(sigmoid) 함수, 다중 분류는 소프트맥스(softmax) 함수를 사용해, 출력값을 각 클래스가 될 확률로 바꿨다는 점이다  보통 은닉층에는 ReLU 함수, 이진 분류의 출력층에는 Sigmoid 함수, 다중 분류의 출력층에는 Softmax 함수를 쓴다손실 함수  분류 문제는 출력층이 특정 클래스가 될 확률 값이다  확률 값을 출력으로 할 때 손실 함수는 어떻게 정의하는게 좋을까?      회귀 문제에서의 MSE 손실함수는, 타겟 값의 확률은 1, 타겟이 아닌 값의 확률은 0으로 동시에 보낼 수 없다    타겟 값(p)이 1일 때 확률(q)를 최대로 하고, 타겟 값이 0일 때 확률을 최소로 하자  이러한 로직을 의사(Pseudo) 코드로 나타내보면 아래와 같다p: 예측할 값 (ex. 0 또는 1)if p == 1:    q 를 최대화else if p == 0:    q 를 최소화 (-q를 최대화 -&amp;gt; 1-q를 최대화)  이를 하나의 값으로 축약하면 아래와 같다  이 값은 곧 Likelihood를 의미한다  그래서 결국 분류 문제는 최대우도추정(MLE)을 하는 것과 같다  하지만 확률은 0과 1사이의 값이기 때문에 여러 데이터에 대해 최대우도추정을 하기에는 그 값이 계속 작아진다  또 모델 학습에서 비용 함수(Cost function)은 무엇인가를 최소화하는 것이 일반적이다  그래서 아래와 같은 과정을 추가한다  위의 파란색을 음의 Log likelihood 값이라 하고 이를 크로스 엔트로피라고 한다  결국 비용함수는 크로스 엔트로피이고, 분류문제는 크로스 엔트로피를 최소로 하도록 한다  이 비용함수에 대한 정규방정식은 없지만, convex function이라서 경사하강법을 이용하면 해(모델 파라미터)를 구할 수 있다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-14T21:01:35+09:00'>14 Jan 2024</time><a class='article__image' href='/ai-basic-classification'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 분류'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-classification'>[AI Basic] 분류</a> </h2><p class='article__excerpt'>분류 모델은 출력층에 이중 분류는 시그모이드 함수, 다중 분류는 소프트맥스 함수를 사용해, 출력값을 각 클래스가 될 확률로 바꿨다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 네트워크(1) VPC",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-network-vpc",
      "date"     : "Jan 13, 2024",
      "content"  : "Table of Contents  VPC  VPC 구성요소          서브넷                  서브넷의 종류          CIDR                    인터넷 게이트웨이      라우트 테이블      보안그룹                  보안그룹 규칙 사항          소스 또는 대상에 올 수 있는 요소                    NACL      NAT Gateway      Bastion Host      VPC  논리적으로 격리된 가상 네트워크 공간을 만들어 AWS 리소스를 이용할 수 있도록 하는 서비스이다  네트워킹 환경(IP주소 범위, 서브넷, 라우팅 테이블, 게이트웨이 등)을 완벽하게 제어할 수 있다  보안성 높은 클라우드 환경을 구현할 수 있다  VPC는 하나의 리전에만 속할 수 있음(다른 리전으로 확장 불가능)Youtube, AWS 강의실 참조VPC 구성요소서브넷  VPC의 하위 단위로 VPC에 할당된 IP블록을 더 작은 단위로 분할한 개념이다  CIDR 방식으로 서브넷의 IP 주소를 지정한다  서브넷은 하나의 가용 영역 안에 있어야 한다서브넷의 종류  퍼블릿 서브넷          인터넷 게이트웨이를 통해 외부 인터넷과 연결되어 있는 서브넷을 말한다      안에 위치한 인스턴스에 퍼블릭 IP 부여할 수 있다      보통 웹서버 처럼 외부 사용자에게 노출되어야 하는 인프라를 포함한다        프라이빗 서브넷          외부 인터넷과 연결되어 있지 않은 서브넷      퍼블릭 IP 가 없다      보통 데이터베이스처럼 외부 사용자에게 노출되면 안되는 인프라를 포함한다      CIDR인터넷 게이트웨이  VPC가 외부의 인터넷과 통신할 수 있도록 하기 위해 만드는 통로와 같은 개념이다  라우트 테이블에서 인터넷 게이트웨이의 IP 주소를 설정해줘야 한다라우트 테이블  들어오고 나가는 트래픽에 대해 해당 트래픽이 어디로 가야 하는지 알려주는 이정표 역할을 한다  서브넷 간의 원활한 통신을 위해 라우팅 테이블을 이용하기도 한다  VPC마다 한 개의 라우트 테이블을 가질 수 있으며 VPC 생성시 자동으로 하나 생성된다  여러 개의 VPC가 하나의 라우트 테이블을 공유할 수도 있다보안그룹  보안그룹은 리소스(ex. EC2 인스턴스)에 대한 인/아웃 바운드 트래픽 규칙을 가지고 트래픽을 제어하는 가상 방화벽과 같다  리소스 단위로 보안그룹을 지정할 수 있다  인바운드 트래픽은 소스(source)와 포트로 리소스의 허용 범위를 지정할 수 있다  아웃바운드 트래픽은 대상(destination)과 포트로 외부(상대방)의 허용 범위를 지정할 수 있다  아웃바운드 트래픽은 대상과 포트를 명시하면 해당 대상, 포트의 트래  Stateful 하다: 아웃바운드 주소를 기억해서, 아웃 바운드 규칙 없이도 아웃바운드 주소로 다시 트래픽을 전송한다          예를 들어 사용자가 인스턴스에서 요청을 전송하면 해당 요청의 응답 트래픽은 인바운드 보안 그룹 규칙에 관계없이 인스턴스에 도달할 수 있다      항상 요청을 받고, 그에 대해 응답하는 형태의 리소스라면 아웃 바운드 규칙이 없어도 된다      먼저 요청을 보내거나, 또는 상대방의 특정 포트로만 트래픽을 전송하고 싶으면 아웃 바운드 규칙을 정한다        여러 개의 보안그룹을 적용하면 OR 연산자로 동작한다보안그룹 규칙 사항  허용 규칙을 지정할 수 있지만 거부 규칙은 지정할 수 없습니다.  보안 그룹을 처음 만들 때 인바운드 규칙이 없습니다. 따라서 보안 그룹에 인바운드 규칙을 추가하기 전에는 어떤 인바운드 트래픽도 허용되지 않습니다.  보안 그룹을 처음 생성하면 리소스의 모든 아웃바운드 트래픽을 허용하는 아웃바운드 규칙이 있습니다. 규칙을 제거할 수 있으며 특정 아웃바운드 트래픽만 허용하는 아웃바운드 규칙을 추가할 수 있습니다. 보안 그룹에 아웃바운드 규칙이 없는 경우 어떤 아웃바운드 트래픽도 허용되지 않습니다.소스 또는 대상에 올 수 있는 요소  단일 IPv4 주소: /32 접두사 길이를 사용해야 합니다. ex. 203.0.113.1/32  단일 IPv6 주소: /128 접두사 길이를 사용해야 합니다. ex. 2001:db8:1234:1a00::123/128  CIDR 블록 표기법으로 표시된 IPv4 주소의 범위: ex. 203.0.113.0/24  CIDR 블록 표기법으로 표시된 IPv6 주소의 범위: ex. 2001:db8:1234:1a00::/64  접두사 목록의 ID: ex. pl-1234abc1234abc123  보안 그룹의 ID: ex. sg-1234567890abcdef0NACL  Network Control Access List  보안그룹과 비슷하게 방화벽 역할을 한다  NACL은 서브넷 단위로 설정할 수 있다  NACL은 특정 IP 주소를 deny 할 수 있다  Stateless 하다: 상대방의 소스, 포트번호를 기억하지 못하기 때문에, 아웃 바운드 규칙을 작성해야 한다  규칙이 리스트 형태로 되어있는데, 각 규칙마다 규칙번호를 가지며, 낮은 번호의 규칙부터 우선 평가된다  우선 적용받는 규칙에서 Allow 된 경우, 이 후의 규칙을 평가하지 않는다Youtube, AWS 강의실 참조NAT Gateway  Network Address Translation Gateway  프라이빗 서브넷 내의 인스턴스를 인터넷과 연결하기 위한 통로 역할을 한다  프라이빗 IP 주소를 노출하지 않고, 인터넷과 통신할 수 있다  프라이빗 서브넷의 인스턴스가 VPC 외부 서비스와의 연결을 시작할 수 있지만, 외부 서비스는 연결을 먼저 시작할 수 없다  (프라이빗 서브넷의 인스턴스는 퍼블릭 NAT 게이트웨이를 통해 인터넷에 연결할 수 있지만 인터넷에서 원치 않는 인바운드 연결을 수신할 수 없습니다. 퍼블릭 서브넷에서 퍼블릭 NAT 게이트웨이를 생성하고 생성 시 탄력적 IP 주소를 NAT 게이트웨이와 연결해야 합니다. 트래픽을 NAT 게이트웨이에서 VPC용 인터넷 게이트웨이로 라우팅합니다)  퍼블릭 서브넷 내에 위치해야 한다Bastion Host  NAT 게이트웨이와 반대로, 외부 서비스가 프라이빗 서브넷 안의 인스턴스와 먼저 연결을 시작하고 싶은 경우 사용한다  (사용자가 프라이빗 서브넷 안의 인스턴스에 접속하고자 할 때 사용한다)  퍼블릭 서브넷 안의 EC2 인스턴스를 Bastion Host로 사용한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-13T21:01:35+09:00'>13 Jan 2024</time><a class='article__image' href='/aws-network-vpc'> <img src='/images/aws_logo.png' alt='[AWS] 네트워크(1) VPC'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-network-vpc'>[AWS] 네트워크(1) VPC</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] PLM(1): Intro (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-plm-intro",
      "date"     : "Jan 13, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-13T21:01:35+09:00'>13 Jan 2024</time><a class='article__image' href='/ai-nlp-plm-intro'> <img src='/images/nlp_logo.png' alt='[NLP] PLM(1): Intro (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-plm-intro'>[NLP] PLM(1): Intro (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[OS]: 프로세스",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/os-process",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/os-process'> <img src='/images/os_logo.png' alt='[OS]: 프로세스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-process'>[OS]: 프로세스</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[OS]: 메모리",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/os-memory",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/os-memory'> <img src='/images/os_logo.png' alt='[OS]: 메모리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-memory'>[OS]: 메모리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[OS]: Intro",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/os-intro",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/os-intro'> <img src='/images/os_logo.png' alt='[OS]: Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-intro'>[OS]: Intro</a> </h2><p class='article__excerpt'>운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 백업 및 복구 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-backup",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/mongodb-backup'> <img src='/images/mongo_logo.png' alt='[MongoDB] 백업 및 복구 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-backup'>[MongoDB] 백업 및 복구 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 컴퓨팅(2) Lambda (준비중)",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-computing-lambda",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/aws-computing-lambda'> <img src='/images/aws_logo.png' alt='[AWS] 컴퓨팅(2) Lambda (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-computing-lambda'>[AWS] 컴퓨팅(2) Lambda (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 문장 임베딩 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-sentence-embedding",
      "date"     : "Jan 12, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-12T21:01:35+09:00'>12 Jan 2024</time><a class='article__image' href='/ai-nlp-sentence-embedding'> <img src='/images/nlp_logo.png' alt='[NLP] 문장 임베딩 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-sentence-embedding'>[NLP] 문장 임베딩 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 복제 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-replicaset",
      "date"     : "Jan 11, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-11T21:01:35+09:00'>11 Jan 2024</time><a class='article__image' href='/mongodb-replicaset'> <img src='/images/mongo_logo.png' alt='[MongoDB] 복제 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-replicaset'>[MongoDB] 복제 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] 컴퓨팅(1) EC2",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-computing-ec2",
      "date"     : "Jan 11, 2024",
      "content"  : "Table of Contents  EC2          EC2의 특성      EC2의 구성      EC2 종류      EC2 생명주기      EC2 인스턴스의 스토리지 옵션        EC2 구성요소          EBS      보안그룹      Auto Scaling                  오토 스케일링 실습                    Load Balancer                  로드 밸런서 실습                    EC2  Elastic Compute Cloud  컴퓨팅 파워를 제공하는 서비스EC2의 특성  Virtual Machine 서비스  다양한 OS 지원  Auto Scaling을 통한 탄력적 확장/축소  다양한 인스턴스 타입 제공  사용한만큼만 지불하는 초단위 온디맨드 가격 모델EC2의 구성  인스턴스: 클라우드에서 사용하는 가상 서버로 CPU, GPU, 메모리 등 연산을 위한 하드웨어를 담당  EBS: Elastic Block Storage의 줄임말로 클라우드에서 사용하는 가상 하드디스크  AMI: EC2 인스턴스를 실행하기 위한 정보를 담고 있는 이미지  보안 그룹: 가상 방화벽EC2 종류  AMI          OS, 파일 시스템과 같은 것들을 담고 있는 이미지      AWS에서 기본적으로 다음과 같은 이미지를 제공하며, 직접 만들어 관리할 수도 있다        인스턴스 타입          아키텍처, CPU, GPU, 메모리, 용량에 따라 구분        범용(T, M): 균형 있는 컴퓨팅, 메모리 및 네트워킹 리소스를 제공하며, 다양한 여러 워크로드에 사용 가능  컴퓨팅 최적화(C): 고성능 프로세서를 활용하는 컴퓨팅 집약적인 애플리케이션에 적합  메모리 최적화(R, X): 메모리에서 대규모 데이터 세트를 처리하는 워크로드를 위한 빠른 성능을 제공  스토리지 최적화(H, I, D): 로컬 스토리지에서 매우 큰 데이터 세트에 대해 많은 순차적 읽기 및 쓰기 액세스를 요구하는 워크로드를 위해 설계  가속 컴퓨팅(G, F, P): 하드웨어 액셀러레이터를 사용하여 부동 소수점 계산이나 그래픽 처리 등의 기능을 훨씬 효율적으로 수행EC2 생명주기  중지          중지 중에는 인스턴스 요금 미청구      (단, EBS, Elastic IP 등 다른 구성요소는 청구됨)      중지 후 재시작시 퍼블릭 IP 주소 변경됨        종료          인스턴스 영구적으로 삭제      EC2 인스턴스의 스토리지 옵션      https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/Storage.html    EBS          EBS(Elastic Block Store)는 인스턴스와 연결하고 분리할 수 있는 내구성이 뛰어난 블록 수준 스토리지 볼륨을 제공합니다. 여러 EBS 볼륨을 하나의 인스턴스에 연결할 수 있습니다. EBS 볼륨은 연결된 인스턴스의 수명과 독립적으로 유지됩니다. EBS 볼륨을 암호화할 수 있습니다. 데이터의 백업 사본을 유지하기 위해 EBS 볼륨에서 스냅샷을 생성할 수 있습니다. 스냅샷은 Amazon S3에 저장됩니다. 스냅샷에서 EBS 볼륨을 생성할 수 있습니다.        인스턴스 스토어          인스턴스 스토어는 인스턴스에 블록 수준의 임시 스토리지를 제공합니다. 인스턴스 스토어 볼륨의 수, 크기 및 유형은 인스턴스 유형과 인스턴스 크기에 따라 결정됩니다. 인스턴스 스토어에 저장된 데이터는 연관 인스턴스의 수명 기간 동안에0000 유지되고, 해당 인스턴스를 중단하거나 최대 절전 모드로 전환하거나 종료하면 인스턴스 스토어 볼륨의 데이터가 손실됩니다.        EFS          EFS는 Amazon EC2에서 사용할 수 있는 확장 가능한 파일 스토리지를 제공합니다. EFS 파일 시스템을 만든 후 파일 시스템을 마운트하도록 인스턴스를 구성할 수 있습니다. 하나의 EFS 파일 시스템을 여러 인스턴스에서 실행하는 워크로드 및 애플리케이션에 대한 공통 데이터 소스로 사용할 수 있습니다.        S3          Amazon S3를 활용하면 저렴하지만 신뢰성이 있는 데이터 스토리지 인프라에 액세스할 수 있습니다. S3은 언제든지 Amazon EC2 내 또는 웹의 어디서나 원하는 데이터의 양을 저장하고 가져올 수 있게 해주어 웹 규모의 컴퓨팅 작업을 쉽게수행할 수 있도록 설계되었습니다. 예를 들어 Amazon S3를 사용하여 데이터 및 애플리케이션의 백업 복사본을 저장할 수 있습니다. Amazon EC2는 Amazon S3를 사용하여 EBS 스냅샷과 인스턴스 스토어 지원 AMI를 저장합니다.      EC2 구성요소EBS  EC2 인스턴스에서 사용할 영구 블록 스토리지 볼륨  EC2 인스턴스와 네트워크로 연결되어 있어 생명주기를 같이 할 수도 있고, 따로 분리할 수도 있음  EC2 인스턴스 하나에 여러개의 EBS를 추가로 구성할 수 있음  EC2 인스턴스와 같은 가용영역에 존재해야함  SSD, HDD 등 다양한 타입 제공  증분 형태로 스냅샷을 찍어서 S3에 저장할 수 있음보안그룹Auto Scaling  EC2 인스턴스의 숫자를 상황에 따라 탄력적으로 변경 가능하도록 도와준다  CPU 부하, 실행중인 인스턴스의 개수와 같이 특정 조건에 따라 여러 가용 영역에 걸쳐 인스턴스 숫자를 조절한다  실행되어야 하는 인스턴스의 최소, 최대 개수를 설정할 수 있다  애플리케이션이 항상 현재 트래픽 수요를 처리할 수 있는 적절한 용량을 보유하도록 보장한다오토 스케일링 실습  AMI, EC2 타입, 보안그룹 등이 정의된 시작 템플릿을 만든다  1개 이상의 가용 영역과 인스턴스의 최소, 최대 개수를 설정한다Load Balancer  트래픽을 지정한 인스턴스 그룹내에서 적절히 분산한다  오토 스케일링과 같은 방식으로 여러 개의 EC2 인스턴스를 실행중인 경우를 생각해보자  로드 밸런서가 없다면          모든 EC2 인스턴스의 IP 주소를 알고 있어야 한다      새로운 인스턴스가 추가, 삭제 될 때마다 해당 인스턴스의 IP 주소를 관리해줘야 한다        로드 밸런서를 생성하면          모든 인스턴스의 IP 주소를 알 필요없이, 로드 밸런서의 도메인 네임에 트래픽을 전송하면 된다      (로드밸런서의 IP 주소는 고정되지 않음)      로드 밸런서가 EC2 인스턴스의 IP 주소를 알아서 관리한다      특정 인스턴스에만 트래픽이 과부하되지 않고, 여러 인스턴스에 골고루 트래픽이 전달되도록 해준다        EC2의 추가/삭제를 자동으로 감지해 트래픽을 실행중인 인스턴스에만 전달한다로드 밸런서 실습  먼저 대상그룹을 통해 로드 밸런싱할 EC2 인스턴스 그룹을 만든다  로드 밸런서 유형을 선택한다  대상그룹을 지정해 로드 밸런서를 만든다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-11T21:01:35+09:00'>11 Jan 2024</time><a class='article__image' href='/aws-computing-ec2'> <img src='/images/aws_logo.png' alt='[AWS] 컴퓨팅(1) EC2'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-computing-ec2'>[AWS] 컴퓨팅(1) EC2</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Pytorch] 학습 코드 짜기",
      "category" : "AI",
      "tags"     : "pytorch",
      "url"      : "/ai-pytorch-train",
      "date"     : "Jan 11, 2024",
      "content"  : "Table of Contents  Data Loader          데이터셋      데이터 로더        학습import torchimport torchvisionimport torchvision.transforms as transformsimport torchvision.models as modelsimport torchvision.datasets as datasetsimport torch.optim as optimimport torch.nn as nnimport torch.nn.functional as Ffrom torch.utils.data import random_splitimport matplotlib.pyplot as pltimport matplotlib.image as imageimport numpy as npData Loader데이터셋  기본적으로 파이토치에서 데이터셋을 구성할 때는 PyTorch의 torch.utils.data에서 Dataset 클래스를 상속해서 만든다  이렇게 생성된 Dataset 클래스는 크게 아래와 같이 3가지 메서드로 구성된다          __init__: 일반적으로 해당 메서드에서는 데이터의 위치나 파일명과 같은 초기화 작업을 위해 동작합니다. 일반적으로 CSV파일이나 XML파일과 같은 데이터를 이때 불러옵니다. 이렇게 함으로써 모든 데이터를 메모리에 로드하지 않고 효율적으로 사용할 수 있습니다. 여기에 이미지를 처리할 transforms들을 Compose해서 정의해둡니다.      __len__: 해당 메서드는 Dataset의 최대 요소 수를 반환하는데 사용됩니다. 해당 메서드를 통해서 현재 불러오는 데이터의 인덱스가 적절한 범위 안에 있는지 확인할 수 있습니다.      __getitem__: 해당 메서드는 데이터셋의 idx번째 데이터를 반환하는데 사용됩니다. 일반적으로 원본 데이터를 가져와서 전처리하고 데이터 증강하는 부분이 모두 여기에서 진행될 겁니다. 이는 이후 transform 하는 방법들에 대해서 간단히 알려드리겠습니다.      class BasicDataset(torch.utils.data.Dataset):    def __init__(self, X, y):        super().__init__()        self.image_df = X        self.label_df = y        def __len__(self):        return len(self.image_df)        def __getitem__(self, idx):        return self.image_df.iloc[idx], self.label_df.iloc[idx]데이터 로더  데이터 로더는 모델 학습을 위해서 데이터를 미니 배치(Mini batch)단위로 제공해주는 역할을 합니다.  dataset은 앞서 우리가 만든 Dataset을 인자로 넣어주시면 됩니다!  보통 batch_size나 collate_fn와 같은 인자를 주로 사용할겁니다!          batch_size: 인자가 나타내고 있는 뜻 그대로 배치 사이즈를 의미합니다.      shuffle: 데이터를 DataLoader에서 섞어서 사용하겠는지를 설정할 수 있습니다. 앞선 경우와 다르게 데이터가 섞인 것을 확인하실 수 있습니다.      sampler / batch_sampler: sampler는 index를 컨트롤하는 방법입니다. 데이터의 index를 원하는 방식대로 조정합니다. 즉 index를 컨트롤하기 때문에 설정하고 싶다면 shuffle 파라미터는 False(기본값)여야 합니다. 불균형 데이터셋의 경우, 클래스의 비율에 맞게끔 데이터를 제공해야할 필요가 있습니다. 이럴 때 사용하는 옵션이 sampler입니다.                  SequentialSampler : 항상 같은 순서          RandomSampler : 랜덤, replacemetn 여부 선택 가능, 개수 선택 가능          SubsetRandomSampler : Samples elements randomly from a given list of indices, without replacement. (랜덤 리스트, 위와 두 조건 불가능)          WeigthRandomSampler : 가중치에 따른 확률          BatchSampler : batch단위로 sampling 가능                    collate_fn: 생각보다 많이 쓰는 옵션입니다. 보통 map-style 데이터셋에서 sample list를 batch 단위로 바꾸기 위해 필요한 기능입니다. zero-padding이나 Variable Size 데이터 등 데이터 사이즈를 맞추기 위해 많이 사용합니다. 데이터가 로더에 의해 return 되기 전에 collate_fn을 거쳐서 그 결과가 return된다      drop_last: batch 단위로 데이터를 불러온다면, batch_size에 따라 마지막 batch의 길이가 달라질 수 있습니다. batch의 길이가 다른 경우에 따라 loss를 구하기 귀찮은 경우가 생기고, batch의 크기에 따른 의존도 높은 함수를 사용할 때 걱정이 되는 경우 마지막 batch를 사용하지 않을 수 있습니다.      from torch.utils.data import DataLoaderdataloader = DataLoader(dataset, batch_size=4, shuffle=True, drop_last=True)학습def train():    start_time = time.time()    print(f&#39;[Epoch: {epoch + 1} - Training]&#39;)    model.train()    total = 0    running_loss = 0.0    running_corrects = 0    for i, batch in enumerate(train_dataloader):        imgs, labels = batch        imgs, labels = imgs.cuda(), labels.cuda()        outputs = model(imgs)        optimizer.zero_grad()        _, preds = torch.max(outputs, 1)        loss = criterion(outputs, labels)                loss.backward()        optimizer.step()                total += labels.shape[0]        running_loss += loss.item()        running_corrects += torch.sum(preds == labels.data)                if i % log_step == log_step - 1:            print(f&#39;[Batch: {i + 1}] running train loss: {running_loss / total}, running train accuracy: {running_corrects / total}&#39;)    print(f&#39;train loss: {running_loss / total}, accuracy: {running_corrects / total}&#39;)    print(&quot;elapsed time:&quot;, time.time() - start_time)    return running_loss / total, (running_corrects / total).item()def validate():    start_time = time.time()    print(f&#39;[Epoch: {epoch + 1} - Validation]&#39;)    model.eval()    total = 0    running_loss = 0.0    running_corrects = 0    for i, batch in enumerate(val_dataloader):        imgs, labels = batch        imgs, labels = imgs.cuda(), labels.cuda()        with torch.no_grad():            outputs = model(imgs)            _, preds = torch.max(outputs, 1)            loss = criterion(outputs, labels)        total += labels.shape[0]        running_loss += loss.item()        running_corrects += torch.sum(preds == labels.data)        if (i == 0) or (i % log_step == log_step - 1):            print(f&#39;[Batch: {i + 1}] running val loss: {running_loss / total}, running val accuracy: {running_corrects / total}&#39;)    print(f&#39;val loss: {running_loss / total}, accuracy: {running_corrects / total}&#39;)    print(&quot;elapsed time:&quot;, time.time() - start_time)    return running_loss / total, (running_corrects / total).item()def test():    start_time = time.time()    print(f&#39;[Test]&#39;)    model.eval()    total = 0    running_loss = 0.0    running_corrects = 0    for i, batch in enumerate(test_dataloader):        imgs, labels = batch        imgs, labels = imgs.cuda(), labels.cuda()        with torch.no_grad():            outputs = model(imgs)            _, preds = torch.max(outputs, 1)            loss = criterion(outputs, labels)        total += labels.shape[0]        running_loss += loss.item()        running_corrects += torch.sum(preds == labels.data)        if (i == 0) or (i % log_step == log_step - 1):            print(f&#39;[Batch: {i + 1}] running test loss: {running_loss / total}, running test accuracy: {running_corrects / total}&#39;)    print(f&#39;test loss: {running_loss / total}, accuracy: {running_corrects / total}&#39;)    print(&quot;elapsed time:&quot;, time.time() - start_time)    return running_loss / total, (running_corrects / total).item()def adjust_learning_rate(optimizer, epoch):    lr = learning_rate    if epoch &amp;gt;= 3:        lr /= 10    if epoch &amp;gt;= 7:        lr /= 10    for param_group in optimizer.param_groups:        param_group[&#39;lr&#39;] = lrlearning_rate = 0.01log_step = 20model = Model1()model = model.cuda()criterion = nn.CrossEntropyLoss()optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)num_epochs = 20best_val_acc = 0best_epoch = 0history = []accuracy = []for epoch in range(num_epochs):    adjust_learning_rate(optimizer, epoch)    train_loss, train_acc = train()    val_loss, val_acc = validate()    history.append((train_loss, val_loss))    accuracy.append((train_acc, val_acc))    if val_acc &amp;gt; best_val_acc:        print(&quot;[Info] best validation accuracy!&quot;)        best_val_acc = val_acc        best_epoch = epoch        torch.save(model.state_dict(), f&quot;best_checkpoint_epoch_{epoch + 1}.pth&quot;)torch.save(model.state_dict(), f&quot;last_checkpoint_epoch_{num_epochs}.pth&quot;)plt.plot([x[0] for x in accuracy], &#39;b&#39;, label=&#39;train&#39;)plt.plot([x[1] for x in accuracy], &#39;r--&#39;,label=&#39;validation&#39;)plt.xlabel(&quot;Epochs&quot;)plt.ylabel(&quot;Accuracy&quot;)plt.legend()test_loss, test_accuracy = test()print(f&quot;Test loss: {test_loss:.8f}&quot;)print(f&quot;Test accuracy: {test_accuracy * 100.:.2f}%&quot;)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-11T21:01:35+09:00'>11 Jan 2024</time><a class='article__image' href='/ai-pytorch-train'> <img src='/images/pytorch_logo.png' alt='[Pytorch] 학습 코드 짜기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-pytorch-train'>[Pytorch] 학습 코드 짜기</a> </h2><p class='article__excerpt'>딥러닝 모델 학습 과정에 데이터를 주입하는 방법과 모델 학습에 공통적으로 적용되는 학습 과정을 코드로 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 워드 임베딩 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-word-embedding",
      "date"     : "Jan 11, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-11T21:01:35+09:00'>11 Jan 2024</time><a class='article__image' href='/ai-nlp-word-embedding'> <img src='/images/nlp_logo.png' alt='[NLP] 워드 임베딩 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-word-embedding'>[NLP] 워드 임베딩 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 경사 하강법",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-gradient-descent",
      "date"     : "Jan 11, 2024",
      "content"  : "Table of Contents  배치 경사 하강법          확률적 경사 하강법(Stochastic Gradient Descent)      배치 경사 하강법(Batch Gradient Descent)                  배치 경사 하강법 수식 과정                    배치 경사 하강법확률적 경사 하강법(Stochastic Gradient Descent)확률적 경사 하강법은 데이터 세트에서 무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산한다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용한다. 그렇기 때문에 굉장히 빠른 속도로 가중치를 업데이트 할 수 있게 된다. 하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌이 난다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것이다. 그래서 느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법이다.배치 경사 하강법(Batch Gradient Descent)배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 m(ex. 16, 32)개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용한다. 다시 말해 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 m개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 것이다.또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정이 아닌 단순 합과 곱의 연산이었다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능하다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어이다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 된다.배치 경사 하강법 수식 과정      데이터를 Batch size(m)만큼 Forward propagation시킨다            Error를 구한다            각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다                가중치를 업데이트 한다      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-11T21:01:35+09:00'>11 Jan 2024</time><a class='article__image' href='/ai-basic-gradient-descent'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 경사 하강법'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-gradient-descent'>[AI Basic] 경사 하강법</a> </h2><p class='article__excerpt'>배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 m개씩 묶음(batch)해서 그 평균 그래디언트를 가중치 업데이트에 사용하는 것이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 트랜잭션 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-transaction",
      "date"     : "Jan 10, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-10T21:01:35+09:00'>10 Jan 2024</time><a class='article__image' href='/mongodb-transaction'> <img src='/images/mongo_logo.png' alt='[MongoDB] 트랜잭션 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-transaction'>[MongoDB] 트랜잭션 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Pytorch] 모델 만들기",
      "category" : "AI",
      "tags"     : "pytorch",
      "url"      : "/ai-pytorch-model",
      "date"     : "Jan 10, 2024",
      "content"  : "Table of Contents  파이토치 모델  모델 직접 만들기          간단한 선형 모델 만들어보기      torch.nn 모듈 사용하기        모델 파헤치기  Pretrained 모델          사전학습된 모델 불러오기      사전학습된 모델 커스텀 하기      사전학습된 모델 파인튜닝 하기        모델 저장하기 / 불러오기          모델 저장하기      저장된 모델 불러오기      파이토치 모델  파이토치 모델은 기본적으로 nn.Module 클래스를 상속하여 사용한다  공식문서에 따르면 nn.Module 은 다음과 같은 기능을 한다          Base class for all neural network modules.      Your models should also subclass this class.      Modules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes      모델 직접 만들기간단한 선형 모델 만들어보기  딥러닝 모델의 기본적인 기능을 제공 받기 위해 nn.Module 을 상속받는다  모델의 가중치는 Parameter()를 통해 생성한다  순전파(forward propagation)를 통해 모델의 예측값을 얻기 위해 forward() 메서드를 구현한다import torchimport torch.nn as nnfrom torch.nn.parameter import Parameterclass MyLinearModel(nn.Module):    def __init__(self, in_features, out_features):        super(MyLinearModel, self).__init__()        self.W = Parameter(torch.ones((out_features, in_features)))        self.b = Parameter(torch.ones(out_features))            def forward(self, x):        mm = torch.mm(x, self.W.T)        output = mm + self.b        return outputmodel = MyLinearModel(3, 2) # 모델 생성x = torch.randn((10, 3)) # 입력 데이터prediction = model(x) # 순전파를 통한 예측값 반환  가중치(weight)와 바이어스(bias)는 업데이트되어야 할 값이기 떄문에 Parameter로 정의한다  이동평균과 같이 학습은 필요없지만 모델을 저장할 때 같이 저장하고 싶은 값들은 Buffer로 정의한다  파이토치에서 사용되는 변수들을 정리하면 다음과 같다          Tensor                  ❌ gradient 계산          ❌ 값 업데이트          ❌ 모델 저장시 값 저장                    Parameter                  ✅ gradient 계산          ✅ 값 업데이트          ✅ 모델 저장시 값 저장                    Buffer                  ❌ gradient 계산          ❌ 값 업데이트          ✅ 모델 저장시 값 저장                    class Model(nn.Module):    def __init__(self):        super().__init__()        self.parameter = Parameter(torch.Tensor([7]))        self.tensor = torch.Tensor([7])        self.register_buffer(&#39;buffer&#39;, self.tensor) # tensor를 buffer로 등록torch.nn 모듈 사용하기  Linear를 비롯해 Convolution, RNN, BatchNorm, Sigmoid, CrossEntropyLoss 등 여러 모듈들을 이미 파이토치에서 만들어놨다import torch.nn as nnmodel = nn.Linear(20, 30)x = torch.randn(128, 20)prediction = model(x)  실제로 논문에서 구현하는 여러 모듈들이 복합적으로 사용하는 모델을 구현할 때에는, 직접 기본적인 모듈을 직접 만들 필요 없이, torch.nn에서 제공해주는 모듈을 활용하면 된다class Model(nn.Module):    def __init__(self):        super(Model, self).__init__()        self.conv1 = nn.Conv2d(in_channels=1, out_channels=3, kernel_size=1, bias=True)        self.bn1 = nn.BatchNorm2d(num_features=3)        self.conv2 = nn.Conv2d(in_channels=3, out_channels=5, kernel_size=1, bias=False)    def forward(self, x):        x = F.relu(self.bn1(self.conv1(x)))        return F.relu(self.conv2(x))  그리고 논문에 있는 모델들을 보면 여러 개의 모듈을 포함하는 레이어가 수십개씩 쌓여있는 모습을 볼 수 있다  그런 모델을 구현할 때에는 모델안의 모듈들을 컨테이너로 구분해서 감싸주는게 좋다  또한 모델에서 다른 부분은 얼려두고, 특정 모듈만 학습하는 파인튜닝을 위해서라도 컨테이너 단위로 모델을 나누는게 좋다  컨테이너 종류로는 Sequential, ModuleList, ModuleDict 등이 있다  Sequential은 입력 데이터가 모든 모듈을 다 통과하고, ModuleList, ModuleDict는 동적으로 특정 모듈만 통과하도록 할 수 있다  아래는 컴퓨터 비전 분야에서 사용되는 AlexNet의 모델 코드이다class AlexNet(nn.Module):    def __init__(self, num_classes=1000):        super(AlexNet, self).__init__()        self.features = nn.Sequential(            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(64, 192, kernel_size=5, padding=2),            nn.ReLU(inplace=True),            nn.MaxPool2d(kernel_size=3, stride=2),            nn.Conv2d(192, 384, kernel_size=3, padding=1),            nn.ReLU(inplace=True),            nn.Conv2d(384, 256, kernel_size=3, padding=1),            nn.ReLU(inplace=True),            nn.Conv2d(256, 256, kernel_size=3, padding=1),            nn.ReLU(inplace=True),            nn.MaxPool2d(kernel_size=3, stride=2),        )        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))        self.classifier = nn.Sequential(            nn.Dropout(),            nn.Linear(256 * 6 * 6, 4096),            nn.ReLU(inplace=True),            nn.Dropout(),            nn.Linear(4096, 4096),            nn.ReLU(inplace=True),            nn.Linear(4096, num_classes),        )    def forward(self, x):        x = self.features(x)        x = self.avgpool(x)        x = torch.flatten(x, 1)        x = self.classifier(x)        return x모델 파헤치기  모델을 구성하는 모듈과 파라미터를 살펴보고 싶을 때가 있다. 이 때는 model 객체의 named_parameters() 메서드를 사용하면 된다for param, weight in model.named_parameters():    print(f&quot;파라미터 명: {param}&quot;)    print(f&quot;파라미터 크기: {weight.size()}&quot;)    print(f&quot;파라미터 값: {weight}&quot;)    print()------------------------------------------------파라미터 명: conv1.weight파라미터 크기: torch.Size([3, 1, 1, 1])파라미터 값: Parameter containing:tensor([[[[0.9726]]],        [[[0.2617]]],        [[[0.4475]]]], requires_grad=True)파라미터 명: conv1.bias파라미터 크기: torch.Size([3])파라미터 값: Parameter containing:tensor([0.4027, 0.9123, 0.1270], requires_grad=True)파라미터 명: bn1.weight파라미터 크기: torch.Size([3])파라미터 값: Parameter containing:tensor([1., 1., 1.], requires_grad=True)파라미터 명: bn1.bias파라미터 크기: torch.Size([3])파라미터 값: Parameter containing:tensor([0., 0., 0.], requires_grad=True)파라미터 명: conv2.weight파라미터 크기: torch.Size([5, 3, 1, 1])파라미터 값: Parameter containing:tensor([[[[-0.3509]],            ...         [[-0.4782]]]], requires_grad=True)  직접 파라미터에 접근할 수도 있다model.conv1.weight--------------------------Parameter containing:tensor([[[[0.9726]]],        [[[0.2617]]],        [[[0.4475]]]], requires_grad=True)  torchvision (또는 torchinfo)를 사용하면 조금 더 보기 좋게 프린트 해볼 수도 있다summary(model, input_size=(1, 1, 1))----------------------------------------------------------------        Layer (type)               Output Shape         Param #================================================================            Conv2d-1              [-1, 3, 1, 1]               6       BatchNorm2d-2              [-1, 3, 1, 1]               6            Conv2d-3              [-1, 5, 1, 1]              15================================================================Total params: 27Trainable params: 27Non-trainable params: 0----------------------------------------------------------------Input size (MB): 0.00Forward/backward pass size (MB): 0.00Params size (MB): 0.00Estimated Total Size (MB): 0.00----------------------------------------------------------------Pretrained 모델  딥러닝 모델은 일반적으로 파라미터가 많은 대용량 크기의 모델을 풍부한 대용량 데이터로 학습할수록 더 성능이 뛰어나다  하지만 이런 모델을 학습하는데에는 많은 비용이 들어가기 때문에 쉽지 않다  그래서 구글, 네이버 등과 같은 기업에서는 이러한 큰 시간과 비용을 들여 학습시킨 모델을 제공하는데 이러한 모델들을 사전 학습된 모델 (pre-trained model)이라고 한다  예를 들어, 대용량의 언어 데이터를 학습한 모델은 다양한 자연어 관련 태스크에 범용적으로 사용될 수 있고, 대량의 이미지 데이터를 학습한 모델은 여러 컴퓨터 비전 관련 태스크에 사용될 수 있다  이렇게 사전 학습된 모델은 그대로 사용할 수도 있고, 우리가 가지고 있는 데이터로 특별한 목적에 더 특화된 모델을 만들기 위해 파인 튜닝(fine-tuning) 할 수도 있다사전학습된 모델 불러오기  파이토치는 비전, 자연어와 같은 각 분야에 특화된 라이브러리를 제공한다. 이러한 라이브러리에는 유명한 모델들을 사전학습된 모델로 제공해준다  뿐만 아니라 허깅페이스(huggingface) 같은 곳에서도 사전학습된 모델을 제공한다import torchvision.models as modelsalexnet = models.alexnet(pretrained=True)# alexnet = models.alexnet() # 가중치는 학습되지 않은 껍데기만 가져온다alexnet---------------------------------------------------------------------------------AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace=True)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))alexnet.classifier--------------------------------------------------------------------Sequential(  (0): Dropout(p=0.5, inplace=False)  (1): Linear(in_features=9216, out_features=4096, bias=True)  (2): ReLU(inplace=True)  (3): Dropout(p=0.5, inplace=False)  (4): Linear(in_features=4096, out_features=4096, bias=True)  (5): ReLU(inplace=True)  (6): Linear(in_features=4096, out_features=1000, bias=True))사전학습된 모델 커스텀 하기  사전학습된 모델은 보통 입력층과 출력층은 우리가 원하는 형태와 다를 수 있다  예를 들어 사전 학습된 모델은 이미지 클래스 100개를 분류할 수 있는 모델이지만, 우리의 태스크는 10개만 분류하면 되는 경우가 있다      그래서 이런 문제를 해결하기 위해서 우리는 사전 학습된 모델의 일부를 추가, 수정, 삭제할 수 있어야 한다    임의의 모듈에 접근하는 방법은 다음과 같다alexnet.classifier[0]alexnet.get_submodule(&quot;classifier&quot;)[0]alexnet.get_submodule(&quot;classifier.0&quot;)alexnet._modules[&quot;classifier&quot;]._modules[&quot;0&quot;]---------------------------------------------Dropout(p=0.5, inplace=False)  모듈의 이름을 모를 때는 다음과 같이 named_modules() 메서드를 사용해 확인해 볼 수 있다for name, module in alexnet.named_modules():    print(f&quot;이름: {name}&quot;)------------------------------------------------이름: features이름: features.0이름: features.1...(생략)이름: features.11이름: features.12이름: avgpool이름: classifier이름: classifier.0...(생략)이름: classifier.5이름: classifier.6  모듈을 수정/삭제 하는 방법은 다음과 같다# 모듈 삭제alexnet.classifier[6] = nn.Identity()# alexnet.classifier.pop(6)# 모듈 수정alexnet.classifier[6] = nn.Linear(4096, 10)  모듈을 추가하는 방법은 다음과 같다# 특정 모듈의 자식 모듈로 추가하는 방법alexnet.features[11].add_module(&#39;100&#39;, nn.BatchNorm2d(256))# 자식 모듈이 아닌 그냥 모듈 추가하는 방법alexnet.features.insert(1, nn.Identity())사전학습된 모델 파인튜닝 하기  특정 부분만 학습하기 위해서는 다른 부분은 학습에서 제외해야 한다(freeze)          Weight freeze 란 해당 모듈의 graident 는 역전파 하지 않아 학습을 하지 않는다는 의미입니다.      예를 들어, 우리가 하려는 태스크가 pretrain 한 태스크와 매우 유사하다면, feature 파트는 freeze 하여 학습하지 않고 새로 정의한 task specific 파트만 학습하는 것이 좋은 방법일 수 있습니다.        모듈 객체의 requires_grad_() 메서드 사용하면 된다alexnet.features[0].requires_grad_(requires_grad=False)# 또는for param in alexnet.features[0].parameters():    param.requires_grad=False  제외되었는지(frozen) 확인해보자alexnet.features[0].weight.requires_gradalexnet.features[0].bias.requires_grad-----------------------------------------------FalseFalse  만약 특정 커스텀 태스크를 위해 모듈을 새로 추가했다면, 해당 모듈의 weight initialization을 해주는게 좋다import torch.nn.init as initdef initialize_weights(model):    &quot;&quot;&quot;    Xavier uniform 분포로 모든 weight 를 초기화합니다.    더 많은 weight 초기화 방법은 다음 문서에서 참고해주세요. https://pytorch.org/docs/stable/nn.init.html    &quot;&quot;&quot;    for m in model.modules():        if isinstance(m, nn.Conv2d):            init.xavier_uniform_(m.weight.data)            if m.bias is not None:                m.bias.data.zero_()        elif isinstance(m, nn.BatchNorm2d):            m.weight.data.fill_(1)            m.bias.data.zero_()        elif isinstance(m, nn.Linear):            m.weight.data.normal_(0, 0.01)            m.bias.data.zero_()# 이렇게 새로 추가한 모듈만 가중치 초기화를 해준다initialize_weights(model.myclassifier)모델 저장하기 / 불러오기모델 저장하기  torch.save(model.state_dict(), save_path)  state_dict()는 모델을 사용하는데 필요한 모든 파라미터와 버퍼를 저장하고 있다save_folder = &quot;./runs/&quot;save_path = os.path.join(save_folder, &quot;best.pth&quot;)   # ./runs/best.pthos.makedirs(save_folder, exist_ok=True)  torch.save(model.state_dict(), save_path)print(f&quot;{save_path} 폴더에 모델이 성공적으로 저장되었습니다.&quot;)print(f&quot;해당 폴더의 파일 리스트: {os.listdir(save_folder)}&quot;)--------------------------------------------------------./runs/best.pth 폴더에 모델이 성공적으로 저장되었습니다.해당 폴더의 파일 리스트: [&#39;best.pth&#39;]저장된 모델 불러오기  model.load_state_dict(torch.load(save_path))new_model = Model()new_model.load_state_dict(torch.load(save_path))print(f&quot;{save_path} 에서 성공적으로 모델을 load 하였습니다.&quot;)--------------------------------------------------------./runs/best.pth 에서 성공적으로 모델을 load 하였습니다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-10T21:01:35+09:00'>10 Jan 2024</time><a class='article__image' href='/ai-pytorch-model'> <img src='/images/pytorch_logo.png' alt='[Pytorch] 모델 만들기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-pytorch-model'>[Pytorch] 모델 만들기</a> </h2><p class='article__excerpt'>파이토치 모델은 기본적으로 nn.Module 클래스를 상속하여 사용한다. 모델을 구현할 때에는, 직접 기본적인 모듈을 직접 만들 필요 없이, torch.nn에서 제공해주는 모듈을 활용하면 된다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 언어 모델링 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-language-modeling",
      "date"     : "Jan 10, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-10T21:01:35+09:00'>10 Jan 2024</time><a class='article__image' href='/ai-nlp-language-modeling'> <img src='/images/nlp_logo.png' alt='[NLP] 언어 모델링 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-language-modeling'>[NLP] 언어 모델링 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 인공 신경망 학습",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-neural_network_train",
      "date"     : "Jan 10, 2024",
      "content"  : "Table of Contents  신경망 학습          손실 함수                  MSE                    가중치 업데이트      신경망 학습  앞에서 살펴본 인공 신경망은 여러 층의 은닉층을 포함하고 있었다  각각의 은닉층에는 가중치라는 매개변수가 있다  학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 찾아내는 것을 말한다  가중치 매개변수의 최적값은 신경망 모델의 출력이 실제 값에 최대한 가까워질 때의 값을 의미한다손실 함수  손실 함수는 신경망 모델의 출력과 실제 값의 차이를 수학적으로 정의한 함수를 의미한다  모델의 출력과 실제 값이 가까워지려면 두 값의 차이가 최소가 되어야 하므로, 우리는 손실 함수가 최소값이 되도록 학습을 하면 되는 것이다  손실 함수의 종류는 대표적으로 평균 제곱 오차(mean squared error, MSE)와 크로스 엔트로피 오차(cross entropy error)가 있다MSE  Mean Squared Error  회귀 문제에서 대표적으로 많이 사용되는 손실 함수이다  (크로스 엔트로피 오차는 분류 문제에서 대표적으로 사용된다. 크로스 엔트로피는 나중에 분류 문제를 다루는 글에서 따로 다루도록 하겠다)  데이터 (x(입력), y(정답레이블)) 쌍이 총 100개 있다고 해보자  입력 데이터 1개마다 신경말 모델의 출력 y_hat이 있고 이 값이 정답 레이블 y 1개와의 차이가 있다  하지만 입력 데이터 1개에만 잘 맞는 모델이 아니라, 데이터 100개에 대해 평균적으로 잘 맞는 모델의 가중치를 찾아야 한다  그래서 전체 데이터 개수만큼 발생한 오차의 평균을 구해야 한다  또한 첫 번째 쌍에서 오차가 -2이고, 두 번째 쌍에서 오차가 +2 였다면 둘을 더하면 0이 되어서 오차가 상쇄되는 문제가 생긴다  그래서 단순히 오차가 아니라, 제곱 오차를 구해야 한다  (절대값이 아니라 제곱을 하는 이유는 제곱이 미분이 가능하기 때문이다. 미분이 되는 것이 나중에 배울 경사 하강법에서 이점이 있다)가중치 업데이트  학습은 최적의 가중치 매개변수를 찾아내는 것이다  그러려면 학습 데이터((x, y))를 통과시킬 때마다 구해지는 오차를 최소화시키도록 가중치(w)를 업데이트 해야한다  은닉층에 있는 수많은 가중치들을 각각 어느정도 더하고 빼야 할까?  딥러닝에서는 이를 위해 경사 하강법(Gradient descent) 방법을 사용한다  그래디언트는 함수값을 가장 빠르게 증가시키는 방향이기 때문에 마이너스(-)를 취해주면 가장 빠르게 감소시키는 방향을 가르킨다      그래서 각 가중치 값마다 손실 함수의 미분을 구한 후, 상수값을 곱해서 이를 원래의 가중치에서 빼주면, 가중치는 점점 최적의 가중치에 가까워진다    가중치가 하나만 있다고 생각하고 경사 하강법을 시각적으로 단순화 시켜보면 아래와 같다  따라서 가중치가 여러개인 일반적인 상황에서는 손실 함수를 각 가중치에 대해 편미분하여, 각각의 가중치에 대해 업데이트 해주면 된다  놀라운 사실은 MSE 함수의 미분 값은 결국 모델의 출력과 정답과의 차이인 오차(error)와 입력값(x)의 곱과 같다는 사실이다  마지막으로 이를 행렬로 나타내면 아래와 같다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-10T21:01:35+09:00'>10 Jan 2024</time><a class='article__image' href='/ai-basic-neural_network_train'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 인공 신경망 학습'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-neural_network_train'>[AI Basic] 인공 신경망 학습</a> </h2><p class='article__excerpt'>학습이란 훈련 데이터로부터 가중치 매개변수의 최적값을 찾아내는 것을 말한다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 쿼리 최적화 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-query-optimization",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/mongodb-query-optimization'> <img src='/images/mongo_logo.png' alt='[MongoDB] 쿼리 최적화 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-query-optimization'>[MongoDB] 쿼리 최적화 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AWS] Intro",
      "category" : "devops",
      "tags"     : "AWS",
      "url"      : "/aws-intro",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents  클라우드 서비스          클라우드 서비스 등장 배경      클라우드 서비스의 주요 특징      클라우드 서비스 분류      온 프레미스와 클라우드 서비스간의 비교        AWS          주요 서비스      리전과 가용 영역      비용 체계      클라우드 서비스클라우드 서비스 등장 배경클라우드 서비스의 주요 특징클라우드 서비스 분류온 프레미스와 클라우드 서비스간의 비교AWS주요 서비스리전과 가용 영역비용 체계",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/aws-intro'> <img src='/images/aws_logo.png' alt='[AWS] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-intro'>[AWS] Intro</a> </h2><p class='article__excerpt'>클라우드 서비스 그리고 AWS의 개요에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Pytorch] 텐서 다루기",
      "category" : "AI",
      "tags"     : "pytorch",
      "url"      : "/ai-pytorch-tensor",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents  파이토치  디바이스 (CPU, GPU, MPS 등)          디바이스 확인하기      디바이스 설정하기      텐서에 디바이스 할당하기        텐서 속성  텐서 생성          torch.Tensor      torch.tensor      torch.as_tensor      torch.zeros, torch.zeros_like, torch.ones, torch.ones_like      torch.empty, torch.empty_like, torch.full, torch.full_like      torch.arange, torch.linspace      torch.rand, torch.randn, torch.randint        텐서 조작          텐서 이어붙이기                  torch.cat          torch.stack, torch.vstack, torch.hstack, torch.dstack                    텐서 쪼개기                  torch.split                    텐서 형변환      텐서 인덱싱                  torch.index_select                    텐서 모양 바꾸기                  torch.reshape          Tensor.view()          torch.transpose          torch.squeeze(), torch.unsqueeze()                      텐서 연산과 함수  텐서 곱          행렬곱      행렬 원소간 곱      벡터 내적      배치 행렬곱        자동 미분과 그래디언트          Autograd      파이토치  파이토치는 딥러닝 프레임워크중 하나다  파이토치는 넘파이(Numpy) 배열과 유사한 텐서(Tensor)를 사용한다  파이토치는 GPU를 활용한 딥러닝 코드를 작성하는데 편리한 기능을 제공한다  텐서는 기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다.  텐서는 “자동 미분” 기능을 제공한다.디바이스 (CPU, GPU, MPS 등)  파이토치는 텐서간의 연산을 할 때, 텐서들이 서로 같은 디바이스(Device)안에 있어야 한다  따라서 가능하면, 연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행하는 것이 좋다  (GPU가 없다면 다른 장비(CPU, MPS)도 된다)디바이스 확인하기torch.cuda.is_available() # GPU 사용 가능 여부torch.cpu.is_available() # CPU 사용 가능 여부torch.backends.mps.is_available() # MPS 사용 가능 여부디바이스 설정하기  만약 device를 지금 사용중인 디바이스가 아닌 다른 디바이스로 설정하려는 경우 &#39;cuda:X&#39; 처럼 X에 디바이스 ordinal을 표기해야 한다. 하지만 지금 현재 장비 사용하려는 경우 0 붙이거나 아니면 아예 생략해도 된다. 그래서 나는 그냥 생략할 예정이다  device로 문자열 &#39;cuda&#39; 이런식으로 전달해도 되고, torch.device(&#39;cuda&#39;) 이런 객체를 전달해도 된다device = &quot;cpu&quot;if torch.cuda.is_available():    device = &quot;cuda:0&quot;elif torch.backends.mps.is_available():    device = &quot;mps&quot;print(device)--------------------&quot;mps&quot;# 이렇게 한 줄로 쓸 수도 있음device = &#39;cuda&#39; if torch.cuda.is_available() else &#39;mps&#39; if torch.backends.mps.is_available() else &#39;cpu&#39;텐서에 디바이스 할당하기  Tensor.cpu(), Tensor.cuda() 써도 되지만,  device를 정의했으면 Tensor.to(device) 를 쓰는게 더 괜찮은 것 같다  (게다가 mps는 .mps() 따로 없어서 무조건 .to() 써야함)  텐서를 디바이스에 옮기는 과정은, 원본 텐서를 새로운 디바이스에 복사한다. 그래서 새로운 디바이스에 할당된 텐서를 다루기 위해서는 변수에 재할당 해야한다x = torch.Tensor([[1, 2], [3, 4]])z = x.to(device) # 이렇게 x.to(device)는 device로 x를 복사한 후 복사한 값을 반환한다print(x.device) # cpuprint(z.device) # mps: 0# 보통은 x = x.to(device) 이렇게 쓰면 된다# 참고로 .to() 는 device 뿐만 아니라 타입까지 변환할 수 있다x.to(torch.float32)x.to(device, dtype=torch.float32)# 결론,# to()를 쓰자# x = x.to(device) 처럼 device로 옮겨진 텐서를 재할당하자# 텐서를 연산할 때는 서로 같은 디바이스에 있어야 한다a = torch.tensor([    [1.0, 1.],    [2., 2.]]).to(&quot;mps&quot;)# CPU 장치의 텐서b = torch.tensor([    [5., 6.],    [7., 8.]])print(torch.matmul(a, b)) # 오류 발생--------------------------------RuntimeError: Tensor for argument #2 &#39;mat2&#39; is on CPU, but expected it to be on GPU (while checking arguments for mm)텐서 속성  텐서의 기본 속성으로는 다음과 같은 것들이 있다.          모양(shape)      자료형(data type)      저장된 장치(device)      tensor = torch.rand(2, 2)print(tensor)print(f&quot;shape: {tensor.shape}&quot;)print(f&quot;type: {tensor.dtype}&quot;)print(f&quot;device: {tensor.device}&quot;)--------------------------------------tensor([[0.0271, 0.0495],        [0.0183, 0.3877]])shape: torch.Size([2, 2])type: torch.float32device: cpu텐서 생성  텐서를 생성하는 데 있어 아래에서 설명한 것 말고도 훨씬 더 많은 방법들이 있다 (파이토치 공식문서 참고)torch.Tensor  torch.Tensor() (= torch.FloatTensor()) 말고도, torch.DoubleTensor(), torch.IntTensor(), torch.LongTensor() 등 다양한 데이터 타입을 이름으로 가지는 생성자가 있다l = [[1, 2], [3, 4]]arr = np.array([[1, 2], [3, 4]])torch.Tensor(l)torch.Tensor(arr)-----------------------tensor([[1., 2.],        [3., 4.]])torch.tensor  torch.tensor()의 가장 큰 특징은 입력으로 받은 data를 항상 새로 복사해 텐서로 만든다는 점이다  복사는 메모리 낭비를 유발하기 때문에, 상황에 맞게 대처 방법을 사용하는게 좋다          데이터를 공유하지만 메모리 효율적인 방법으로 텐서를 만들고 싶은 경우: torch.as_tensor(data)      텐서의 단순 requires_grad() 플래그만 바꾸고 싶은 경우: Tensor.requires_grad(bool)        torch.tensor()는 새로운 메모리에 텐서를 만들고 autograph에서 떼어낸다 (creates a new leaf tensor)          그래서 텐서를 복사할 때는 torch.tensor() 보다는 같은 역할이지만 더 명시적인, Tensor.clone().detach() 를 권장하기도 한다      torch.as_tensor  torch.as_tensor()는 데이터를 공유하며, autograph에 붙어있다 (preserves autograd history if possible)  이미 텐서인 경우 같은 타입, 디바이스면 그냥 원본 반환하고, 타입이 달라지거나 디바이스가 달라지면 아예 새로 복사한다  넘파이 배열의 경우 torch.from_numpy() 가 조금 더 빠르다torch.zeros, torch.zeros_like, torch.ones, torch.ones_like  torch.zeros()는 size 형태의 텐서를 0으로, torch.ones()는 size 형태의 텐서를 1로 채운다torch.zeros(size=(2, 3))torch.zeros(2, 3)  torch.zeros_like()는 input과 같은 형태의 텐서를 0으로, torch.ones_like()는 input과 같은 형태의 텐서를 1로 채운다  이 때 input은 텐서여야 한다. 넘파이 배열 같은 다른 타입을 쓰려면 torch.ones(input.shape) 이렇게 써야 한다x = torch.tensor([[1, 2], [3, 4]])z = torch.zeros_like(x)torch.empty, torch.empty_like, torch.full, torch.full_like  torch.empty(), torch.empty_like()는 비어있는 것은 아니고, 0에 가까운 초기화되지 않은 값을 가진다a = torch.empty(2, 3, dtype=torch.int32) # [2, 3] 이렇게 넣어줘도 된다b = torch.empty_like(a)  torch.full(), torch.full_like()는 fill_value로 텐서를 채워준다a = torch.full(size=(2, 2), fill_value=5, dtype=torch.float64)b = torch.full_like(a, fill_value=10)torch.arange, torch.linspace  torch.arange()는 간격을 지정할 수 있다  torch.linspace()는 개수를 지정할 수 있다torch.arange(5) # tensor([0, 1, 2, 3, 4])torch.arange(1, 4) # tensor([1, 2, 3])torch.arange(0, 2.5, 0.5) # tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000])torch.linspace(3, 10, 5) # tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])torch.rand, torch.randn, torch.randint  torch.rand()는 0과 1사이의 uniform distribution 에서 size 만큼 샘플링한 텐서를 반환한다  torch.randn()은 0과 1사이의 normal distribution 에서 size 만큼 샘플링한 텐서를 반환한다  torch.randint()는 low와 high-1 사이의 정수 값을 uniform distribution 에서 size 만큼 샘플링한 텐서를 반환한다  (rand_like(), randn_like(), randint_like()도 있다)  더 많은 샘플링 방법들이 있다 (파이토치 공식문서 참고)torch.rand(2, 3) # [2, 3] 형태의 텐서에 uniform distribution으로 샘플링한 값을 채운다torch.randn(2, 3) # [2, 3] 형태의 텐서에 normal distribution으로 샘플링한 값을 채운다torch.randint(3, 10, (2, 2)) # [2, 2] 형태의 텐서에 3과 9 사이의 정수 값을 uniform distribution으로 샘플링한 값을 채운다텐서 조작텐서 이어붙이기torch.cat  torch.cat(tensors, dim)은 dim 방향으로 텐서를 이어 붙인다. dim 방향 제외한 나머지 부분은 shape이 서로 같아야 한다x = torch.ones(size=(2, 2))y = torch.zeros(size=(2, 2))torch.cat(tensors=(x, y), dim=0)--------------------------------tensor([[1., 1.],        [1., 1.],        [0., 0.],        [0., 0.]])torch.cat(tensors=(x, y), dim=1)--------------------------------tensor([[1., 1., 0., 0.],        [1., 1., 0., 0.]])x = torch.ones(size=(2, 3))y = torch.zeros(size=(2, 4))torch.cat(tensors=(x, y), dim=1)--------------------------------tensor([[1., 1., 1., 0., 0., 0., 0.],        [1., 1., 1., 0., 0., 0., 0.]])torch.stack, torch.vstack, torch.hstack, torch.dstack  torch.stack(tensors, dim)은 새로운 차원으로 텐서를 이어 붙인다.  텐서들의 사이즈는 모두 같아야 한다  새로운 차원은 크기는 쌓은 len(tensors)가 된다# 예를 들어, 3 X 4 짜리 행렬을 5개를 새로운 차원에 이어붙인다고 하면,# 3 X 4 는 2차원 행렬이기 때문에 사이즈가 [3, 4] 이렇게 되어있다.# 여기서 새로운 차원이 생길 수 있는 자리는 [new, 3, 4], [3, new, 4] 또는 [3, 4, new] 이렇게 있다# 우리는 5개를 이어붙인다고 했기 때문에 new에 5가 들어간다a = torch.ones((3, 4))torch.stack([a, a, a, a, a], dim=0).shape # [5, 3, 4]torch.stack([a, a, a, a, a], dim=1).shape # [3, 5, 4]torch.stack([a, a, a, a, a], dim=2).shape # [3, 4, 5]  torch.vstack(tensors)은 dim=0 (세로)방향으로 쌓는다  torch.hstack(tensors)은 dim=1 (가로)방향으로 쌓는다  torch.dstack(tensors)은 dim=2 (3차원)방향으로 쌓는다  dim 방향 제외한 다른 차원의 사이즈는 같아야 한다  쌓으면 결과는 다른 차원의 사이즈는 그대로고, dim 방향의 차원만 모두 더해진다          vstack의 경우: (2, 3), (3, 3) -&amp;gt; (5, 3)      hstack의 경우: (1, 2, 3), (1, 5, 3) -&amp;gt; (1, 7, 3)      dstack의 경우: (1, 2), (1, 2) -&amp;gt; (1, 2, 2)      텐서 쪼개기torch.split  torch.split(tensor, split_size or selections, dim)  split_size(int)인 경우 split_size 값 만큼의 size 갖는 텐서로 나눈다 (batch size같은 개념)  selections(list)인 경우 selections의 element값 만큼을 split_size로 가진다 -&amp;gt; selections 원소의 합이 split하고자 하는 차원의 값과 같아야 함          (3, 16) -&amp;gt; selections: [1, 5, 10] ok      (3, 16) -&amp;gt; selections: [1, 5, 11] X        쪼개기도 vsplit, hsplit, dsplit 있음x = torch.randn(5, 16)y = torch.split(x, 3, dim=1)for batch in y:    print(batch.shape)------------------------------torch.Size([5, 3])torch.Size([5, 3])torch.Size([5, 3])torch.Size([5, 3])torch.Size([5, 3])torch.Size([5, 1])z = torch.split(x, [1, 3, 5, 7], dim=1)for batch in z:    print(batch.shape)------------------------------torch.Size([5, 1])torch.Size([5, 3])torch.Size([5, 5])torch.Size([5, 7])텐서 형변환  텐서의 자료형(정수, 실수 등)을 변환할 수 있다  Tensor.type()  Tensor.int(), Tensor.float()x = torch.ones(2, 3)print(x.dtype)---------------------torch.float32x_int = x.type(torch.int32)print(x.dtype)print(x_int.dtype)---------------------torch.float32torch.int32x_float = x.float()print(x_float.dtype)---------------------torch.float32텐서 인덱싱torch.index_select  torch.index_select(input, dim, index)  index를 통해 특정 index의 tensor를 가져온다  index는 IntTensor 또는 LongTensor type의 1-D tensor이다  input.dim()은 변하지 않는다 (3차원이면 계속 3차원)  input의 dim 사이즈는 len(index)와 같아진다x = torch.IntTensor([[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]])indices = torch.tensor([0, 2])y0 = torch.index_select(x, dim=0, index=indices)y1 = torch.index_select(x, dim=1, index=indices)----------------------------------------------------------------------xshape: torch.Size([3, 4])tensor([[ 1,  2,  3,  4],        [ 5,  6,  7,  8],        [ 9, 10, 11, 12]], dtype=torch.int32)------------------------y0shape: torch.Size([2, 4])tensor([[ 1,  2,  3,  4],        [ 9, 10, 11, 12]], dtype=torch.int32)------------------------y1shape: torch.Size([3, 2])tensor([[ 1,  3],        [ 5,  7],        [ 9, 11]], dtype=torch.int32)------------------------텐서 모양 바꾸기torch.reshape  torch.reshape(input, shape)  input과 data는 같다, shape만 다르다  가능하면 view of input 아니면 copyx = torch.arange(8.)y = torch.reshape(x, (1, 8))z = torch.reshape(y, (-1, 4))print(x.shape, y.shape, z.shape)----------------------------------torch.Size([8]) torch.Size([1, 8]) torch.Size([2, 4])Tensor.view()  Tensor.view(shape)  torch.reshape()과 비슷하지만 기존 텐서와 메모리를 공유하기 때문에 메모리를 낭비하지 않는 방식이다  텐서의 형태를 바꿀 때 인덱스는 바뀌었으나 실제 메모리 상에서는 배열을 바꾸지 않는다(위치 바꾸는 연산이 잦으면 성능을 떨어트리므로)  contiguous한 텐서만 지원한다          (contiguous한 텐서인지 확인: Tensor.is_contiguous())      (contiguous한 텐서 만들고 싶을 때: Tensor.contiguous())      Tensor.contiguous().view() 이렇게 쓰면 된다      torch.transpose  torch.transpose(input, dim0, dim1)  dim0과 dim1이 바뀐 tensor를 리턴  input과 memory를 공유x = torch.randn(2, 3, 4)y = torch.transpose(x, 0, 2)y.shape # (4, 3, 2)torch.squeeze(), torch.unsqueeze()  torch.squeeze(tensor, dim): 특정(dim) 차원의 값이 1이면 차원을 없애버린다  torch.unsqueeze(tensor, dim): 특정(dim) 차원을 추가한다x = torch.randn(2, 1, 4)y0 = torch.squeeze(x, dim=0)y1 = torch.squeeze(x, dim=1)y2 = torch.squeeze(x, dim=2)y0.shapey1.shapey2.shape--------------torch.Size([2, 1, 4])torch.Size([2, 4])torch.Size([2, 1, 4])x = torch.randn(2, 1, 4)y0 = torch.unsqueeze(x, dim=0)y1 = torch.unsqueeze(x, dim=1)y2 = torch.unsqueeze(x, dim=2)y3 = torch.unsqueeze(x, dim=3)y0.shapey1.shapey2.shapey3.shape---------------torch.Size([1, 2, 1, 4])torch.Size([2, 1, 1, 4])torch.Size([2, 1, 1, 4])torch.Size([2, 1, 4, 1])텐서 연산과 함수# 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능# 기본적으로 요소별(element-wise) 연산a = torch.tensor([    [1, 2],    [3, 4]])b = torch.tensor([    [5, 6],    [7, 8]])print(a + b)print(a - b)print(a * b)print(a / b)---------------------------tensor([[ 6,  8],        [10, 12]])tensor([[-4, -4],        [-4, -4]])tensor([[ 5, 12],        [21, 32]])tensor([[0.2000, 0.3333],        [0.4286, 0.5000]])t = torch.Tensor([[1, 3], [2, 0]])torch.sum(t, dim=0)torch.sum(t, dim=1)-------------------------------------tensor([3., 3.])tensor([4., 2.])torch.mean(t, dim=1)-------------------------------------tensor([2., 1.])torch.argmax(t, dim=0)-------------------------------------tensor([1, 0])torch.sort(t, dim=0, descending=False)-------------------------------------torch.return_types.sort(values=tensor([[1., 0.],        [2., 3.]]),indices=tensor([[0, 1],        [1, 0]]))torch.clamp(t, min=2, max=10)-------------------------------------tensor([[2., 3.],        [2., 2.]])torch.any(t)torch.all(t)-------------------------------------tensor(True)tensor(False)t = torch.arange(1, 11)t[[0, 2]]-------------------------------------tensor([1, 3])t[(t &amp;gt; 2) &amp;amp; (t &amp;lt; 8)]-------------------------------------tensor([3, 4, 5, 6, 7])t[t.remainder(2) == 0]-------------------------------------tensor([ 2,  4,  6,  8, 10])torch.where(t &amp;gt; 5, t*2, 0)-------------------------------------tensor([ 0,  0,  0,  0,  0, 12, 14, 16, 18, 20])torch.tensor([0, 0, 1, 1, 1, 2]).unique()-------------------------------------tensor([0, 1, 2])텐서 곱행렬곱# 모두 같다A.matmul(B)torch.matmul(A, B)A.mm(B)torch.mm(A, B) # This function does not broadcast. For broadcasting matrix products, see torch.matmul()A @ B-----------------tensor([[ 8., 18.],        [14., 32.]])행렬 원소간 곱# 모두 같다A.mul(B)torch.mul(A, B)A * B------------tensor([[ 2.,  9.],        [ 8., 20.]])벡터 내적# 모두 같다vec_A = torch.ravel(A)vec_B = torch.ravel(B)vec_A.dot(vec_B)torch.dot(vec_A, vec_B)----------tensor(39.)tensor(39.)배치 행렬곱batch = 32n = 10m = 20p = 30t1 = torch.rand((batch, n, m))t2 = torch.rand((batch, m, p))torch.bmm(t1, t2).shape--------------------------torch.Size([32, 10, 30])자동 미분과 그래디언트  신경망 학습은 크게 두 단계로 나뉘어진다          순전파: forward propagation을 통해prediction을 구하고 이를 이용해 loss 계산      역전파: back prop을 통해 gradients를 구하고 이를 이용해 parameter들을 adjust      # forward propprediction = model(data)# backward proploss.backward()Autograd  https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html  https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html  텐서에 requires_grad=True를 적용하면 이는 autograd가 이 tensor의 모든 연산들을 추적하도록 합니다  loss가 계산되고 loss.backward()를 호출하면 autograd는 gradient를 계산하고 이를 텐서의 .grad 속성(attribute)에 저장합니다  autograd는 실행 시점에 정의되는(define-by-run) 프레임워크입니다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/ai-pytorch-tensor'> <img src='/images/pytorch_logo.png' alt='[Pytorch] 텐서 다루기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-pytorch-tensor'>[Pytorch] 텐서 다루기</a> </h2><p class='article__excerpt'>파이토치는 딥러닝 프레임워크중 하나다. 파이토치는 넘파이(Numpy) 배열과 유사한 텐서(Tensor)를 사용한다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NLP] 텍스트 전처리 (준비중)",
      "category" : "AI",
      "tags"     : "NLP",
      "url"      : "/ai-nlp-text-preprocessing",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/ai-nlp-text-preprocessing'> <img src='/images/nlp_logo.png' alt='[NLP] 텍스트 전처리 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-nlp-text-preprocessing'>[NLP] 텍스트 전처리 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Computer Vision] Object detection (준비중)",
      "category" : "AI",
      "tags"     : "CV",
      "url"      : "/ai-cv-object-detection",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/ai-cv-object-detection'> <img src='/images/cv_logo.png' alt='[Computer Vision] Object detection (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-cv-object-detection'>[Computer Vision] Object detection (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Computer Vision] 이미지 분류 (준비중)",
      "category" : "AI",
      "tags"     : "CV",
      "url"      : "/ai-cv-image-classification",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/ai-cv-image-classification'> <img src='/images/cv_logo.png' alt='[Computer Vision] 이미지 분류 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-cv-image-classification'>[Computer Vision] 이미지 분류 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 인공신경망이란",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-neural-network",
      "date"     : "Jan 9, 2024",
      "content"  : "Table of Contents  인공신경망          퍼셉트론      DNN      비선형 활성 함수        결론인공신경망  Artificial Neural Network  인공신경망은 딥러닝을 이루는 핵심 개념이다  딥러닝은 결국 인공신경망 레이어를 깊이(deep) 쌓아서 딥러닝인 것이다퍼셉트론  퍼셉트론은 가장 간단한 인공 신경망 구조 중 하나로 1957년 프랑크 로젠블라트가 제안했다  퍼셉트론은 현대의 인공 신경망과 비교하면 상당히 원시적인 신경망이지만, 노드, 가중치, 레이어와 같은 인공 신경망의 중요한 구성요소들을 공부하는데에 큰 의미가 있다Photo by Daeki Yoon  퍼셉트론 (혹은 인공신경망)은 위 그림과 같이, 입력을 받아 각각의 가중치를 곱해 더한 후 활성 함수를 통과시켜 값을 출력하는 일종의 함수다  하지만 퍼셉트론은 비선형 분류 문제를 풀 수 없다는 치명적인 약점이 있다  이 문제는 1970년대 민스키의 『Perceptrons』에서 지적되었고, 이 후 한동안 신경망 연구가 정체기를 겪었다  이 문제를 해결하기 위해 은닉층 이라는 개념이 도입되었다  다시 XOR 문제로 돌아와 보면, XOR 문제는 주어진 x1, x2 공간에서는 데이터를 분류하는 모델을 만들 수 없다. 따라서 분류가 가능하도록 해주는 특징공간으로 옮겨야 하는데, 이를 가능하게 해주는 것이 바로 은닉층의 역할이다.  밑에 그림과 같이 두 개의 퍼셉트론을 이용해 새로운 특징공간 z1, z2로 옮기면 우리의 데이터를 분류할 수 있게 된다(z1, z2층이 바로 은닉층)DNN  Deep Neural Network  인공신경망은 일종의 함수인데, 이러한 인공신경망을 깊이 쌓으면 그것이 바로 깊은 인공 신경망(DNN)이 된다  이것은 여러 개의 함수가 적용된 합성 함수와 같다  깊은 층을 통해 비선형 분류 문제를 해결하게 되며 이를 계기로 다양한 문제에 층을 깊이 쌓은 신경망 구조가 주목을 받기 시작했다  신경망을 깊게 쌓음으로써 기계가 여러 가지 특징 공간에서 데이터를 볼 수 있게 되었고 이 방법은 실제로 비정형 데이터(음성, 사진 등)를 다루는 데에 굉장한 성능을 보여주었다비선형 활성 함수  깊은 인공신경망. 이것이 바로 딥러닝의 핵심이다  은닉층을 깊게 쌓으면 모델이 데이터를 더 다양한 특징 공간에서 볼 수 있고 이것은 마치 사람의 생각이 더 깊어지고 더 다양한 관점에서 사물을 바라보는 것과 비슷하다  하지만 은닉층을 깊이 쌓으려면 은닉층에 반드시 포함되어야 하는 요소가 있는데 바로 비선형 활성 함수(Non linear activation function)이다  깊은 인공 신경망은 여러 개의 함수가 적용된 합성 함수와 같은데, 합성 함수가 linear한 관계라면, 결국은 하나의 linear function과 다를게 없고, 이는 신경망을 깊어지는 효과를 만들어내지 못한다  그래서 인공신경망을 깊게 만들려면 하나로 합쳐지지 않는 비선형 활성 함수가 필요한 것이다결론  인공신경망은 일종의 함수다  인공신경망을 깊게 쌓으면 비선형 문제를 비롯한 복잡한 문제를 풀 수 있게 된다  인공신경망을 깊게 쌓기 위해서는 각 은닉층마다 비선형 활성 함수가 있어야 한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-09T21:01:35+09:00'>09 Jan 2024</time><a class='article__image' href='/ai-basic-neural-network'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 인공신경망이란'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-neural-network'>[AI Basic] 인공신경망이란</a> </h2><p class='article__excerpt'>인공신경망은 합과 곱으로 이루어진 일종의 함수이며, 인공신경망을 깊이 쌓으면 더 복잡한 문제를 풀 수 있게 된다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 인덱스 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-index",
      "date"     : "Jan 8, 2024",
      "content"  : "Table of Contents  인덱스  인덱스 만들기          인덱스명 표기하기        인덱스 종류          Single FIeld Index      Compound Index      Multikey Index      Text Index        인덱스 특징인덱스  인덱스는 쿼리를 더욱 효율적으로 실행하도록 도와준다  만약 인덱스가 없으면 쿼리를 실행하기 위해 컬렉션내의 모든 도큐먼트를 스캔해야 한다  인덱스가 있으면 쿼리 결과를 반환하기 위해 스캔해야 하는 도큐먼트의 수를 제한시킬 수 있다  하지만 인덱스는 쓰기 실행에 있어서는 안좋은 영향을 미친다  그래서 쓰기 비율이 훨씬 높은 컬렉션의 경우에는 인덱스를 사용하는 것을 신중히 해야한다  몽고DB에서 인덱스는 B-tree 구조이다  인덱스는 도큐먼트의 1개 이상의 특정 필드를 정렬된 상태로 저장한다  몽고DB는 컬렉션 생성시 자동으로 _id 필드로 구성된 유니크 인덱스를 생성한다인덱스 만들기  인덱스명은 변경할 수 없다. 새로운 이름을 얻기 위해서는 삭제후 새로 생성해야 한다  대부분의 케이스에 ESR 원칙을 적용해 인덱스를 만들면 효율적인 인덱스를 만들 수 있다인덱스명 표기하기  인덱스명을 직접 만들면 나중에 쿼리 결과를 분석할 때 어떤 인덱스가 사용됐는지 식별하기 쉽다db.&amp;lt;collection&amp;gt;.createIndex(   { &amp;lt;field&amp;gt;: &amp;lt;value&amp;gt; },   { name: &quot;&amp;lt;indexName&amp;gt;&quot; })  인덱스명을 따로 지정하지 않으면 다음과 같은 규칙으로 인덱스명이 자동 설정된다            Index      Default Name              { score : 1 }      score_1              { content : “text”, “description.tags”: “text” }      content_text_description.tags_text              { category : 1, locale : “2dsphere”}      category_1_locale_2dsphere              { “fieldA” : 1, “fieldB” : “hashed”, “fieldC” : -1 }      fieldA_1fieldB_hashed_fieldC-1      인덱스 종류Single FIeld Index  필드 하나로 이루어진 인덱스를 말한다  단일 필드 인덱스를 만드는 방법은 아래와 같다db.&amp;lt;collection&amp;gt;.createIndex( { &amp;lt;field&amp;gt;: &amp;lt;sortOrder&amp;gt; } )  정렬 기준은 1이 오름차순, -1이 내림차순 정렬이다db.students.createIndex( { gpa: 1 } )  내장된 도큐먼트가 가지는 필드에 대해서도 인덱스를 생성할 수 있다  아래와 같은 형태의 도큐먼트가 있을 때,{    &quot;name&quot;: &quot;Bob&quot;,    &quot;gpa&quot;: 3.2,    &quot;location&quot;: { city: &quot;Albany&quot;, state: &quot;New York&quot; }}  다음과 같이 location 필드의 내장 필드인 state에 대해서 인덱스를 만들 수 있다db.students.createIndex( { &quot;location.state&quot;: 1 } )Compound Index  여러 필드로 이루어진 인덱스를 말한다{ &quot;item&quot;: 1, &quot;location&quot;: 1, &quot;stock&quot;: 1 }  위와 같은 인덱스는 다음과 같은 필드 조합으로 이루어진 쿼리에 사용될 수 있다          item      item, location      item, location, stock      item, stock (여기서는 item만 인덱스 효과를 누릴 수 있음)        아래와 같은 필드 조합의 쿼리는 인덱스를 사용하지 않는다          location      stock      location, stock        만약 item을 사용하는 쿼리와, item, location 을 사용하는 쿼리가 있다면 { &quot;item&quot;: 1, &quot;location&quot;: 1} 인덱스 하나만 있으면 된다. { &quot;item&quot;: 1 } 인덱스도 만드는 것은 낭비이다Multikey Index      배열 필드로 이루어진 인덱스를 말한다        다음과 같은 형태의 도큐먼트가 있을 때,  { _id: 5, type: &quot;food&quot;, item: &quot;apple&quot;, ratings: [ 5, 8, 9 ] }  아래와 같이 ratings 필드를 이용해 멀티키 인덱스를 만들 수 있다db.inventory.createIndex( { ratings: 1 } )Text Index인덱스 특징",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-08T21:01:35+09:00'>08 Jan 2024</time><a class='article__image' href='/mongodb-index'> <img src='/images/mongo_logo.png' alt='[MongoDB] 인덱스 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-index'>[MongoDB] 인덱스 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[AI Basic] 인공지능이란",
      "category" : "AI",
      "tags"     : "AI_basic",
      "url"      : "/ai-basic-intro",
      "date"     : "Jan 8, 2024",
      "content"  : "Table of Contents  AI          인공지능, 머신러닝 그리고 딥러닝      딥러닝 개요                  분류와 회귀          딥러닝 맛보기                    AI  Artificial Intelligence  인간의 지능을 인공적으로 만든 것을 말한다  인공지능을 활용하면 컴퓨터가 마치 사람이 생각하는 것처럼 동작하도록 할 수 있다  하드웨어의 발달과 축적된 데이터로 급격히 발전하며 현재는 상품 추천, 자율 주행, 생성형 AI 등 우리 생활에서 다양하게 활용되고 있다인공지능, 머신러닝 그리고 딥러닝  인공지능          인간의 지능을 컴퓨터를 통해 구현해낸 모든 기술들을 일컫는 말이다      머신러닝, 딥러닝도 결국 인공지능을 위한 하나의 방법이다        머신러닝          특정 알고리즘을 데이터를 통해 학습한 인공지능이다      머신러닝 모델을 구축함으로써 입력하지 않은 정보에 대해서다 판단이나 결정을 할 수 있게 된다      머신러닝 기법은 주로 정형 데이터를 다룬다        딥러닝          특정 알고리즘보다는 수많은 파라미터를 가지는 인공 신경망을 데이터를 통해 학습한 인공지능이다      실제로 인간의 뇌가 학습하는 방법과 가장 유사하다      음성, 이미지, 동영상 같은 알고리즘으로 잡아내기 힘든 추상적인 특성을 잘 찾기 때문에 비정형 데이터에서 특히 강점을 보인다      딥러닝 개요  딥러닝은 인공지능을 구현하기 위해 깊은 인공 신경망을 학습시킨다  학습된 인공 신경망 모델은 새로운 데이터들을 보고 결과물을 예측한다분류와 회귀  딥러닝으로 풀 수 있는 문제의 종류는 정말 다양하다  분류(Classification)          입력 데이터의 범주를 고르는 문제다                  ex. 사진을 보고 고양이인지 강아지인지 고른다          ex. 문장을 보고 긍정인지 부정인지 고른다                    분류 모델은 곧, 데이터를 잘 분류하는 decision boundary 와 같다        회귀(Regression)          입력 데이터를 보고 연속적인 값을 예측한다                  ex. 집 주변의 데이터를 보고 집 값을 예측한다          ex. 부모님의 유전자 정보를 통해 아이의 키를 예측한다                    회귀모델은 곧, 데이터의 일반적인 경향을 잘 나타내는 함수와 같다        최근에는 딥러닝의 활용도가 급격히 증가함에 따라, 자연어 처리, 컴퓨터 비전, 자율주행, 생성형 AI 로 영역을 분리하기도 한다딥러닝 맛보기  딥러닝 과정을 간단하게 살펴보면 다음과 같다          목적에 맞는 모델을 선택한다      그에 맞는 손실함수를 선택한다      경사하강법을 통해 학습시킨다      검증 데이터를 통해 학습을 언제 종료할지 결정한다      검증 과정에서 가장 좋은 성능을 낸 모델을 선택한다      실전에서 새로운 데이터에 대해 학습된 모델이 결과를 반환한다            손실함수는 딥러닝 학습에 있어서 목표를 설정해주는 것과 같다. 예를 들어 a와 b가 있을 때, a와 b가 같아지는게 목표라면(a = b) 우리는 a와 b의 차이(a-b)가 작아지도록 해야 한다. 이 때 손실함수 Loss function = a - b 이렇게 하고, 이 손실함수가 최소가 되도록 학습하면 점점 a와 b가 비슷해질 것이다    경사하강법은 학습을 최대한 효율적으로 하도록한다. 다시 말해 손실함수가 최대한 빠르게 최소가 되도록 한다. 손실 함수를 마치 깜깜한 골짜기와 같다고 생각해보자. 이 때 우리는 어떻게 가장 빠르게 밑으로 내려갈 수 있을까? 경사하강법은 여기서 ‘주변에 발을 짚었을 때 경사가 가장 급한 곳으로 가다보면 가장 빨리 골짜기 밑으로 갈 수 있지 않을까?’ 라는 방법을 제시한다. 그리고 이 방법은 현재 딥러닝에서 가장 많이 사용하는 모델 학습 방법이다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-08T21:01:35+09:00'>08 Jan 2024</time><a class='article__image' href='/ai-basic-intro'> <img src='/images/ai_basic_logo.png' alt='[AI Basic] 인공지능이란'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/ai-basic-intro'>[AI Basic] 인공지능이란</a> </h2><p class='article__excerpt'>인공지능은 인간의 지능을 컴퓨터를 통해 구현해낸 모든 기술들을 일컫는 말이다. 머신러닝, 딥러닝도 결국 인공지능을 위한 하나의 방법이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] 데이터 모델링 (준비중)",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-data-modeling",
      "date"     : "Jan 7, 2024",
      "content"  : "Table of Contents  데이터 모델링  모델링 팩터          데이터베이스      컬렉션      도큐먼트        모델링 고려사항  모델링 디자인 패턴데이터 모델링  몽고DB는 느슨한 스키마를 지원한다. 그래서 도큐먼트마다 다른 필드를 가질 수 있고, 심지어 필드마다 다른 타입의 데이터를 가질 수도 있다. 그래서 자칫 몽고DB에서 데이터 모델링은 중요하지 않다고 생각될 수도 있다.  하지만 몽고DB에서의 데이터 모델링은 RDBMS에서 모델링 만큼이나 중요하다. 왜냐하면 몽고DB에서 데이터 모델링은 성능뿐만 아니라 데이터 일관성, 샤딩 등 다양한 요소에 영향을 끼치기 때문이다  몽고DB는 스키마 변경이 자유롭기 때문에, DB서버가 중지되지 않고도 스키마를 바꿀 수 있다. 그래서 몽고DB에서는 어플리케이션의 변화에 맞게 스키마를 반복적으로 변경할 수 있다.  (하지만 변경 이전의 데이터는 변경된 후의 스키마를 따르지 않기 때문에, 필요한 경우 백엔드단에서 직접 스키마에 맞게 변경되는 스크립트를 실행해줘야 한다)모델링 팩터  데이터를 모델링할 때 데이터를 어느 단위로 분리할지 생각해보자데이터베이스  하나의 몽고DB 인스턴스는 여러 개의 데이터베이스를 가질 수 있고, 각 데이터베이스는 다시 여러 개의 컬렉션을 가질 수 있다  몽고DB에서는 데이터베이스 이름과 컬렉션 이름의 조합을 네임스페이스라고 한다  서비스의 특성에 맞게 데이터베이스와 컬렉션을 적절히 분리하고 통합해야 성능과 관리의 편리성을 모두 얻을 수 있다  몽고DB 초기 버전에서는 도큐먼트 변경 작업이 데이터베이스 글로벌 락을 발생시켰다  지금은 WiredTiger 스토리지 엔진 도입으로, 다른 RDBMS와 동등한 수준의 도큐먼트 수준의 잠금을 제공한다  (하지만 스키마 변경, 관리자 명령, 컬렉션 또는 인덱스 추가/변경과 같이 여전히 데이터베이스의 잠금을 발생시키는 것들도 있다)  그래서 데이터베이스 잠금을 발생시키는 작업이 자주 발생한다면 데이터베이스를 작은 단위로 분리하는게 성능상 이점이 있다  하지만 대부분의 경우 데이터베이스 분리는 성능보다는 샤딩이나 서비스 통합과 관련해서 많이 고려하는 부분이다  WiredTiger나 RocksDB 스토리지 엔진을 사용한다면 동시성 처리를 위해 데이터베이스나 컬렉션을 물리적으로 분리할 필요는 없다컬렉션  몽고DB에 관한 매뉴얼이나 많은 관련 글들에서는 몽고DB의 제한적인 형태의 조인 때문에 컬렉션에 가능하면 많은 데이터를 내장(embedding)할 것을 권장한다  이는 모델링적인 측면에선느 맞는 이야기일지 모르지만 성능적인 측면에서는 그렇지 않을 수도 있다  (항상 같이 필요한 데이터 묶음이라면 괜찮지만 그렇지 않다면 불필요한 디스크 읽기 작업, 네트워크 비용이 발생)  몽고DB는 내부적으로 이미 샤딩 기능을 가지고 있기 때문에 컬렉션 단위의 샤딩은 별도로 크게 고려하지 않아도 된다 (WiredTiger, RocksDB 사용할 경우)  이미 몽고DB 3.0부터 도입된 WiredTiger 스토리지 엔진에서는 도큐먼트 단위의 잠금을 지원하기 때문에 하나의 컬렉션에 많은 도큐먼트가 저장된다고 해서 동시 처리 성능 저하가 발생하지 않는다  또한 몽고DB에서 컬렉션 단위의 샤딩을 지원하므로 컬렉션을 잘게 쪼개서 얻을 수 있는 이점 또한 제한적이다  컬렉션 설계에서 무엇보다 중요한 것은 샤드 키의 선정이다. 샤드 키를 잘못 선정하면 몽고DB의 데이터 분산 효과를 무효로 만들어 버릴 정도로 많은 영향을 미친다도큐먼트  일반적으로 몽고DB에 저장되는 도큐먼트는 RDBMS의 레코드보다 용량이 큰 경향이 있다. 몽고DB의 BSON 도큐먼트는 최대 16MB 까지의 데이터만 저장할 수 있다. 그래서 모든 도큐먼트 하나에 모든 관련 데이터를 저장하는 것은 향후 응용 프로그램의 기능을 제한시킬 수 있다  도큐먼트 내에서 변경이 자주 발생하는 필드를 별도의 컬렉션으로 분리하는 것은 안정적인 서비스를 위해 고려해 볼만하다. 왜냐하면 특정 필드 하나만 변경하더라도, 실제로는 도큐먼트를 통째로 변경하는 작업이 일어나서 굉장히 비효율적인 디스크 쓰기 작업이 발생하고, 이러한 부하가 엄청나게 커지면 그동안 다른 읽기 쿼리를 실행하지 못하게 된다  반면에 몽고DB에서 하나의 도큐먼트 처리에 대해서만 원자적인 오퍼레이션을 보장한다는 점을 생각하면 도큐먼트를 묶어서 저장하는게 낫다고 할 수 있다모델링 고려사항모델링 디자인 패턴  https://www.mongodb.com/blog/post/building-with-patterns-a-summary",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-07T21:01:35+09:00'>07 Jan 2024</time><a class='article__image' href='/mongodb-data-modeling'> <img src='/images/mongo_logo.png' alt='[MongoDB] 데이터 모델링 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-data-modeling'>[MongoDB] 데이터 모델링 (준비중)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] CRUD 쿼리 기본편",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-query-basic",
      "date"     : "Jan 6, 2024",
      "content"  : "Table of Contents  몽고DB CRUD 연산          Insert      Delete      Find      Update      몽고DB CRUD 연산  다양한 언어를 지원한다 (ex. MongoDB Shell, Python, Node.js 등)  (나는 MongoDB Shell 기준으로 작성했다)  Read, Update, Delete 관련 작업은 SQL의 WHERE와 같은 필터(filter)를 잘 작성하는 것이 중요하다  (공식문서 참고)Insert  데이터베이스에 데이터를 삽입하는 작업과 관련된 쿼리이다  insertOne(), insertMany() 명령어를 주로 사용한다  Update 관련 명령어에서 upsert: true 옵션을 주면 Insert 역할을 수행할 수도 있다          (ex. findOneAndReplace(filter, replacement, { upsert: true }))        몽고DB는 데이터를 저장할 때 _id 필드를 반드시 포함해야 한다. 그래서 데이터를 삽입할 때 _id 필드를 생략하면, 몽고DB가 ObjectId 타입의 _id 필드를 자동으로 생성한다  반환 값은 삽입된 문서가 아닌, 결과에 관한 메타정보를 포함하는 도큐먼트를 반환한다# 하나의 도큐먼트 삽입 예시db.inventory.insertOne(   { item: &quot;canvas&quot;, qty: 100, tags: [&quot;cotton&quot;], size: { h: 28, w: 35.5, uom: &quot;cm&quot; } })# 여러 도큐먼트 삽입 예시db.inventory.insertMany([   { item: &quot;journal&quot;, qty: 25, tags: [&quot;blank&quot;, &quot;red&quot;], size: { h: 14, w: 21, uom: &quot;cm&quot; } },   { item: &quot;mat&quot;, qty: 85, tags: [&quot;gray&quot;], size: { h: 27.9, w: 35.5, uom: &quot;cm&quot; } },   { item: &quot;mousepad&quot;, qty: 25, tags: [&quot;gel&quot;, &quot;blue&quot;], size: { h: 19, w: 22.85, uom: &quot;cm&quot; } }])# 반환 도큐먼트 예시{   &quot;acknowledged&quot; : true,   &quot;insertedIds&quot; : [      ObjectId(&quot;562a94d381cb9f1cd6eb0e1a&quot;),      ObjectId(&quot;562a94d381cb9f1cd6eb0e1b&quot;),      ObjectId(&quot;562a94d381cb9f1cd6eb0e1c&quot;)   ]}Delete  데이터베이스에 데이터를 삭제하는 작업과 관련된 쿼리이다  deleteOne(), deleteMany() 명령어를 주로 사용한다          deleteOne()의 경우 필터에 여러 도큐먼트가 매칭되어도 가장 첫 번째 매칭된 도큐먼트만 삭제한다      deleteOne()이 의도한대로 동작하도록 하기 위해서는 필터에서 사용하는 필드는 _id 같은 유니크 인덱스를 갖는 필드를 쓰는게 좋다      findOneAndDelete() 명령어는 sort 옵션이 있어서, 정렬된 결과에서 첫 번째 도큐먼트를 삭제하도록 유도할 수 있다        컬렉션의 모든 도큐먼트를 삭제하더라도 컬렉션 안에 포함된 인덱스는 여전히 남아있다# 컬렉션 안에 있는 모든 도큐먼트 삭제 예시db.inventory.deleteMany({})# 필터에 매칭되는 도큐먼트 한 개 삭제 예시db.inventory.deleteOne( { status: &quot;D&quot; } )# 필터에 매칭되는 도큐먼트 모두 삭제 예시db.inventory.deleteMany( { status : &quot;A&quot; } )Find  데이터베이스의 데이터를 읽는 작업과 관련된 쿼리이다  findOne(), find() 명령어를 주로 사용한다          (findOne()은 내부적으로 find()에서 limit: 1 옵션을 주는 것과 똑같이 동작한다 )        쿼리에 사용되는 필터를 어떻게 작성할지가 가장 중요하면서도 어렵다          제공되는 연산자가 다양하다 (공식문서 참고)      어떤 경우에는 필드가 먼저오고, 또 어떤 경우에는 연산자가 먼저온다      몽고DB는 필드의 타입으로 도큐먼트, 배열도 가능해서 이와 관련한 필터 작성법도 익혀야 한다      # 컬렉션 내에서 전체 도큐먼트 조회 예시db.inventory.find( {} )# 필드 값의 동등 비교를 통한 도큐먼트 조회 예시db.inventory.find( { status: &quot;D&quot; } )# 필드 값의 조건 비교를 통한 도큐먼트 조회 예시## 대소 비교 ## (관련 연산자: $gt, $gte, $lt, $lte)db.inventory.find( { quantity: { $gt: 20 } } )## 동등 비교 ## (관련 연산자: $eq, $ne) (위에서 살펴본 것과 같이 $eq는 생략해 축약 표현 가능하다)db.inventory.find( { qty: { $eq: 20 } } )## 포함 여부 비교 ## (관련 연산자: $in, $nin)db.inventory.find( { quantity: { $in: [ 5, 15 ] } }, { _id: 0 } )# AND, OR 을 이용한 도큐먼트 조회 예시# (관련 연산자: $and, $or)## AND (콤마(,) 또는 $and 를 통해 구현할 수 있다)db.inventory.find( { status: &quot;A&quot;, qty: { $lt: 30 } } )## 같은 필드에 대한 AND 쿼리문은 콤마로 사용하면 편하다db.inventory.find( { $and: [ { price: { $ne: 1.99 } }, { price: { $exists: true } } ] } )db.inventory.find( { price: { $ne: 1.99, $exists: true } } )## ORdb.inventory.find( { $or: [ { status: &quot;A&quot; }, { qty: { $lt: 30 } } ] } )## x AND (y OR Z) db.inventory.find( {     status: &quot;A&quot;,     $or: [ { qty: { $lt: 30 } }, { item: /^p/ } ]} )## AND Queries With Multiple Expressions Specifying the Same Operator## (The query cannot use an implicit AND operation because it uses the $or operator more than once.)## (정확히 잘 모르겠지만 아래처럼 같은 연산자(여기서는 $or)를 여러 번 쓰는 경우에는 콤마가 아닌 반드시 $and 연산자로 묶어야 하는 것 같다)db.inventory.find( {    $and: [        { $or: [ { qty: { $lt : 10 } }, { qty : { $gt: 50 } } ] },        { $or: [ { sale: true }, { price : { $lt : 5 } } ] }    ]} )## AND 쿼리는, 같은 필드에 대한 경우에만 콤마 연산자로 간단히 쓰고, 좀 복잡해지는 경우에는 $and 연산자를 쓰는게 좋은 것 같다# 필드의 존재 여부를 통한 도큐먼트 조회# (관련 연산자: $exists)db.inventory.find( { qty: { $exists: true, $nin: [ 5, 15 ] } } )# 필드의 타입 체크를 통한 도큐먼트 조회# (관련 연산자: $type)db.addressBook.find( { &quot;zipCode&quot; : { $type : &quot;number&quot; } } )# 표현식을 이용한 도큐먼트 조회# (관련 연산자: $expr)## 도큐먼트 안의 다른 필드 값을 참조db.monthlyBudget.find( { $expr: { $gt: [ &quot;$spent&quot; , &quot;$budget&quot; ] } } ) // 도큐먼트의 spent 필드 값이 budget 필드 값보다 큰 도큐먼트// Aggregation expression to calculate discounted pricelet discountedPrice = {   $cond: {      if: { $gte: [&quot;$qty&quot;, 100] },      then: { $multiply: [&quot;$price&quot;, NumberDecimal(&quot;0.50&quot;)] },      else: { $multiply: [&quot;$price&quot;, NumberDecimal(&quot;0.75&quot;)] }   }};// Query the supplies collection using the aggregation expressiondb.supplies.find( { $expr: { $lt:[ discountedPrice,  NumberDecimal(&quot;5&quot;) ] } }); // 컬렉션 내에서 각 도큐먼트마다 discountedPrice를 구한 후, 그 값이 5보다 작은지 비교# Aggregation을 이용한 도큐먼트 조회 (이건 분량이 많다)# 배열 필드를 이용한 도큐먼트 조회# (관련 연산자: $all, $elemMatch, $size)## 일치db.inventory.find({ tags: [’red’, ‘blank’] }) // tags 필드가 정확히 [‘red’, ‘blank’]인 도큐먼트 (순서까지 같아야함)db.inventory.find({ tags: { $all: [’red’, ‘blank’] } }) // tags 필드가 정확히 [‘red’, ‘blank’]인 도큐먼트 (순서는 상관 없음)## 포함db.inventory.find({ tags: ‘red’ }) // tags 필드에 red가 포함된 도큐먼트db.inventory.find({ dim_cm: { $gt: 30 } }) // dim_cm 필드에 30보다 큰 원소가 하나라도 포함된 도큐먼트db.inventory.find({ dim_cm: { $gt: 15, $lt: 20 } }) // dim_cm 필드에 15보다 큰 원소 또는 20보다 작은 원소 또는 15~20인 원소를 하나라도 포함하는 도큐먼트db.inventory.find({ dim_cm: { $elemMatch: { $gt: 15, $lt: 20 } } }) // dim_cm 필드에 15~20인 원소가 하나라도 포함된 도큐먼트## 일부db.inventory.find({ dim_cm.1: { $gt: 25}) // dim_cm 필드의 2번째 원소 값이 25보다 큰 도큐먼트## 개수db.inventory.find({ dim_cm: { $size: 3}) //dim_cm 필드의 원소 개수가 3개인 도큐먼트Update  데이터베이스의 데이터를 수정하는 작업과 관련된 쿼리이다  updateOne(), updateMany(), replaceOne() 명령어를 주로 사용한다  위의 Find 처럼 필터로 특정 도큐먼트를 찾고, 추가로 해당 도큐먼트를 어떻게 업데이트 할지에 관해서도 나타내야 한다# 관련 연산자: $currentDate, $min, $max, $inc, $mul, $rename, $set, $unset, $setOnInsert## db.collection.updateMany(&amp;lt;filter&amp;gt;, &amp;lt;update&amp;gt;, &amp;lt;options&amp;gt;)db.inventory.updateMany(   { &quot;qty&quot;: { $lt: 50 } },   {     $set: { &quot;size.uom&quot;: &quot;in&quot;, status: &quot;P&quot; },     $currentDate: { lastModified: true }   })## db.collection.replaceOne(&amp;lt;filter&amp;gt;, &amp;lt;update&amp;gt;, &amp;lt;options&amp;gt;)db.inventory.replaceOne(   { item: &quot;paper&quot; },   { item: &quot;paper&quot;, instock: [ { warehouse: &quot;A&quot;, qty: 60 }, { warehouse: &quot;B&quot;, qty: 40 } ] })",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-06T21:01:35+09:00'>06 Jan 2024</time><a class='article__image' href='/mongodb-query-basic'> <img src='/images/mongo_logo.png' alt='[MongoDB] CRUD 쿼리 기본편'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-query-basic'>[MongoDB] CRUD 쿼리 기본편</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MongoDB] Intro",
      "category" : "data_engineering",
      "tags"     : "mongodb",
      "url"      : "/mongodb-intro",
      "date"     : "Jan 5, 2024",
      "content"  : "Table of Contents  MongoDB          몽고DB의 역사      NoSQL        MongoDB의 장점  MongoDB의 특징          데이터 구조      BSON      Replica Set      Sharded Cluster      WiredTiger Storage Engine        참고MongoDB  C++로 개발된 도큐먼트 기반 NoSQL 오픈소스 데이터베이스이다몽고DB의 역사  Humongous(거대한) Database를 줄인 MongoDB로 명명  2007년 뉴욕 기반의 10gen 이라는 기관에서 시작  기존의 회사가 운영하던 관계형 데이터베이스의 확장성에 심각한 한계점을 느끼고 나서 높은 확장성을 제공하는 데이터베이스를 만들기로 결심  Kevin P. Ryan, Dwight Merriman 그리고 Eliot Horowitz가 10gen을 나와 MongoDB Inc 라는 회사를 따로 설립  2009년에 오픈소스로 처음 MongoDB가 세상에 등장NoSQL  NoSQL은 RDBMS 처럼 엄격하게 스키마를 정의하지 않고, Semi-Structured한 형태의 데이터 저장을 지원하는 데이터베이스이다  NoSQL의 일반적인 장점: 유연한 스키마, 유연한 확장성, 높은 가용성  데이터 형태를 표현하는 방법에 따라 Document store, Key-value store, Wide column store 등으로 더욱 세분화된다MongoDB의 장점  유연한 스키마  유연한 확장성: Scale-out, 샤딩, 자체 로드 밸런싱(mongos)  높은 가용성: Replica Set  인덱스 지원: 프라이머리 키 인덱스, 세컨더리 인덱스  복잡하고 다양한 쿼리 가능: Ad-hoc 쿼리, 전문 검색  BSON 형태의 데이터 저장 포맷: 데이터 크기가 가벼워짐, 다양한 데이터 타입 지원, 빠른 쿼리MongoDB의 특징데이터 구조  Database          사용자 정의 db      admin db: 인증과 권한 부여와 관련된 데이터베이스      local db: 복제 처리와 관련된 데이터베이스      config db: 분산처리, 샤드(shard)에 관련된 데이터베이스        Collection          RDBMS의 테이블과 비슷한 개념      컬렉션 단위로 동적인 스키마를 가진다      컬렉션 단위로 인덱스를 생성할 수 있다      컬렉션 단위로 샤딩할 수 있다        Document          RDBMS의 레코드와 비슷한 개념      다른 도큐먼트, 배열과 같은 필드를 가질 수 있다      JSON 형태로 표현하고, BSON 형태로 저장한다      도큐먼트마다 고유한 _id 프라이머리 키 값이 있다.      도큐먼트 한 개의 최대 크기는 16MB 이다        Field          필드를 추가하기 위해 컬렉션의 스키마를 다시 정의할 필요가 없다      필드의 추가, 삭제가 자유롭다      _id 필드  어떤 데이터 타입이어도 상관 없지만 ObjectId가 기본이다  하나의 컬렉션에서 모든 도큐먼트는 고유한 _id 값을 가진다  ObjectId 데이터 타입을 사용하는 주된 이유는 몽고DB의 분산 특성 때문이다  샤딩된 환경에서 고유 식별자를 쉽게 생성하도록 도와준다  (여러 서버에 걸쳐 자동으로 증가하는 기본 키를 동기화하는 작업은 어렵고 시간이 걸린다)  (https://www.mongodb.com/docs/manual/core/document/#the-_id-field 참고)BSON  데이터 입출력 시에는 JSON 형식을 사용  데이터 저장 시에는 BSON 형식을 사용  JSON 데이터를 이진 형식으로 인코딩한 포맷  컴퓨터가 쉽게 이해할 수 있는 이진 포맷으로 검색 속도가 빠름  날짜 및 이진 데이터 타입을 지원함  (https://www.mongodb.com/docs/manual/reference/bson-types/ 참고)Replica Set  높은 가용성을 제공하기 위해 같은 데이터 셋을 저장 및 관리하는 몽고DB의 서버 그룹  크게 Primary와 Secondary로 구성  Primary          Leader Server      Read/Write 모두 처리 가능      Replica Set당 한 개만 존재        Secondary          Follower Server      Read에 대한 요청만 처리 가능 -&amp;gt; Secondary 수를 늘림으로써 Read 분산 처리 가능      복제를 통해 Primary와 동일한 데이터 셋을 유지        지속적으로 하트비트(Heartbeat)를 주고 받으며 서버 살아 있는지 확인  Primary 서버가 Replica Set에서 이탈하면 Secondary중에서 새로운 리더를 선출해야함 (Leader election)  local db의 Oplog 컬렉션을 통해 복제를 수행Sharded Cluster  유연한 확장성을 제공하기 위해 데이터 셋을 일정 기준에 따라 나누어 여러 대의 서버에 분산 저장 및 처리(Write 작업)하는 몽고DB의 서버 그룹  모든 Shard는 Replica Set으로 구성 (높은 가용성과, 분산 Read 작업도 함께 지원)  샤딩을 위해서는 Shard Key를 선정해야 하고, 해당 필드에 인덱스가 만들어져 있어야 한다          Ranged Sharding: 값의 범위에 따라 분산      Hashed Sharding: 값의 해시 결과에 따라 분산 (가능하면 이 방법을 통해 분산한다)        비교적 관리가 복잡하다는 단점이 있다WiredTiger Storage Engine  MongoDB 3.2부터 기본 스토리지 엔진으로 WiredTiger를 사용  데이터 압축 지원: 4~6배 정도의 압축  도큐먼트 레벨의 잠금 지원참고  MongoDB 공식문서, Home  MongoDB 공식문서, Why Use MongoDB and When to Use It?  NHN 클라우드, mongoDB Story 1: mongoDB 정의와 NoSQL  BYTESCOUT, MONGODB HISTORY AND ADVANTAGES  Guru99, What is MongoDB? Introduction, Architecture, Features &amp;amp; Example  MongoDB 공식문서, Replication  프리킴: [MongoDB] 몽고DB 기본 명령어  Confluent hub: Debezium MongoDB CDC Source Connector",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2024-01-05T21:01:35+09:00'>05 Jan 2024</time><a class='article__image' href='/mongodb-intro'> <img src='/images/mongo_logo.png' alt='[MongoDB] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-intro'>[MongoDB] Intro</a> </h2><p class='article__excerpt'>몽고DB는 도큐먼트 기반 NoSQL 오픈소스 데이터베이스이다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 배치 처리",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-batch",
      "date"     : "Dec 20, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-20T21:01:35+09:00'>20 Dec 2023</time><a class='article__image' href='/backend_theory-batch'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 배치 처리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-batch'>[Backend Thoery] 배치 처리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 의존성 주입(Dependency Injection)",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-dependency-injection",
      "date"     : "Dec 20, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-20T21:01:35+09:00'>20 Dec 2023</time><a class='article__image' href='/backend_theory-dependency-injection'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 의존성 주입(Dependency Injection)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-dependency-injection'>[Backend Thoery] 의존성 주입(Dependency Injection)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 암호화/복호화",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-cryption",
      "date"     : "Dec 20, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-20T21:01:35+09:00'>20 Dec 2023</time><a class='article__image' href='/backend_theory-cryption'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 암호화/복호화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-cryption'>[Backend Thoery] 암호화/복호화</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] SSE (Server-Sent-Event)",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-sse",
      "date"     : "Dec 20, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-20T21:01:35+09:00'>20 Dec 2023</time><a class='article__image' href='/backend_theory-sse'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] SSE (Server-Sent-Event)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-sse'>[Backend Thoery] SSE (Server-Sent-Event)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 웹소켓",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-web-socket",
      "date"     : "Dec 20, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-20T21:01:35+09:00'>20 Dec 2023</time><a class='article__image' href='/backend_theory-web-socket'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 웹소켓'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-web-socket'>[Backend Thoery] 웹소켓</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 페이지네이션",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-pagination",
      "date"     : "Dec 15, 2023",
      "content"  : "Table of Contents  웹 개발이란  백엔드란  백엔드 요소  백엔드 아키텍처  백엔드 코드 아키텍처  백엔드 대표적 기능  백엔드 고려사항웹 개발이란  프로그래밍 세계에서 웹 서비스를 개발하는 분야를 웹 개발(Web Development)이라고 함  웹 개발은 크게 UI를 담당하는 프론트엔드(Frontend)와 데이터를 백엔드(Backend)로 나뉨  백엔드는 유저로부터 받은 데이터를 저장하고, 유저가 요청한 데이터를 반환하는 역할을 함백엔드란  백엔드는 서버에서 실행되는 코드  유저의 요청을 받아, 로직을 처리하고 응답으로 적절한 데이터를 반환하는 역할  또 데이터를 영구적으로 저장하는 데이터베이스도 백엔드에 포함되어 있음  유저가 직접 백엔드와 통신하지는 않고, 유저는 프론트엔드와 통신하며, 프론트엔드가 백엔드와 통신해 유저가 요청한 데이터를 반환백엔드 요소  백엔드 코드를 작성할 때, 기본적으로 작성하는 요소들은 다음과 같다          요청 라우팅: 유저가 보낸 요청(Request)에 알맞는 응답(Response)을 반환하기 위해 적절한 위치로 분기      데이터 처리: 데이터베이스에서 데이터 읽기/저장/수정/삭제      비즈니스 로직: 구현한 기능을 수행하는데 필요한 비즈니스 로직      예외 처리: 어떠한 상황에도 서버는 하나의 요청에 대해 하나의 응답을 반환해야함. 상황에 맞는 적절한 응답 반환        그 밖의 구성하면 도움이 되는 요소들은 다음과 같다          캐싱: 자주 사용되는 데이터를 캐싱      API 문서화: 다른 개발자들과의 협업을 위해 API를 문서화      테스트 코드      백엔드 아키텍처  API Gateway: 요청을 적절한 API 서버로 전달하는 역할  Load Balancer: 하나의 서버에 과부하가 걸리지 않도록 요청 트래픽을 적절히 배분하는 역할  API Server: 들어온 요청에 대해 적절한 로직을 수행하는 역할  Database: 데이터 영구 저장하는 역할  Cache: 자주 사용되는 데이터를 임시 저장해 빠른 읽기를 지원하는 역할  Queue: 들어온 요청을 비동기적으로 처리하는 역할백엔드 코드 아키텍처  MVC  Clean code  Hexagonal백엔드 대표적 기능  회원가입/로그인  페이지네이션  검색  알림  채팅  국제화  결제  장바구니백엔드 고려사항  확장성(Scalability)  대용량 트래픽",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-15T21:01:35+09:00'>15 Dec 2023</time><a class='article__image' href='/backend_theory-pagination'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 페이지네이션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-pagination'>[Backend Thoery] 페이지네이션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 비동기 프로그래밍",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-asynchronous",
      "date"     : "Dec 14, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-14T21:01:35+09:00'>14 Dec 2023</time><a class='article__image' href='/backend_theory-asynchronous'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 비동기 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-asynchronous'>[Backend Thoery] 비동기 프로그래밍</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 에러 핸들링",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-error-handling",
      "date"     : "Dec 12, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-12T21:01:35+09:00'>12 Dec 2023</time><a class='article__image' href='/backend_theory-error-handling'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 에러 핸들링'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-error-handling'>[Backend Thoery] 에러 핸들링</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 데이터 패칭 (with Tanstack query)",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-data-fetching",
      "date"     : "Dec 10, 2023",
      "content"  : "Table of Contents  참고참고  Adding Interactivity, react.dev",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-10T21:01:35+09:00'>10 Dec 2023</time><a class='article__image' href='/react-data-fetching'> <img src='/images/react_logo.png' alt='[React]: 데이터 패칭 (with Tanstack query)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-data-fetching'>[React]: 데이터 패칭 (with Tanstack query)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Code Architecture] 헥사고날 아키텍처",
      "category" : "language",
      "tags"     : "Code_Architecture",
      "url"      : "/code_architecture-hexagonal",
      "date"     : "Dec 10, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-10T21:01:35+09:00'>10 Dec 2023</time><a class='article__image' href='/code_architecture-hexagonal'> <img src='/images/code_architecture_logo.png' alt='[Code Architecture] 헥사고날 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/code_architecture-hexagonal'>[Code Architecture] 헥사고날 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 쿠키와 세션",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-cookie-session",
      "date"     : "Dec 10, 2023",
      "content"  : "Table of Contents  쿠키  세션쿠키  쿠키(cookie)란 웹 사이트에 접속할 때 서버에 의해 사용자의 컴퓨터에 저장되는 정보를 의미한다  웹 사이트는 이렇게 저장된 사용자의 정보를 클라이언트(client) 측의 컴퓨터에 남겨서 필요할 때마다 재사용 한다  즉, 특정 호스트에서 생성된 쿠키는 이후 모든 요청마다 서버로 쿠키를 다시 전송한다  사용자의 컴퓨터에 마치 과자 부스러기가 남아 있는 것과 같다고 해서 ‘쿠키(cookie)’라는 명칭이 붙었다  쿠키는 로그인 정보나 비회원의 장바구니 정보를 저장하는 용도로 많이 활용된다  하지만 사용자의 정보가 컴퓨터에 고스란히 남기 때문에 사생활 침해의 우려가 있으며, 보안과 관련된 이슈를 가지고 있다세션  세션(Session)이란 HTTP 프로토콜을 사용하는 인터넷 사용자가 어떤 웹사이트를 방문해서 브라우저를 닫기까지의 기간, 그리고 그 기간동안 임시 데이터를 저장하는 공간을 일컫는 말이다  방문자가 웹서버에 접속해 있는 상태를 하나의 단위로 보고 세션이라고 한다  앞서 살펴본 쿠키는 클라이언트 측의 컴퓨터에 모든 데이터를 저장한다  세션은 서버 내부에 저장되며, 세션의 키값만을 클라이언트 측에 남겨둔다  브라우저는 필요할 때마다 이 키값을 이용하여 서버에 저장된 데이터를 사용하게 된다  저장된 값은 반영구적이며, 사용자가 특정 시간동안 사용되지 않을 경우 폐기될 수 있는 정보이다                세션의 장단점                      장점                      클라이언트의 상태 정보가 서버에 저장되므로 상대적으로 안전하다            서버에서 클라이언트 상태 정보를 저장하므로, 사용자의 로그인 여부 확인이 용이하고, 경우에 따라 강제 로그아웃도 시킬 수 있다                          단점                      클라이언트 수에 따라 서버에 걸리는 부하가 커진다            서버 확장이 어렵다                            ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-10T21:01:35+09:00'>10 Dec 2023</time><a class='article__image' href='/backend_theory-cookie-session'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 쿠키와 세션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-cookie-session'>[Backend Thoery] 쿠키와 세션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 데이터 폼 양식 만들기",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-data-form",
      "date"     : "Dec 8, 2023",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-08T21:01:35+09:00'>08 Dec 2023</time><a class='article__image' href='/react-data-form'> <img src='/images/react_logo.png' alt='[React]: 데이터 폼 양식 만들기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-data-form'>[React]: 데이터 폼 양식 만들기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 인증/인가",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-authentication",
      "date"     : "Dec 6, 2023",
      "content"  : "Table of Contents  인증과 인가  인증(Authentication)          세션 인증 방식      JWT 토큰 인증 방식      OAuth 인증 방식        인가(Authorization)  참고인증과 인가  인증(Authentication)          유효한 사용자인지를 확인한다      ex. 로그인 여부를 확인한다        인가(Authorization)          권한이 있는지를 확인하고 있으면 특정 행동을 허가한다      인가는 인증 절차를 거친 후에 진행된다      ex. 불법 광고 영상을 올린 사용자를 탈퇴시키려 할 때, 먼저 관리자 권한이 있는지 확인하고 있는 경우 허가한다      ex. 자신이 작성했던 댓글을 수정하려 할 때, 댓글 작성자가 맞는지 확인하고 맞는 경우 허가한다      인증(Authentication)  보통 인증은 웹 서비스에서 로그인한 유저에게만 제공하는 정보에 접근하려고 할 때 요구된다  이 때 HTTP는 무상태(stateless)한 특성 때문에 로그인한 정보를 저장하고 있지 않으면, 매번 인증 절차를 진행해야 된다  그래서 백엔드에서 인증 절차를 구현할 때는 사용자의 로그인 정보를 어디에 저장할지 고민해야 한다  아래의 세가지 주요 인증 방식을 살펴보자세션 인증 방식  서버에 포함된 세션 저장소에 로그인한 사용자의 정보를 저장한다  사용자가 로그인 성공시 세션 저장소에 (세션ID-사용자정보)라는 키-밸류 형태의 데이터가 저장되고, 사용자의 웹 브라우저 쿠키에 세션ID가 저장된다  사용자는 매번 로그인 인증 절차를 거치지 않고, 쿠키에 저장된 세션ID를 요청(request)과 함께 보냄으로써 인증된 사용자임을 증명할 수 있다                세션 인증 방식의 장단점                      장점                      사용자 민감 정보는 서버에 저장하고, 세션 ID만으로 인증이 가능하다            쿠키에는 세션 ID 정보만 있기 때문에, 탈취되어도 세션 ID만 무효화하면 사용자 정보는 안전하다는 점에서 사후대처가 된다                          단점                      해커가 세션 ID를 탈취하여 위장 접근할 수 있다.(하이재킹 공격)            서버를 스케일아웃 시킬 경우 모든 서버가 세션 정보를 가지고 있어야 하므로, 서버의 확장성을 떨어트린다 (유튜브 7분 30초 참고)                            JWT 토큰 인증 방식  로그인한 사용자 정보를 JWT 토큰으로 만들어 사용자의 브라우저 쿠키에 저장한다  사용자가 로그인 성공시 서버는 로그인 정보를 이용해 JWT 토큰을 만들어 사용자의 브라우저 쿠키에 저장한다  사용자는 매번 로그인 인증 절차를 거치지 않고, 쿠키에 저장된 JWT 토큰을 요청(request)과 함께 보냄으로써 인증된 사용자임을 증명할 수 있다                JWT 토큰 인증 방식의 장단점                      장점                      서버의 확장성에 영향을 미치지 않는다                          단점                      세션 ID와 비교해 JWT 토큰은 비교적 문자열이 길다. 그래서 네트워크 오버헤드가 비교적 크다            암호화되어 있긴 하지만, 어쨋든 사용자 정보가 클라이언트 측에 저장된다 (보안에 치명적인 데이터는 담지 않는게 좋다)            해커에게 탈취되었을 때, 딱히 취할 수 있는 대처방법이 없다                            OAuth 인증 방식  Open Standard for Authorization  개방형 Authorization 표준  여러 웹사이트에 사용자의 민감한 정보를 저장하지 않고, 신뢰할 수 있는 외부 서비스(ex. 구글, 네이버 등)에 저장된 사용자 정보로 인증/인가 하는 방식이다  OAuth는 제 3자의 클라이언트(보통 우리가 만든 웹서비스를 말함)에게 보호된 리소스를 제한적으로 접근하게 해주는 프레임워크를 말한다  OAuth 프레임워크에는 다음과 같은 주체들이 있다          리소스 소유자: 사용자      클라이언트: 사용자의 정보를 접근하는 제 3자의 서비스      인증 서버: 클라이언트의 접근을 관리하는 서버      리소스 서버: 리소스 소유자의 데이터를 관리하는 서버      인가(Authorization)  인가는 인증을 성공한 사용자가 특정 요청에 대해 권한이 있는지를 확인한다  ex. 로그인 후 내가 작성한 댓글 수정 요청을 보낼 때, 요청을 보낸 사용자가 댓글 작성자와 일치하는지 확인한다  보통 사용자의 이메일(email)을 통해 사용자를 확인하거나 또는 사용자의 역할(role)을 통해 권한이 있는지 확인한다참고  Basic 인증과 Bearer 인증의 모든, 토스 페이먼츠  [Web] 로그인 인증 방식, dee_thinking  [10분 테코톡] 토닉, 후디의 인증과 인가, 우아한테크 유튜브",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-06T21:01:35+09:00'>06 Dec 2023</time><a class='article__image' href='/backend_theory-authentication'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 인증/인가'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-authentication'>[Backend Thoery] 인증/인가</a> </h2><p class='article__excerpt'>세션, JWT, OAuth 방식의 인증과 인가에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Redis] NestJS에서 사용하기 (준비중)",
      "category" : "data_engineering",
      "tags"     : "redis",
      "url"      : "/redis-nestjs",
      "date"     : "Dec 5, 2023",
      "content"  : "Table of Contents  준비중준비중",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-05T21:01:35+09:00'>05 Dec 2023</time><a class='article__image' href='/redis-nestjs'> <img src='/images/redis_logo.png' alt='[Redis] NestJS에서 사용하기 (준비중)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/redis-nestjs'>[Redis] NestJS에서 사용하기 (준비중)</a> </h2><p class='article__excerpt'>레디스를 NestJS에서 사용하는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 리액트 훅(hook)",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-hook",
      "date"     : "Dec 5, 2023",
      "content"  : "Table of Contents  훅(Hook)  State Hooks          useState        Context Hooks          useContext        Ref Hook          useRef        Effect Hook          useEffect        Performance Hooks          useMemo      useCallback        참고훅(Hook)  훅은 리액트에서 제공하는 여러 특징들을 사용할 수 있도록 도와준다State Hooks  상태(state)는 컴포넌트가 어떤 값을 기억하도록 한다useState  상태(state) 변수를 선언할 때 useState 훅을 사용한다function ImageGallery() {  const [index, setIndex] = useState(0);  // ...Context Hooks  컨텍스트(context)는 부모 컴포넌트가 멀리 떨어져 있는 자식 컴포넌트에게 값을 전달하도록 해준다useContext  정의한 컨텍스트를 읽고 구독할 때 useContext 훅을 쓴다function Button() {  const theme = useContext(ThemeContext);  // ...Ref Hook  참조(ref)는 상태(state)처럼 어떤 값을 저장하는 목적으로 쓰인다  하지만 상태(state)와 다르게 참조(ref)가 바뀌어도 컴포넌트가 리렌더링 되지는 않는다  (When you change the ref.current property, React does not re-render your component)useRef  참조(ref)를 정의할 때 useRef 훅을 쓴다  어떤 값도 저장할 수 있지만, 대부분의 경우 DOM 노드를 참조하는 목적으로 쓴다function Form() {  const inputRef = useRef(null);  // ...Effect Hook  효과(effect)는 컴포넌트를 다른 외부 시스템과 동기화되도록 연결시켜 준다  네트워크, 브라우저의 DOM, 애니메이션 등 non-React 코드를 다룰 때 사용한다useEffect  컴포넌트를 다른 외부 시스템과 연결할 때 useEffect 훅을 사용한다useEffect(setup, dependencies?)function ChatRoom({ roomId }) {  useEffect(() =&amp;gt; {    const connection = createConnection(roomId);    connection.connect();    return () =&amp;gt; connection.disconnect();  }, [roomId]);  // ...  setup          optionally return a cleanup function      컴포넌트가 처음 DOM에 추가될 때 setup 함수가 실행된다      dependencies 변화로 리렌더링 될 때마다 cleanup 함수 실행 with old props and state      그리고 setup 함수 실행 with new props and state      컴포넌트가 DOM 에서 제거될 때 cleanup 함수 실행        dependencies          setup 함수 안에서 참조된 reactive values 를 목록으로 가지는 배열      (reactive values: props, state, variables, 컴포넌트 안에서 정의된 함수)      리액트는 배열안의 값들을 하나씩 비교해서 변경된 경우 setup 함수 실행        If some of your dependencies are objects or functions defined inside the component, there is a risk that they will cause the Effect to re-run more often than needed. To fix this, remove unnecessary object and function dependencies. You can also extract state updates and non-reactive logic outside of your Effect.  Effects only run on the client. They don’t run during server rendering.Performance Hooks  리액트에서는 리렌더링을 최적화 하기 위해 캐시된 계산 결과를 재사용해서 불필요한 리렌더링을 방지한다useMemo  useMemo는 어떤 계산 결과를 캐시하도록 해준다useCallback  useCallback lets you cache a function definition before passing it down to an optimized component.참고  Built-in React Hooks, react.dev",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-05T21:01:35+09:00'>05 Dec 2023</time><a class='article__image' href='/react-hook'> <img src='/images/react_logo.png' alt='[React]: 리액트 훅(hook)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-hook'>[React]: 리액트 훅(hook)</a> </h2><p class='article__excerpt'>리액트에서 제공하는 훅에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Code Architecture] 클린 코드 아키텍처",
      "category" : "language",
      "tags"     : "Code_Architecture",
      "url"      : "/code_architecture-clean-code",
      "date"     : "Dec 5, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-05T21:01:35+09:00'>05 Dec 2023</time><a class='article__image' href='/code_architecture-clean-code'> <img src='/images/code_architecture_logo.png' alt='[Code Architecture] 클린 코드 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/code_architecture-clean-code'>[Code Architecture] 클린 코드 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 직렬화와 역직렬화",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-serialization",
      "date"     : "Dec 5, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-05T21:01:35+09:00'>05 Dec 2023</time><a class='article__image' href='/backend_theory-serialization'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 직렬화와 역직렬화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-serialization'>[Backend Thoery] 직렬화와 역직렬화</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Redis] 데이터 타입",
      "category" : "data_engineering",
      "tags"     : "redis",
      "url"      : "/redis-data-types",
      "date"     : "Dec 4, 2023",
      "content"  : "Table of Contents  String          간단한 실습        Number          간단한 실습        Hash          간단한 실습        Set          간단한 실습        Sorted Set          간단한 실습      String  레디스에서는 문자열과 관련하여 아래와 같은 명령어를 제공한다  복잡해 보이지만 약간의 규칙이 있다          앞에 M이 붙어있는 것은 여러 개의 데이터를 다룰 수 있다는 의미다      뒤에 EX가 붙어있는 것은 만료 시간을 함께 지정한다는 의미다      뒤에 NX가 붙어있는 것은 키(key)가 존재하지 않을 때 명령어를 실행한다는 의미다        위에서 살펴본 EX, NX 같은 요소들은 레디스에서 제공하는 옵션의 일부를 붙여서 새로운 명령어로 만든 것에 불과하다          SET key apple EX 60 와 SETEX key 60 apple 는 같은 기능을 수행한다        레디스 공식문서를 보면 아래와 같이 명령어에 어떤 옵션을 사용할 수 있는지 알려준다간단한 실습SET apple red -- OKGET apple -- red-- 3초간 유효한 문자열 데이터를 저장한다SET banana yellow EX 3 -- OK-- 3초 뒤GET banana -- null-- 문자열의 길이를 리턴한다. 없으면 0STRLEN apple -- 3STRLEN banana -- 0-- 여러 개의 데이터를 읽고 쓴다MSET grape violet pear white avocado green -- OKMGET grape pear banana -- [ &quot;violet&quot;, &quot;white&quot;, null]-- 일부 문자열을 교체한다SETRANGE apple 2 dish -- 6GET apple -- redishNumber  레디스에는 숫자형 데이터를 따로 구분하지 않지만, 여기서는 그냥 편의상 구분했다  좋아요나 조회수 같은 카운터 기반 기능 구현할 때 유용하다간단한 실습SET like 5 -- OKINCR like -- 6INCRBY like 100 -- 106INCRBYFLOAT like 0.6 -- &quot;106.6&quot;Hash  레디스에서는 해시 데이터 타입과 관련된 명령어를 다음과 같이 제공한다  해시 데이터 타입은 아래와 같이 값(value)이 딕셔너리 형태로 되어 있다간단한 실습HSET user name kim age 20 address Seoul -- 3HGET user name -- &quot;kim&quot;HGETALL user -- { &quot;name&quot;: &quot;kim&quot;, &quot;age&quot;: &quot;20&quot;, &quot;address&quot;: &quot;Seoul&quot; }Set  레디스에서는 셋 데이터 타입과 관련된 명령어를 다음과 같이 제공한다  셋 데이터 타입은 아래와 같이 값(value)이 셋 형태로 되어 있다간단한 실습SADD color red blue green -- 3SMEMBERS color -- [ &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;]SADD myfavorite blue -- 1SDIFF color myfavorite -- [ &quot;red&quot;, &quot;green&quot; ]Sorted Set  레디스에서는 정렬된 셋 데이터 타입과 관련된 명령어를 다음과 같이 제공한다  역할별로 구분해서 정리해보면 다음과 같다  정렬된 셋 데이터 타입은 아래와 같이 값(value)이 실제 값(member)과 점수(score)를 가지는 형태로 되어 있다  실제 값(member)은 셋(Set)이기 때문에 유니크(unique)해야 한다. 점수(score)는 유니크할 필요 없다간단한 실습ZADD midterm 80 math 60 english 90 science -- 3-- 오름차순ZRANK midterm english -- 0ZRANK midterm math -- 1ZRANK midterm english WITHSCORE -- [0, &quot;60&quot;]-- 내림차순ZREVRANK midterm english -- 2",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-04T21:01:35+09:00'>04 Dec 2023</time><a class='article__image' href='/redis-data-types'> <img src='/images/redis_logo.png' alt='[Redis] 데이터 타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/redis-data-types'>[Redis] 데이터 타입</a> </h2><p class='article__excerpt'>레디스에서 지원하는 다양한 데이터 타입의 종류에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 컴포넌트간 데이터 전달 (with Props)",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-props",
      "date"     : "Dec 4, 2023",
      "content"  : "Table of Contents  Sharing state between components  Passing Data Deeply with Context  Responding to Events  참고Sharing state between components  부모 컴포넌트에서 자식 컴포넌트로 속성을 전달할 수 있다  하나의 상태 변수로 두 개의 컴포넌트가 함께 변하도록 하려면, 두 컴포넌트의 공통 부모 컴포넌트에서 상태 변수를 정의하고, 상태 변수를 두 자식 컴포넌트에게 속성(props)으로 전달하면 된다  Finally, pass the event handlers down so that the children can change the parent’s state.Passing Data Deeply with Context  Usually, you will pass information from a parent component to a child component via props  passing props can become verbose and inconvenient if you have to pass them through many components in the middle, or if many components in your app need the same information.  Context lets the parent component make some information available to any component in the tree below it without passing it explicitly through props  Context lets a component provide some information to the entire tree below it.  To pass context:          Create and export it with export const MyContext = createContext(defaultValue).      Pass it to the useContext(MyContext) Hook to read it in any child component, no matter how deep.      Wrap children into &amp;lt;MyContext.Provider value={…}&amp;gt; to provide it from a parent.        Context passes through any components in the middle.  Context lets you write components that “adapt to their surroundings”.  Before you use context, try passing props or passing JSX as children.Responding to Events  React lets you add event handlers to your JSX. Event handlers are your own functions that will be triggered in response to interactions like clicking, hovering, focusing form inputs, and so on.  You can handle events by passing a function as a prop to an element like &amp;lt;button&amp;gt;.  Event handlers must be passed, not called! onClick={handleClick}, not onClick={handleClick()}.  You can define an event handler function separately or inline.  Event handlers are defined inside a component, so they can access props.  You can declare an event handler in a parent and pass it as a prop to a child.  You can define your own event handler props with application-specific names.  Events propagate upwards. Call e.stopPropagation() on the first argument to prevent that.  Events may have unwanted default browser behavior. Call e.preventDefault() to prevent that.  Explicitly calling an event handler prop from a child handler is a good alternative to propagation.참고  Adding Interactivity, react.dev  Responding to Events, react.dev",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-04T21:01:35+09:00'>04 Dec 2023</time><a class='article__image' href='/react-props'> <img src='/images/react_logo.png' alt='[React]: 컴포넌트간 데이터 전달 (with Props)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-props'>[React]: 컴포넌트간 데이터 전달 (with Props)</a> </h2><p class='article__excerpt'>상위 컴포넌트에서 하위 컴포넌트로 데이터를 전달하는 법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 데이터베이스",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-database",
      "date"     : "Dec 4, 2023",
      "content"  : "Table of Contents  데이터베이스 선택  모델링  ERD데이터베이스 선택모델링ERD",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-04T21:01:35+09:00'>04 Dec 2023</time><a class='article__image' href='/backend_theory-database'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 데이터베이스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-database'>[Backend Thoery] 데이터베이스</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Redis] Intro",
      "category" : "data_engineering",
      "tags"     : "redis",
      "url"      : "/redis-intro",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  Redis  Redis를 쓰는 이유Redis  Redis: Remote Dictionary Server  레디스는 인메모리(In-Memory) 저장소이다  레디스는 키-값(Key-Value) 형태로 값을 저장하고 있다  값에 들어갈 수 있는 데이터 형태로는 String, Hash, Set, Sorted Set 등으로 다양한 데이터 타입을 지원한다  레디스의 주요 저장 장치인 메모리는 휘발성이기 때문에 세션 데이터 또는 캐시 데이터를 저장하는 용도로 주로 사용된다Redis를 쓰는 이유  메모리 기반, 해시 기반의 키-값 저장소이기 때문에 빠르다  다양한 데이터 타입을 지원해 카운터(ex. 좋아요, 조회수), 랭킹 등과 같은 기능들을 쉽게 구현할 수 있다  해시값으로 데이터에 접근하는 키-값 형태의 저장소이기 때문에 수평적 확장(Scale-Out)이 가능하다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/redis-intro'> <img src='/images/redis_logo.png' alt='[Redis] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/redis-intro'>[Redis] Intro</a> </h2><p class='article__excerpt'>레디스의 특징과 레디스를 사용하는 이유에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 리렌더링 (with State)",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-state",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  동적인 웹페이지  State          useState      State의 특성        Re-rendering          State as a snapshot      Queueing a series of state updates      Updating objects in state        참고동적인 웹페이지  요즘 웹페이지는 대부분 동적이다  동적인 요소는 시간에 따라 변하는 것도 있고, 유저의 이벤트에 따라 변하는 것도 있다  예를 들어 검색창에 입력한 텍스트가 UI에 반영되는 것, 구매할 상품의 개수를 +,- 버튼으로 누를 때마다 숫자가 변하는 것, 다크모드/라이트모드에 따라 화면 색깔이 바뀌는 것 등이 동적인 요소라고 할 수 있다  리액트에서는 이렇게 변하는 데이터를 상태(state)라고 한다  상태가 변할 때마다 상태를 포함하고 있는 컴포넌트가 업데이트되는 것을 리렌더링(re-rendering)이라고 한다State  상태는 현재 값을 저장해둔 변수와 비슷하며 리액트 컴포넌트가 기억하고 있는 값이라고 할 수 있다  일반 변수는 값이 변경돼도 화면이 업데이트 되지 않지만, 리액트의 상태는 변경되면 화면이 업데이트 된다  리액트 컴포넌트가 상태의 현재 값을 저장해뒀다가 상태 값이 현재 값과 비교해 변경된 경우, 리액트는 이러한 변화를 업데이트해 화면에 반영한다useState  리액트에서 상태는 useState 라는 훅을 이용해 정의할 수 있다  useState를 호출하는 것은 리액트 컴포넌트에게 무언가를 기억하라고 하는 것과 같다  useState 함수는 인자로 상태의 초기 값을 받고, [state, setState] 쌍을 반환한다  리액트에서는 상태(state)가 변화할 때마다 화면에 결과를 반영한다  리액트는 상태가 세터 함수(setState)에 의해 변경될 때만 화면에 결과를 반영한다const [count, setCount] = useState(0)State의 특성  State is isolated and private          State is not tied to a particular function call or a place in the code, but it’s “local” to the specific place on the screen      What if you wanted both galleries to keep their states in sync? The right way to do it in React is to remove state from child components and add it to their closest shared parent      Re-rendering  The component’s (or one of its ancestors’) state has been updated  Updating your component’s state automatically queues a render  “Rendering” is React calling your components.          On initial render, React will call the root component.      For subsequent renders, React will call the function component whose state update triggered the render            After rendering (calling) your components, React will modify the DOM.          For the initial render, React will use the appendChild() DOM API to put all the DOM nodes it has created on screen.      For re-renders, React will apply the minimal necessary operations (calculated while rendering!) to make the DOM match the latest rendering output.        React only changes the DOM nodes if there’s a difference between renders. For example, here is a component that re-renders with different props passed from its parent every second. Notice how you can add some text into the &amp;lt;input&amp;gt;, updating its value, but the text doesn’t disappear when the component re-renders:  After rendering is done and React updated the DOM, the browser will repaint the screen. Although this process is known as “browser rendering”, we’ll refer to it as “painting”State as a snapshot  Setting state requests a new re-render, but does not change it in the already running code  “Rendering” means that React is calling your component, which is a function. The JSX you return from that function is like a snapshot of the UI in time. Its props, event handlers, and local variables were all calculated using its state at the time of the render.  State actually “lives” in React itself—as if on a shelf!—outside of your function. When React calls your component, it gives you a snapshot of the state for that particular render. Your component returns a snapshot of the UI with a fresh set of props and event handlers in its JSX, all calculated using the state values from that render!  아래 코드를 보자  버튼 클릭 한 번에 setNumber 함수가 세 번 호출된다. 하지만 클릭 하면 숫자는 1 밖에 증가하지 않는다  왜냐하면 setter 함수는 상태(state)를 코드 실행 중에 바꾸는 것이 아니라, 컴포넌트가 반환되는 시점에 바꾸기 때문이다  그래서 함수가 세 번 호출되는 동안 number는 계속 0이다      다시 말해 상태 변수는 렌더링중에는 고정되어 있다    Setting state requests a new render.  React stores state outside of your component, as if on a shelf.  When you call useState, React gives you a snapshot of the state for that render.  Variables and event handlers don’t “survive” re-renders. Every render has its own event handlers.  Every render (and functions inside it) will always “see” the snapshot of the state that - React gave to that render.  You can mentally substitute state in event handlers, similarly to how you think about the rendered JSX.  Event handlers created in the past have the state values from the render in which they were created.Queueing a series of state updates  여러 번의 setter 함수 호출을 다음 렌더링 하나에 반영하고 싶을 때, React 배치 상태 업데이트를 알면 도움이 된다  리액트는 이벤트 핸들러 안에 있는 모든 코드를 실행한 후에 상태 변수를 업데이트 한다  만약 다음 렌더링 전까지 여러 번에 걸쳐 상태 값이 바뀌길 원하면, setter 함수 안에 화살표 함수 형태로 인자를 주면 된다. setter 함수 안에 있는 화살표 함수는 상태 값의 이전 값을 바탕으로 값을 변경한다  n =&amp;gt; n + 1 과 같은 함수를 updater 함수라고 한다  updater 함수를 사용해서 setter 함수를 작성한 경우 이벤트 핸들러 코드는 다음과 같이 동작한다          updater 함수를 큐에 담아두고 먼저 이벤트 핸들러 안에 있는 다른 코드를 모두 실행한다      리액트는 큐에 있는 모든 updater 함수를 지나며 상태를 업데이트 하고 최종 상태를 반환한다            updater 함수뿐만 아니라 그냥 replace 값도 큐에 저장됨    Setting state does not change the variable in the existing render, but it requests a new render.  React processes state updates after event handlers have finished running. This is called batching.  To update some state multiple times in one event, you can use setNumber(n =&amp;gt; n + 1) updater function.Updating objects in state  Treat all state in React as immutable.  When you store objects in state, mutating them will not trigger renders and will change the state in previous render “snapshots”.  Instead of mutating an object, create a new version of it, and trigger a re-render by setting state to it.  You can use the {…obj, something: ‘newValue’} object spread syntax to create copies of objects.  Spread syntax is shallow: it only copies one level deep.  To update a nested object, you need to create copies all the way up from the place you’re updating.  To reduce repetitive copying code, use Immer.참고  Adding Interactivity, react.dev  React re-renders guide: everything, all at once, Developer Way  Re-rendering Components in ReactJS, Geeksforgeeks  How and when to force a React component to re-render, LogRocket",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/react-state'> <img src='/images/react_logo.png' alt='[React]: 리렌더링 (with State)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-state'>[React]: 리렌더링 (with State)</a> </h2><p class='article__excerpt'>상태 변화에 따른 리렌더링을 통해 웹 페이지를 동적으로 만드는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 컴포넌트 (with JSX)",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-jsx",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  JSX  JSX 문법          하나의 부모 요소로 감싸야 한다      인라인 표기법이 HTML과 살짝 다르다      반복문을 쓸 수 없다      조건문을 쓸 수 없다        트랜스파일링JSX  JavaScript XML  자바스크립트 코드 안에 HTML 코드를 작성하기 위해 만든 문법이다  리액트에서 컴포넌트를 만들기 위해 사용하는 문법이다function App() {    return (        &amp;lt;div&amp;gt;            &amp;lt;h1&amp;gt;Hello React!&amp;lt;/h1&amp;gt;        &amp;lt;/div&amp;gt;    )}                JSX 등장 배경              예전에는 HTML, CSS, JS를 모두 분리해서 작성했다. 하지만 컴포넌트를 지향하는 모던 프론트엔드에 오면서 JS 컴포넌트 파일 하나에 HTML, CSS를 같이 관리하는 방법이 선호되게 되었다. JSX는 HTML 코드를 JS 코드 안에서 작성하도록 해준다    JSX 문법하나의 부모 요소로 감싸야 한다  컴포넌트는 DOM 트리의 일부가 되는데, 컴포넌트에 포함되는 모든 노드가 트리의 상위 노드에 속하기 위해서는 하나의 부모 요소로 감싸져야 한다  보통 &amp;lt;div&amp;gt;&amp;lt;/div&amp;gt;로 감싸거나 &amp;lt;&amp;gt;&amp;lt;/&amp;gt;로 감싼다인라인 표기법이 HTML과 살짝 다르다  원래 HTML의 태그에서 클래스를 설정할 때 class=&#39;item&#39; 이렇게 표기하지만 JSX 에서는 className=&#39;item&#39; 이런식으로 표기한다  JSX는 자바스크립트의 확장된 문법이고, class는 자바스크립트에서 예약어이기 때문에 className을 사용하는 것이다반복문을 쓸 수 없다  JSX 안에서는 반복문을 쓸 수가 없다  그래서 고차함수 map() 을 쓰거나, 배열을 리턴하는 함수를 호출한다조건문을 쓸 수 없다  JSX 안에서는 조건문을 쓸 수 없다  그래서 삼항연산자를 이용한다트랜스파일링  브라우저는 JSX를 이해하지 못하기 때문에 바벨(Babel)과 같은 컴파일러로 JS 코드로 트랜스파일링 해야 한다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/react-jsx'> <img src='/images/react_logo.png' alt='[React]: 컴포넌트 (with JSX)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-jsx'>[React]: 컴포넌트 (with JSX)</a> </h2><p class='article__excerpt'>JSX를 이용해 컴포넌트를 만드는 방법에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: Intro",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-intro",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  리액트  리액트의 특징          Virtual DOM      선언형 UI      컴포넌트 기반      단방향 데이터 전달        JSX  렌더링          초기 렌더링      리렌더링        리액트 훅  참고리액트  리액트는 2013년 페이스북에서 공개한 프론트엔드 라이브러리이다  컴포넌트 기반으로 유저 인터페이스(UI)를 만들도록 해준다리액트의 특징Virtual DOM  Virtual DOM은 메모리에 구현해놓은 가상 DOM을 말한다  기존의 Virtual DOM과 컴포넌트 함수 재실행으로 변경된 Virtual DOM을 비교해, 변경된 부분을 React DOM에 알린다  React DOM은 Real DOM과 비교해 변경된 부분만 Real DOM에 업데이트 하여 재렌더링 한다  Virtual DOM은 다수의 변경을 그룹화 하여 한 번에 처리해주기 때문에 렌더링 성능을 개선해준다  개발자는 직접 Real DOM을 조작하는 명령형 코드가 아닌, 원하는 상태를 기술하는 선언형 코드를 통해 DOM 조작을 프레임워크에 위임한다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다단방향 데이터 전달  컴포넌트간의 데이터 전달을 양방향이 아닌 단방향으로 한정함으로써 상태 관리를 좀 더 쉽게 할 수 있게 되었다  (양방향은 상태 변화의 흐름을 추적하기 어려워, 문제가 발생했을 때 문제의 원인을 찾기 힘들게 만들었다)  (Angular, Vue는 양방향을 사용한다)  부모 컴포넌트에서 자식 컴포넌트로만 데이터를 전달할 수 있다JSX  JavaScript XML  자바스크립트 코드 안에 HTML 코드를 작성하기 위해 만든 문법이다  리액트에서 컴포넌트를 만들기 위해 사용하는 문법이다function App() {    return (        &amp;lt;div&amp;gt;            &amp;lt;h1&amp;gt;Hello React!&amp;lt;/h1&amp;gt;        &amp;lt;/div&amp;gt;    )}                JSX 등장 배경              예전에는 HTML, CSS, JS를 모두 분리해서 작성했다. 하지만 컴포넌트를 지향하는 모던 프론트엔드에 오면서 JS 컴포넌트 파일 하나에 HTML, CSS를 같이 관리하는 방법이 선호되게 되었다. JSX는 HTML 코드를 JS 코드 안에서 작성하도록 해준다    렌더링  HTML 코드가 사용자의 화면에 나타나기 까지의 과정을 렌더링이라 한다  Ajax와 SPA의 등장으로 페이지 단위 렌더링에서 컴포넌트 단위의 렌더링이 가능하게 됐다  리액트에서는 크게 두 가지 렌더링이 있다: 초기 렌더링(initial-rendering), 리렌더링(re-rendering)초기 렌더링  사용자가 처음 사이트에 접속할 때, 리액트는 root라는 진입점을 만들어 그 밑에 우리가 만든 컴포넌트로 묶인 DOM 트리를 연결한다  root.render(&amp;lt;App/&amp;gt;) 로 초기 렌더링을 실행한다리렌더링  컴포넌트 안에 있는 데이터가 변경되면 리액트는 이것을 감지해 해당 컴포넌트를 리렌더링 한다  리액트는 감지할 데이터를 상태(state)라고 한다  즉, 상태에 변경이 감지되면 리액트는 컴포넌트를 업데이트 하기위해 해당 상태를 포함하는 컴포넌트를 리렌더링 한다리액트 훅  리액트에서는 상태 관리, 요소 참조와 같은 여러 기능들을 쉽게 사용할 수 있도록 훅(hook)으로 제공한다  React 16.8 버전에 새로 추가된 기능으로, 함수형 컴포넌트 안에서만 사용할 수 있다  자주 사용하는 훅으로는 useState(), useEffect(), useRef(), useMemo() 등이 있다참고  Render and Commit, react.dev  리액트란?, 코딩젤리  [React] - What is a Root Element in React?, She Codes",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/react-intro'> <img src='/images/react_logo.png' alt='[React]: Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-intro'>[React]: Intro</a> </h2><p class='article__excerpt'>리액트에 대한 소개와, 리액트의 특징에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[React]: 프론트엔드 개발 흐름의 변화",
      "category" : "frontend",
      "tags"     : "react",
      "url"      : "/react-frontend-history",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  프론트엔드 영역의 등장          HTML      자바스크립트      CSS      Ajax      SPA        리액트의 등장  모던 프론트엔드          Virtual DOM      선언형 UI      컴포넌트 기반      전략적인 렌더링        참고프론트엔드 영역의 등장HTML  HyperText Markup Language  텍스트를 h1, div 와 같은 태그로 구조화 하였다  텍스트로 된 문서에 링크를 입혀 웹 공간에서 문서간 이동을 가능하게 했다자바스크립트  웹 브라우저에 동적인 요소를 구현하기 위해 프로그래밍 언어를 개발했다CSS  텍스트의 서식을 따로 분리하기 위해 만들어졌다Ajax  Ajax의 등장으로 브라우저는 필요한 부분의 데이터만 비동기적으로 받아와 변경된 부분만 재렌더링할 수 있게 되었다SPA출처: Scalable Path  Single Page Application  하나의 HTML 파일(index.html)만 이용해 서비스를 제공하는 어플리케이션을 말한다  하나의 HTML 파일에 변경되는 요소만 재렌더링 하는 방식으로 훨씬 부드러운 사용자 경험을 제공한다  프론트엔드의 랜더링 방식을 페이지 단위의 렌더링에서 컴포넌트 단위의 렌더링으로 변화시켰다  Ajax가 SPA를 촉발시켰으며, SPA의 등장으로 서버는 이제 더이상 전체 HTML 파일을 제공할 필요 없이, 필요한 데이터만 JSON 형식으로 보내면 되게 되었다  이로 인해 서버사이드 진영에서는 자연스럽게 화면을 담당하는 코드와 데이터를 담당하는 코드를 분리하는 MVC 패턴 형태로 코드를 작성하게 되었다  그리고 웹 애플리케이션의 규모가 점점 커지게 되면서, 프론트엔드라는 영역이 따로 분리되어 생겨났다                SPA의 장단점                      장점                      페이지 이동에 있어 유저에게 더 높은 UX 를 제공한다            JSON API 를 통해 느슨한 결합 형태로 설계할 수 있다            필요한 데이터만 요청하면 되므로 서버의 부하가 감소한다                          단점                      유저가 처음 접속시 이후 요소를 만드는데 필요한 모든 자바스크립트 코드를 불러오기 때문에 오래 걸린다            HTML 파일이 데이터로 모두 채워져 있지 않기 때문에 SEO의 성능이 낮다            이후에 알아볼 리액트 또한 SPA 기반 프레임워크인데, 이러한 단점을 서버사이드 렌더링(SSR)으로 보완했다                            리액트의 등장  UI 만을 담당하는 프론트엔드라는 영역이 따로 분리된 데에는 그만큼 프론트엔드 영역의 규모와 복잡성이 커졌기 때문이다  페이스북은 프론트엔드 영역에 프레임워크의 필요성을 느끼고 개발 끝에 2013년 리액트를 세상에 공개했다  리액트는 SPA 방식의 프론트엔드 라이브러리이다  리액트는 모던 프론트엔드 철학이 잘 반영되어있고 이에 필요한 기능들을 제공한다  (리액트에 관한 더 자세한 내용은 이 후 포스트들을 참고한다)                리액트가 라이브러리인 이유                      리액트는 UI에 꼭 필요한 기능들만 가지고 있고, 라우팅, 테스트, 빌드와 같은 부가 요소는 서드파티를 임포트하는 방식으로 사용하도록 설계되었다.        또한 파일 구조, 코드 등을 프레임워크 처럼 강제하지 않는다. 이러한 이유로 리액트를 프레임워크가 아닌 라이브러리라고 한다          모던 프론트엔드  모던 프론트엔드는 인터랙티브하고 사용자 친화적, 미학적인 웹 인터페이스를 만드는데 사용되는 프론트엔드 기술의 현재 경향을 말한다  모던 프론트엔드의 특징은 다음과 같다          리액트에서 제공: Virtual DOM, 선언형 UI, 컴포넌트 기반      next.js에서 제공: 전략적인 렌더링      Virtual DOM  메모리에 가상의 DOM을 구현해놓고, 변경이 발생했을 때 변경된 부분을 실제 DOM에 반영한다  가상 DOM에서의 변화는 렌더링을 유발하지 않아 연산 비용이 낮고, 다수의 변경을 그룹화하여 한 번에 처리할 수도 있어 효율적이다  또한 리액트에서는 개발자가 직접하기 부담스러운 명령적인 DOM 조작, 관리를 프레임워크에 위임한다  Virtual DOM은 렌더링 성능을 개선해준다선언형 UI  직접 UI가 어떻게 변경되어야 하는지에 관해 명령형으로 코드를 작성하지 않고, 원하는 UI의 상태에 관해 선언적으로 코드를 작성한다  프레임워크가 선언된 UI 상태와 일치하도록 알아서 렌더링한다  선언형 UI는 코드의 유지보수성을 높여준다컴포넌트 기반  SPA의 등장으로, 렌더링 단위가 페이지 단위에서 컴포넌트 단위로 변화함에 따라 컴포넌트 기반으로 UI를 개발하는게 선호되었다  컴포넌트 기반 개발은 코드의 재사용성이 더 높인다전략적인 렌더링  렌더링은 크게 서버 사이드 렌더링(SSR)과 클라이언트 사이드 렌더링(CSR)으로 나뉜다  SSR          서버에서 완성된 HTML을 클라이언트에게 서빙하는 방식      사용자가 요청한 페이지에서 필요한 모든 데이터를, 서버에서 HTML 파일에 담아 완성된 HTML 파일을 반환한다      초기 랜딩 페이지, 레이아웃, 정적 페이지와 같이 변경이 자주 일어나지 않는 경우에 적합하다        CSR          클라이언트에서 자바스크립트 코드를 실행해 동적으로 HTML 파일을 생성하는 방식      사용자가 요청한 데이터만 서버에서 받아온 후, 변경된 컴포넌트만 클라이언트에서 재렌더링 한다      인터랙티브하며 데이터 변경이 자주 발생하는 경우에 적합하다        next.js 프레임워크는 상황에 적합한 렌더링 방식을 취하는 전략적인 렌더링 기능을 제공한다  전략적인 렌더링은 두 렌더링 방식의 장점을 모두 이용하도록 해준다                SSR과 CSR의 장단점                  SSR                  장점                          초기 사이트 접속시 로딩 시간이 짧다              HTML 파일에 모든 데이터가 담겨있기 때문에 SEO 성능이 좋다                                단점                          사용자가 페이지 이동시마다 전체 렌더링이 일어나기 때문에 낮은 사용자 경험을 제공하며, 서버에 부하가 높아진다                                          CSR                  장점                          전체 렌더링이 아닌 일부만 렌더링 되기 때문에 높은 사용자 경험을 제공하며, 서버 부하를 낮춘다                                단점                          사용자가 처음 사이트 접속시 필요한 모든 자바스크립트 코드를 로드하고, 랜딩 페이지에 해당하는 자바스크립트 코드를 실행해 렌더링해야 하기 때문에, 처음 접속시 페이지 로딩이 느리다              데이터가 포함된 HTML 파일이 클라이언트에 의해 완성되기 때문에, SEO 성능이 낮아진다                                            참고  모던 웹 프론트엔드의 이해, sejinkim  What Is a Single-Page Application (SPA)? Pros &amp;amp; Cons With Examples, Scalable path",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/react-frontend-history'> <img src='/images/react_logo.png' alt='[React]: 프론트엔드 개발 흐름의 변화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/react-frontend-history'>[React]: 프론트엔드 개발 흐름의 변화</a> </h2><p class='article__excerpt'>프론트엔드, 리액트, 모던 프론트엔드 순으로 프론트엔드 개발 흐름의 변화에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(6) DTO",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series8",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  DTO          DTO 사용에 필요한 라이브러리      DTO 만들기      DTO  Data Transfer Object의 약자  들어오고 나가는 데이터의 형태를 애플리케이션에서 규격화한 클래스(DTO)에 맞는지 검증하는 데이터 검증 방식  (DTO는 Nest에서 정해진 형태로 제공하는 구성요소는 아니지만, 여러 백엔드 애플리케이션에서 데이터 검증 목적으로 많이 사용)DTO 사용에 필요한 라이브러리  Nest에서 DTO를 적용하려면 아래의 라이브러리를 설치해야함  class-validator: DTO 클래스에서 프로퍼티를 검증할 때 사용할 유용한 데코레이터를 제공해줌  class-transformer: plain json과 class object 간의 직렬화/역직렬화에 유용한 기능을 제공? Nest에서 제공하는 빌트인 파이프인 ValidationPipe가 직렬화/역직렬화 기능을 함. 그래서 class-transformer가 꼭 필요한 건 아닌 것 같음. (하지만 여전히 다른 유용한 기능도 많이 제공하기 때문에 설치해두면 좋음)npm i class-validator class-transformerDTO 만들기// create-cat.dto.tsimport { IsString, IsInt } from &#39;class-validator&#39;;export class CreateCatDto {  @IsString()  name: string;  @IsInt()  age: number;  @IsString()  breed: string;}// main.tsconst app = await NestFactory.create(AppModule);app.useGlobalPipes(new ValidationPipe())// controller.ts@Post()create(@Body() createCatDto: CreateCatDto) {  ...}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/nestjs-series8'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(6) DTO'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series8'>[NestJS] 구성요소(6) DTO</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(5) Pipes",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series7",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  Pipes          Built-in Pipes                  ValidationPipe          DefaultValuePipe          Parse* Pipe                    Pipe 적용하기                  Parameter-level 적용 예시          Global-level 적용 예시                    커스텀 파이프 만들기      Pipes  들어온 데이터를 검증/변환하는 용도의 클래스  PipeTransform 를 implements 하면 됨Built-in Pipes  Nest에서 기본적으로 다음과 같은 빌트인 파이프 제공ValidationPipe  글로벌한 규칙을 적용하고 싶은 경우에 많이 사용  ValidationPipeOptions 객체를 통해 옵션 전달할 수 있음  ex. whitelist, forbidNonWhitelisted, transform 등과 같은 인자로 글로벌하게 규칙 적용  어플리케이션에서 정해둔 인풋 데이터를 whitelist 라고 함  whitelist: true          정해둔 인풋 데이터만 받음      (정해두지 않은 예상치 않은 인풋 데이터를 허용하면, 보안상 취약점 생길 수 있음)        forbidNonWhitelisted: true          정해두지 않은 예상치 않은 인풋 데이터가 들어오면, 400 예외 발생시킴으로써 아예 어플리케이션에서 추가 실행되지 않도록 할 수 있음      (whitelist: true 옵션도 함께 줘야 예상대로 동작함)        transform: true          타입 힌팅으로 전달해준 타입으로 알아서 변환해줌      (ex. ParseIntPipe 안써도, int로 타입 힌팅해주면 numeric한 문자열을 정수 타입으로 변환해줌)      DefaultValuePipe  Parse* 파이프를 통과해야 하는 파라미터는 그냥 단순히 =’기본값’ 형태로 디폴트를 줄 수 없음  DefaultValuePipe 파이프를 사용해 디폴트 값을 줘야함Parse* Pipe  ParseIntPipe, ParseFloatPipe, ParseBoolPipe, ParseArrayPipe, ParseUUIDPipe, ParseEnumPipe, ParseFilePipe  Parse* 파이프들은 Int, Float, .. 등으로 데이터를 변환시켜줌으로써 해당 데이터 타입임을 확실할 수 있도록 해줌  변환 불가능한 타입이면 Bad Request 예외 발생 (ex. ParseIntPipe에 대해 ‘Apple’: 예외 발생, ‘1’: 1로 변환)  대부분 파라미터에 사용Pipe 적용하기Parameter-level 적용 예시  @Param(), @Query() 등과 같은 데코레이터에 전달  여러 파이프를 순차적으로 전달할 수도 있음  의존성 주입 방식으로 그냥 클래스만 전달 할 수도 있고, 옵션을 넣어줌으로써 조금 더 커스텀하게 쓰고 싶은 경우 직접 인스턴스를 전달할 수도 있음@Get(&#39;:id&#39;)async findOne(@Param(&#39;id&#39;, ParseIntPipe) id: number) {  return this.catsService.findOne(id);}@Get(&#39;:id&#39;)async findOne(  @Param(&#39;id&#39;, new ParseIntPipe({ errorHttpStatusCode: HttpStatus.NOT_ACCEPTABLE }))  id: number,) {  return this.catsService.findOne(id);}  Parse* 파이프들은 항상 어떤 값이 들어오기를 기대 -&amp;gt; null, undefined 들어오면 예외 발생  그래서 Parse* 파이프 앞에 먼저 DefaultValuePipe 파이프 넣어주면 좋음@Get()async findAll(  @Query(&#39;activeOnly&#39;, new DefaultValuePipe(false), ParseBoolPipe) activeOnly: boolean,  @Query(&#39;page&#39;, new DefaultValuePipe(0), ParseIntPipe) page: number,) {  return this.catsService.findAll({ activeOnly, page });}Global-level 적용 예시  useGlobalPipes()를 통해 main.ts 파일에 적용const app = await NestFactory.create(AppModule);app.useGlobalPipes(new ValidationPipe(  {     whitelist: true,    forbidNonWhitelisted: true,  }))커스텀 파이프 만들기  PipeTransform 을 implements 하면 됨  transform() 을 구현해야함. 다음의 두 인자를 받음          value: 파이프를 지나는 데이터      metadata: 파이프가 적용된 데이터의 메타데이터                  metadata 객체는 다음과 같은 속성을 가짐          type: 인자가 요청의 어느 부분으로 들어오는지 (‘body’, ‘query’, ‘param’, ‘custom’)          metatype: 데이터에 선언된 타입 (ex. @Param(&#39;id&#39;, ParseIntPipe) id: number) 인 경우 number)          data: 데코레이터에 넘겨진 문자열 (ex. @Param(&#39;id&#39;, ParseIntPipe) id: number) 인 경우 id)                    import { PipeTransform, Injectable, ArgumentMetadata } from &#39;@nestjs/common&#39;;@Injectable()export class MyValidationPipe implements PipeTransform {  transform(value: any, metadata: ArgumentMetadata) {    return value;  }}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/nestjs-series7'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(5) Pipes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series7'>[NestJS] 구성요소(5) Pipes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(4) Guards",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series6",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  Guards          Guard 만들기      Guard 적용하기                  컨트롤러에 적용하고 싶은 경우          라우트 핸들러에 적용하고 싶은 경우          글로벌하게 적용하고 싶은 경우                    Guards  인증/인가 용도의 클래스  CanActivate 를 implements 하면 됨  가드 또한 constructor() 생성자 함수로 의존성 주입할 수 있음Guard 만들기  CanActivate 를 implements -&amp;gt; canActivate() 메서드 구현해야함  canActivate() 함수는 boolean을 리턴해야함          true면 요청을 처리함      false면 요청을 거부함        canActivate() 함수는 Execution Context가 인자로 제공됨 -&amp;gt; 이를 통해 요청 객체를 참조할 수 있음import { Injectable, CanActivate, ExecutionContext } from &#39;@nestjs/common&#39;;import { Observable } from &#39;rxjs&#39;;@Injectable()export class RolesGuard implements CanActivate {  canActivate(    context: ExecutionContext,  ): boolean | Promise&amp;lt;boolean&amp;gt; | Observable&amp;lt;boolean&amp;gt; {    const req = context.switchToHttp().getRequest()    const isAdmin = req.user.roles == &#39;admin&#39;    return isAdmin  }}Guard 적용하기컨트롤러에 적용하고 싶은 경우@Controller(&#39;users&#39;)@UseGuards(RolesGuard)export class UserController {}라우트 핸들러에 적용하고 싶은 경우@Controller(&#39;users&#39;)export class UserController {  @UseGuards(RolesGuard)  @Get()  readUser() {}}글로벌하게 적용하고 싶은 경우  의존성 주입이 적용되지 않은 방법  모듈 밖에서 가드가 등록되었기 때문에 의존성 주입이 적용되지 않음const app = await NestFactory.create(AppModule);app.useGlobalGuards(new RolesGuard());  의존성 주입이 적용되는 방법  아무 모듈중 하나에 다음과 같은 방법으로 의존성 주입할 수 있음import { Module } from &#39;@nestjs/common&#39;;import { APP_GUARD } from &#39;@nestjs/core&#39;;@Module({  providers: [    {      provide: APP_GUARD,      useClass: RolesGuard,    },  ],})export class AppModule {}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/nestjs-series6'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(4) Guards'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series6'>[NestJS] 구성요소(4) Guards</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(3) Interceptors",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series5",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  Interceptors          Interceptor 기능      Interceptor 만들기      Interceptor 적용하기                  컨트롤러에 적용하고 싶은 경우          라우트 핸들러에 적용하고 싶은 경우          글로벌하게 적용하고 싶은 경우                    Interceptors  미들웨어와 비슷한 용도  NestInterceptor 를 implements 하면 됨  인터셉터 또한 constructor() 생성자 함수로 의존성 주입할 수 있음Interceptor 기능  메서드 실행 전/후로 추가적인 로직을 실행하도록 해줌  함수로부터 리턴 받은 결과를 변환하도록 해줌  함수로부터 던져진 예외를 변환하도록 해줌  특별한 상황에서 함수를 완전 오버라이딩 하도록 해줌 (ex. 캐싱)Interceptor 만들기  NestInterceptor 를 implements -&amp;gt; intercept() 메서드 구현해야함  intercept() 는 두 개의 인자 제공됨 (Execution Context, Call Handler)          Execution Context                  현재 상태에 대한 정보를 가지고 있는 utility class(helper class)          컨트롤러, 메서드, 실행 문맥 등 다양한 환경에 제네릭한 execution process를 제공                    Call Handler                  handle() 메서드를 호출함으로써 Observable을 리턴하고, 핸들러를 트리거          핸들러의 response stream이 Observable 안에서 흐르게 되고, Observable 안에서는 이 response에 추가적인 연산을 수행할 수 있음          (Observable을 쓰는 이유는 RxJS 라이브러리의 강력한 연산들을 사용할 수 있기 때문)                    import { Injectable, NestInterceptor, ExecutionContext, CallHandler } from &#39;@nestjs/common&#39;;import { Observable } from &#39;rxjs&#39;;import { tap } from &#39;rxjs/operators&#39;;@Injectable()export class LoggingInterceptor implements NestInterceptor {  intercept(context: ExecutionContext, next: CallHandler): Observable&amp;lt;any&amp;gt; {    // context 통해 요청객체(req)에 접근할 수 있음    const req = context.switchToHttp().getRequest()    // 응답객체에 별다른 작업이 필요 없다면, next.handle()만 호출하면 됨    return next.handle()    // 응답객체에 별도의 작업을 원하면, handle.pipe(map( data =&amp;gt; data + 1 )) 이런식으로 map() 함수 사용    // 응답객체에 별다른 작업 필요 없지만, 별개의 함수 실행 원하면, handle.pipe(tap( () =&amp;gt; console.log(&#39;tap! tap!&#39;) )) 이런식으로 tap() 함수 사용    return next.handle().pipe(tap(() =&amp;gt; console.log(`After... ${Date.now() - now}ms`)),);  }}Interceptor 적용하기컨트롤러에 적용하고 싶은 경우@UseInterceptors(LoggingInterceptor)export class UserController {}라우트 핸들러에 적용하고 싶은 경우export class UserController {  @UseInterceptors(LoggingInterceptor)  @Get()  readUser() {}}글로벌하게 적용하고 싶은 경우  의존성 주입이 적용되지 않은 방법  모듈 밖에서 인터셉터가 등록되었기 때문에 의존성 주입이 적용되지 않음// main.tsconst app = await NestFactory.create(AppModule)app.useGlobalInterceptors(new LoggingInterceptor() )  의존성 주입이 적용되는 방법  아무 모듈중 하나에 다음과 같은 방법으로 의존성 주입할 수 있음// app.module.tsimport { Module } from &#39;@nestjs/common&#39;;import { APP_INTERCEPTOR } from &#39;@nestjs/core&#39;;@Module({  providers: [    {      provide: APP_INTERCEPTOR,      useClass: LoggingInterceptor,    },  ],})export class AppModule {}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/nestjs-series5'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(3) Interceptors'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series5'>[NestJS] 구성요소(3) Interceptors</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(2) Providers",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series4",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  Provider          Service      Repository      Provider  NestJS 어플리케이션에서 의존성으로서 주입될 수 있는 클래스들 (ex. Service, Repository 등)  @Injectable() 데코레이터를 이용하면 Nest IoC Container에 의해 클래스가 관리됨  별도의 스코프를 지정해주지 않으면 일반적으로 싱글턴 인스턴스가 생성  Nest는 typescript의 type을 이용해 클래스들간의 의존성 관계를 쉽게 파악하고, 필요한 인스턴스를 생성, 리턴함  (일반적으로 사용되는 싱글턴의 경우, 다른 곳에서 이미 인스턴스가 생성됐으면, 새로 생성되지 않고, 다른 곳에서 만들었던 인스턴스를 리턴)@Controller(&#39;cats&#39;)export class CatsController {  // 컨트롤러에서는 서비스 프로바이더를 생성자 함수를 통해 주입한다  constructor(private catsService: CatsService) {}  @Post()  async create(@Body() createCatDto: CreateCatDto) {    this.catsService.create(createCatDto);  }  @Get()  async findAll(): Promise&amp;lt;Cat[]&amp;gt; {    return this.catsService.findAll();  }}? Strategy, Guard 도 @Injectable() 붙는데 프로바이더인가? 근데 왜 Module의 providers에 등록하지 않아도 별다른 에러가 없는걸까?Service  비즈니스 로직을 수행하는 프로바이더의 일종  또한 Service 프로바이더는 Repository 프로바이더를 의존성으로 가지고 있음// items.service.ts@Injectable()export class ItemService {  constructor(@InjectRepository(ItemEntity) private itemRepo: Repository&amp;lt;ItemEntity&amp;gt;) {}  async readItem(id: number) {    const item = await this.itemRepo.findOneBy({ id })    return item  }}// item.controller.ts@Controller(&#39;items&#39;)export class ItemController {  constructor(private itemService: ItemService) {}  @Get(&#39;:id&#39;)  async readItem(@Param(&#39;id&#39;) id: number) {    const item = await this.itemService.readItem(id)    return item  }}// items.module.ts@Module(  {    controllers: [ItemController],    providers: [ItemService],    exports: [ItemService]  })export class ItemModule {}// app.module.ts@Module(  {    imports: [ItemModule],    controllers: [AppController],    providers: [AppServie],    exports: [AppService]  })export class AppModule {}Repository  데이터베이스와 관련된 작업을 담당하는 프로바이더의 일종  리포지토리 프로바이더를 제공하는 모듈을 직접 만들어도 되지만, 이미 해당 ORM에서 훌륭한 모듈 제공해줌  여기서는 TypeORM을 사용할 예정이므로, TypeOrmModule 사용  TypeOrmModule에서는 엔티티 정의, CRUD 메서드, 관계 정의 등 DB와 관련된 기능들을 제공해줌// items.service.ts@Injectable()export class ItemService {  constructor(@InjectRepository(ItemEntity) private itemRepo: Repository&amp;lt;ItemEntity&amp;gt;) {}  async readItem(id: number) {    const item = await this.itemRepo.findOneBy({ id })    return item  }}// items.module.ts@Module({  imports: [    TypeOrmModule.forFeature([ItemEntity]),  ],  controllers: [ItemController],  providers: [ItemService],  exports: [ItemService]})export class ItemModule {}// app.module.ts@Module({  imports: [    TypeOrmModule.forRoot({      type: &#39;mysql&#39;,      host: &#39;localhost&#39;,      port: 3300,      username: &#39;admin&#39;,      password: &#39;admin&#39;,      database: &#39;mydb&#39;,      entities: [ItemEntity]    }),    ItemModule  ],  controllers: [AppController],  providers: [AppService],})export class AppModule {}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/nestjs-series4'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(2) Providers'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series4'>[NestJS] 구성요소(2) Providers</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[HTML/CSS]: Intro",
      "category" : "frontend",
      "tags"     : "HTML-CSS",
      "url"      : "/htmlcss-intro",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents  DTO          DTO 사용에 필요한 라이브러리      DTO 만들기      DTO  Data Transfer Object의 약자  들어오고 나가는 데이터의 형태를 애플리케이션에서 규격화한 클래스(DTO)에 맞는지 검증하는 데이터 검증 방식  (DTO는 Nest에서 정해진 형태로 제공하는 구성요소는 아니지만, 여러 백엔드 애플리케이션에서 데이터 검증 목적으로 많이 사용)DTO 사용에 필요한 라이브러리  Nest에서 DTO를 적용하려면 아래의 라이브러리를 설치해야함  class-validator: DTO 클래스에서 프로퍼티를 검증할 때 사용할 유용한 데코레이터를 제공해줌  class-transformer: plain json과 class object 간의 직렬화/역직렬화에 유용한 기능을 제공? Nest에서 제공하는 빌트인 파이프인 ValidationPipe가 직렬화/역직렬화 기능을 함. 그래서 class-transformer가 꼭 필요한 건 아닌 것 같음. (하지만 여전히 다른 유용한 기능도 많이 제공하기 때문에 설치해두면 좋음)npm i class-validator class-transformerDTO 만들기// create-cat.dto.tsimport { IsString, IsInt } from &#39;class-validator&#39;;export class CreateCatDto {  @IsString()  name: string;  @IsInt()  age: number;  @IsString()  breed: string;}// main.tsconst app = await NestFactory.create(AppModule);app.useGlobalPipes(new ValidationPipe())// controller.ts@Post()create(@Body() createCatDto: CreateCatDto) {  ...}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/htmlcss-intro'> <img src='/images/htmlcss_logo.png' alt='[HTML/CSS]: Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/htmlcss-intro'>[HTML/CSS]: Intro</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] HTTP/HTTPS",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_theory-http",
      "date"     : "Dec 3, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-03T21:01:35+09:00'>03 Dec 2023</time><a class='article__image' href='/backend_theory-http'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] HTTP/HTTPS'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_theory-http'>[Backend Thoery] HTTP/HTTPS</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] 구성요소(1) Modules, Controllers",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series3",
      "date"     : "Dec 2, 2023",
      "content"  : "Table of Contents  Module          @Module() 데코레이터      Module 공유      Global Module        Controller          Routing      Request Object                  Path Parameter          Query Parameter                    Response Object      Status Code      Redirect      Module  백엔드에서 대부분의 애플리케이션들은 재사용성과 확장성을 위해 코드를 모듈화  NestJS에서도 @Module() 데코레이터를 이용해 코드를 쉽게 모듈화하도록 해줌  모듈화된 application graph의 시작점이 되는 root module은 항상 존재해야함  (application graph: 모듈의 관계와, 프로바이더의 의존성 관계를 파악하기 위해 사용하는 NestJS의 내부 데이터 구조)@Module() 데코레이터  @Module() 데코레이터는 ModuleMetadata라는 객체를 인자로 받음  ModuleMetadata는 다음과 같은 프로퍼티를 가짐          imports: 이 모듈에서 필요로 하는 프로바이더를 export 하고 있는 모듈들      controllers: 이 모듈에서 정의하고 있는 컨트롤러들      providers: Nest Injector에 의해 초기화(instantiate) 되고, 사용될 프로바이더들      exports: 이 모듈에서 다른 모듈에 제공하는 프로바이더들      ? Guard는 @Injectable() 데코레이터가 붙지만 왜 providers에 추가하지 않아도 괜찮은걸까?Module 공유  A 모듈에서 B 모듈의 BService 프로바이더를 사용하고 싶은 경우// B.module.ts@Module({  controllers: [BController],  providers: [BService],  exports: [BService]})export class BModule {}// A.module.ts@Module({  controllers: [AController],  imports: [BModule]})export class AModule {}Global Module  Global module은, 다른 모듈에서 import 없이도 사용할 수 있음  가능한 Global module은 적게 사용하는 것이 좋음// User 모듈을 글로벌하게 사용하고 싶은 경우// app.module.ts@Module({  imports: [UserModule, ItemModule],  controllers: [AppController],  providers: [AppService],})export class AppModule {}// user.module.ts@Global()@Module({    controllers: [UserController],    providers: [UserService],    exports: [UserService]})export class UserModule {}// Item 모듈에서 글로벌 모듈인 User 모듈을 사용하는 예시// Item 모듈에서 import 없이도 UserService 프로바이더를 사용할 수 있다// item.module.ts@Module({    controllers: [ItemController],    providers: [ItemService]})export class ItemModule {}// item.controller.ts@Controller(&#39;items&#39;)export class ItemController {    constructor(        private itemService: ItemService,        private userService: UserService        ) {}    @Get()    readItem() {        const user = this.userService.readUser()        const item = this.itemService.readItem()        return user + item    }}Controller  요청(Request)을 알맞은 컨트롤러의 알맞은 핸들러로 보내고, 응답(Response)을 유저에게 반환하는 역할  @Controller() 데코레이터를 사용Routing  @Controller() 의 인자로 관련된 엔드포인트들을 그룹화할 수 있음  @Get(), @Post(), ..와 같은 데코레이터의 인자로 각각의 핸들러에 구체적인 엔드포인트를 만들 수 있음// user.controller.ts// http://localhost:3000/users/...@Controller(&#39;users&#39;)export class UserController {    // http://localhost:3000/users/form  @Get(&#39;form&#39;)  readUserForm() {    return &#39;this is form&#39;  }}Request Object  핸들러는 종종 유저가 보낸 요청 객체에 접근할 수 있어야함  포괄적인 방법으로 @Req() 데코레이터로 요청 객체 자체에 접근하는 방법과,  구체적인 방법으로 @Body(), @Param(), @Query(), @Header(), @Session(), @Ip() 등을 사용할 수도 있음  Express의 이점을 얻기 위해서는, 타입을 Request 명시해 주는 것이 좋음 (import { Request } from &#39;express&#39;;)Path Parameter  정적 경로 파라미터와, 동적 경로 파라미터가 있음  요청이 들어오면 코드가 위에서 부터 실행되기 때문에, 구체적인 정적 경로 파라미터를 가지는 엔드포인트가 먼저 오도록 해야함// user.controller.ts@Controller(&#39;users&#39;)export class UserController {  // 정적 경로 파라미터  // http://localhost:3000/users/form  @Get(&#39;form&#39;)  readUserForm() {    return &#39;this is form&#39;  }  // 동적 경로 파라미터  // http://localhost:3000/users/1  // http://localhost:3000/users/2  // ...  @Get(&#39;:id&#39;)  readUser(@Param(&#39;id&#39;) id: number) {    return `user id: ${id}`  }}Query Parameter  key, value 쌍으로 추가 정보를 보낼 수 있음  쿼리 파라미터로는 엔드포인트를 분기할 수 없음  요청 객체에 쿼리 파라미터에 값을 주지 않으면 undefined 가 됨  디폴트 값 줄 수 있음 (@Query(&#39;id&#39;) id: number = 0)  (Parse 로 시작하는 파이프 (ex. ParseIntPipe)를 사용할 경우 디폴트 값을 위와 같이 줄 수 없음. 이 때는 DefaultValuePipe로 디폴트 줘야함)// item.controller.ts@Controller(&#39;items&#39;)export class UserController {  // 아래의 핸들러는 http://localhost/items/1?category=phone URL도 받아들임  @Get(&#39;:id&#39;)  readItem(@Param(&#39;id&#39;) id: number) {    return `item id: ${id}`  }}// item.controller.ts@Controller(&#39;items&#39;)export class UserController {  // 이렇게 두 개의 핸들러를 만들어도   // http://localhost/items/1?category=phone  // http://localhost/items/1  // 이 두 개의 URL 모두 무조건 위에서 먼저 만난 핸들러에서 받아들여짐 -&amp;gt; 쿼리 파라미터는 핸들러 분기의 기준이 안됨  @Get(&#39;:id&#39;)  readItem(@Param(&#39;id&#39;) id: number) {    return `item id: ${id}`  }  @Get(&#39;:id&#39;)  readItem(@Param(&#39;id&#39;) id: number, @Query(&#39;category&#39;) category: string) {    return `item id: ${id}, category: ${category}`  }}@Controller(&#39;items&#39;)export class UserController {  // http://localhost:3000/items?category=television --&amp;gt; { &quot;category&quot;: &quot;television&quot; }  // http://localhost:3000/items --&amp;gt; {}  @Get()  readItem(@Query(&#39;category&#39;) category: string) {    return { category }  }  // http://localhost:3000/items?category=television --&amp;gt; { &quot;category&quot;: &quot;television&quot; }  // http://localhost:3000/items --&amp;gt; { &quot;category&quot;: null }  // 만약 JSON 객체를 리턴할 때, 값이 없더라도 키 값을 할당하고 싶으면 null을 디폴트로,  // 값이 없으면 키 또한 JSON 객체에 할당하고 싶지 않다면 디폴트를 안주는 것이 낫다  @Get()  readItem(@Query(&#39;category&#39;) category: string = null) {    return { category }  }  // http://localhost:3000/items?id=1 --&amp;gt; { &quot;id&quot;: 1 }  // http://localhost:3000/items --&amp;gt; { &quot;message&quot;: &quot;Validation failed&quot;, &quot;error&quot;: &quot;Bad Request&quot;, &quot;statusCode&quot;: 400 }  // Parse * Pipe를 쓸 때는, 디폴트를 다음과 같은 방법으로 줄 수 없다  @Get()  readItem(@Query(&#39;id&#39;, ParseIntPipe) id: number = 0) {    return { id }  }  // http://localhost:3000/items?id=1 --&amp;gt; { &quot;id&quot;: 1 }  // http://localhost:3000/items --&amp;gt; { &quot;id&quot;: 0 }  @Get()  readItem(@Query(&#39;id&#39;, new DefaultValuePipe(0), ParseIntPipe) id: number) {    return { id }  }}Response Object  응답 객체를 다루는 두 가지 방식이 있음: Standard(권장되는 방식)와, Library-specific  Library-specific 방식은 응답 객체에 대한 모든 제어를 직접할 수 있다는 장점이 있음  하지만, Library (Express, Fastify)에 따라 코드가 달라짐  또한, @HttpCode(), @Header(), 포스트 인터셉터(Interceptor)가 제대로 동작하지 않게 됨  (이 부분은 { passthrough: true }를 인자로 넣어주면 해결됨)  (그래서 쿠키를 설정해야 하는 코드는 위의 옵션을 추가하고 Library-specific 방식을 사용하면서 Standard 방식의 이점도 가져감)  (Now you can interact with the native response object (for example, set cookies or headers depending on certain conditions), but leave the rest to the framework)// fully Library-specific 방식@Get()readItem(@Res() response: Response) {  const token = { foo: &quot;bar&quot; }  response.cookie(&#39;token&#39;, JSON.stringify(token))  response.status(HttpStatus.CREATED).send(token)}// Library-specific 방식과 Standard 방식 함께 쓰는 방식@Get()@HttpCode(201)readItem(@Res({ passthrough: true }) response: Response) {  const token = { foo: &quot;bar&quot; }  response.cookie(&#39;token&#39;, JSON.stringify(token))  return true}Status Code  200번대: @HttpCode(201)  300번대: @Redirect(&#39;https://docs.nestjs.com&#39;, 302)  400번대: throw new NotFoundException()  또는 @Res() 쓰는 경우에는, res.status().send() 이렇게 쓰면 됨Redirect  @Redirect(‘리다이렉트할 URL’, 응답 상태 코드)  상태코드가 3xx 여야 리다이렉션이 실제로 일어남 (ex. 200으로 하면 리다이렉션 안됨) (디폴트는 302)  코드 실행도중 에러나면 리다이렉션 안됨  코드 실행도중 예외를 발생시키면 리다이렉션 안됨  return문 실행되지만, 실제로 반환은 되지 않고, 리다이렉션됨  동적으로 리다이렉션 하고 싶은 경우, HttpRedirectResponse 인터페이스를 따르는 객체를 리턴하면 됨 ({ url: &#39;https://docs.nestjs.com/v5/&#39;, statusCode: 301 })// console.log(1)에 1 찍히지만, users로 리다이렉션 하기 때문에 &#39;read user&#39;가 반환됨@Get()@Redirect(&#39;http://localhost:3000/users&#39;)readItem() {  console.log(1)  return &#39;read item&#39;}// NotFoundException 만나기 때문에 리다이렉션 안됨@Get()@Redirect(&#39;http://localhost:3000/users&#39;)readItem() {  throw new NotFoundException()}",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-02T21:01:35+09:00'>02 Dec 2023</time><a class='article__image' href='/nestjs-series3'> <img src='/images/nest_logo.png' alt='[NestJS] 구성요소(1) Modules, Controllers'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series3'>[NestJS] 구성요소(1) Modules, Controllers</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[NestJS] Intro",
      "category" : "backend",
      "tags"     : "NestJS",
      "url"      : "/nestjs-series2",
      "date"     : "Dec 1, 2023",
      "content"  : "Table of Contents  NestJS란  Express와 비교  NestJS 시작하기  NestJS 코드 구조  NestJS Request LifecycleNestJS란  node.js 런타임 위에서 동작하는 백엔드 프레임워크  HTTP 요청과 같은 기본적인 기능은 디폴트로 Express를 사용하고 있음 (Fastify로 교체 가능)Express와 비교  Express를 래핑한 NestJS를 통해 얻게 되는 이점은 다음과 같다          구조화되어 있는 아키텍처      Typescript 친화적      강력한 CLI 제공      OOP 개발의 핵심 개념인 제어의 역전을 위해 필요한 의존성 주입 기능을 제공      NestJS 시작하기      Nest CLI 사용하면 쉽게 템플릿 코드 생성할 수 있음        Nest CLI 설치  npm i -g @nestjs/cli  템플릿 코드 생성nest new .  개발 서버 실행npm run start:devNestJS 코드 구조  NestJS는 코드를 모듈화 함으로써 재사용성을 높임  시작점이 되는 루트 모듈과, 각각의 기능을 분리시켜 놓은 모듈로 이루어짐  NestJS는 크게 다음과 같은 요소들로 이루어짐          Module: 모듈화 하는 역할      Controller: 요청을 받고 응답을 돌려주는 역할      Service: 비즈니스 로직을 수행하는 역할      Repository: DB와 관련된 작업을 수행하는 역할      DTO: 데이터를 검증하고, 변환하는 역할      Middleware: 요청과 관련된 공통적으로 자주 사용되는 작업을 수행하는 역할 (NestJS에서는 Middleware보다는 Guard, Interceptor, Pipe 사용 권장)      Guard: 인증/인가와 관련된 작업을 수행하는 역할      Interceptor: Middleware와 역할이 비슷하지만, 다른 점은 Interceptor는 요청 뿐만 아니라, 응답에 대해서도 가능      Pipe: 데이터가 Controller에게 전달되기 전에 검증/변환하는 역할      Filter: 예외를 처리하는 역할      NestJS Request Lifecycle  NestJS 백엔드 어플리케이션 안에서 요청(Request) 객체는 다음과 같은 순서로 순회한다          Middleware -&amp;gt; Guard -&amp;gt; Interceptor -&amp;gt; Pipe -&amp;gt; Controller -&amp;gt; Service      Guard, Interceptor, Pipe에 대해 글로벌, 컨트롤러, 핸들러 스코프 순서로 순회        Service에서 반환한 응답(Response) 객체는 다음과 같은 순서를 순회하고 유저에게 반환된다          Service -&amp;gt; Controller -&amp;gt; Interceptor -&amp;gt; Filter      Interceptor, Filter에 대해 핸들러, 컨트롤러, 글로벌 스코프 순서로 순회      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-01T21:01:35+09:00'>01 Dec 2023</time><a class='article__image' href='/nestjs-series2'> <img src='/images/nest_logo.png' alt='[NestJS] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/nestjs-series2'>[NestJS] Intro</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Code Architecture] 계층형 아키텍처",
      "category" : "language",
      "tags"     : "Code_Architecture",
      "url"      : "/code_architecture-layer",
      "date"     : "Dec 1, 2023",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-01T21:01:35+09:00'>01 Dec 2023</time><a class='article__image' href='/code_architecture-layer'> <img src='/images/code_architecture_logo.png' alt='[Code Architecture] 계층형 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/code_architecture-layer'>[Code Architecture] 계층형 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Backend Thoery] 백엔드 개요",
      "category" : "backend",
      "tags"     : "backend_theory",
      "url"      : "/backend_thoery-intro",
      "date"     : "Dec 1, 2023",
      "content"  : "Table of Contents  웹 개발이란  백엔드란  백엔드 요소  백엔드 아키텍처  백엔드 코드 아키텍처  백엔드 대표적 기능  백엔드 고려사항  백엔드 프로그래밍 흐름웹 개발이란  프로그래밍 세계에서 웹 서비스를 개발하는 분야를 웹 개발(Web Development)이라고 한다  웹 개발은 크게 UI를 담당하는 프론트엔드(Frontend)와 데이터를 담당하는 백엔드(Backend)로 나뉜다백엔드란  백엔드는 서버에서 실행되는 코드로, 유저로부터 받은 데이터를 저장하고, 유저가 요청한 데이터를 반환하는 역할을 한다  유저의 요청을 받아, 로직을 처리하고 응답으로 적절한 데이터를 반환하는 역할을 한다  또 데이터를 영구적으로 저장하는 데이터베이스도 백엔드 범주에 보통 포함시킨다  유저가 직접 백엔드와 통신하지는 않고, 유저는 프론트엔드와 통신하며, 프론트엔드가 유저 대신 백엔드와 통신한다백엔드 요소  필수요소          요청 라우팅: 유저가 보낸 요청(Request)에 알맞는 응답(Response)을 반환하기 위해 적절한 위치로 분기      데이터 처리: 데이터베이스에서 데이터 읽기/저장/수정/삭제      비즈니스 로직: 구현한 기능을 수행하는데 필요한 비즈니스 로직      예외 처리: 어떠한 상황에도 서버는 하나의 요청에 대해 하나의 응답을 반환해야함        부가요소          캐싱: 자주 사용되는 데이터를 캐싱      API 문서화: 다른 개발자들과의 협업을 위해 API를 문서화      테스트 코드: 안정적인 서비스를 위해 배포하기 전에 다양한 케이스에 대해 테스트      백엔드 아키텍처  API Gateway: 요청을 적절한 API 서버로 전달하는 역할  Load Balancer: 하나의 서버에 과부하가 걸리지 않도록 요청 트래픽을 적절히 배분하는 역할  API Server: 들어온 요청에 대해 적절한 로직을 수행하는 역할  Database: 데이터 영구 저장하는 역할  Cache: 자주 사용되는 데이터를 임시 저장해 빠른 읽기를 지원하는 역할  Queue: 들어온 요청을 비동기적으로 처리하는 역할백엔드 코드 아키텍처  MVC  Clean code  Hexagonal백엔드 대표적 기능  회원가입/로그인  페이지네이션  검색  알림  채팅  국제화  결제  장바구니백엔드 고려사항  확장성(Scalability)  대용량 트래픽백엔드 프로그래밍 흐름  내가 백엔드 프로그래밍할 때 따르는 작성 흐름이다  주관적이기 때문에 참고만 하고 본인 또는 속한 팀원들과 개발하기 편한 방식을 따르면 된다- 라우팅  1. 엔드포인트를 REST API 형태로 작성  2. 요청에 적절한 상태 코드와 응답을 반환하도록 함- 데이터베이스  1. 데이터베이스와의 연결  2. ERD로 엔티티 정의  3. 정의한 엔티티 바탕으로 모델 클래스 정의- 데이터  1. CRUD에 사용할 data validation 구현  2. 데이터 직렬화/역직렬화 구현- 보안  1. 회원가입/로그인/로그아웃 구현  2. 비밀번호 암호화  3. 쿠키 서명- 비즈니스 로직- 에러 처리- 페이지네이션- 유지/보수  1. API 문서화  2. 테스트 코드  3. 로깅- 부가 기능  1. 캐싱  2. 웹소켓  3. SSE  4. 배치 처리",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-12-01T21:01:35+09:00'>01 Dec 2023</time><a class='article__image' href='/backend_thoery-intro'> <img src='/images/backend_theory_logo.png' alt='[Backend Thoery] 백엔드 개요'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/backend_thoery-intro'>[Backend Thoery] 백엔드 개요</a> </h2><p class='article__excerpt'>웹 서비스 그리고 백엔드에 대해서 간단히 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Github Action] Docker Container",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/github_action-docker-container",
      "date"     : "Jan 20, 2023",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-20T21:01:35+09:00'>20 Jan 2023</time><a class='article__image' href='/github_action-docker-container'> <img src='/images/git_logo.png' alt='[Github Action] Docker Container'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action-docker-container'>[Github Action] Docker Container</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Github Action] Environment variables &amp; Secrets",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/github_action-environment_variable-secrets",
      "date"     : "Jan 16, 2023",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-16T21:01:35+09:00'>16 Jan 2023</time><a class='article__image' href='/github_action-environment_variable-secrets'> <img src='/images/git_logo.png' alt='[Github Action] Environment variables &amp; Secrets'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action-environment_variable-secrets'>[Github Action] Environment variables &amp; Secrets</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Github Action] Filter &amp; Activity types",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/github_action-filter-activity_types",
      "date"     : "Jan 14, 2023",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-14T21:01:35+09:00'>14 Jan 2023</time><a class='article__image' href='/github_action-filter-activity_types'> <img src='/images/git_logo.png' alt='[Github Action] Filter &amp; Activity types'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action-filter-activity_types'>[Github Action] Filter &amp; Activity types</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Github Action] Workflow",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/github_action-workflow",
      "date"     : "Jan 13, 2023",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-13T21:01:35+09:00'>13 Jan 2023</time><a class='article__image' href='/github_action-workflow'> <img src='/images/git_logo.png' alt='[Github Action] Workflow'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action-workflow'>[Github Action] Workflow</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Github Action] Intro",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/github_action-intro",
      "date"     : "Jan 13, 2023",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-13T21:01:35+09:00'>13 Jan 2023</time><a class='article__image' href='/github_action-intro'> <img src='/images/git_logo.png' alt='[Github Action] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action-intro'>[Github Action] Intro</a> </h2><p class='article__excerpt'>GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "[Git] 중급",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/git-intermediate",
      "date"     : "Jan 13, 2023",
      "content"  : "Table of Contents  git add -p  git commit –amend  stash  cherry-pick  rebase –onto  깃은 어렵다          원격이란 존재에 브랜치까지 더해지니 엄청 헷갈림      만약 로컬에서 커밋하지 않은 작업 내용이 있는 상황에서 원격을 pull 하라는 요청이 들어온다면?        참고git add -p  파일 한 개 내에서도 라인별로 커밋할지 안할지 결정하고 싶을 때  (기존 git add는 파일별로 커밋여부를 결정했다면 -p 옵션은 더 세분화해서 라인별로 가능하다는 말)git commit –amend  마지막 커밋에 코드를 수정하거나 메시지를 변경하고 싶을 때  stashcherry-pickrebase –onto  B라는 브랜치에서 분기된 C 브랜치를 A 브랜치에 rebase하고 싶은 경우  rebase –onto “브랜치 A” “브랜치 B” “브랜치 C”깃은 어렵다원격이란 존재에 브랜치까지 더해지니 엄청 헷갈림  나의 로컬에 있는 브랜치는 다른 사람들에게 안보임  나의 로컬에 있는 브랜치를 원격 브랜치와 연결해 푸시하면 다른 사람들에게 원격에 있는 브랜치는 보임만약 로컬에서 커밋하지 않은 작업 내용이 있는 상황에서 원격을 pull 하라는 요청이 들어온다면?  작업 내용을 로컬에 저장만 하고, 커밋은 안한 상황이다. 이 때 위에서 현재 원격 코드를 pull하라는 요청이 들어왔다  이 상태에서 pull을 하면 내 로컬의 최근 커밋 + 원격의 최근 커밋간의 병합이 일어날 것이고,  내가 저장만 한 작업 내용들은? 이럴때 stash를 쓰면 되겠지?  내 작업 내용들이 사라지진 않지만 pull 중에 에러가 난다고 한다참고  Happy Git and GitHub for the useR, Pull, but you have local work",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2023-01-13T21:01:35+09:00'>13 Jan 2023</time><a class='article__image' href='/git-intermediate'> <img src='/images/git_logo.png' alt='[Git] 중급'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git-intermediate'>[Git] 중급</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part14]: 네트워크 용어(5): ssh",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series15",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  참고  ssh 설치          apt update      apt install openssh-server (양쪽 다 서버로 설치하면 됨)        접속을 시도할 클라이언트에서 개인키/공용키를 만든다          클라이언트: ssh-keygen -t rsa        만약 접속할 서버의 종류가 점점 늘어난다면 config 파일로 관리하는 것도 좋은 방법이다          클라이언트: vim ~/.ssh/config        Host spark-worker1 (ssh 명령어 뒤에 붙일 호스트 ex. ssh spark-worker1)    HostName worker1 (실제 호스트명)    IdentityFile ~/.ssh/id_rsa (내가 가지고 있는 개인키)                      이제 서버에 .ssh 폴더를 만들고 그 안에 authorized_keys 라는 파일에 공용키를 복사해 붙여넣는다          접속할 서버: mkdir ~/.ssh      접속할 서버: vim authorized_keys      접속할 서버: 클라이언트의 ~/.ssh/id_rsa.pub (공용키) 복사해 붙여넣는다        이제 클라이언트에서 ssh &amp;lt;호스트&amp;gt; 또는 ssh &amp;lt;호스트명&amp;gt;:&amp;lt;포트번호&amp;gt; 로 접속되면 성공  에러가 나면 일단 양쪽 다 .ssh 폴더는 700, authorized_keys 는 600 권한을 맞춰준다  만약 ssh: connect to host 192.168.0.6 port 22: Connection refused 에러가 뜬다면 서버의 22번 포트를 열어줘야 한다          접속할 서버: /etc/ssh/sshd_config 파일에서 Port 22 앞에 #을 제거한다      접속할 서버: netstat -ntl 로 22번 포트 LISTEN 중인지 확인 (설치는 apt update 후 apt install net-tools)      접속할 서버: /etc/init.d/ssh restart 로 ssh 재시작        만약 패스워드를 물어본다면 서버의 PubkeyAuthentication 설정을 바꿔줘야 한다          접속할 서버: /etc/ssh/sshd_config 파일에서 PubkeyAuthentication yes를 추가해 준다      접속할 서버: /etc/init.d/ssh restart 로 ssh 재시작      참고  TAEWAN.KIM ssh config 설정 방법  JAMES YOON, sh 공개키 인증시 Permission denied (publickey,password) 오류 해결  IBM Support, Configuring an SSH login without password",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series15'> <img src='/images/network_logo.png' alt='Network Series [Part14]: 네트워크 용어(5): ssh'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series15'>Network Series [Part14]: 네트워크 용어(5): ssh</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part14]: 네트워크 용어(4): DNS",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series14",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  DNS(Domain Name System)  Main components:          Domain Registrar      Nameserver      DNS Records        How Does DNS Work?  There are 4 DNS servers involved in loading a webpage:  Resolution          Components      Iterative Resolution      Recursive Resolution      Recursive Resolution: Pros/Cons      Responsibility: Recursive vs Iterative        What is DNS caching? Where does DNS caching occur?          Browser DNS caching      Operating system (OS) level DNS caching        DNS 질의 과정과 우선순위          /etc/hosts      /etc/host.conf      /etc/resolv.conf        Domain Name Hierarchy  참고DNS(Domain Name System)  도메인명을 이용해 웹 서버에 접근할 수 있도록 도와주는 서비스  도메인명을 IP주소로 변환해준다  인터넷계의 전화번호부Main components:  Domain Registrar (도메인 등록 대행 업체)  Nameservers (네임 서버)  DNS Records (DNS 레코드)Let’s talk about these components and how they work together.Domain Registrar  도메인명은 도메인 등록 대행 업체로부터 구매한다  도메인 등록 대행 업체는 기본적으로 네임서버를 제공해준다  DNS 레코드는 이러한 등록 대행 업체에 의해 관리된다  다음은 도메인명을 판매하는 도메인 등록 대행 업체의 목록이다- AWS Route53- Google Domains- Cloudflare- Namecheap- GoDaddy- HoverNameserver  DNS 레코드를 저장하고 있는 서버  네임서버는 등록 대행 업체(ex. GoDaddy) 또는 다른 외부 서비스(ex. Cloudflare)가 제공If you’re trying to point your domain name to your web hosting, you’ve probably come across the term nameserver. So, what is a nameserver?Nameservers help connect URLs with the IP address of web servers. Nameservers are an important part of the Domain Name System (DNS), which many people call the “phone book of the Internet”.What Is a Nameserver? Explained in More DetailWhen a user enters a URL in their browser, like “kinsta.com, there needs to be some way to connect that URL with the underlying web server that powers the website at that domain name.Think how difficult it would be if you had to enter the actual IP address of a web server every time you wanted to visit a website. You wouldn’t be able to remember whether it was 159.89.229.118 or 159.89.229.119 — it would be a mess!Nameservers play an important role in connecting a URL with a server IP address in a much more human-friendly way.Nameservers look like any other domain name. When you look at a website’s nameservers, you’ll typically see a minimum of two nameservers (though you can use more). Here’s an example of what they look like:  ns-380.awsdns-47.com  Ns-1076.awsdns-06.orgOnly instead of serving up a website, those nameservers help direct traffic.To illustrate the role that nameservers play in directing traffic on the Internet, let’s look at a real example.Let’s say you want to visit the Kinsta homepage. On the surface, this action is simple: you type “kinsta.com” into your browser’s address bar and you see the Kinsta homepage. Easy, right?But behind-the-scenes, the high-level process actually goes something like this:  You type “kinsta.com” into the address bar and hit enter  Your browser sends a request to that domain’s nameservers  The nameservers respond back with the IP address of the website’s server  Your browser requests the website content from that IP address  Your browser retrieves the content and renders it in your browserIn order to visit a website you must first access a Domain Name Server. If there’s an issue with In the the decentralized naming systems responsible for turning hostnames into IP addresses, you might experience a message such as “DNS server not responding”.Nameservers vs DNS RecordsIn the example above, we left out one point for simplicity:DNS records are what contain the actual information that other browsers or services need to interact with, like your server’s IP address.Nameservers, on the other hand, help store and organize those individual DNS records.Earlier, we referred to DNS as the phone book of the Internet. But a more specific analogy would be that:  Nameservers are the physical phone book itself.  DNS records are the individual entries in the phone book.If you wanted to find someone’s phone number (back when phone books existed!), you’d first grab the phone book itself. Then, you’d open the phone book and go through the entries to find the specific information that you need.Armed with that knowledge, let’s look at a fuller sequence of what happens when you visit a website:  You type “kinsta.com” into the address bar and hit enter  Your browser uses DNS to retrieve the domain’s nameservers  Your browser asks for the A record that contains the IP address of the web server (a specific DNS record)  The nameservers provide the IP address from the A record  Your browser requests the website content from that IP address  Your browser retrieves the content and renders it in your browserHow to Use Nameservers in the Real WorldIn the real world, you’ll use nameservers and DNS records primarily to point your domain name towards your hosting.You also might use the DNS records supplied by your nameservers in other ways, like setting up your email account with MX records or verifying your domain name with Google Search Console.Where Are Your Domain’s Nameservers Located?The answer to this question is that “it depends”.When you register your domain name through a domain registrar, your domain is usually pointed towards your domain registrar’s nameservers at first. Your domain registrar is also where you can edit your domain’s nameservers.If you wanted to, you could leave your nameservers at your domain registrar and just edit the DNS records to point your domain name towards your web hosting.However, many web hosts recommend that you change your domain’s nameservers to nameservers provided by the host. For example, here at Kinsta, we provide premium nameservers powered by Amazon Route 53 that you can use (though you don’t have to):To change your nameservers, you’ll need to use the interface at the domain registrar where you purchased your domain name.For example, here’s what it looks like to change the nameservers at a domain registered through Google Domains.You can see that the domain is originally configured to use the Google Domains nameservers:DNS Records  네임서버에 저장되어 있는 데이터  zone files라고도 불림  변환 요청을 처리하기 위해 필요한 정보를 가지고 있음  DNS 서버는 DNS 레코드를 보고 요청을 처리함  보통 DNS 레코드는 1~4시간마다 업데이트가 일어남 이 값을 TTL이라고 함여러 종류의 DNS 레코드가 있지만 아래의 4 가지 정도가 가장 많이 사용된다  A records: Used to point a domain or a subdomain at an IPv4 address. This is the rule used to point a domain like example.com to the web server where the example.com website lives. (Note: If a web server uses and IPv6 address rather than an IPv4 address, then an AAAA record is used rather than an A record).  CNAME records: Used to associate a subdomain to the primary or canonical domain. This type of rule is commonly used to associate a www subdomain with the primary domain, such as www.example.com with example.com.  MX records: Used to associate a domain with an email service. This is the type of rule used if you want mail for example.com to be delivered to a specific email service such as Gmail.  TXT records: Used to associate any arbitrary text to a domain. Most commonly, TXT records are used to associate SPF records with a domain to improve email deliverability and protect against spammers misusing the domain name when sending out spam. Check out our in-depth blog post on email authentication and why it’s important.How Does DNS Work?    DNS는 요청받은 도메인 명을 IP주소로 변환시켜주는데 이를 DNS resolution이라고 함  DNS resolution은 IP주소를 알아내기 위해 아래의 4가지 종류의 DNS 서버를 필요로함There are 4 DNS servers involved in loading a webpage:  DNS recursor (DNS resolver): DNS 리졸버는 클라이언트로부터 요청을 받고 받은 요청을 적절한 네임서버로 전달한다  Root nameserver: 루트 네임서버는 resolving을 위한 첫 단계로, 어떤 TLD 네임 서버로 가야 하는지 알려준다  TLD nameserver: Top-level 도메인 서버는 다음으로 특정 호스트명을 가지는 네임서버를 안내한다  Authoritative nameserver: 요청의 마지막 종착지로, 도메인명에 해당하는 정보가 있으면 IP 주소를 돌려준다Note: Often DNS lookup information will be cached either locally inside the querying computer orremotely in the DNS infrastructure. There are typically 8 steps in a DNS lookup. When DNS information is cached, steps are skipped from the DNS lookup process which makes it quicker. The example below outlines all 8 steps when nothing is cached.  The 8 steps in a DNS lookup:  A user types ‘example.com’ into a web browser and the query travels into the Internet and is received by a DNS recursive resolver.  The resolver then queries a DNS root nameserver (.).  The root server then responds to the resolver with the address of a Top Level Domain (TLD) DNS server (such as .com or .net), which stores the information for its domains. When searching for example.com, our request is pointed toward the .com TLD.  The resolver then makes a request to the .com TLD.  The TLD server then responds with the IP address of the domain’s nameserver, example.com.  Lastly, the recursive resolver sends a query to the domain’s nameserver.  The IP address for example.com is then returned to the resolver from the nameserver.  The DNS resolver then responds to the web browser with the IP address of the domain requested initially.Once the 8 steps of the DNS lookup have returned the IP address for example.com, the browser is able to make the request for the web page:  The browser makes a HTTP request to the IP address.  The server at that IP returns the webpage to be rendered in the browser (step 10).ResolutionResolution is the process of asking for the resource records of a fully-qualified domain name (FQDN) and receiving back an answer. Every time that your computer does not have an IP address cached for a required FQDN, a resolution takes place. In this post, I discuss the main components involved in DNS resolution and explain the two main methods in which resolution is performed.ComponentsThere are five main components that play a role in DNS resolution.The first component is the client. This is the host that is asking the question, “Where is www.netflix.com on the internet?”The second component is the DNS resolver. Typically provided by your ISP, this serves as the first component that the client reaches out to if the answer to the DNS query is not cached by the client. Its role is to query the other components to find the answer to the original question. The way it does this depends on the type of DNS resolution being performed.Clients can configure their settings to use a DNS resolver not provided by their ISP. Google, Cloudflare, Verisign, and Cisco are just a handful of companies that offer free-alternative DNS resolvers. Be aware of which resolver you choose though! Every site you visit will likely send a DNS query which is handled by your chosen DNS resolver. This gives that resolver an ability to see what you request and may sell this data to advertisers. Always read the policies of DNS resolvers that you are considering to use.The third component is the DNS Root Zone, which is questioned if the DNS resolver does not have the answer in cache. Its role is to return the nameservers for the requested TLD. There are 13 root servers in the world operated by 12 organizations. These servers are anycasted and I go into more detail about them in my DNS Architecture post.The fourth component is the TLD’s Nameservers. Its role is to return the authoritative nameservers of the requested second-level domain.Finally, the fifth component is the Authoritative Nameservers. These servers are the responsibility of the registrant to provide, and their role is to return the resource record for the requested third-level domain (or apex domain).Iterative ResolutionThere are two types of resolution, the first is iterative. In an iterative resolution, it is the responsibility of the DNS resolver to keep querying nameservers until it gets an answer.A flowchart describing iterative resolution.Let’s go through each step in more detail.  The client sends an iterative DNS query for www.blakes.site..  The DNS resolver receives this query. If it doesn’t have an answer for this query already cached, it will continue by asking a root server where the nameservers for .site are. If it is cached, the answer will be returned here and the process will terminate. Sidenote: The DNS resolver could also store cache entries for the .site TLD nameservers and the blakes.site authoritative nameservers and skip the appropriate steps.  The root server returns the IP addresses for the .site nameservers. It also can cache the .site nameservers for future usage.  The DNS resolver now has to ask the .site TLD nameservers for the IP addresses of the blakes.site authoritative nameservers. It also can cache the .site nameservers for future usage.  The .site TLD nameservers return the IP addresses for the blakes.site authoritative nameservers. The DNS resolver can cache the blakes.site authoritative nameservers for future usage.  The DNS resolver asks the blakes.site authoritative nameservers for the resource records for the entry www.  The blakes.site authoritative nameservers return the resource records for the entry www.  The DNS resolver caches the response and returns it back to the client.Recursive ResolutionThe alternative to iterative resolution is recursive resolution. Instead of an address to the next nameserver being sent back to the DNS resolver to then query, the nameserver makes the request itself and returns the result all the way back up to the DNS resolver.A flowchart describing recursive resolution.Let’s also go through this resolution step-by-step.  The client sends a recursive DNS query for www.blakes.site.. Nothing new.  The DNS resolver receives this query. If it doesn’t have an answer for this query already cached, it will continue by asking a root server for the answer to www.blakes.site.. If it is cached, the answer will be returned here and the process will terminate.  If the root server did not have an answer cached, then it asks the next component that could have an answer: the TLD nameservers. The root servers can also cache the TLD nameservers for the requested domain for future use.  If the TLD nameservers did not have an answer cached, then it asks the next component that could have an answer: the authoritative nameservers. The TLD nameservers can also cache the authoritative nameservers for the requested domain for future use.  The authoritative nameservers find an answer for www.blakes.site. and pass the answer up back to the TLD nameservers.  The TLD nameservers pass the answer back up to the root server.  The root server passes the answer back to the DNS resolver.  The DNS resolver caches and passes the answer back to the client.There is caching at each component, so it is possible that only a partial resolution has to take place for a query. If the requested FQDN is popular and the DNS resolver is being used by a lot of people, then it is completely possible that the root servers are never contacted.Recursive Resolution: Pros/ConsIn general, recursive resolution tends to be faster than its iterative counterpart due to caching of final answers. However, this type of resolution creates security flaws including cache poisoning and DNS amplification attacks.Responsibility: Recursive vs IterativeIn recursive resolution, the burden of having to contact nameservers belongs to the server. On the flip side, for iterative resolution, the burden of contacting nameservers belongs to the client.What is DNS caching? Where does DNS caching occur?  도메인명-IP주소를 간단히 매핑하여 저장함으로써 리졸빙을 빠르게 할 수 있다Browser DNS caching  브라우저에 매핑 관계를 저장해둔다  브라우저에 도메인명을 입력하면 브라우저는 가장 처음으로 브라우저에 캐시된 매핑 관계를 확인한다In Chrome, you can see the status of your DNS cache by going to chrome://net-internals/#dns.Operating system (OS) level DNS caching  요청을 ISP가 제공해주는 리졸버에게 전달하기 전에 마지막으로 들리는 로컬 환경이다DNS 질의 과정과 우선순위/etc/hostsDNS가 없던 아주 옛날에는 모든 서버의 /etc/hosts 파일에는 아래와 같은 형식으로 domain과 IP주소의 짝을 직접 등록하여 도메인에 대한 IP주소를 찾아가도록 하였다. DNS서버를 운영할때 기본값으로 /etc/hosts 파일을 먼저 읽어 들인다. 요청받은 도메인이 이 곳에 등록되어 있다면 DNS요청을 네임서버에 보내지않고 이 곳에 등록되어 있는 주소로 연결이 된다. 그리하여 일종의 트릭으로 원하는 IP와 도메인명을 등록하고 사용할 수도 있다.&amp;lt;IP주소&amp;gt;    &amp;lt;hostname&amp;gt;127.0.0.1  localhost/etc/host.conf이 파일은 어떤 특정 도메인에 대해 IP주소값을 찾을 때, 주소 값을 어디에서 찾을 것인가를 결정하는 파일이다.hosts : /etc/hosts파일을 말한다.bind : DNS를 말한다. 즉, /etc/resolv.conf에 정의된 nameserver를 의미한다.nis : NIS에 의한 도메인 쿼리를 말한다. 위의 내용으로 예를 들면 어떤 PC로 부터 자신(DNS서버)에게 도메인 주소를 IP로 알려달라는 질의 요청이 왔다. 맨 처음은 /etc/hosts 파일에서 찾아본 후, 없으면 /etc/resolv.conf파일에 정의된 nameserver에게 쿼리하는 순서이다. 즉, 도메인 네임 서비스를 어디서 받을 것인가를 정의해 놓은 파일이라는 것이다./etc/resolv.conf이 파일은 사용하고자 하는 네임서버를 지정하는 파일이다.구성을 보도록 하자.search는 호스트+도메인으로 사용할 도메인 명을 지정해둔 것이다. 거의 모두 호스트명과 도메인 명을 함께 사용한다. 하지만 특별하게 호스트명만 사용되었을 때 사용하게될 기본 도메인명을 의미하는 것이다.예를 들어 search abc.com 이라고 가정하자. 그럼 “telnet 호스트명”과 같이 “telnet www”라고 하였을 경우에 자동으로 “telnet www.abc.com”으로 인식하는 것이다. nameserver는 말 그대로 이 서버에서 사용할 네임서버를 지정해둔 것이다.DNS 처리과정 우선순위 1. Local 환경 쿼리 (자기 자신)2. 내부 네트워크 쿼리3. 외부 공인 DNS 쿼리Domain Name HierarchyAlternatively referred to as a namespace, a domain namespace is a name service provided by the Internet for Transmission Control Protocol networks/Internet Protocol (TCP/IP). DNS is broken up into domains, a logical organization of computers that exist in a larger network. Below is an example of the hierarchy of domain naming on the Internet.In the example above, all websites are broken into regional sections based on the TLD (top-level domain). With http://support.computerhope.com, it has a “.com” TLD, “computerhope” as its second level domain (local to the .com TLD), and “support” as its subdomain, which is determined by its server.참고  How To Set Permanent DNS Nameservers in Ubuntu and Debian  kinsta, What Is DNS? Domain Name System Explained  kinsta, What Is a Nameserver? Why Are Nameservers Important?  CLOUDFLARE, What is DNS?  IT_Dexter:티스토리  Whackur’s Blog:티스토리  hozero.log, [SAA] 섹션 10: Route 53  Blake Khan, DNS Explained. Hierarchy and Architecture",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series14'> <img src='/images/network_logo.png' alt='Network Series [Part14]: 네트워크 용어(4): DNS'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series14'>Network Series [Part14]: 네트워크 용어(4): DNS</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part13]: 네트워크 용어(3): IPC, RPC",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series13",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  IPC          Use Cases      공유 메모리      파일      파이프      메세지 큐      소켓        RPC          When the client calls the server, the RPC system must take care of:      The following steps take place during a RPC      RPC의 장점      RPC의 단점      RPC의 대표적인 구현체        REST (REpresentational State Transfer)  gRPC          HTTP 2.0      Protocol Buffer      gRPC 이점        참고IPC  Inter-Process Communication의 줄임말 -&amp;gt; 프로세스간 통신을 의미  프로세스끼리 서로 데이터를 주고 받는 방법들을 일컫어 IPC라고 함  스레드끼리는 메모리를 공유한다 -&amp;gt; 공유하는 공간이 있기 때문에 데이터를 주고 받는데 크게 어려움이 없다  프로세스는 별도의 메모리에 생성된다 -&amp;gt; 공유하는 공간이 없다 -&amp;gt; 이를 위해 운영체제 커널에서 IPC를 위한 여러 도구를 제공  공유 메모리, 파일, 파이프, 소켓, 메세지 큐, RPCUse CasesThere are several good reasons and common use cases for IPC:  Sharing information/data: 프로세스간 데이터 동기화  Computational Speedups: 데이터 처리를 다른 쪽에 넘김  Modularity: 같은 응용 프로그램을 여러 프로세스로 띄울 때 프로세스간 데이터 공유 (ex. 크롬 브라우저의 탭)  Development: 하나의 서비스안에 있는 여러 어플리케이션을 독립적으로 개발하고, 필요한 데이터를 IPC 통신을 통해 공유공유 메모리  Shared memory IPCs refer to sharing a physical memory location where multiple processes read and write to. The processes do this by mapping local memory to the shared physical memory location (via pointers or some other method). The physical memory is then used throughout the processes life spawn, meaning some processes can finish and close, but the physical memory remains until it is released (and all processes detach).  It is possible (and advised) to address the issue of mutual exclusion by building a mutex that spans processes. Programers must recognize this and program accordingly.  프로세스간 read, write를 모두 필요로 할 때 사용  중개자 없이 메모리에 바로 접근 가능 -&amp;gt; 모든 IPC 방법 중에 가장 빠름  공유 메모리 모델의 장점          커널의 관여 없이 메모리를 직접 사용하여 IPC 속도가 빠르다.      프로그램 레벨에서 통신 기능을 제공하여, 자유로운 통신이 가능하다.        공유 메모리 모델의 단점          구현하기 어렵다는 단점이 있다.        컨텍스트 스위칭 관점          공유 메모리 모델에서의 IPC는 해당 프로세스가 CPU를 사용하는 행위이다.      즉, IPC를 많이 한다고 컨텍스트 스위칭 많이 일어나지 않는다.        동기화 관점          메모리 영역에 대한 동시적인 접근을 제어하기 위한 방법이 필요하다.      커널이 동기화를 제공하지 않으며, 부가적인 방법이 필요하다.      접근 제어 방식은 locking이나 세마포어(semaphore) 등이 있다.        공유 메모리 모델의 활용의 예 : 데이터베이스파일  파일을 이용한 통신은 부모-자식 프로세스 간 통신에 많이 사용  운영체제가 별다른 동기화를 제공하지 않음  동기화를 위해 주로 부모 프로세스가 wait() 함수를 이용하여 자식 프로세스의 작업이 끝날 때까지 기다렸다가 작업을 시작파이프  Pipes are relatively straight forward. Instead of using shared/mapped memory to share data among processes, instead we “pipe” data across processes with the help of the operating system. Although this does not mean that less memory is used necessarily (by the system), but it does remove the programmers requirement to manage it. The major advantage they have over the shared or mapped memory IPCs is that the programmer does not have to worry about does not have to worry about mutual exclusion for read/writing, the operating system handles it all.  익명 파이프 (Anonymous Pipe)          기본 파이프      부모-자식, 형제 프로세스간 통신에 사용      기본적으로 하나의 프로세스는 read, 다른 하나는 write만 가능한 단방향 통신 -&amp;gt; 양방향 통신을 위해서는 2개의 파이프 필요      파이프를 사용하는 프로세스가 없으면 자동으로 제거됨        네임드 파이프 (Named Pipe)          부모-자식, 형제 프로세스가 아닌 서로 무관한 프로세스간 통신에도 사용 가능      FIFO라 불리는 특수한 파일 사용      네임드 파이프로 양방향 통신을 위해서는 2개의 파이프 필요      파이프를 사용하는 프로세스가 없어도 제거되지 않고 남아있음      메세지 큐  파이프와 비슷하게 단방향 통신  파이프와 다른점은 메세지 큐는 메모리를 사용  다수의 프로세스간 데이터 전달 가능  메시지 큐를 사용하기 위해서는, “메시지 큐 ID”를 알아야 함 (cf.소켓은 상대방 프로세스의 “포트 번호”만 알면 가능)위에서 살펴본 파이프, 소켓, 메세지 큐와 같은 방식을 메세지 전달 모델이라고 한다.  메시지 전달 모델의 장점          구현하기에 간단하여 사용하기 편리하다.        메시지 전달 모델의 단점          커널을 경유하므로, 속도가 느리다.        컨텍스트 스위칭 관점          메시지 전달 모델에서의 IPC는 해당 프로세스 입장에서 일종의 입출력(I/O)로 볼 수 있다.      즉, IPC를 하면 할수록 컨텍스트 스위칭이 많이 일어난다. =&amp;gt; 예를 들어, send하고 상대방이 받을 때까지 기다려야 하며, 이 때 컨텍스트 스위칭이 발생한다. =&amp;gt; 마찬가지로, receive하면 상대방이 보낼 때까지 기다려야 하며, 이 때 컨텍스트 스위칭이 발생한다.        동기화 관점          send와 receive와 같은 연산에 대해서는 커널이 동기화를 제공한다.      send와 receive를 수행할 때에 프로그램은 동기화에 대한 고려 없이 사용할 수 있다.        메시지 전달 모델 활용의 예 : 서버-클라이언트 방식의 통신  메시지 전달 모델의 구현 IPC : PIPE, Message Queue, Socket소켓  sockets are similar to pipes, but capable of network connections (i.e. communication across computers). The data then travels through the network to another computer and to the desired process. You can also use sockets locally by using the “localhost” to essentially loop-back to the another process on the system.  소켓은 두 프로세스간 통신을 위해 제공되는 엔드포인트  원격에 있는 프로세스간 통신을 제공  양방향 통신이 가능  서버/클라이언트 구조의 프로세스간 통신에 자주 사용소켓은 대부분의 언어에서 API 형태로 제공하는 편리함 때문에 지금도 많이 사용되고 있지만, 일련의 통신 과정을 직접 구현하므로 통신 관련 장애를 처리하는 것은 고스란히 개발자의 몫이 됩니다. 서비스가 고도화될 수록 수백 수천가지 데이터가 돌아다니게 될텐데, 이에 따라 data formatting 을 하는 것도 점점 어려워지게 되죠이런 소켓의 한계에서 RPC(Remote Procedure Call)라는 기술이 등장RPC  Remote Procedure Call is a technique for building distributed systems. Basically, it allows a program on one machine to call a subroutine on another machine without knowing that it is remote  IPC의 한 종류  분산 시스템에서 많이 사용하는 방식  원격에 있는 프로세스와의 통신을 마치 로컬에 있는 프로시저를 실행하듯 사용  (프로시저(procedure)는 서브루틴(subroutine)이 될 수도 있고, 서비스가 될 수도 있고, 함수가 될 수도 있다)  MSA(Micro Service Architecture)패턴으로 서비스를 개발하는 환경에서 유용하게 활용됨  RPC는 Java, Python, Go와 같은 다양한 언어로 구현할 수 있음  RPC는 프로세스간 통신 과정을 추상화  네트워크 장비, 프로토콜, 운영체제에 무관하게 개발 가능RPC: 네트워크로 연결된 서버 상의 프로시저(함수, 메서드 등)를 원격으로 호출할 수 있는 기능네트워크 통신을 위한 작업 하나하나 챙기기 귀찮으니. 통신이나 call 방식에 신경쓰지 않고 원격지의 자원을 내 것처럼 사용할 수 있죠. IDL(Interface Definication Language) 기반으로 다양한 언어를 가진 환경에서도 쉽게 확장이 가능하며, 인터페이스 협업에도 용이하다는 장점이 있습니다.RPC의 핵심 개념은 ‘Stub(스텁)’이라는 것인데요. 서버와 클라이언트는 서로 다른 주소 공간을 사용 하므로, 함수 호출에 사용된 매개 변수를 꼭 변환해줘야 합니다. 안그러면 메모리 매개 변수에 대한 포인터가 다른 데이터를 가리키게 될 테니까요. 이 변환을 담당하는게 스텁입니다.client stub은 함수 호출에 사용된 파라미터의 변환(Marshalling, 마샬링) 및 함수 실행 후 서버에서 전달 된 결과의 변환을, server stub은 클라이언트가 전달한 매개 변수의 역변환(Unmarshalling, 언마샬링) 및 함수 실행 결과 변환을 담당하게 됩니다. 이런 Stub을 이용한 기본적인 RPC 통신 과정을 살펴보겠습니다.  IDL(Interface Definition Language)을 사용하여 호출 규약 정의합니다.          함수명, 인자, 반환값에 대한 데이터형이 정의된 IDL 파일을 rpcgen으로 컴파일하면 stub code가 자동으로 생성됩니다.        Stub Code에 명시된 함수는 원시코드의 형태로, 상세 기능은 server에서 구현됩니다.          만들어진 stub 코드는 클라이언트/서버에 함께 빌드합니다.        client에서 stub 에 정의된 함수를 사용할 때,  client stub은 RPC runtime을 통해 함수 호출하고  server는 수신된 procedure 호출에 대한 처리 후 결과 값을 반환합니다.  최종적으로 Client는 Server의 결과 값을 반환받고, 함수를 Local에 있는 것 처럼 사용할 수 있습니다.구현의 어려움/지원 기능의 한계 등으로 제대로 활용되지 못했습니다. 그렇게 RPC 프로젝트도 점차 뒷길로 가게되며 데이터 통신을 우리에게 익숙한 Web을 활용해보려는 시도로 이어졌고, 이 자리를 REST가 차지하게됩니다.When the client calls the server, the RPC system must take care of:  Taking all the parameters which are passed to the subroutine and transferring them to the remote node;  Having the subroutine executed on the remote node; and  Transferring back all the parameters which are returned to the calling routine.The most common method of doing this is by the use of stub modules. The client program is linked to a client stub module. This is a subroutine which looks (from the outside) in every respect like the remote subroutine. On the inside, it is almost empty: all it does is take the values of the parameters which are passed to it, and put them in a message. This is known as marshalling.The client stub then uses a routine in the RPC Run-Time System (RTS) to send the message off and wait for a reply message. When the reply arrives, the stub unmarshals the parameters that were returned in the reply message, putting their values into the variables of the calling program. The client stub then returns to the calling program just like a normal subroutine.The following steps take place during a RPC  A client invokes a client stub procedure, passing parameters in the usual way. The client stub resides within the client’s own address space.  The client stub marshalls(pack) the parameters into a message. Marshalling includes converting the representation of the parameters into a standard format, and copying each parameter into the message.  The client stub passes the message to the transport layer, which sends it to the remote server machine.  On the server, the transport layer passes the message to a server stub, which demarshalls(unpack) the parameters and calls the desired server routine using the regular procedure call mechanism.  When the server procedure completes, it returns to the server stub (e.g., via a normal procedure call return), which marshalls the return values into a message. The server stub then hands the message to the transport layer.  The transport layer sends the result message back to the client transport layer, which hands the message back to the client stub.  The client stub demarshalls the return parameters and execution returns to the caller.RPC의 장점  비즈니스 로직에 집중할 수 있음.  다양한 언어를 가진 환경에서 쉽게 확장할 수 있음.  쉽게 인터페이스 협업이 가능함.RPC의 단점  새로운 학습 비용이 듬.  사람의 눈으로 읽기 힘듬.RPC의 대표적인 구현체  Google의 ProtocolBuffer  Facebook의 Thrift  Twitter의 FinalgeREST (REpresentational State Transfer)REST는 HTTP/1.1 기반으로 URI를 통해 모든 자원(Resource)을 명시하고 HTTP Method를 통해 처리하는 아키텍쳐 입니다. 자원 그 자체를 표현하기에 직관적이고, HTTP를 그대로 계승하였기에 별도 작업 없이도 쉽게 사용할 수 있다는 장점으로 현대에 매우 보편화되어있죠. 하지만 REST에도 한계는 존재합니다. REST는 일종의 스타일이지 표준이 아니기 때문에 parameter와 응답 값이 명시적이지 않아요. 또한 HTTP 메소드의 형태가 제한적이기 때문에 세부 기능 구현에는 제약이 있습니다.덧붙여, 웹 데이터 전달 format으로 xml, json을 많이 사용하는데요.XML은 html과 같이 tag 기반이지만 미리 정의된 태그가 없어(no pre-defined tags) 높은 확장성을 인정 받아 이기종간 데이터 전송의 표준이었으나, 다소 복잡하고 비효율적인 데이터 구조탓에 속도가 느리다는 단점이 있었습니다. 이런 효율 문제를 JSON이 간결한 Key-Value 구조 기반으로 해결하는 듯 하였으나, 제공되는 자료형의 한계로 파싱 후 추가 형변환이 필요한 경우가 많아졌습니다. 또한 두 타입 모두 string 기반이라 사람이 읽기 편하다는 장점은 있으나, 바꿔 말하면 데이터 전송 및 처리를 위해선 별도의 Serialization이 필요하다는 것을 의미합니다.gRPCgRPC는 google 사에서 개발한 오픈소스 RPC(Remote Procedure Call) 프레임워크입니다. 이전까지는 RPC 기능은 지원하지 않고, 메세지(JSON 등)을 Serialize할 수 있는 프레임워크인 PB(Protocol Buffer, 프로토콜 버퍼)만을 제공해왔는데, PB 기반 Serizlaizer에 HTTP/2를 결합하여 RPC 프레임워크를 탄생시킨 것이죠.REST와 비교했을 때 기반 기술이 다르기에 특징도 많이 다르지만, 가장 두드러진 차이점은 HTTP/2를 사용한다는 것과 프로토콜 버퍼로 데이터를 전달한다는 점입니다. 그렇기에 Proto File만 배포하면 환경과 프로그램 언어에 구애받지 않고 서로 간의 데이터 통신이 가능합니다.Monolithic 구조에서는 하나의 프로그램으로 동작하기 때문에 그 안에서 구조적인 2개의 서비스간의 데이터는 공유 메모리를 통해서 주고받을 수 있습니다. 따라서 이 경우 서비스간 메시지 전송 성능은 큰 이슈가 되지 않습니다. 반면 MSA에서는 여러 모듈로 분리되어있고 동일 머신에 존재하지 않을 수 있습니다. 따라서 일반적으로는 보편화된 방식인 REST 통신을 통해 메시지를 주고 받습니다. 문제는 Frontend 요청에 대한 응답을 만들어내기 위해 여러 마이크로 서비스간의 협력이 필요하다면, 구간별 REST 통신에 따른 비효율로 인해 응답속도가 저하된다는 점입니다.HTTP 2.0http/1.1은 기본적으로 클라이언트의 요청이 올때만 서버가 응답을 하는 구조로 매 요청마다 connection을 생성해야만 합니다. cookie 등 많은 메타 정보들을 저장하는 무거운 header가 요청마다 중복 전달되어 비효율적이고 느린 속도를 보여주었습니다. 이에 http/2에서는 한 connection으로 동시에 여러 개 메시지를 주고 받으며, header를 압축하여 중복 제거 후 전달하기에 version1에 비해 훨씬 효율적입니다. 또한, 필요 시 클라이언트 요청 없이도 서버가 리소스를 전달할 수도 있기 때문에 클라이언트 요청을 최소화 할 수 있습니다.gRPC는 HTTP 2.0 기반위에서 동작하기 때문에 지금까지 HTTP 2.0의 특징에 대해서 살펴봤습니다. 짧게 정리하자면, Header 압축, Multiplexed Stream 처리 지원 등으로 인해 네트워크 비용을 많이 감소시켰습니다. 그렇다면 HTTP 2.0 특징을 제외한 gRPC만의 특징은 무엇이 있을까요?먼저 REST API 통신의 문제점에 대해서 먼저 살펴본 다음 gRPC의 특징에 대해서 살펴보도록 하겠습니다. REST 구조에서는 JSON 형태로 데이터를 주고 받습니다. JSON은 데이터 구조를 쉽게 표현할 수 있으며, 사람이 읽기 좋은 표현 방식입니다. 하지만 사람이 읽기 좋은 방식이라는 의미는 머신 입장에서는 자신이 읽을 수 있는 형태로 변환이 필요하다는 것을 의미합니다. 따라서 Client와 Server간의 데이터 송수신간에 JSON 형태로 Serialization 그리고 Deserialization 과정이 수반되어야합니다. JSON 변환은 컴퓨터 CPU 및 메모리 리소스를 소모하므로 수많은 데이터를 빠르게 처리하는 과정에서는 효율이 떨어질 수 밖에 없습니다.두 번째 이슈는 JSON 구조는 값은 String으로 표현됩니다. 따라서 사전에 타입 제약 조건에 대한 명확한 합의가 없거나 문서를 보고 개발자가 인지하지 못한다면, Server에 전달전에 이를 검증할 수 없습니다. 가령 위 예시와 같이 Server에서 zipCode는 숫자 타입으로 처리되어야하지만 Client에서는 이에 대한 제약 없이 문자열을 포함시켜 전달할 수 있음을 의미합니다.그렇다면 gRPC 기술은 위 두 가지 이슈를 어떻게 풀어내었을까요?Client에서 Server측의 API를 호출하기 위해서 기존에는 어떤 Endpoint로 호출해야할 지 그리고 전달 Spec에 대해서 API 문서 작성 혹은 Client와 Server 개발자간의 커뮤니케이션을 통해 정의해야했습니다. 그리고 이는 별도의 문서 생성이나 커뮤니케이션 비용이 추가로 발생합니다.이러한 문제를 감소시키기 위해 다양한 방법이 존재합니다. 그 중 한가지는 Server의 기능을 사용할 수 있는 전용 Library를 Client에게 제공하는 것입니다. 그러면 Client는 해당 Library에서 제공하는 Util 메소드를 활용해서 호출하면 내부적으로는 Server와 통신하여 올바른 결과를 제공할 수 있습니다. 또한 해당 방법은 Server에서 요구하는 Spec에 부합되는 데이터만 보낼 수 있게 강제화 할 수 있다는 측면에서 스키마에 대한 제약을 가할 수 있습니다.gRPC에서는 위 그림과 같이 이와 유사한 형태인 Stub 클래스를 Client에게 제공하여 Client는 Stub을 통해서만 gRPC 서버와 통신을 수행하도록 강제화 했습니다.그렇다면 Stub 클래스는 무엇이고 위 그림에서 보이는 Proto는 무엇일까요?Protocol BufferProtocol Buffer는 google 사에서 개발한 구조화된 데이터를 직렬화(Serialization)하는 기법입니다.직렬화란, 데이터 표현을 바이트 단위로 변환하는 작업을 의미합니다. 아래 예제처럼 같은 정보를 저장해도 text 기반인 json인 경우 82 byte가 소요되는데 반해, 직렬화 된 protocol buffer는 필드 번호, 필드 유형 등을 1byte로 받아서 식별하고, 주어진 length 만큼만 읽도록 하여 단지 33 byte만 필요하게 됩니다.Protocol Buffer는 Google이 공개한 데이터 구조로써, 특정 언어 혹은 특정 플랫폼에 종속적이지 않은 데이터 표현 방식입니다. 하지만 Protocol Buffer는 특정 언어에 속하지 않으므로 Java나 Kotlin, Golang 언어에서 직접적으로 사용할 수 없습니다.따라서 Protocol Buffer를 언어에서 독립적으로 활용하기 위해서는 이를 기반으로 Client 혹은 Server에서 사용할 수 있는 Stub 클래스를 생성해야합니다. 이때 protoc 프로그램을 활용해서 다양한 언어에서 사용할 수 있는 Stub 클래스를 자동 생성할 수 있습니다.Stub 클래스를 생성하면, 해당 클래스 정보를 Server와 Client에 공유한 다음 Stub 클래스를 활용하여 서로 양방향 통신을 수행할 수 있습니다.지금까지 학습한 Protocol Buffer 내용을 정리하면 다음과 같은 장점을 지닌 것을 확인할 수 있습니다.  스키마 타입 제약이 가능하다  Protocol buffer가 API 문서를 대체할 수 있다.위 두가지 특징은 이전에 REST에서 다룬 이슈 중 하나인 API Spec 정의 및 문서 표준화 부재의 문제를 어느정도 해소해줄 수 있습니다. 그렇다면 또 하나의 이슈인 JSON Payload 비효율 문제와 대비하여 gRPC는 어떠한 이점을 지니고 있을까요?JSON 타입은 위와같이 사람이 읽기는 좋지만 데이터 전송 비용이 높으며, 해당 데이터 구조로 Serialization, Deserialization 하는 비용이 높음을 앞서 지적했습니다.gRPC의 통신에서는 데이터를 송수신할 때 Binary로 데이터를 encoding 해서 보내고 이를 decoding 해서 매핑합니다. 따라서 JSON에 비해 payload 크기가 상당히 적습니다.또한 JSON에서는 필드에 값을 입력하지 않아도 구조상에 해당 필드가 포함되어야하기 때문에 크기가 커집니다.  반면 gRPC에서는 입력된 값에 대해서만 Binary 데이터에 포함시키기 때문에 압축 효율이 JSON에 비해 상당히 좋습니다.결론적으로 이러한 적은 데이터 크기 및 Serialization, Deserialization 과정의 적은 비용은 대규모 트래픽 환경에서 성능상 유리합니다.gRPC 이점  성능          네트워크 요청으로 보내기 위해 Protobuf를 직렬화(Serialization) 및 역직렬화(Deserialization)하는 작업은 JSON 형태의 직렬화/역직렬화 보다 빠르다. 또한 gRPC의 네트워크 속도가 HTTP POST/GET 속도보다 빠르다. 특히 POST 요청 시 많은 차이를 보인다. 자세한 내용은 Mobile gRPC Benchmarks 문서를 참고한다.        API-우선 방식          Protobuf를 통해 기능을 개발하기 전에 API를 먼저 정의할 수 있다. API가 먼저 정의될 경우 개발팀이 병렬적으로 일을 진행할 수 있고, 개발 속도가 빨라지며, API를 좀 더 안정적으로 제공할 수 있다. 자세한 내용은 Understanding the API-First Approach to Building Products 문서를 참고한다.        REST API 지원          Protobuf로 정의된 API는 envoyproxy나 grpc-gateway 같은 gateway 를 통해 REST API로 제공 가능하다. gRPC로 정의된 API를 OpenAPI 프로토콜로 변환하여 REST API를 사용하는 클라이언트에도 API-우선 방식을 적용할 수 있다.      참고  Austin G. Walters, Intro to IPC  DR-kim, 프로세스간 통신 방법  우당탕탕 히온이네, [운영체제] IPC 프로세스간 통신  bycho211, 프로세스간 통신(IPC)  얼음연못, [개발상식] 프로세스간 통신(IPC)  nesoy, RPC란?  jakeseo, RPC란?  w3, What is Remote Procedure Call?  grpc공식문서, What is gRPC?  navercloud, [네이버클라우드 기술&amp;amp;경험] 시대의 흐름, gRPC 깊게 파고들기 #1  cla9, 1. gRPC 개요  buzzvil Tech, gRPC",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series13'> <img src='/images/network_logo.png' alt='Network Series [Part13]: 네트워크 용어(3): IPC, RPC'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series13'>Network Series [Part13]: 네트워크 용어(3): IPC, RPC</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part12]: 네트워크 용어(2): 소켓(Socket)",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series12",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  Socket          소켓을 이용한 통신 메커니즘      소켓의 종류        Web Socket          Web Socket Pros and Cons      Applications of Web Socket      What is HTTP?                  How HTTP works?          HTTP Pros and Cons          Applications of HTTP                      참고Socket  소켓은 응용 계층에서 두 프로세스(클라이언트-서버)간의 통신을 위한 엔드포인트를 제공하는 소프트웨어다 -&amp;gt; 서로 다른 서버의 프로세스간의 통신  유닉스 계열의 운영체제에서 IPC를 위한 엔드포인트 -&amp;gt; 같은 서버안에 있는 프로세스간의 통신  소켓은 결국 어떤 종류의 통신이든 필요로 하는 가상의 네트워크 장비이다  SSH, FTP, HTTP, 웹소켓 통신과 같은 모든 응용 계층에서의 통신은 결국 소켓을 사용한다  소켓 프로그래밍을 통해 HTTP와 같은 단방향 통신이 아닌 양방향 통신을 가능하게 하는 프로토콜을 직접 만들어 쓸 수도 있다(ex. 웹소켓)  소켓은 응용 계층에 사용되는 프로토콜을 실제로 코드로 구현해 놓은 소프트웨어라고 할 수 있다  각 소켓은 IP주소와 Port번호로 이루어진 특정 주소를 가지고 있다소켓을 이용한 통신 메커니즘- 클라이언트와 서버에서 각각 소켓을 생성한다- 서버는 클라이언트가 서버를 찾을 수 있도록 소켓에 주소를 bind 한다- 그리고 클라이언트의 요청을 기다린다- 클라이언트가 서버에 연결을 요청하고, 서버가 이를 수락하면 통신이 가능해진다  socket()을 호출하면 통신을 위한 엔드포인트를 나타내는 소켓 디스크립터를 리턴한다  서버는 bind()를 호출해 클라이언트가 서버를 찾을 수 있도록 소켓에 주소를 바인딩한다  listen()은 서버가 클라이언트의 요청을 받을 준비가 되었다는 신호를 의미한다  클라이언트는 connect()를 이용해 서버에 연결을 요청한다  클라이언트는 accept()를 통해 요청을 받아들인다  서버와 클라이언트는 recv(), send(), read(), write()와 같은 전송 API를 이용해 데이터를 주고 받는다  서버 또는 클라이언트가 연결을 종료하고 싶을 때는 close()를 호출하여 연결을 종료한다The socket APIs are located in the communications model between the application layer and the transport layer. The socket APIs are not a layer in the communication model. Socket APIs allow applications to interact with the transport or networking layers of the typical communications model.소켓의 종류  소켓의 종류에 따라 L4(Transport Layer)의 프로토콜이 달라진다      또는 원하는 L4의 프로토콜에 따라 소켓의 종류를 다르게 사용해야 한다    Datagram Socket          데이터그램 소켓의 연결은 단방향이고 신뢰할 수 없다.      또한 수신 측에서 데이터를 순서대로 받는다고 보장할 수도 없다. 데이터그램은 L4 계층에서 UDP(User Datagram Protocol)라는 표준 프로토콜을 사용한다.      안전장치도 별로 없어서 단순하고 간단하고, 가벼운 방법이다. 고로, 부하가 매우 적다.      패킷 손실이 허용되기도 한다.      보통, 네트워크 게임이나 미디어 스트리밍에서 자주 쓰인다.        Stream Socket          전화와 같이 믿을 수 있는 양방향 통신을 제공한다.      한쪽에서 다른 한쪽으로의 연결을 초기화하고, 연결이 생성된 후에는 어느 쪽에서든 다른 쪽으로 통신할 수 있다.      보낸 내용이 실제로 도착했는지도 즉각 확인할 수 있다. 스트림 소켓은 TCP(Transmission Control Protocol)라 불리는 표준 통신 프로토콜을 사용한다.      TCP는 패킷이 오류 없이 순서대로 도착하도록 설계되었다.      웹서버, 메일서버, 각 클라이언트 애플리케이션 모두는 TCP와 스트림 소켓을 사용한다.      Web Socket  웹 소켓은 양방향 통신을 위한 응용 계층 표준 프로토콜  이 때 전송계층은 TCP  Web Socket is designed to work over HTTP ports 443 and 80 to support HTTP proxies and interfaces.There are two significant API types when it comes to web communication. Whether to use HTTPS or Web Socket is a crucial decision while deploying projects / applications. The choice would depend on which technology best fits actual client requirements and situations.  웹소켓은 HTTP통신과 자주 비교된다  왜냐하면 HTTP통신의 단방향, 실시간 통신에 약하다는 단점을 웹소켓이 해결해주기 때문이다Web Socket Pros and Cons  장점          빠르고, 리소스 소모가 적다      실시간에 가까운 요청/응답      양방향 통신        단점          웹 브라우저는 완전히 HTML5 규칙을 따라야함      Intermediary / Edge caching not possible      단방향에 비해 통신 방식이 조금 더 복잡      Applications of Web Socket  비트코인 거래소, 게임, 채팅 어플리케이션에 이상적What is HTTP?  HTTP is a communication protocol of the World Wide Web.  Http works as a request-response protocol in the client-server computing model.  HTTP is a uni-directional protocol where the client sends the request and the server sends the response.  Each request is associated with a corresponding response , after the response is sent and connection gets closed each HTTP or HTTPS request establishes a new connection to the server every time and post getting response connection gets terminated itself.  HTTP is a stateless protocol that runs on TCP which is a connection-oriented protocolHow HTTP works?HTTP message information is encoded in ASCII and each HTTP request message comprising HTTP protocol version (HTTP/1.1, HTTP/2), HTTP methods (GET/POST etc.) , HTTP headers (content type, content length) , host details etc. and the body which contain the actual message sent to server. HTTP headers size varies from 200 bytes to 2 KB in size.HTTP Pros and Cons  장점          Advanced addressing scheme by assigning IP Addresses with recognizable names for ease of identification on World Wide web      Capability to download extensions or plugins and display relevant data      Chance of interception during transmission is minimized as each file download happens from independent connection and gets closed      Less latency due to no handshaking following the request except during initial stage when connection is established      All HTTP page gets stored inside the Internet cache for quick content loading        단점          Data integrity is an issue as hacker manage to intercept the request, they can view all the content present on web page      Client don’t take any measures to close connection once all data is received hence during this time-period server may not be present      HTTP needs multiple connections causing administrative overhead      Not suitable for IoT devices as it uses number of system resources which leads to more consumption of power      Applications of HTTP  Fetching /transferring old data  Simple RESTful applications use HTTP참고  [소켓과 웹소켓] 한 번에 정리 (1),소켓이란?, 소켓 API의 실행 흐름, 클라이언트 소켓과 서버 소켓  baeldung, The Difference Between a Port and a Socket  Web Socket vs HTTP: What to choose for your next API Design  What is web socket and how it is different from the HTTP?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series12'> <img src='/images/network_logo.png' alt='Network Series [Part12]: 네트워크 용어(2): 소켓(Socket)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series12'>Network Series [Part12]: 네트워크 용어(2): 소켓(Socket)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part11]: 네트워크 용어(1): NAT, DHCP",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series11",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  NAT          NAT란 무엇인가      NAT를 사용하는 이유                  IP 주소 절약          보안                    NAT의 종류                  Static NAT          Dynamic NAT          PAT(Port Address Translation)                    NAT의 동작 원리        DHCP  참고NATNAT란 무엇인가  NAT stands for network address translation. It’s a way to map multiple local private addresses to a public one before transferring the information. Organizations that want multiple devices to employ a single IP address use NAT, as do most home routers.  Network Address Translation의 줄임말  사설 IP 주소를 가지는 여러 대의 로컬 장비를 하나의 공인 IP 주소로 변환해 외부 네트워크와 통신하도록 해준다NAT를 사용하는 이유IP NAT(Network Address Translation)는 원래 제한된 수의 인터넷 IPv4 주소 문제를 해결하기 위해 개발되었습니다. 여러 디바이스에서 인터넷에 액세스해야 하지만 하나의 IP 주소만 ISP(인터넷 서비스 공급자)에 의해 할당되는 경우 NAT가 필요합니다. NAT는 로컬 호스트가 공용 인터넷에 액세스할 수 있게 하고 외부에서는 직접 액세스할 수 없도록 보호합니다.IP 주소 절약  NAT를 이용하면 여러 대의 장비를 하나의 공인 IP주소를 사용해 인터넷과 연결시킬 수 있다  공유기가 대표적으로 이러한 NAT 기능을 탑재하고 있다보안  외부 네트워크와 통신하기 위해 사설 IP를 알려주지 않아도 된다NAT의 종류Static NAT  사설 IP주소와 공인 IP주소를 1:1로 매핑  이 경우에는 IP주소 절약 효과는 없음  서버의 여러 포트로 포트포워딩하는 목적으로 사용Dynamic NAT  사설 IP주소와 공인 IP주소를 N:M로 매핑 (N &amp;gt; M)  공인 IP주소로 데이터가 들어오면 NAT가 적절한 사설 IP주소로 변환해 준다PAT(Port Address Translation)  GeeksforGeeks, Port Address Translation (PAT) mapping to Private IPsNAT의 동작 원리한 가지 궁금한 것은 NAT가 무엇을 이용해 공인 IP주소를 N개의 사설 IP주소중 하나로 변환할까?As a NAT network address translation example, an inside host may want to communicate with a destination network address translation web server address in the outside world. For further communication, it will send a data packet to the network’s NAT gateway router.The NAT gateway router determines whether the packet meets the condition for translation by learning the source IP address of the packet and looking it up in the table. It can locate authenticated hosts for the internal network translation purposes on its access control list (ACL), and then complete the translation, producing an inside global IP address from the inside local IP address.Finally, the NAT gateway router will route the packet to the destination after saving the translation in the NAT table. The packet reverts to the global IP address of the router when the internet’s web server reverts to the request. Referring back to the NAT table, the router can determine which translated IP address corresponds to which global address, translate it to the inside local address, and deliver the data packet to the host at their IP address. The data packet is discarded if no match is found.DHCPIP 주소를 할당하기란 쉽지 않은 일입니다. 특히 IP 주소 할당 과정에서 문제가 생기면 누군가의 인터넷 연결이 끊어질 수도 있습니다. 이때 DHCP를 이용하면 IP 주소 할당을 더욱 수월하게 진행할 수 있습니다.DHCP는 ‘Dynamic Host Configuration Protocol(동적 호스트 구성 프로토콜)’의 약자로, IP 네트워크에 사용되는 네트워크 프로토콜입니다. DHCP는 IP 주소 및 기타 통신 매개변수를 네트워크에 연결된 장치에 자동으로 할당합니다. 대부분의 가정용 네트워크에서는 라우터가 IP 주소를 장치에 할당하는 DHCP 서버의 역할을 합니다.DHCP는 네트워크 관리자가 해야 할 작업을 간소화합니다. DHCP 사용 없이는 수동으로 IP 주소를 할당해야 합니다. DHCP 설정 없이 수동으로 IP 주소를 할당하면 비효울적이고 시간이 지나치게 많이 소요되며 오류가 발생할 가능성이 높습니다.이제 DHCP 동작 과정에 대해 알아보겠습니다. 장치는 네트워크에 연결 시 IP 주소를 요청합니다. 요청은 DHCP 서버로 전달되며, 서버는 주소를 할당하고 주소의 이용을 모니터링하며 장치의 연결이 해제되면 주소를 다시 가져옵니다. 해당 IP 주소는 다른 장치에 재할당할 수 있으며, 장치는 IP 주소를 이용해 내부 및 공용 네트워크와 통신할 수 있습니다.DHCP 서버는 DHCP 매개변수(DHCP 옵션)를 제공합니다. DHCP 매개변수에서는 IP 주소의 다양한 정보(사용 가능 시간)를 확인할 수 있습니다. DHCP 옵션에서는 보통 다음 내용이 포함됩니다.  현지 네트워크와 인터넷 사이의 데이터를 라우팅하는 기본 게이트웨이  IP 주소 내의 호스트 주소와 네트워크 주소를 분리하는 서브넷 마스크  IP 주소의 이름을 사람이 기억할 수 있는 이름으로 변환하는 DNS 서버DHCP의 IP 주소 할당 방식은 다음과 같습니다.  동적 할당(Dynamic Allocation): 관리자가 DHCP에 IP 주소를 유보해놓는 경우에 동적 할당 방식이 활용됩니다. 로컬 네트워크에 있는 DHCP 클라이언트는 네트워크 초기화 단계에서 DHCP 서버 로부터 IP를 요청합니다. 이 모든 과정은 DHCP 서버가 이용되지 않은 IP 주소를 다시 클레임하고 다시 할당할 수 있는 시간 동안 진행됩니다.  자동 할당(Automatic allocation): DHCP 서버는 관리자가 정한 규칙에 따라 IP 주소를 클라이언트에 영구 할당하는 방식입니다. 자동 할당 방식은 DHCP 서버에 이전 IP 주소 할당 데이터가 있고 동일한 IP 주소를 동일한 클라이언트에게 재할당할 수 있다는 점에서 동적 할당과는 다릅니다.  수동 할당(Manual allocation): 관리자가 각 클라이언트에 대한 고유한 식별자를 IP 주소에 수동 할당하는 방식입니다. DHCP 서버는 DHCP 서버에 연결할 수 없을 때 다른 할당 방식으로 전환하도록 구성되어 있습니다.참고  NordVPN, DHCP의 정의와 DHCP를 이용해야 하는 이유  Microsoft, NAT(Network Address Translation) 소개  STEVEN J.LEE, NAT이해하기  CompTIA, What Is NAT?  GeeksforGeeks, Network Address Translation (NAT)  AVI Networks, Network Address Translation Definition  GeeksforGeeks, Port Address Translation (PAT) mapping to Private IPs",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series11'> <img src='/images/network_logo.png' alt='Network Series [Part11]: 네트워크 용어(1): NAT, DHCP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series11'>Network Series [Part11]: 네트워크 용어(1): NAT, DHCP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series10",
      "date"     : "Jul 16, 2022",
      "content"  : "Table of Contents  로드 밸런싱의 정의  로드 밸런서의 특징  로드 밸런서가 제공하는 이점  로드 밸런싱 방법          Round Robin Algorithm      Least connections      Source IP hash        Software Load Balancers vs. Hardware Load Balancers  로드 밸런서 종류  참고로드 밸런싱의 정의  Load balancing is the process of distributing network traffic across multiple servers. This ensures no single server bears too much demand. By spreading the work evenly, load balancing improves application responsiveness. It also increases availability of applications and websites for users. Modern applications cannot run without load balancers. Over time, software load balancers have added additional capabilities including application security.로드 밸런서의 특징  로드 밸런서는 어떤 서버가 해당 트래픽을 처리할지 결정한다. 이것은 좋은 사용자 경험을 유지시키는데 도움을 준다  로드 밸런서는 계속해서 서버의 상태를 체크한다로드 밸런서가 제공하는 이점  서버 장애를 자동으로 감지  Provide automated disaster recovery to backup sites  Add and remove application servers without disruption  Monitor and block malicious content로드 밸런싱 방법Round Robin Algorithm  가중 라운드 로빈 방식 (Weighted Round Robin): 서버의 사양에 맞게 트래픽을 배분한다  동적 라운드 로빈 방식 (Dynamic Round Robin): 실시간 계산을 통해 트래픽을 배분한다Least connections  커넥션이 가장 적은 서버에 연결한다Source IP hashIn a source IP Hash, load balancing a server is selected based on a unique hash key. The Hash key is generated by taking the source and destination of the request. Based on the generated hash key, servers are assigned to clients.  요청의 출발지 IP 주소와 도착지 IP 주소에 기반해 해시 키를 생선한다  생성된 해시 키에 기반해 클라이언트에 서버가 할당된다Software Load Balancers vs. Hardware Load Balancers로드 밸런서 종류      L7:    Network Server Load Balancers(NLB)          L4 로드 밸런서      IP주소 + 포트 번호에 기반해 트래픽을 로드 밸런싱        Application Load Balancers(ALB)          L7 로드 밸런서      HTTP 헤더, URL 과 같은 속성을 이용해 트래픽을 로드 밸런싱        Elastic Load Balancers(ELB): Elastic Load Balancing scales traffic to an application as demand changes over time. It uses system health checks to learn the status of application pool members (application servers) and routes traffic appropriately to available servers, manages fail-over to high availability targets, or automatically spins-up additional capacity.참고  AVI Networks: What Is Load Balancing?  Progress Kemp: What is load balancing?  Jae Honey, L4 / L7 로드밸런서 차이 (Load balancer)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-16T21:01:35+09:00'>16 Jul 2022</time><a class='article__image' href='/network-series10'> <img src='/images/network_logo.png' alt='Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series10'>Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series9",
      "date"     : "Jul 15, 2022",
      "content"  : "Table of Contents  네트워크 관련 리눅스 명령어          ifconfig      netstat      iptables      ping      wget      curl      ssh        참고네트워크 관련 리눅스 명령어ifconfig, ip, netstat, ss, iptables, ping, ssh, telnet, route, curl, wgetifconfig  네트워크 인터페이스의 활성/비활성 여부, ip 관련 정보를 확인할 수 있음ifconfig----------------en0: flags=8863&amp;lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&amp;gt; mtu 1500...inet &amp;lt;ip주소&amp;gt; netmask &amp;lt;서브넷 마스크&amp;gt; broadcast &amp;lt;브로드캐스트 주소&amp;gt;status: activenetstat      Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships    mac에서 netstat은 ubuntu, redhat 계열과는 가지고 있는 옵션이 다름          -a 기본 출력에 포함되지 않은 netstat의 출력에 서버 포트를 포함합니다.      -g 멀티 캐스트 연결과 관련된 정보를 표시합니다.      -I interface는 지정된 인터페이스에 대한 패킷 데이터를 제공합니다. 사용 가능한 모든 인터페이스는 -i 깃발이지만 en0 일반적으로 기본 발신 네트워크 인터페이스입니다. (소문자에 유의하십시오.)      -n 이름이있는 원격 주소의 레이블을 억제합니다. 이것은 제한된 정보만을 희생시키면서 netstat의 출력 속도를 크게 향상시킵니다.      -p 프로토콜은 특정 네트워킹 프로토콜과 관련된 트래픽을 나열합니다. 전체 프로토콜 목록은 / etc / protocols,하지만 가장 중요한 것은 udp 와 TCP.      -r 네트워크에서 패킷이 라우팅되는 방식을 보여주는 라우팅 테이블을 표시합니다.      -s 활성 여부에 관계없이 모든 프로토콜에 대한 네트워크 통계를 표시합니다.      -v 특히 각 열린 포트와 연관된 프로세스 ID (PID)를 표시하는 열을 추가하여 자세한 정보를 제공합니다.      자주 사용하는 예시        netstat -avp tcp# 포트 열고 LISTEN 하고 있는 네트워크 엔드포인트netstat -a | grep -i LISTEN                      ubuntu 계열          자주 사용하는 예시        netstat -nlp                            iptables  administration tool for IPv4 packet filtering(방화벽) and NATping  특정 서버가 켜져있는지/접근 가능한지 확인    ping &amp;lt;ip&amp;gt;ping -c &amp;lt;ping날릴 횟수&amp;gt; &amp;lt;ip&amp;gt;        포트 번호를 지정하고 싶은 경우에는 tcping 라는 커맨드 툴 설치해야함    brew install tcpingtcping &amp;lt;ip&amp;gt; &amp;lt;port&amp;gt;      wget  network downloader  기본적으로 다운이 디폴트    wget &amp;lt;URL&amp;gt;          curl  다양한 통신 프로토콜(HTTP, FTP, LDAP 등)을 지원하여 데이터를 전송할 수 있는 도구  기본적으로 터미널 화면에 출력이 디폴트    curl &amp;lt;URL&amp;gt;curl &amp;lt;URL&amp;gt; &amp;gt; &amp;lt;원하는 파일명&amp;gt;curl -o &amp;lt;원하는 파일명&amp;gt; &amp;lt;URL&amp;gt;curl -O &amp;lt;URL&amp;gt;/&amp;lt;원하는 파일명&amp;gt;            뭔가 curl은 파일의 다운로드 경로를 넣어줘도 HTML 코드가 출력된다 (파일을 다운로드하고자 할 때는 wget인건가 무조건?)  서버에 REST API 형태로 HTTP Request를 전송할 수도 있다    curl -X POST -H &amp;lt;헤더&amp;gt; &amp;lt;URL&amp;gt; -d &amp;lt;전송할 데이터&amp;gt;curl -X GET &amp;lt;URL&amp;gt;      ssh  remote login program    ssh -i &amp;lt;pem파일&amp;gt; &amp;lt;username&amp;gt;@&amp;lt;host&amp;gt;        어플리케이션 계층에서 제공참고  kibua20, 자주 사용하는 curl 명령어 옵션과 예제  ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-15T21:01:35+09:00'>15 Jul 2022</time><a class='article__image' href='/network-series9'> <img src='/images/network_logo.png' alt='Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series9'>Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part8]: 네트워크 활용편(2) REST API",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series8",
      "date"     : "Jul 14, 2022",
      "content"  : "Table of Contents  HTTP API? REST API?  REST API          REST 장단점      REST 구성 요소        RESTful          RESTful의 개념      RESTful의 목적      RESTful 하지 못한 경우      URI 규칙      HTTP Method 활용        참고HTTP API? REST API?  HTTP API와 REST API는 사실 거의 같은 의미  HTTP API는 HTTP를 사용해서 서로 정해둔 스펙으로 데이터를 주고 받으며 통신하는 것  REST API는 HTTP API에 여러가지 제약 조건이 추가REST는 다음 4가지 제약조건을 만족해야 한다.  자원의 식별  메시지를 통한 리소스 조작  자기 서술적 메세지  애플리케이션 상태에 대한 엔진으로서 하이퍼미디어 (대표적으로 구현하기 어려운 부분)여러가지가 있지만 대표적으로 구현하기 어려운 부분이 마지막에 있는 부분인데요. 이것은 HTML처럼 하이퍼링크가 추가되어서 다음에 어떤 API를 호출해야 하는지를 해당 링크를 통해서 받을 수 있어야 합니다.그리고 이런 부분을 완벽하게 지키면서 개발하는 것을 RESTful API라고 하는데요. 실무에서 이런 방법으로 개발하는 것은 현실적으로 어렵고, 또 추가 개발 비용대비 효과가 있는 것도 아닙니다.그런데 이미 많은 사람들이 해당 조건을 지키지 않아도 REST API라고 하기 때문에, HTTP API나 REST API를 거의 같은 의미로 사용하고 있습니다.REST API  REpresentational State Transfer  프론트엔드에서 백엔드의 데이터를 가져올 때 가장 많이 사용하는 자원 식별/처리 방식  웹의 기존 기술과 HTTP 프로토콜을 그대로 활용하기 때문에 웹의 장점을 최대한 활용할 수 있는 아키텍처  URI로 자원을 요청하여 특정 형태로 표현  HTTP API를 활용해 행동을 표현  웹 사이트의 이미지, 텍스트, DB 내용 등의 모든 자원에 고유한 ID인 HTTP URI를 부여      REST API의 요청에 대한 응답은 대체로 JSON으로 표현    REST API는 무상태(Stateless)환경에서 동작하는 것을 전제로 함  따라서 보안 및 인증에 대해서는 JWT(JSON Web Token), OAuth와 같은 토큰 인증이 사용  상태 유지를 위한 세션(Sessions)은 사용하지 않음  자주 사용하는 자원에 대해서는 ETag, Last-Modified 헤더를 통한 캐싱도 가능HTTP URI(Uniform Resource Identifier)를 통해 자원(Resource)을 명시하고, HTTP Method(Post, Get, Put, Delete)를 통해 해당 자원에 대한 CRUD Operation을 적용하는 것을 의미한다.REST 장단점  장점          여러 가지 서비스 디자인에서 생길 수 있는 문제를 최소화해준다.      Hypermedia API의 기본을 충실히 지키면서 범용성을 보장한다.      HTTP 프로토콜의 표준을 최대한 활용하여 여러 추가적인 장점을 함께 가져갈 수 있게 해준다.        단점          브라우저를 통해 테스트할 일이 많은 서비스라면 쉽게 고칠 수 있는 URL보다 Header 값이 왠지 더 어렵게 느껴진다.      구형 브라우저가 아직 제대로 지원해주지 못하는 부분이 존재한다.                  PUT, DELETE를 사용하지 못하는 점          pushState를 지원하지 않는 점                    REST 구성 요소  자원(Resource): URI          모든 자원에 고유한 ID가 존재하고, 이 자원은 Server에 존재한다.      자원을 구별하는 ID는 ‘/groups/:group_id’와 같은 HTTP URI 다.      Client는 URI를 이용해서 자원을 지정하고 해당 자원의 상태(정보)에 대한 조작을 Server에 요청한다.        행위(Verb): HTTP Method          HTTP 프로토콜의 Method를 사용한다.      HTTP 프로토콜은 GET, POST, PUT, DELETE, HEAD 와 같은 메서드를 제공한다.        표현(Representation of Resource)          Client가 자원의 상태(정보)에 대한 조작을 요청하면 Server는 이에 적절한 응답(Representation)을 보낸다.      REST에서 하나의 자원은 JSON, XML, TEXT, RSS 등 여러 형태의 Representation으로 나타내어 질 수 있다.      JSON 혹은 XML를 통해 데이터를 주고 받는 것이 일반적이다.      RESTful  URI로 자원을 표현하고, HTTP Method로 행동을 표현RESTful의 개념  RESTful은 일반적으로 REST라는 아키텍처를 구현하는 웹 서비스를 나타내기 위해 사용되는 용어이다.          즉, REST 원리를 따르는 시스템은 RESTful이란 용어로 지칭된다.        RESTful은 REST를 REST답게 쓰기 위한 방법으로, 누군가가 공식적으로 발표한 것이 아니다.RESTful의 목적  이해하기 쉽고 사용하기 쉬운 REST API를 만드는 것  RESTful API를 구현하는 근본적인 목적이 퍼포먼스 향상에 있는게 아니라, 일관적인 컨벤션을 통한 API의 이해도 및 호환성을 높이는게 주 동기이니, 퍼포먼스가 중요한 상황에서는 굳이 RESTful API를 구현하실 필요는 없습니다.RESTful 하지 못한 경우  Ex1) CRUD 기능을 모두 POST로만 처리하는 API  Ex2) route에 resource, id 외의 정보가 들어가는 경우(/students/updateName)URI 규칙            규칙      좋은 예      나쁜 예              마지막이 /로 끝나면 안된다      http://api.test.com/users      http://api.test.com/users/              _대신 -를 사용한다      http://api.test.com/tag/rest-api      http://api.test.com/tag/rest_api              소문자로 구성한다      http://api.test.com/tag/rest-api      http://api.test.com/tag/REST-api              동사를 URI로 포함시키지 않는다      http://api.test.com/user/1      http://api.test.com/delete-user/1              파일 확장자는 표시하지 않는다      http://api.test.com/users/1/profile      http://api.test.com/users/1/profile.png              리소스 간에는 연관 관계가 있는 경우             http://api.test.com/users/{userid}/devices      HTTP Method 활용  행위에 대한 표현은 URI에서 하지 않고 HTTP Method를 이용하도록 한다  가장 많이 사용하는 메서드는 GET, POST, PUT, DELETE 이다            상태코드      설명              200 OK      요청이 올바르게 수행되었음(GET, PUT)              201 Created      서버가 새로운 리소스를 생성했음(POST)              204 No Content      응답할 데이터가 없음(HTTP Body가 없음) (DELETE, PUT)              400 Bad Request      요청이 잘못되었음              401 Unauthorized      인증(로그인)이 필요함              403 Forbidden      로그인 되었으나 해당 자원에 대한 권한이 없음              404 Not Found      존재하지 않는 자원에 대해 요청했음 (URI가 잘못된 경우?)              405 Method Not Allowed      자원이 지원하지 않는 메소드임 (Method가 잘못된 경우?)              409 Confilct      비지니스 로직상 요청을 처리하지 못한 경우              429 Too Many Requests      요청을 너무 많이한 경우      참고  이영한님의 HTTP API와 REST API의 차이에 관한 질문에 대한 답변  WeareSoft/tech-interview  REST 논문을 정리한 자료  정상우, REST를 보다 RESTful 하게 API 만들기  위키백과 REST",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-14T21:01:35+09:00'>14 Jul 2022</time><a class='article__image' href='/network-series8'> <img src='/images/network_logo.png' alt='Network Series [Part8]: 네트워크 활용편(2) REST API'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series8'>Network Series [Part8]: 네트워크 활용편(2) REST API</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series7",
      "date"     : "Jul 13, 2022",
      "content"  : "Table of Contents  웹 브라우저 렌더링 과정  캐시와 쿠키          캐시                  웹 브라우저 캐시와 CPU 캐시          웹 브라우저 캐시의 종류          프록시 캐시                    쿠키      세션        Do web browsers use different outgoing ports for different tabs?  참고웹 브라우저 렌더링 과정대기열: 브라우저는 주소창을 통해 입력한 요청을 대기열에 넣는다캐싱: 요청에 대한 응답값을 프록시 서버 캐시 또는 웹 브라우저 캐시에 저장하고 재요청시 캐싱된 값을 리턴브라우저 캐시: 쿠키, 로컬 스토리지, 세션 스토리지 등을 포함한 캐시공유 프록시 캐시: 서버 앞 단에 클라이언트와 가깝게 프록시 서버를 배치한다. 프록시 서버에 저장된 응답값이 프록시 캐시(Node.js 앞에 있는 nginx)DNS: URL의 도메인명을 IP주소로 변환한다. 이 과정 또한 바로 DNS로 요청을 보내지 않고, 호스트 파일과 같이 캐시된 데이터가 있는지 먼저 확인한다라우팅: DNS가 resolve한 IP주소와 ARP 과정을 통해 얻은 MAC 주소를 이용해 목적지 IP 주소로 라우팅 한다.초기연결: 라우팅할 때 처음부터 데이터를 요청하는 것이 아니라 먼저 TCP 3 way handshake를 이용해 연결을 한다다운로드: 이제 요청에 대한 응답값과, 그외에 필요한 HTML, CSS, 이미지 등을 서버로부터 받는다렌더링: 웹 브라우저가 서버로부터 받아온 데이터를 이용해 렌더링하여 사용자에게 제공한다캐시와 쿠키캐시  The goal of a caching is to store some resource “closer” to the access point than it was before  나중에 요청올 결과를 미리 저장해두었다가 빠르게 돌려주는 것웹 브라우저 캐시와 CPU 캐시  The CPU cache consists of special circuits within the CPU that are designed to cache memory, i.e. the cache is implemented in hardware as part of the physical CPU. This special cache hardware has absolutely nothing to do with the browser cache  웹 브라우저 캐시  We all have cache stored in our internet browser. It’s essentially a folder which contains small pieces of information taken from our browsing experience and the sites we visit. The aim is to make pages we revisit load faster.  CPU 캐시  The purpose of the cache memory component is to increase speed. It’s located physically closer to the processor than the RAM component making it 10 to 100 times faster than RAM. However, its function is very different. The cache memory stores instructions and data that are used repeatedly by programs, enhancing the overall performance.웹 브라우저 캐시의 종류  로컬 스토리지          키/값 쌍의 형태로 클라이언트 브라우저에서 데이터를 저장하는 방법      수동으로 삭제하지 않는 한 영구적으로 저장소에 남아 있습니다.        mac의 경우~/Library/Application Support/Google/Chrome/Default/Local Storage/                      세션 스토리지          로컬 스토리지와 유사하지만 탭을 닫을 때 삭제됨        쿠키프록시 캐시공유 프록시 캐시: 서버 앞 단에 클라이언트와 가깝게 프록시 서버를 배치한다. 프록시 서버에 저장된 응답값이 프록시 캐시(Node.js 앞에 있는 nginx, AWS의 CloudFront)쿠키쿠키는 웹 서버가 생성하여 웹 브라우저로 보내는 정보의 작은 파일입니다. 웹 브라우저는 미리 정해진 시간 동안 또는 웹 사이트에서 사용자의 세션 기간 동안 받은 쿠키를 저장합니다. 사용자가 웹 서버에 대해 향후 요청할 경우 관련 쿠키를 첨부합니다.쿠키는 웹 사이트가 사용자 경험을 개인화할 수 있도록 사용자에 대한 정보를 웹 사이트에 제공하는 데 도움이 됩니다. 예를 들어, 전자 상거래 웹사이트는 사용자들이 쇼핑 카트에 어떤 상품을 넣었는지 알기 위해 쿠키를 사용한다. 또한 일부 쿠키는 인증 쿠키와 같은 보안 목적으로 필요합니다(아래 참조).인터넷에서 사용되는 쿠키는 “HTTP 쿠키”라고도 불립니다. 대부분의 웹과 마찬가지로 쿠키는 HTTP 프로토콜을 사용하여 전송됩니다.cat ~/Library/Application Support/Google/Chrome/Default/Cookies세션Do web browsers use different outgoing ports for different tabs?Each connection to a website uses a different socket with default destination TCP port 80 for plain HTTP and 443 for HTTPS. For the socket to be unique, the combination of the source IP address, source TCP port, destination IP address and destination TCP port must be different.If you have multiple connections to the same website (assuming the website uses only 1 IP address) from the same computer, a different source TCP port must be used. This way, each connection is unique.참고  정상우, 쿠키와 세션에 대해 알아보자  CLOUD FLARE: What are cookies?, Cookies definition",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-13T21:01:35+09:00'>13 Jul 2022</time><a class='article__image' href='/network-series7'> <img src='/images/network_logo.png' alt='Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series7'>Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-ex-series5",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  JOIN과 SUBQUERY 둘 다 사용할 수 있다면 어떤 것을 쓰는게 좋을까  LEFT OUTER JOIN을 사용할 때 조건을 만족하는 행이 2개인 경우  FROM에서 콤마(,)가 사용된 경우  날짜 관련 함수  ROW_NUMBER()  윈도우 함수  OVER()  CTE(Common Table Expression)  LEAD, LAG 함수  3 NOT IN (null, 1, 2)  WHERE 조건절에 레코드(튜플) 사용할 수도 있음  JOIN 쿼리에서 ON절과 WHERE절에 표기하는 것의 차이  Subset  Comparing Sets  INNER JOIN은 ON이 없어도 된다?  ROW_NUMBER(), RANK(), DENSE_RANK()  서브쿼리에서는 메인쿼리의 컬럼을 사용할 수 있다  컬럼명이 SQL 문법에 포함되는 경우 쌍따옴표로 묶어주면 된다  UNION 말고 UNION ALL도 있다  참고JOIN과 SUBQUERY 둘 다 사용할 수 있다면 어떤 것을 쓰는게 좋을까  Sub-query solutionSELECT Name as Employee FROM Employee eWHERE Salary &amp;gt; (    Select Salary FROM Employee m WHERE m.Id = e.ManagerId)Advantages Of Subquery  Complex query can be broken down into a series of logical steps.  Subquery is easy to read, understand and maintain.  It allow to use the results of another query in the outer query.Disadvantages of Subquery  Execution is slower than JOIN.  We cannot modify a table and select from the same table within a subquery in the same SQL statement.  JOIN solutionSELECT     a.NAME AS EmployeeFROM Employee AS a JOIN Employee AS b     ON a.ManagerId = b.Id     AND a.Salary &amp;gt; b.SalaryAdvantage of a JOIN  Execution and retrieval time faster than subqueries.Disadvantages Of JOIN  Database server has to do more work when it comes to a lot of joins in a query =&amp;gt; more time consuming to retrieve data  Developer can be confused to choose the appropriate type among many types of joins.  Conclusion  Most developers prioritize speed optimizing while others prioritize logic. It ups to you in your specific case.LEFT OUTER JOIN을 사용할 때 조건을 만족하는 행이 2개인 경우문제 Consecutive Available Seats여기서 나는 LEFT OUTER JOIN을 2번 썼다. [현재 좌석, 이전 좌석, 이후 좌석] 이런식으로.정답은 맞았지만 이렇게 조인을 2번이나 써야하나 라는 생각에 다른 사람들의 풀이를 구경해봤다.풀이 중에select distinct a.seat_idfrom cinema ajoin cinema bon abs(a.seat_id - b.seat_id) = 1and a.free=true and b.free=trueorder by a.seat_id;을 봤다. 조인을 1번만 쓰고 있다. 근데 이 방법은 ON절에 사용된 조건이 driving table 행에 조인되는 derived table의 행이 2개가 조인되는 결과를 가져오게 될 것 같았다. 그래서 이러한 경우에는 결과 테이블이 어떻게 될지 궁금해 검색해봤다.haerong22, LEFT OUTER JOIN 의 함정위 블로그 내용을 보니이렇게 id 값이 중복되는 데이터 뻥튀기(?) 현상이 일어났다.이 때는  테이블간 제약 조건을 명확히 한다.  조인 조건을 추가  distinct 사용  group by 사용  top 1, limit 사용와 같은 방법을 이용해 해결할 수 있다. 위의 풀이에서는 distinct를 추가했다.FROM에서 콤마(,)가 사용된 경우FROM 절에서 여러 테이블을 함께 사용할 때 사람들마다 쿼리 작성법이 조금씩 달랐다. 표기법의 차이일 뿐 다음은 같은 역할을 한다.  INNER JOIN = JOIN = ,  LEFT OUTER JOIN = LEFT JOIN  RIGHT OUTER JOIN = RIGHT JOIN날짜 관련 함수  DATE_SUB()          INTERVAL        DATE_FORMAT()ROW_NUMBER()  Assigns a sequential integer to every row within its partition  We will show you how to add a sequential integer to each row or group of rows in the result set.  ROW_NUMBER() is a window function that returns a sequential number for each row, starting from 1 for the first row.윈도우 함수  특정 범위마다 함수를 적용하는 것을 윈도우 함수라고 함  MySQL에서 제공하는 윈도우 함수라고 따로 정의해둔 윈도우 함수 묶음이 있음  집계 함수도 OVER절을 이용해 범위를 정의하면 윈도우 함수로 사용할 수 있음(Most aggregate functions also can be used as window functions, MySQL 공식문서)  사용 방법: [윈도우 함수] + [OVER 절] or [집계 함수] + [OVER 절]  범위마다 함수를 적용한다는 점에서 GROUP BY와 비슷하게 느껴지지만, GROUP BY는 집계된 결과를 테이블로 보여주는 반면, 윈도우 함수는 집계된 결과를 기존 테이블에 하나의 열로 추가하여 결과를 볼 수 있음OVER()  If you want to learn window functions in MySQL, you need to understand the OVER clause  In 2018, MySQL introduced a new feature: window functions, which are accessed via the OVER clause. Window functions are a super powerful resource available in almost all SQL databases. They perform a specific calculation (e.g. sum, count, average, etc.) on a set of rows; this set of rows is called a “window” and is defined by the MySQL OVER clause.  OVER clause which has three possible elements: partition definition, order definition, and frame definition.    [window_function(expression)][aggregation_function(expression)] OVER (  [partition_defintion] [order_definition] [frame_definition])        PARTITION BY: 윈도우 범위 결정  ORDER BY: 정렬하여 계산CTE(Common Table Expression)  문제: All People Report to the Given Manager  In MySQL every query generates a temporary result or relation. In order to give a name to those temporary result set, CTE is used.  A CTE is defined using WITH clause  A recursive CTE is a subquery which refer to itself using its own name          The recursive CTEs are defined using WITH RECURSIVE clause      There should be a terminating condition to recursive CTE.      The recursive CTEs are used for series generation and traversal of hierarchical or tree-structured data      WITH RECURSIVE CTE AS (    SELECT employee_id    FROM Employees    WHERE manager_id = 1 AND employee_id != 1    UNION ALL    SELECT e.employee_id    FROM CTE c INNER JOIN Employees e ON c.employee_id = e.manager_id)SELECT employee_idFROM CTEORDER BY employee_idLEAD, LAG 함수  Non Aggregation Window Function 중 하나  lead -&amp;gt; 이끌다 -&amp;gt; 현재 행 다음  lag -&amp;gt; 질질 끌다 -&amp;gt; 현재 행 이전  LEAD(expr, N, default) OVER(PARTITION BY ~ ORDER BY ~)3 NOT IN (null, 1, 2)  Tree Node 문제  True로 여겨질 줄 알았으나 False였다WHERE 조건절에 레코드(튜플) 사용할 수도 있음SELECT student_id, MIN(course_id) AS course_id, gradeFROM EnrollmentsWHERE (student_id, grade) IN                            (SELECT student_id, MAX(grade)                            FROM Enrollments                            GROUP BY student_id)GROUP BY student_id, gradeORDER BY student_idJOIN 쿼리에서 ON절과 WHERE절에 표기하는 것의 차이SubsetComparing SetsINNER JOIN은 ON이 없어도 된다?  INNER JOIN, JOIN 또는 그냥 콤마(,)를 이용해 두 테이블을 조인할 때는 ON이 없어도 된다  (OUTER JOIN은 없으면 에러남)  ON없이 사용하는 경우를 Cartesian Product(곱집합) 이라고 함  A: {1, 2, 3}, B: {x, y} -&amp;gt; FROM A, B를 하면 -&amp;gt; {[1, x], [1, y], [2, x], [2, y], [3, x], [3, y]}  이 개념을 활용하면 LeetCode의 Shortest Distance in a Plane 문제를 풀 수 있다ROW_NUMBER(), RANK(), DENSE_RANK()            ROW_NUMBER()      공동 순위를 무시함 (ex: 1,2,3,4 …)              RANK()      공동 순위만큼 건너뜀 (ex: 1,2,2,4 …)              DENSE_RANK()      공동 순위를 뛰어넘지 않음 (ex: 1,2,2,3 …)      서브쿼리에서는 메인쿼리의 컬럼을 사용할 수 있다SELECT S1.Score, (    SELECT COUNT(DISTINCT Score) FROM Scores WHERE Score &amp;gt;= S1.Score) AS &quot;rank&quot;FROM Scores S1ORDER BY S1.Score DESC컬럼명이 SQL 문법에 포함되는 경우 쌍따옴표로 묶어주면 된다SELECT  score,  RANK() OVER (ORDER BY score) AS &quot;rank&quot;FROM ScoresUNION 말고 UNION ALL도 있다{[1,  ‘kim’]}과 {[1,  ‘kim’]}을 합칠 때  UNION -&amp;gt; {[1,  ‘kim’]}  UNION ALL -&amp;gt; {[1,  ‘kim’], [1,  ‘kim’]}참고  MySQL tutorial: MySQL ROW_NUMBER, This is How You Emulate It  SQL OVER 절  [MySQL] 윈도우함수(Window Function)  MySQL 공식문서: 12.21.1 Window Function Descriptions  LearnSQL: What Is the MySQL OVER Clause?  GeeksforGeeks: MySQL Recursive CTE (Common Table Expressions)  horang, [MySQL] 계층 쿼리 - WITH, WITH RECURSIVE 사용법",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/mysql-ex-series5'> <img src='/images/mysql_logo.png' alt='[MySQL] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-series5'>[MySQL] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] SQL 문제",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-ex-series4",
      "date"     : "Jul 11, 2022",
      "content"  : "Table of Contents  SQLSQLLeetcode: Database 💟 ✅ ❎문제 리스트--------------------------------------------- EASY- Rearrange Products Table 💟 ❎- Customer Who Visited but Did Not Make Any Transactions 💟 ✅ - Bank Account Summary II 💟 ✅- Group Sold Products By The Date 💟 ✅- Immediate Food Delivery I 💟 ✅- Reformat Department Table 💟 ✅- Product Sales Analysis I ✅- Product Sales Analysis II ✅- The Latest Login in 2020 ✅- Employees With Missing Information 💟 ✅- Swap Salary ✅- Game Play Analysis I ✅- Primary Department for Each Employee ✅- Find Customer Referee ✅- Big Countries ✅- Customer Placing the Largest Number of Orders ✅- Customer Order Frequency 💟 ✅- Combine Two Tables ✅- Duplicate Emails ✅- Consecutive Available Seats 💟 ✅- Employees Earning More Than Their Managers ✅- Reported Posts 💟 ✅- Customers Who Never Order ✅- Fix Product Name Format 💟 ✅- Delete Duplicate Emails ✅- User Activity for the Past 30 Days I 💟 ✅- Sales Analysis II 💟 ✅- The Number of Employees Which Report to Each Employee 💟 ✅- Classes More Than 5 Students ✅- Rising Temperature 💟 ✅- Friend Requests I: Overall Acceptance Rate 💟 ✅--------------------------------------------- MEDIUM- Find the Start and End Number of Continuous Ranges 💟 ❎ - Running Total for Different Genders 💟 ❎ - All People Report to the Given Manager ✅- Number of Calls Between Two Persons 💟 ✅- Game Play Analysis III 💟 ✅- Customers Who Bought Products A and B but Not C 💟 ✅- Biggest Window Between Visits 💟 ❎ - Evaluate Boolean Expression 💟 ✅- Tree Node ✅- Highest Grade For Each Student 💟 ✅- Number of Accounts That Did Not Stream ✅- Restaurant Growth 💟 ❎ - Exchange Seats 💟 ✅- Product Price at a Given Date 💟 ❎ - Customers Who Bought All Products ❎- Managers with at Least 5 Direct Reports ✅- Market Analysis I 💟 ✅- Find Interview Candidates 💟 ❎ - Immediate Food Delivery II 💟 ❎ - Shortest Distance in a Plane ✅- Rank Scores 💟 ✅- Movie Rating 💟 ✅- Countries You Can Safely Invest In 💟 ✅- League Statistics 💟 ✅- The Number of Passengers in Each Bus I- Department Highest Salary- Consecutive Numbers- Game Play Analysis IV- Active Users- Nth Highest Salary- Second Degree Follower- Second Highest Salary- Reported Posts II--------------------------------------------- HARD- Sales by Day of the Week- Median Employee Salary- Total Sales Amount by Year- Average Salary: Departments VS Company- Longest Winning Streak- Dynamic Pivoting of a Table- First and Last Call On the Same Day- Tournament Winners- The Number of Passengers in Each Bus II- Human Traffic of Stadium- Department Top Three Salaries- Find Median Given Frequency of Numbers- The Number of Seniors and Juniors to Join the Company- Trips and Users",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-11T21:01:35+09:00'>11 Jul 2022</time><a class='article__image' href='/mysql-ex-series4'> <img src='/images/mysql_logo.png' alt='[MySQL] SQL 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-series4'>[MySQL] SQL 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Git] 깃허브",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/git-github",
      "date"     : "Jul 8, 2022",
      "content"  : "Table of Contents  Github  Personal Access Token  원격 저장소(Remote Repository) 생성  로컬과 원격 저장소 연동을 위한 세팅  원격 저장소를 이용해 협업하기          git pull      git push      Github  깃허브는 깃을 통해 생성한 파일의 여러 버전들을 원격 저장소에 저장하도록 해주는 웹 서비스입니다.  깃허브를 이용하면 버전관리 뿐만 아니라 전세계 개발자들과 함께 코드를 공유하며 협업할 수 있게 됩니다.  깃허브의 중요한 특징 중 하나는 누구든 자신의 커밋을 깃허브에 반영(push)하기 위해서는 먼저, 깃허브의 최신 상태를 자신의 로컬 컴퓨터에 먼저 반영(pull) 해야 한다. 이 과정에서 충돌이 발생할 가능성이 높으며 충돌이 발생한 곳을 잘 병합해야 한다.Personal Access Token원격 저장소(Remote Repository) 생성로컬과 원격 저장소 연동을 위한 세팅  로컬의 깃으로 관리되는 디렉토리를 원격 저장소와 연결    git remote add origin &quot;원격 저장소 주소&quot;              origin은 원격 저장소를 지칭하는 이름. 다른 것으로 해도 되지만 origin이 암묵적 규칙        현재 로컬 브랜치를 원격의 브랜치와 연결    git push -u origin &quot;원격 저장소 브랜치&quot;# 현재 로컬에서 나의 브랜치가 main인 경우# origin의 main 브랜치와 연결하겠다git push -u origin main              이렇게 한 번 연결하고 나면 다음부터는 그냥 git push하면 됨        현재 나의 로컬은 어느 리모트 저장소와 연결되어 있나    git remote      원격 저장소를 이용해 협업하기git pullgit push",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-08T21:01:35+09:00'>08 Jul 2022</time><a class='article__image' href='/git-github'> <img src='/images/git_logo.png' alt='[Git] 깃허브'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git-github'>[Git] 깃허브</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Git] 브랜치",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/git-branch",
      "date"     : "Jul 7, 2022",
      "content"  : "Table of Contents  Branch  Branch와 관련한 기본 명령어          브랜치 생성      브랜치 목록      브랜치 이동      브랜치 이름 변경      브랜치 삭제        Branch 만들어보기  Branch 합치기          git merge      git rebase        Branch 충돌 해결하기          git merge 도중 충돌이 생긴 경우      git rebase 도중 충돌이 생긴 경우        HEAD          git checkout      Branch브랜치는 크게 다음과 같은 2가지의 경우에 사용된다  브랜치는 기존 브랜치에서 독립적인 작업을 하고 싶을 때 -&amp;gt; 신기능 개발  하나의 메인 브랜치를 두고 여러 모습으로 코드를 제공하고 싶을 때 -&amp;gt; 배포용, 테스트용, 개발용여기서는 신기능 개발과 같은 독립적인 작업을 하고 싶은 상황을 예로 들어 실습해 볼 것이다.(그래야 다시 브랜치를 합치는 상황도 예로 만들어 해볼 수 있기 때문에)먼저 브랜치와 관련된 기본적인 명령어들을 한 번 짚고 넘어가자.Branch와 관련한 기본 명령어브랜치 생성git branch &quot;생성할 브랜치명&quot;브랜치 목록git branch브랜치 이동git switch &quot;이동할 브랜치명&quot;브랜치 이름 변경git branch -m &quot;기존 브랜치명&quot; &quot;새 브랜치명&quot;브랜치 삭제git branch -d &quot;삭제할 브랜치명&quot;# 만약 다른 브랜치에는 없고, 오직 &quot;삭제할 브랜치&quot;만 가지는 내용의 커밋이 있으면 -d 옵션으로 삭제 안됨 -&amp;gt; Forcing 느낌의 -D 옵션 필요git branch -D &quot;삭제할 브랜치명&quot;Branch 만들어보기  기존의 main 브랜치  각 yaml 파일의 상단 멤버를 coach로 승격한 add-coach 브랜치  Zebra team을 새로 추가한 new-team 브랜치Branch 합치기git merge  두 브랜치를 하나로 합치는 커밋을 새로 생성  브랜치가 많아지면 히스토리가 복잡해짐    merge는 “기준 브랜치”에서 “대상 브랜치”를 merge 함    git switch &quot;기준 브랜치&quot;git merge &quot;대상 브랜치&quot;        만약 merge한 “대상 브랜치”가 마음에 안들면 git reset –hard “이전 커밋” 명령어로 돌아가도 됨  merge를 완료했으면 “대상 브랜치”는 삭제해도 됨git rebase  하나의 브랜치를 다른 브랜치 뒤에 이어 붙이는 방법  히스토리 내역이 깔끔해짐  협업할 때 이미 공유된 커밋들은 rebase 하지 않는 것이 좋음  rebase는 나 혼자 여러 개 브랜치를 만든 상황에서 하나로 정리할 때 사용하기 좋음  rebase는 merge와 반대로 “대상 브랜치”로 이동 후, “기준 브랜치”를 rebase 함    git switch &quot;대상 브랜치&quot;git rebase &quot;기준 브랜치&quot;        rebase 하고나면 “대상 브랜치”가 “기준 브랜치” 뒤에 잘 붙음. 근데 “기준 브랜치”가 아직 뒤에 머물러 있음. 앞으로 이동해야함    # 왜 merge인지 모르겠지만, 일단 merge를 해야 &quot;기준 브랜치&quot;가 rebase된 커밋까지 이동함# 이런 merge를 Fast-forward라고 함git switch &quot;기준 브랜치&quot;git merge &quot;대상 브랜치&quot;      Branch 충돌 해결하기  두 사람이 만든 커밋이 같은 파일 같은 줄에 서로 다른 내용을 입력했을 때  두 브랜치의 커밋이 같은 파일 같은 줄에 서로 다른 내용을 입력했을 때  해결 방법: 처음부터 방지. 그럴 수 없다면 merge를 한 사람이 직접 수정해야함git merge 도중 충돌이 생긴 경우git rebase 도중 충돌이 생긴 경우위의 예시로 main 브랜치가 변경되었다. 이 부분을 먼저 reset 명령어를 이용해 다시 돌려놓고 rebase 실습을 하겠다.이제 rebase로 브랜치를 합쳐보자.HEAD  현재 속한 브랜치의 최신 커밋git checkout  HEAD를 이동시킴  보통 특정 커밋에서 새로운 브랜치를 분기하고 싶은 경우 사용  또한 로컬내의 Remote branch로 이동할 때는 switch가 아닌 checkout 사용    git checkout origin/main        git checkout을 사용하는 몇 가지 방법    git checkout &quot;이동하고 싶은 커밋 ID&quot;  # 현재 HEAD에서 뒤로 이동하고 싶은만큼 ^를 붙인다git checkout HEAD^^^# ^ 대신 물결표시(~)를 붙여도 된다. 물결표시(~)는 뒤로 이동하고 싶은 칸 수를 숫자로 적을 수 있다git checkout HEAD~~~git checkout HEAD~3        위 그림을 보면 HEAD를 이동하는 즉시 새로운 브랜치가 생기는 것을 알 수 있음  이렇게 하는 이유는 git checkout의 주요 용도인 “새로운 브랜치 분기”의 특성을 최대한 살리기 위함  나는 그냥 과거의 코드 내용을 살피기 위한 목적으로 checkout한건데 이럴 때마다 브랜치 생긴다고? 그럼 너무 부담되는데          걱정할 필요 없음 -&amp;gt; 기존에 존재하던 브랜치중 아무데로 이동하는 즉시 임시 브랜치는 사라짐        새로운 브랜치를 분기해보자",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-07T21:01:35+09:00'>07 Jul 2022</time><a class='article__image' href='/git-branch'> <img src='/images/git_logo.png' alt='[Git] 브랜치'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git-branch'>[Git] 브랜치</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Git] 커밋",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/git-commit",
      "date"     : "Jul 6, 2022",
      "content"  : "Table of Contents  시작하기 전에  git restore  git reset  git revert시작하기 전에시작하기 전에 한 가지 커밋을 추가하도록 하자.현재 yalco 디렉토리의 commit 이력은 다음과 같다.git restore  Working direcotry 또는 Staging Area에 기록된 변경 사항을 되돌리는 방법git reset  특정 커밋으로 돌아간다  soft, mixed, hard 옵션이 있음  git reset –soft “원하는 시점의 커밋ID”          돌아온 시점 이후의 변경사항들이 사라지지 않고 Staging Area에 올라와 있음      당연히 Working directory에도 반영되어 있음      돌아온 시점을 반영하고 싶으면 git restore --staged .를 쓰면 됨      가장 안전한 방법        git reset –mixed “원하는 시점의 커밋 ID”          돌아온 시점 이후의 변경사항들이 사라지지 않고 Working directory에 올라와 있음      돌아온 시점을 반영하고 싶으면 git restore .        git reset –hard “원하는 시점의 커밋 ID”          3가지 영역에서의 상태가 모두 돌아온 시점으로 동기화 되어 있음      가장 과감한 방법        돌아왔는데 다시 앞의 커밋으로 돌아가고 싶으면,          git reflog 통해 앞의 커밋을 나타내는 HEAD${숫자} 또는 커밋id를 확인한다 (빠져나올때는 q)      git reset --hard HEAD${숫자} or 커밋id 를 하면 다시 앞의 커밋으로 돌아오고, 그동안의 커밋도 그대로 tree에 남게된다      git revert  특정 커밋에서 이루어진 작업을 취소하는 커밋을 새로 생성  만약 특정 커밋에서 이루어진 작업을 취소까지는 하는데 일단 Staging Area까지만 완성해놓고 커밋은 미루어 두고 싶다면          git revert –no-commit “되돌릴 커밋 ID”      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-06T21:01:35+09:00'>06 Jul 2022</time><a class='article__image' href='/git-commit'> <img src='/images/git_logo.png' alt='[Git] 커밋'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git-commit'>[Git] 커밋</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Git] 시작하기",
      "category" : "devops",
      "tags"     : "git",
      "url"      : "/git-intro",
      "date"     : "Jul 5, 2022",
      "content"  : "Table of Contents  Git 소개          Github는        Git 초기 설정하기  Git 시작하기          git init      git status      git add                  Git의 3가지 작업 영역                    git commit      git log                  Git Graph Extention을 이용하면 Git log를 시각화 할 수 있다                    .gitignore        git 명령어 정리          하나의 브랜치에서 사용하는 명령어      여러 브랜치 &amp;amp; 협업시 사용하는 명령어        다음 포스트에서는  참고Git 소개깃은 리눅스(Linux)의 아버지 리누스 토발즈(Linus Torvalds)라는 사람이 만들었습니다. 리누스 토발즈는 리눅스를 만든 이후에 리눅스의 버전을 관리하기 위해 BitKeeper라고 하는 툴을 이용하고 있었는데요. 그러다 리눅스 개발자 중 한 명이 BitKeeper의 내부 동작원리를 분석하려고 했다가 리눅스 커뮤니티와 BitKeeper 사이가 안좋아지게 되었습니다. 이 때문에 BitKeeper는 리눅스에게 서비스를 유료화 시켰고 리누스 토발즈는 결국 버전 관리 프로그램을 직접 만들어버렸습니다. 이렇게 만들어진 것이 바로 깃입니다.Github는  깃 허브는 깃을 통해 생성한 파일의 여러 버전들을 원격 저장소에 저장하도록 해주는 웹 서비스입니다.  깃 허브를 이용하면 버전관리 뿐만 아니라 전세계 개발자들과 함께 코드를 공유하며 협업할 수 있게 됩니다.Git 초기 설정하기# 사용자 정보 설정git config --global user.name &quot;(본인 이름)&quot;git config --global user.email &quot;(본인 이메일)&quot;# 설정값 확인하기git config --global user.namegit config --global user.email# defaultBranch명 main으로 설정하기git config --global init.defaultBranch mainGit 시작하기git init# Git으로 관리하고 싶은 폴더git init.git이라는 숨겨진 폴더가 생성되고 Git이 관리하는 모든 버전들이 이 폴더 안에 저장되게 된다.yalco라는 폴더를 Git으로 버전관리 하려고 한다. git init 명령어를 치고나면 Git repository가 .git 폴더에서 초기화 되었다고 말해준다..git 폴더의 역할aaa우선 간단하게 yaml 파일 2개를 만들어보았다.git statusGit 상태를 확인해 보자.git status보면 Untracked files에 방금 생성한 두 파일이 보인다. Untracked라는 말은 Git이 한 번도 관리해본 적 없는 파일이라는 말이다. 위의 두 yaml 파일을 이제 Git에게 맡겨보도록 하자.git add  git add는 Working directory(로컬 디렉토리)의 변경사항을 Staging Area(스냅샷을 찍기위한 무대)로 올림  나는 이러이러한 변경사항을 무대로 올리고(add), 스냅샷으로 찍어서 저장(commit)해둘거야  참고로 매번 git add 명령어의 대상이된 파일만 커밋되는 것은 아님 (tracked된 파일들은 git add 안하더라도 가장 최근에 커밋된 상태로 계속 커밋의 대상이됨. 이 내용은 아래의 카카오 프렌즈를 이용한 그림 참고) (이를 스냅샷 방식이라고 함 &amp;lt;-&amp;gt; 델타 방식과 대비)# 특정 파일만 맡기려는 경우git add &quot;파일명&quot;# 생성/삭제/변경된 모든 파일을 맡기려는 경우git add .new file 이라는 표시가 뜬다. Git 입장에서 이 두 파일은 새로 생성되었기 때문이다.이 상태에서 lions.yaml 파일의 manager를 Conan으로 변경해보자.그리고 다시 Git 상태를 보면Changes not staged for commit 라는 항목에 lions.yaml이 modified 되었다고 뜬다.변경되었지만 아직 Staging Area에 올라가지 않은 경우이다.Untracked files: 한 번도 Git이 관리한 적 없는 파일들Changes to be committed: git add한 파일들Changes not staged for commit: 수정하고 저장만 된 파일들Git의 3가지 작업 영역git add 되지 않은 파일들은 not staged 라고 한다. Git은 이렇게 어떤 파일이 최종적으로 하나의 버전으로 남기까지 3가지 영역을 오간다. 그림으로 보자.아래 그림은 명령어를 통해 파일들이 오고가는 영역을 나타낸 것이다. Github를 포함하면 remote repository라는 영역도 추가된다.다시 돌아가 보면 지금 우리의 파일 상태는 다음과 같았다.여기서 이제 커밋을 해보자.git commit커멋이 되었다고 한다. 커밋이 되면 Local Repository에 하나의 버전으로 남게된다.Local Repository는 git log 명령으로 확인할 수 있다.git log‘Create two files’라는 메시지로 잘 커밋이 되었다. 커밋된 버전은 f0c374e…이라는 식별자를 가지게 된다. 이 식별자를 이용해 나중에 버전을 되돌리거나 하는 등의 작업을 할 수 있다.Git Graph Extention을 이용하면 Git log를 시각화 할 수 있다위의 Uncommitted Changes는 무엇인지 보자.Conan으로 변경하고 커밋에는 포함하지 않았었음 -&amp;gt; 가장 최근 커밋된 모습(현재 Staging Area의 모습)과의 diff를 보여준다이 변경사항도 하나의 버전으로 만들어 보자..gitignore비밀번호와 같은 민감한 정보는 Git에 의해 관리되지 않는 것이 낫다.나중에 Github(Remote Repository)에 커밋을 저장(push)하면 다른 사람들에게 내 비밀번호가 공개될 수 있다.이러한 경우 .gitignore라는 파일을 이용해 관리하고 싶지 않은 파일들을 표기해주면 된다.git 명령어 정리하나의 브랜치에서 사용하는 명령어여러 브랜치 &amp;amp; 협업시 사용하는 명령어다음 포스트에서는지금까지는 계속 새로운 버전을 만들면서 앞으로 가는 방법만 배웠다.다음 포스트에서는 과거로 돌아가거나(reset), 과거의 특정 버전에서 했던 행동을 돌이키는 방법(revert)을 알아보자.참고  Sébastien Dubois, Git concepts for newcomers — Part 2: Git repository, working tree and staging area  Git committing unchanged files",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-05T21:01:35+09:00'>05 Jul 2022</time><a class='article__image' href='/git-intro'> <img src='/images/git_logo.png' alt='[Git] 시작하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git-intro'>[Git] 시작하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 그라파나를 이용해 MySQL 서버 모니터링 하기",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-ex-series3",
      "date"     : "Jul 3, 2022",
      "content"  : "Table of Contents  모니터링 적용 과정          도커 컴포즈 파일에 prometheus, grafana, exporter 컨테이너를 추가한다      prometheus.yml 파일을 작성한다      localhost:3000 을 통해 그라파나 웹 UI에 접속한다      프로메테우스를 데이터 소스로 등록한다      대시보드 템플릿을 다운받는다      대시보드를 등록한다      결과        참고모니터링 적용 과정도커 컴포즈 파일에 prometheus, grafana, exporter 컨테이너를 추가한다version: &#39;3.2&#39;services:  mysql:    hostname: mysql    image: ziontkim0510/mysql-server:1.2    ports:      - 3306:3306    environment:      MYSQL_USER: root      MYSQL_ROOT_HOST: &quot;%%&quot;      MYSQL_DATABASE: test      MYSQL_ROOT_PASSWORD: passwd    command: mysqld      --server-id=1234      --max-binlog-size=4096      --binlog-format=ROW      --log-bin=bin-log      --sync-binlog=1      --binlog-rows-query-log-events=ON    volumes:      - ./dataset:/var/lib/mysql-files  mysqld-exporter:    hostname: mysqld-exporter    image: prom/mysqld-exporter    ports:      - 9104:9104    environment:      DATA_SOURCE_NAME: root:passwd@(mysql:3306)/  prometheus:    hostname: prometheus    image: prom/prometheus    ports:      - 9090:9090    volumes:      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml  grafana:    hostname: grafana    image: grafana/grafana    ports:      - 3000:3000prometheus.yml 파일을 작성한다  프로메테우스가 메트릭을 읽어오도록 잡을 생성한다global:  scrape_interval: 15s  scrape_timeout: 10s  evaluation_interval: 15sscrape_configs:- job_name: prometheus  metrics_path: /metrics  static_configs:  - targets:    - localhost:9090- job_name: mysql  metrics_path: /metrics  static_configs:  - targets:    - mysqld-exporter:9104localhost:3000 을 통해 그라파나 웹 UI에 접속한다프로메테우스를 데이터 소스로 등록한다대시보드 템플릿을 다운받는다  대시보드 템플릿 제공 사이트MySQL Overview 말고도 다양한 대시보드를 템플릿으로 제공해준다. 템플릿은 JSON 파일로 작성돼 있다.JSON 파일을 직접 다운 받아도 되고, 그냥 번호만 알고 있어도 된다.대시보드를 등록한다나는 번호로 그냥 입력했다.아까 등록했던 프로메테우스를 데이터 소스로 사용한다.결과아래는 MySQL InnoDB Metrics 라는 템플릿을 사용했을 때의 모습이다참고  44bits, 그라파나(Grafana)란?  How-To Geek, How to Monitor MySQL Server Activity With a Grafana Dashboard  Grafana 공식문서, Grafana documentation  jssvs, 프로메테우스(prometheus) 구성 및 기본 사용법  alice_k106, 169. [Prometheus] 1편 : Prometheus (프로메테우스) 사용 방법, 기본 개념, 데이터 구조  ExporterHub, MySQL Server Exporter  고양이 중독, 프로메테우스 Aurora MySQL 성능 지표 모니터링 구성  대시보드 템플릿 제공 사이트  Docker Hub, prom/mysqld-exporter 컬렉터 플래그 종류 알 수 있음",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-03T21:01:35+09:00'>03 Jul 2022</time><a class='article__image' href='/mysql-ex-series3'> <img src='/images/mysql_logo.png' alt='[MySQL] 그라파나를 이용해 MySQL 서버 모니터링 하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-series3'>[MySQL] 그라파나를 이용해 MySQL 서버 모니터링 하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 성능 최적화",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-ex-series2",
      "date"     : "Jul 2, 2022",
      "content"  : "Table of Contents  쿼리 분석  시스템 분석          연결(Connection)                  모니터링을 위한 status          튜닝을 위한 system variables                    스토리지 엔진                  MyISAM과 InnoDB 비교          InnoDB 관련 설정                    MySQL 서버        참고# 점검 대상이 되는 것- 연결(Connection)- 쿼리- 인덱스- 스토리지 엔진- 메모리- MySQL 서버쿼리 분석  내가 실행한 쿼리에 대한 분석  쿼리, 인덱스 튜닝에 대한 힌트를 제공  (쿼리 튜닝 방법은 이전 포스트 참고)EXPLAINSELECT...EXPLAIN ANALYZESELECT...# 슬로우 로그 조회하기SELECT * FROM mysql.slow_log;# 또는SELECT start_time, user_host, query_time, lock_time, rows_sent, rows_examined, db, CONVERT(sql_text USING utf8 ) sql_textFROM mysql.slow_log;시스템 분석  시스템/서버 설정에 대한 분석  그라파나와 같은 모니터링 툴을 이용해 상황에 맞게 설정해야함연결(Connection)  MySQL은 오픈 소스임에도 높은 성능을 제공해준다는 점에서 웹 서비스의 DB로 많이 사용됨  웹 서비스에서 사용되는 OLTP는 대용량 데이터 처리보다는 많은 사용자의 동시 접속을 제어하는 것이 중요한 요소임모니터링을 위한 statusSHOW STATUS LIKE &#39;%connect%&#39;;SHOW STATUS LIKE &#39;%client%&#39;;  Aborted_clients: 클라이언트 프로그램이 비 정상적으로 종료된 수  Aborted_connects: MySQL 서버에 접속이 실패된 수  Connections: MySQL 서버에 대한 연결 시도 횟수  Max_used_connections: 최대로 동시에 접속한 수  Threads_cached: Thread Cache의 Thread 수  Threads_connected: 현재 연결된 Thread 수  Threads_created: 접속을 위해 생성된 Thread 수  Threads_running: Sleeping 되어 있지 않은 Thread 수  Aborted_connects / Connections -&amp;gt; 접속 실패율튜닝을 위한 system variablesSHOW VARIABLES LIKE &#39;%connection%&#39;  wait_timeout: 종료전까지 요청이 없이 기다리는 시간 ( TCP/IP 연결, Shell 상의 접속이 아닌 경우 )  thread_cache_size: thread 재 사용을 위한 Thread Cache 수로써, Cache 에 있는 Thread 수보다 접속이 많으면 새롭게 Thread를 생성한다.  max_connections: 최대 동시 접속 가능 수. 늘어나면 날수록 메모리가 고갈되고 스케줄링 오버헤드도 증가스토리지 엔진  default_storage_engine: 기본 스토리지 엔진MyISAM과 InnoDB 비교  InnoDB 의 경우 INSERT 속도가 MyISAM에 비해 느리다InnoDB 관련 설정  innodb_log_group_home_dir: 리두 로그 파일의 위치  innodb_log_files_in_group: 리두 로그 파일의 수  innodb_log_file_size: 리두 로그 파일의 크기          지나치게 크면 복구 시간이 길어지면서 비효율적이 될 수 있음      innodb_buffer_pool_size의 25% 정도 할당        innodb_buffer_pool_size: 버퍼 풀의 크기          버퍼 풀의 크기가 클수록 성능에 유리 (다다익선)        innodb_buffer_pool_instances: 버퍼 풀 인스턴스 수          인스턴스 수를 늘리면 트랜잭션 간의 Lock 경합을 줄일 수 있다      CPU 코어 수가 많은 시스템일수록 인스턴스 수를 늘릴 수 있다고 보면 된다        innodb_flush_log_at_trx_commit: 트랜잭션을 얼마나 엄격하게 지킬 것인지          default 1: 트랜잭션의 ACID 특성을 완전히 준수 -&amp;gt; 트랜잭션이 커밋될 때마다 로그 버퍼상의 로그를 디스크의 로그 파일로 바로 플러시      0: 1초마다 로그 버퍼에 로그가 쓰여지고 플러시 된다. 트랜잭션을 수행했지만 그 사이에 충돌이 나서 로그를 쓰지 못하게 될 경우 데이터 손실이 발생할 수 있다      2: 로그 버퍼에 로그를 쓰는 시점은 각 트랜잭션이 커밋된 후다. 버퍼에 쓰인 로그가 플러시 되는 주기는 0과 같이 1초다.      어떤 모드를 선택하는가에 따라 안전한 트랜잭션 처리 vs 성능 간의 트레이드 오프가 있다        innodb_flush_method: 데이터 플러쉬 방식  innodb_io_capacity: 백그라운드에서 가능한 IOPS(I/O operations per second)의 수          현재 사용하고 있는 디스크의 IOPS 와 유사한 값으로 설정        innodb_flush_log_at_timeout: 메모리 상의 로그 버퍼를 디스크에 있는 로그 파일로 플러시하는 주기MySQL 서버참고  happist, MySql 최적화로 빨라질 사이트 DB 튜닝 방법  OGG, MySQL Tuning point  islove8587, [MySQL]시스템 관련 튜닝  MySQL Performance Tuning pdf",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-02T21:01:35+09:00'>02 Jul 2022</time><a class='article__image' href='/mysql-ex-series2'> <img src='/images/mysql_logo.png' alt='[MySQL] 성능 최적화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-series2'>[MySQL] 성능 최적화</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 쿼리 튜닝/최적화하기",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-ex-series1",
      "date"     : "Jul 1, 2022",
      "content"  : "Table of Contents  Optimization Overview  Things to Consider for Optimization          테이블      인덱스 설계      스토리지 엔진 선택      DB서버 설정        Optimizer          쿼리 실행 절차        Query Optimization          SELECT      WHERE                  IN vs EXISTS                    GROUP BY      JOIN      ORDER BY                  정렬 처리 방법                    DISTINCT 처리      Subquery      Temporary Table        WHERE, GROUP BY, ORDER BY의 인덱스 사용          WHERE 절의 인덱스 사용      GROUP BY 절의 인덱스 사용      ORDER BY 절의 인덱스 사용      WHERE + (GROUP BY | ORDER BY) 의 인덱스 사용      GROUP BY + ORDER BY 의 인덱스 사용      WHERE + GROUP BY + ORDER BY 의 인덱스 사용        INSERT, UPDATE, DELETE문  실행 계획          통계정보      실행 계획 분석                  id          select_type          table          type          possible_keys          key          key_len          ref          rows          extra                      참고Optimization Overview  데이터베이스의 성능은 테이블, 쿼리, 서버 설정과 같은 몇 가지 요소에 따라 달라짐  데이터베이스를 잘 설계함으로써 CPU, I/O 작업과 같은 하드웨어적인 요소들을 최적화 할 수 있음Things to Consider for Optimization테이블  컬럼의 타입  컬럼 개수  제약조건      인덱스    applications that perform frequent updates often have many tables with few columns,  while applications that analyze large amounts of data often have few tables with many columns인덱스 설계  Are the right indexes in place to make queries efficient?스토리지 엔진 선택  In particular, the choice of a transactional storage engine such as InnoDB or a nontransactional one such as MyISAM can be very important for performance and scalabilityDB서버 설정  Does the application use an appropriate locking strategy? For example, by allowing shared access when possible so that database operations can run concurrently, and requesting exclusive access when appropriate so that critical operations get top priority. Again, the choice of storage engine is significant.      The InnoDB storage engine handles most locking issues without involvement from you, allowing for better concurrency in the database and reducing the amount of experimentation and tuning for your code    Are all memory areas used for caching sized correctly? That is, large enough to hold frequently accessed data, but not so large that they overload physical memory and cause paging. The main memory areas to configure are the InnoDB buffer pool and the MyISAM key cacheOptimizer  쿼리를 최적으로 실행하기 위해 각 테이블의 데이터가 어떤 분포로 저장돼 있는지 통계 정보를 참조하여 최적의 실행 계획을 수립쿼리 실행 절차  MySQL 엔진의 SQL파서에서 SQL을 트리 형태로 파싱한다  MySQL 엔진의 옵티마이저에서 파싱 정보를 확인하면서 통계 정보를 활용해 어떤 인덱스를 이용해 테이블을 읽을지 선택한다  2 단계가 완료되면 실행 계획이 만들어진다  스토리지 엔진에 실행 계획대로 레코드를 읽어오도록 요청한다  MySQL 엔진의 SQL 실행기가 스토리지 엔진으로부터 받아온 레코드를 조인하거나 정렬하는 작업을 수행한다Query OptimizationSELECT  컬럼을 선택할 때 * 사용을 피해라  DISTINCT 사용을 피해라 -&amp;gt; 중복 데이터 제거를 위해 테이블 풀 스캔 해야함WHERE  인덱스를 잘 활용해라  % 를 맨 앞에 쓰지마라  함수 사용을 피해라  BETWEEN, IN, &amp;lt;, &amp;gt; 사용을 피해라IN vs EXISTS  IN          IN은 다수의 OR 조건을 사용한 것과 같다      IN 안에 포함된 값들을 모두 비교한다      The IN operator cannot compare anything with NULL values      IN 뒤에 서브 쿼리가 사용되면 서브 쿼리의 SELECT 절 결과를 조건에 사용한다      서브쿼리 결과의 크기가 작으면 IN이 EXISTS보다 더 빠르다        EXISTS          서브 쿼리가 반환하는 행이 있는지를 확인한다      EXISTS 조건절에 하나라도 True로 평가되는 경우에는 더 이상 매칭 여부를 확인하지 않는다 (지연 평가)      EXISTS 다음에 직접적으로 값을 명시할 수 없다. 서브쿼리가 주어져야 한다      (내 생각: EXISTS를 의미있게 사용하려면 서브 쿼리에서 메인 쿼리 테이블의 컬럼을 외래키로 사용하고 있어야할 것 같다)      (그렇지 않으면 EXISTS를 쓰는게 아무 의미가 없어 보인다)      서브쿼리 결과의 크기가 크면 EXISTS가 더 빠르다      For checking against more than one single column, you can use the EXISTS Operator      The EXISTS clause can compare everything with NULLs      GROUP BY  HAVING절은 인덱스를 사용해서 처리될 수 없으므로 굳이 튜닝하려고 할 필요 없다  GROUP BY 작업은 크게 인덱스를 사용하는 경우와 사용할 수 없는 경우(임시 테이블을 사용)  인덱스를 사용할 수 없는 경우, 전체 테이블을 스캔하여 각 그룹마다 새 임시 테이블을 만든 다음 이 임시 테이블을 사용하여 그룹을 검색하고 집계 함수를 적용하는 것          이렇게 인덱스를 사용할 수 없을 때 할 수 있는 최선의 방법은 WHERE절을 이용해 GROUP BY 하기 전에 데이터량을 줄이는 것        인덱스를 잘 설정한다면 임시 테이블을 생성하지 않고 빠르게 데이터를 가져올 수 있다          인덱스를 최대로 활용하기 위해서는 GROUP BY 컬럼과, 인덱스되어 있는 컬럼간의 순서가 중요함      SELECT절에 사용되는 집계함수의 경우 MIN(), MAX()는 인덱스의 성능을 최대로 활용할 수 있도록 함            참고로 MySQL 8.0부터는 GROUP BY를 한다고 해서 암묵적으로 정렬이 이루어지지 않음 -&amp;gt; 정렬 필요하면 명시적으로 ORDER BY 써야함    루스 인덱스 스캔을 사용할 수 있는 경우          루스 인덱스 스캔은 레코드를 건너뛰면서 필요한 부분만 가져오는 스캔 방식      EXPLAIN을 통해 실행 계획을 확인해보면 Extra 컬럼에 ‘Using index for group-by’ 라고 표기됨      MIN(), MAX() 이외의 함수가 SELECT 절에 사용되면 루스 인덱스 스캔을 사용할 수 없음      인덱스와 GROUP BY의 컬럼 순서가 처음부터 일치해야함                  ex. 인덱스가 (col1 col2, col3) 일 때 , GROUP BY col1, col2 과 같아야 함 (GROUP BY col2, col3은 안됨)                    SELECT 절과 GROUP BY 절의 컬럼이 일치해야 함                  ex. SELECT col1, col2, MAX(col3) GROUP BY col1, col2                      타이트 인덱스 스캔을 사용하는 경우          SELECT 절과 GROUP BY 절의 컬럼이 일치하지 않지만, 조건절을 이용해 범위 스캔이 가능한 경우                  ex. SELECT c1, c2, c3 FROM t1 WHERE c2 = ‘a’ GROUP BY c1, c3;          ex. SELECT c1, c2, c3 FROM t1 WHERE c1 = ‘a’ GROUP BY c2, c3;                    JOIN  OUTER JOIN보다는 INNER JOIN이 낫다  드라이빙 테이블(Driving Table)은 레코드 수가 적은 것이 낫다  드리븐 테이블은 인덱스를 가지는 것이 중요하다  JOIN의 조건절로 자주 사용하는 칼럼은 인덱스로 등록한다  INNER JOIN은 순서를 신경쓰지 않고 편하게 사용 가능하다  OUTER JOIN은 결과가 상관 없다면, 인덱스를 가지는 테이블이 드리븐 테이블로 오도록 하는 것이 중요하다ORDER BY  대부분의 SELECT 쿼리에서 정렬은 필수적  정렬을 처리하는 방법은 인덱스를 이용하는 방법과 Filesort라는 별도의 처리를 이용하는 방법정렬 처리 방법  인덱스를 사용한 정렬          인덱스를 이용해 정렬을 하기 위해서는 반드시 ORDER BY의 순서대로 생성된 인덱스가 있어야 함      인덱스를 이용해 정렬이 가능한 이유는 B-Tree 인덱스가 키 값으로 정렬되어 있기 때문        Filesort를 사용한 정렬          인덱스를 사용할 수 없는 경우, WHERE 조건에 일치하는 레코드를 검색해 정렬 버퍼에 저장하면서 정렬을 처리(FIlesort)함                  방법      장점      단점              인덱스 이용      SELECT 문을 실행할 때 이미 인덱스가 정렬돼 있어 순서대로 읽기만 하면 되므로 매우 빠르다      INSERT, UPDATE, DELETE 작업시 부가적인 인덱스 추가/삭제 작업이 필요하므로 느리다              Filesort 이용      인덱스 이용과 반대로 INSERT, UPDATE, DELETE 작업이 빠르다      정렬 작업이 쿼리 실행 시 처리되어 쿼리의 응답 속도가 느려진다      Filesort를 사용해야 하는 경우  정렬 기준이 너무 많아서 모든 인덱스를 생성하는 것이 불가능한 경우  어떤 처리의 결과를 정렬해야 하는 경우  랜덤하게 결과 레코드를 가져와야 하는 경우소트 버퍼  MySQL은 정렬을 수행하기 위해 별도의 메모리 공간을 할당받아서 사용하는데 이 메모리 공간을 소트 버퍼라고 한다  정렬해야 할 레코드의 건수가 소트 버퍼의 크기보다 크다면 어떻게 해야 할까?          정렬해야 할 레코드를 여러 조각으로 나눠서 처리하게 됨. 이 과정에서 임시 저장을 위해 디스크를 사용      일부를 처리하고 디스크에 저장하기를 반복 수행함      정렬 알고리즘  정렬 대상 컬럼과 프라이머리 키만 가져와서 정렬하는 방식          정렬 대상 컬럼과 프라이머리 키 값만 소트 버퍼에 담아 정렬을 수행      그리고 다시 정렬 순서대로 프라이머리 키로 테이블을 읽어서 SELECT할 컬럼을 가져옴      가져오는 컬럼이 두 개 뿐이라 소트 버퍼에 많은 레코드를 한 번에 읽어올 수 있음      단점은 테이블을 두 번 읽어야 함        정렬 대상 컬럼과 SELECT문으로 요청한 컬럼을 모두 가져와서 정렬하는 방식          최신 버전의 MySQL에서 일반적으로 사용하는 방식      SELECT 문에서 요청한 컬럼의 개수가 많아지면 계속 분할해서 소트 버퍼에 읽어와야함      레코드의 크기나 건수가 작은 경우 성능이 좋음      DISTINCT 처리  DISTINCT는 SELECT하는 레코드를 유니크하게 SELECT 하는 것이지, 특정 컬럼만 유니크하게 조회하는 것이 아님  SELECT DISTINCT(first_name), last_name FROM employees;          DISTINCT는 함수가 아니라서 위처럼 괄호를 해놓아도 그냥 무시함      그래서 결론적으로 first_name과 last_name의 조합이 유니크한 레코드를 가져오게 됨        집합 함수(COUNT(), MIN(), MAX()) 같은 집합 함수 내에서 DISTINCT 키워드가 사용된 경우는 함수의 인자로 전달된 컬럼값이 유니크한 것들을 가져온다Subquery  JOIN으로 해결되면 서브쿼리 대신 JOIN을 사용하자  서브쿼리 안에 where절과 group by를 통해 불러오는 데이터양을 감소시킬 수 있습니다  서브쿼리는 인덱스 또는 제약 정보를 가지지 않기 때문에 최적화되지 못한다  윈도우 함수를 고려해보자Temporary Table  Use a temporary table to handle bulk data  Temporary table vs Using index accessWHERE, GROUP BY, ORDER BY의 인덱스 사용WHERE, GROUP BY, ORDER BY 모두 인덱스를 사용하려면 컬럼의 값을 변환하지 않고 사용해야함WHERE 절의 인덱스 사용  작업 범위를 결정하기 위해 인덱스 사용  조건절에 사용된 컬럼과, 인덱스의 컬럼 구성이 왼쪽부터 비교해 얼마나 일치하는가에 따라 달라짐  순서가 다르더라도 MySQL 서버 옵티마이저는 인덱스를 사용할 수 있는 조건들을 뽑아서 최적화를 수행  WHERE 조건절에 사용된 컬럼의 순서는 중요하지 않다 -&amp;gt; WHERE절의 조건 순서는 편하게 나열해도 된다  인덱스의 컬럼 순서는 중요하다  WHERE 조건절에 사용된 컬럼의 구성은 중요하다  지금까지 설명은 WHERE 조건들이 서로 AND 관계일 때를 얘기한 것이다  OR로 연결될 경우          OR에 연결된 모든 컬럼들이 인덱스를 가지는 경우: 인덱스 머지 후 스캔                  (풀 테이블 스캔보단 빠르지만 인덱스 레인지 스캔보다는 느림)                    OR에 연결된 컬럼중 하나라도 인덱스가 없는 경우: 풀 테이블 스캔                  (풀 테이블 스캔 + 인덱스 레인지 스캔보다는 풀 테이블 스캔 1번이 더 빠르므로)                    GROUP BY 절의 인덱스 사용  WHERE절과는 달리 인덱스의 컬럼 순서와 GROUP BY 절의 컬럼 순서, 위치가 같아야 함  인덱스의 컬럼과 GROUP BY 절의 컬럼은 왼쪽부터 일치해야함  GROUP BY 에서 인덱스에 없는 컬럼을 하나라도 사용하면 인덱스 사용 못함ORDER BY 절의 인덱스 사용  GROUP BY와 거의 유사하다  한 가지 추가되는 조건은 인덱스의 각 컬럼의 ASC/DESC 이,  ORDER BY의 각 컬럼의 ASC/DESC 과 모두 같거나 모두 반대인 경우에만 인덱스를 사용할 수 있다WHERE + (GROUP BY | ORDER BY) 의 인덱스 사용  WHERE 절의 컬럼과 GROUP(ORDER) BY 절의 컬럼을 순서대로 나열했을 때 연속하면 둘 다 인덱스 사용 가능  중간에 빠지는 컬럼이 있으면 WHERE 절만 인덱스 사용 (가장 오른쪽 그림 참고)GROUP BY + ORDER BY 의 인덱스 사용  두 절 모두 컬럼의 구성과 순서가 서로 같아야함  둘중 하나라도 인덱스를 사용할 수 없으면 둘 다 인덱스 사용 못함WHERE + GROUP BY + ORDER BY 의 인덱스 사용INSERT, UPDATE, DELETE문  INSERT, UPDATE, DELETE 문은 크게 성능에 대해 고려할 부분이 많지 않음  테이블의 구조가 더 큰 영향을 미침  인덱스를 잘 설계하는 것이 중요          단조 증가하는 컬럼을 프라이머리 키로 선택하는 것이 좋음      (SELECT문은 쿼리의 조건을 기준으로 프라이머리 키를 선택하는 것이 좋음)      보통 INSERT를 위한 프라이머리 키와 SELECT를 위한 프라이머리 키는 성능적인 측면에서 둘 중 하나를 선택해야함      INSERT를 위해서는 보조인덱스가 적을수록 좋음      (SELECT를 위해서는 쿼리에 이용되는 인덱스면 있는게 좋음)      실행 계획  실행 계획을 통해 쿼리의 불합리한 부분을 찾아내고, 더 최적화된 방법을 모색해보자  옵티마이저가 마법처럼 가장 최적의 방법을 찾아내진 못한다 -&amp;gt; 개발자의 역량이 중요  EXPLAIN 명령을 통해 실행 계획 확인-- 실행 계획 출력EXPLAINSELECT...-- 쿼리의 실행 계획과 단계별 소요된 시간 정보 출력EXPLAIN ANALYZESELECT...통계정보  옵티마이저는 실행 계획을 세울 때 통계 정보를 가장 많이 활용  통계 정보는 다음과 같은 것들을 의미          테이블의 전체 레코드 수      인덱스된 컬럼이 가지는 유니크한 값의 개수      각 컬럼의 데이터 분포도(히스토그램)      실행 계획 분석EXPLAINSELECT *FROM employees AS eINNER JOIN salaries AS sON s.emp_no=e.emp_noWHERE first_name=&#39;ABC&#39;;             id      select_type      table      partitions      type      possible_keys      key      key_len      ref      rows      filtered      Extra              1      SIMPLE      e      NULL      ref      PRIMARY,ix_firstname      ix_firstname      58      const      1      100.00      NULL              1      SIMPLE      s      NULL      ref      PRIMARY      PRIMARY      4      employees.e.emp_no      10      100.00      NULL      표의 각 라인은 쿼리 문장에서 사용된 테이블의 개수만큼 출력된다. 실행 순서는 위에서 아래로 순서대로 표시된다            구분      설명              id      select 아이디로 SELECT를 구분하는 번호              select_type      select에 대한 타입              table      참조하는 테이블              type      조인 혹은 조회 타입              possible_keys      데이터를 조회할 때 DB에서 사용할 수 있는 인덱스 리스트              key      실제로 사용할 인덱스              key_len      실제로 사용할 인덱스의 길이              ref      Key 안의 인덱스와 비교하는 컬럼(상수)              rows      쿼리 실행 시 조회하는 행 (통계에 기반한 추정)              filtered      조회되지 않은 행 (통계에 기반한 추정)              extra      추가 정보      id  행이 어떤 SELECT 구문을 나타내는 지를 알려주는 것으로 구문에 서브 쿼리나 UNION이 없다면 SELECT는 하나밖에 없기 때문에 모든 행에 대해 1이란 값이 부여되지만 이외의 경우에는 원 구문에서 순서에 따라 각 SELECT 구문들에 순차적으로 번호가 부여된다.  (SELECT 문장은 하나인데, 여러 개의 테이블이 조인되는 경우에는 id값이 증가하지 않고 같은 id 값이 부여된다).  테이블 접근 순서와는 무관하다select_type  select에 대한 타입            구분      설명              SIMPLE      단순 SELECT (Union 이나 Sub Query 가 없는 SELECT 문)              PRIMARY      메인 쿼리 (첫 번째 쿼리)              UNION      UNION 쿼리에서 Primary를 제외한 나머지 SELECT              DEPENDENT_UNION      UNION 과 동일하나, 외부쿼리에 의존적임 (값을 공급 받음)              UNION_RESULT      UNION 쿼리의 결과물              SUBQUERY      Sub Query 또는 Sub Query를 구성하는 여러 쿼리 중 첫 번째 SELECT문              DEPENDENT_SUBQUERY      Sub Query 와 동일하나, 외곽쿼리에 의존적임 (값을 공급 받음)              DERIVED      SELECT로 추출된 테이블 (FROM 절 에서의 서브쿼리 또는 Inline View)              UNCACHEABLE SUBQUERY      Sub Query와 동일하지만 공급되는 모든 값에 대해 Sub Query를 재처리. 외부쿼리에서 공급되는 값이 동이라더라도 Cache된 결과를 사용할 수 없음              UNCACHEABLE UNION      UNION 과 동일하지만 공급되는 모든 값에 대하여 UNION 쿼리를 재처리      table  행이 어떤 테이블에 접근하는 지를 보여주는 것으로 대부분의 경우 테이블 이름이나 SQL에서 지정된 별명 같은 값을 나타낸다.type  쿼리 실행 계획에서 type 이후의 컬럼은 MySQL서버가 각 테이블의 레코드를 어떤 방식으로 읽었는지를 나타냄  (type, possible_keys, key, ..)  여기서 말하는 방식은 인덱스 레인지 스캔, 인덱스 풀 스캔, 테이블 풀 스캔 등을 의미  보통 WHERE, ON과 같은 조건절에 어떤 컬럼, 어떤 연산자를 사용했는지에 따라 달라짐  만약 type의 결과가 system, const, eq_ref, ref인 경우에는 쿼리문을 튜닝하지 않아도 됨  아래의 접근 방식은 성능이 빠른 순서대로 나열된 것            구분      설명              system      1개 이하의 레코드를 가지는 테이블을 참조하는 경우              const      쿼리의 WHERE 조건절에 프라이머리 키 또는 유니크 키 컬럼을 이용해 1건의 레코드를 반환하는 경우              eq_ref      조인이 포함된 쿼리에서 조인할 때 이용한 컬럼이 프라이머리 키인 경우 -&amp;gt; 1건의 레코드만 존재 -&amp;gt; const만큼 빠름              ref      조인을 할 때 프라이머리 키 또는 유니크 키가 아닌 키로 매칭하는 경우              fulltext      전문 검색 인덱스를 사용해 레코드를 읽는 접근 방법              ref_or_null      ref 와 같지만 null 이 추가되어 검색되는 경우              unique_subquery      WHERE col1 IN (subquery) 에서 subquery의 결과가 col1에 대해 유니크한 경우              index_subquery      WHERE col1 IN (subquery) 에서 subquery의 결과가 col1에 대해 유니크하지 않은 경우              range      인덱스 레인지 스캔. 인덱스를 &amp;lt;, &amp;gt;, IS NULL, BETWEEN, IN, LIKE와 같은 연산자를 이용해 검색할 때              index_merge      두 개의 인덱스가 병합되어 검색이 이루어지는 경우              index      인덱스 풀 스캔. 주어진 조건절로 인덱스 레인지 스캔을 할 수 없는 경우. 인덱스 컬럼만으로 결과를 반환할 수 있는 경우. 테이블 풀 스캔보다는 빠름              ALL      테이블 풀 스캔. 위에서 하나도 해당되는 경우가 없을 때, 테이블 크기가 그렇게 크지 않을 때 사용      possible_keys  쿼리에서 접근하는 컬럼들과 사용된 비교 연산자들을 바탕으로 어떤 인덱스를 사용할 수 있는지를 표시해준다.key  테이블에 접근하는 방법을 최적화 하기 위해 어떤 인덱스를 사용하기로 결정했는지를 나타낸다.  표시된 값이 PRIMARY인 경우에는 프라이머리 키를 사용한다는 의미. 나머지는 인덱스에 부여한 이름key_len  MySQL이 인덱스에 얼마나 많은 바이트를 사용하고 있는 지를 보여준다. MySQL에서 인덱스에 있는 컬럼들 중 일부만 사용한다면 이 값을 통해 어떤 컬럼들이 사용되는지를 계산할 수 있다.ref  동등 비교 조건으로 어떤 값을 참조했는지를 나타낸다  상수값이면 const, 다른 테이블의 컬럼을 참조한 경우 컬럼명으로 표시된다rows  원하는 행을 찾기 위해 얼마나 많은 행을 읽어야 할 지에 대한 예측값을 의미한다.extra  추가 정보            구분      설명              using index      커버링 인덱스라고 하며 인덱스 자료 구조를 이용해서 데이터를 추출              using where      where 조건으로 데이터를 추출. type이 ALL 혹은 Indx 타입과 함께 표현되면 성능이 좋지 않다는 의미              using filesort      데이터 정렬이 필요한 경우로 메모리 혹은 디스크상에서의 정렬을 모두 포함. 결과 데이터가 많은 경우 성능에 직접적인 영향을 줌              using temporary      쿼리 처리 시 내부적으로 temporary table이 사용되는 경우를 의미함      MySQL Explain 상 일반적으로 데이터가 많은 경우 Using Filesort 와 Using Temporary 상태는 좋지 않으며 쿼리 튜닝 후 모니터링이 필요하다.참고  MySQL 공식문서: Optimizing SELECT Statements  MySQL Performance Tuning and Optimization Tips  nomadlee, MySQL Explain 실행계획 사용법 및 분석  ETL 성능 향상을 위한 몇 가지 팁들  전지적 송윤섭시점 TIL, GROUP BY 최적화  SQL 성능을 위한 25가지 규칙  패스트캠퍼스 SQL튜닝캠프 4일차 - 조인의 기본 원리와 활용  취미는 공부 특기는 기록, Nested Loop Join, Driving Table  stackoverflow, Does the join order matter in SQL?  코딩팩토리, [DB] 데이터베이스 NESTED LOOPS JOIN (중첩 루프 조인)에 대하여  고동의 데이터 분석, [SQL] “성능 관점”에서 보는 결합(Join)  고동의 데이터 분석, [SQL] 성능 관점에서의 서브쿼리(Subquery)  GeeksforGeeks, IN vs EXISTS in SQL  인파, [MYSQL] 📚 서브쿼리 연산자 EXISTS 총정리 (성능 비교)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-01T21:01:35+09:00'>01 Jul 2022</time><a class='article__image' href='/mysql-ex-series1'> <img src='/images/mysql_logo.png' alt='[MySQL] 쿼리 튜닝/최적화하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-series1'>[MySQL] 쿼리 튜닝/최적화하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Linux Series [Part4]: 리눅스 쉘 스크립트",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/linux-series4",
      "date"     : "Jun 18, 2022",
      "content"  : "Table of Contents  변수          Local variable      Environment variable      Positional variable      Special variable        조건문          조건문에 들어가는 대상이 숫자인 경우      조건문에 들어가는 대상이 문자열인 경우      조건문에 논리 연산자를 추가할 경우      조건문에 파일 속성이 들어가는 경우        반복문          for      while        함수변수  대부분의 쉘 프로그램은 변수의 타입을 미리 선언하지 않아도됨  변수를 사용하는 방법에는 $var, ${var}, &quot;$var&quot;, &quot;${var}&quot; 방법이 있음  문자열 안에서는 $var, 또는 ${var}로 사용 (ex. echo &quot;var is $var&quot;)  (참고로 문자열 안에 변수를 쓰고 싶으면 무조건 쌍따옴표로 감싼다. 홑따옴표는 안에 다 문자열로 인식)  조건문 안에 변수를 쓸 때는 쌍따옴표로 감싸는 것이 안전 -&amp;gt; &quot;$var&quot;, &quot;${var}&quot; (ex. [[ &quot;$var&quot; -eq 10 ]] )  declare -i var -&amp;gt; var 변수에 정수만 오도록 타입 지정, -a는 배열#!/bin/zshvar_1=&quot;Hello var_1&quot;int_1=1echo $var_1echo ${var_1}echo &quot;$var_1&quot;echo &quot;${var_1}&quot;echo $int_1------------------------------Hello var_1Hello var_1Hello var_1Hello var_11Local variable  함수내에서만 사용하고 싶은 변수는 앞에 local 붙여준다export#!/bin/zshfunction test1(){  x=123  echo &quot;x is $x&quot;}function test2(){  local y=345  echo &quot;y is $y&quot;}test1test2if [ -n &quot;$x&quot; ]; then  echo &quot;x(=$x) is not local variable&quot;else  echo &quot;x is local variable&quot;fiif [ -n &quot;$y&quot; ]; then  echo &quot;y(=$y) is not local variable&quot;else  echo &quot;y is local variable&quot;fi-------------------x is 123y is 345x(=123) is not local variabley is local variableEnvironment variable  쉘 프로그램 전체에서 인식하는 변수  환경 변수 선언 방법: export ENV_A=123  쉘 프로그램을 실행할 때마다 환경 변수로 적용하고 싶다면,          ~/.zshrc 또는 ~/.bashrc 스크립트에 export 구문 추가      (~/.zshrc 또는 ~/.bashrc 스크립트는 쉘 프로그램 시작할 때 자동으로 실행되는 스크립트)      (쉘 프로그램 시작하면 내부적으로 자동으로 source ~/.zshrc 를 실행)        환경 변수에서 삭제하고 싶으면 unset ENV_A  env 명령어 입력하면 쉘 프로그램에 정의된 모든 환경 변수를 볼 수 있음# env_variable_set.sh#!/bin/zshexport ENV_A=123export ENV_B=234# env_variable.sh #!/bin/zshfunction test1(){  echo &quot;environment variable ENV_A is $ENV_A&quot;}source ~/shell_script_test_folder/env_variable_set.shtest1unset ENV_Atest1------------------------------------------------------environment variable ENV_A is 123environment variable ENV_A is Positional variable  함수에 아규먼트 전달한 것처럼, 스크립트도 실행할 때 아규먼트 전달 가능Special variable  $0: 호출된 스크립트 이름  $#: 파라미터의 개수  $*: 파라미터 전체를 하나의 word로 취급  $@: 파라미터 전체를 각각의 word로 취급  $?: exit status  $$: shell의 PID#!/bin/zshecho &quot;The file name is $0&quot;echo &quot;The number of parameters is $#&quot;echo &quot;The parameters are $*&quot;echo &quot;The PID is $$&quot;for arg in $@do  echo &quot;arg is $arg&quot;done-------------------------------------------The file name is ./special_variable.shThe number of parameters is 0The parameters areThe PID is 86466조건문  조건문은 [ 조건문 ], [[ 조건문 ]], (( 조건문 )) 방식으로 표현 (조건문 앞뒤로 무조건 띄어쓰기 해야함)  끝나면 fi 붙여줘야함  조건문 안에서 변수는 쌍따옴표 붙여주는 것이 가장 안전 -&amp;gt; ex. if [[ “$var” -eq 10 ]];if [ 조건문 ]; then    명령문elif    명령문else    명령문fi조건문에 들어가는 대상이 숫자인 경우  -eq, -ne, -gt, -ge, -lt, -le  ex. [ “$var” -ne 0 ]#!/bin/zsha=123b=123if [ $a -eq $b ]; then  echo &quot;a is ${b}&quot;else  echo &quot; a is not ${b}&quot;fi-----------------------------a is 123조건문에 들어가는 대상이 문자열인 경우  ==, !=, &amp;gt;, &amp;lt;, -z, -n  -z: is null  -n: is not null  ex. [[ “$var” &amp;gt; “ABC” ]]  ex. [[ -z “$var” ]]#!/bin/zsha=&quot;&quot;b=&quot;DEF&quot;if [[ &quot;$a&quot; &amp;lt; &quot;$b&quot; ]]; then  echo &quot;a=$a &amp;lt; b=$b&quot;else  echo &quot;a=$a &amp;gt; b=$b&quot;fiif [ -z &quot;$a&quot; ]; then  echo &quot;a=$a is null&quot;else  echo &quot;a=$a is not null&quot;fi------------------------------a= &amp;lt; b=DEFa= is null조건문에 논리 연산자를 추가할 경우  -a (and), -o (or) 를 두 조건문 사이에 넣는 방법  [ 조건문 ] &amp;amp;&amp;amp; [ 조건문 ], [ 조건문 ] || [ 조건문 ]  [[ 조건문 &amp;amp;&amp;amp; 조건문 ]], [[ 조건문 || 조건문 ]]#!/bin/zsha=123b=234c=345if [ &quot;$a&quot; -gt &quot;$b&quot; -a &quot;$a&quot; -gt &quot;$c&quot; ]; then  echo &quot;a=$a is max&quot;elif [ &quot;$b&quot; -gt  &quot;$a&quot; ] &amp;amp;&amp;amp; [ &quot;$b&quot; -gt &quot;$c&quot; ]; then  echo &quot;b=$b is max&quot;elif [[ &quot;$c&quot; -gt &quot;$a&quot; &amp;amp;&amp;amp; &quot;$c&quot; -gt &quot;$b&quot; ]]; then  echo &quot;c=$c is max&quot;fi------------------------------------------------------c=345 is max조건문에 파일 속성이 들어가는 경우  -e: 파일이 존재하는지  -d: 디렉토리인지  -r: 읽기가 허용되는지  -w: 쓰기가 허용되는지  -x: 실행이 허용되는지#!/bin/zshfile=&quot;/etc/passwd&quot;if [ -e &quot;$file&quot; ]; then  echo &quot;$file file exists&quot;else  echo &quot;$file file not exists&quot;fiif [ -d &quot;$file&quot; ]; then  echo &quot;$file is directory&quot;else  echo &quot;$file is not directory&quot;fiif [ -r &quot;$file&quot; ]; then  echo &quot;$file can be read&quot;fiif [ -w &quot;$file&quot; ]; then  echo &quot;$file can be written&quot;fiif [ -x &quot;$file&quot; ]; then  echo &quot;$file can be executed&quot;fi----------------------------------------/etc/passwd file exists/etc/passwd is not directory/etc/passwd can be read반복문for  반복될 값이 변수로 표현된 경우 쌍따옴표를 붙이면 안됨 (붙이면 안에 원소가 하나씩 안 뽑히고 통째로 뽑아냄)for ...do    ...done#!/bin/zshCOLORS=(red yellow green blue)for color in $COLORSdo  echo &quot;color is $color&quot;doneecho &quot;--------------------------------&quot;for i in {0..10..2}do  echo &quot;index is $i&quot;doneecho &quot;--------------------------------&quot;for ((i=0; i &amp;lt;= 5; i++))do  echo &quot;index is $i&quot;doneecho &quot;-------------------------------&quot;x=10((x+=10))echo &quot;$x&quot;echo &quot;-------------------------------&quot;let &quot;x+=100&quot;echo &quot;$x&quot;------------------------------------------------color is redcolor is yellowcolor is greencolor is blue--------------------------------index is 0index is 2index is 4index is 6index is 8index is 10--------------------------------index is 0index is 1index is 2index is 3index is 4index is 5-------------------------------20-------------------------------120while#!/bin/zshi=0while [ &quot;$i&quot; -le 10 ]do  if [ &quot;$i&quot; -eq 4 ]; then    echo &quot;I hate $i&quot;    ((i++))    continue  fi  echo &quot;i is $i&quot;  if [ &quot;$i&quot; -eq 7 ]; then    break  fi  let &#39;i++&#39;  done------------------------------i is 0i is 1i is 2i is 3I hate 4i is 5i is 6i is 7함수  함수는 인자를 받을 수도 있음. 함수 안에서 인자는 $1, $2, …, ${10} 과 같은 변수를 이용해 사용함  return 문으로는 숫자만 리턴 가능. 0을 리턴하는 것을 제대로 실행한 것으로 간주#!/bin/zshfunction test1(){  echo &quot;This is test1 function&quot;}function test2(){  x=&quot;$1&quot;  echo &quot;This is test2 function&quot;  echo &quot;I got argument $x&quot;}function test3(){  echo &quot;This is test3 function&quot;  return 1}test1test2 200test3if [ &quot;$?&quot; -eq 0 ]; then  echo &quot;test3 function is Success&quot;else  echo &quot;test 3 function is Fail&quot;fi------------------------------------------This is test1 functionThis is test2 functionI got argument 200This is test3 functiontest3 function is Fail",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-18T21:01:35+09:00'>18 Jun 2022</time><a class='article__image' href='/linux-series4'> <img src='/images/linux_logo.png' alt='Linux Series [Part4]: 리눅스 쉘 스크립트'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux-series4'>Linux Series [Part4]: 리눅스 쉘 스크립트</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Linux Series [Part3]: 리눅스 파일 시스템",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/linux-series3",
      "date"     : "Jun 18, 2022",
      "content"  : "Table of Contents  리눅스 파일시스템의 주요 디렉토리  파일 타입  스토리지 사용량 명령어          df      du      리눅스 파일시스템의 주요 디렉토리  /bin: 모든 사용자에게 제공하는 명령어 바이너리 (cat, ls, cp 등)  /sbin: 필수 시스템 바이너리 (init, ip, mount 등)  /usr/bin: 대부분의 명령과 실행 파일  /opt: 선택 가능한 응용 프로그램 패키지  /usr/local: 로컬 프로그램이나 환경설정 데이터  /etc: 시스템에 필수적인 시작 및 설정 파일  /var/log: 시스템 로그 파일  /tmp: 재부팅 시 삭제 될 수 있는 임시 파일파일 타입      크게 일반 파일, 디렉토리, 하드링크, 소프트링크    하드링크는 디스크에 저장된 데이터를 직접 가리킴  소프트링크는 원본 파일의 이름을 가리키는 링크  파일의 수정은 원본, 하드링크, 소프트링크 어느 곳에 하더라도 다른 파일에 다 같이 적용됨  원본 파일의 삭제는 하드링크에는 영향을 주지 않음. 소프트링크는 더 이상 가르킬 원본 파일이 없음스토리지 사용량 명령어df  디스크 여유 용량 확인  df -hdu  디스크 사용량 확인  du -h -d 1  h: 사람이 읽기 쉬운 방식, d: depth를 의미. 현재 위치에서 얼마나 파일을 세분화하여 볼 것인지 의미",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-18T21:01:35+09:00'>18 Jun 2022</time><a class='article__image' href='/linux-series3'> <img src='/images/linux_logo.png' alt='Linux Series [Part3]: 리눅스 파일 시스템'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux-series3'>Linux Series [Part3]: 리눅스 파일 시스템</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Linux Series [Part2]: 리눅스 프로세스 관리",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/linux-series2",
      "date"     : "Jun 17, 2022",
      "content"  : "Table of Contents  프로세스와 스레드  프로세스 관련 명령어          ps      htop      kill      프로세스와 스레드  프로세스          실행중인 프로그램      커널 공간에서는 프로세스 디스크립터        스레드          프로세스 내의 실행 흐름 (프로세스당 최소 1개 이상의 스레드가 존재)      리눅스에서는 스레드를 경량 프로세스라고도 함 -&amp;gt; 리눅스에서는 스레드 단위로 스케줄링      스레드는 주소 공간이나 열린 파일 등 여러 자원을 공유할 수 있음 -&amp;gt; 공유자원을 접근할 때 동기화 요구됨      프로세스 관련 명령어ps  프로세스 상태 모니터링  ps aux  a: 모든 프로세스, u: 사용자 지향적 출력, x: 터미널이 없는 프로세스도 표시htop  3초 동안 수집한 리눅프 프로세스 정보를 지속적으로 제공  top 명령어도 있으나 htop이 더 나은 인터페이스 제공  ps, htop과 같은 명령어는 /proc/ 에서 커널 정보를 가져옴kill  프로세스 종료  kill -번호 pid  ex. kill -15 4324  번호는 시그널 의미          15 (SIGTERM): 실행을 완전하게 종료하라는 요청      9 (SIGKILL): 프로세스를 커널 수준에서 종료      2 (SIGINT): 터미널에 Ctrl + C 와 같이 인터럽트 요청      15번 우선적으로 사용. 안되면 9번      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-17T21:01:35+09:00'>17 Jun 2022</time><a class='article__image' href='/linux-series2'> <img src='/images/linux_logo.png' alt='Linux Series [Part2]: 리눅스 프로세스 관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux-series2'>Linux Series [Part2]: 리눅스 프로세스 관리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Linux Series [Part1]: 리눅스 커널",
      "category" : "CS",
      "tags"     : "OS",
      "url"      : "/linux-series1",
      "date"     : "Jun 17, 2022",
      "content"  : "Table of Contents  리눅스          3개의 주요 리눅스 계열        리눅스 커널          리눅스 커널 구조        리눅스 커널의 핵심 역할          하드웨어 관리 및 추상화      프로세스와 스레드 관리      메모리 관리      I/O 관리        컨테이너 기술을 위한 커널의 핵심 기능          cgroup      namespace      union file system      리눅스  전 세계적으로 약 300여 가지의 리눅스 배포판이 존재3개의 주요 리눅스 계열  Debian          Debian, Ubuntu, Mint Linux      오픈 소스, 안정성에 초점        Red Hat/Fedora          Fedora, Red Hat, CentOS, Amazon Linux      엔터프라이즈 서버 환경을 타겟        openSUSE          openSUSE, SUSE LInux Enterprise Server      오픈소스, 안정성에 초점      리눅스 커널  커널: 컴퓨터 운영 체제의 핵심이 되는 컴퓨터 프로그램  운영 체제의 다른 부분, 응용 프로그램에 필요한 여러 가지 서비스를 제공 (open, read, write, close, wait, fork, exec 등)  응용 프로그램이 하드웨어에 직접 접근하도록 허용하는 것은 위험 -&amp;gt; 커널이 중간에서 그 역할을 담당  오픈 소스 유닉스 계열의 모놀리틱 구조리눅스 커널 구조  모노리틱 커널          장점: 구현이 간단하다. 성능이 좋다(커널 문맥에서 많은 부분이 처리되어 시스템 자원을 효율적으로 사용)      단점: 커널 코드에 오류가 생기면 시스템 전체에 영향을 끼친다      리눅스 커널의 핵심 역할  하드웨어 관리 및 추상화  프로세스와 스레드 관리  메모리 관리  I/O 관리하드웨어 관리 및 추상화  서버 관리자 입장에서는 응용 프로그램이 서버의 하드웨어에 직접 접근하지 못하기 때문에 하드웨어 관리에 대한 부담이 없음  사용자 프로그램은 데이터가 디스크의 어느 위치에 있는지 몰라도 됨. 어떤 디바이스(HDD, SSD, USB 등)를 사용하는지 걱정하지 않아도 됨프로세스와 스레드 관리  프로세스(또는 스레드)에 CPU 사용 시간을 적절히 할당해 줌으로써 멀티 태스킹을 가능하게 함메모리 관리  개별 프로세스에 가상의 연속된 메모리 공간을 제공  물리 메모리봅다 더 큰 크기의 프로그램을 실행 가능하도록 해줌I/O 관리  모든 것은 파일(파일 디스크립터)이다  VFS -&amp;gt; 하부 시스템 구성에 상관없이 파일 입출력 제어 가능컨테이너 기술을 위한 커널의 핵심 기능  cgroups  namespaces  union file systemcgroup  프로세스들이 사용하는 시스템 자원을 제한  제한 가능한 자원: CPU, Memory, Network, Block I/O  활용 예시: runc, YARNnamespace  프로세스별로 커널이 사용하는 공간을 분할  논리적 구분 공간 -&amp;gt; 볼 수 잇는 범위를 제한union file system  하나의 디렉토리 위치에 여러 개의 디렉토리를 마운트 -&amp;gt; 하나의 통합된 디렉토리를 제공  CoW(Copy-on-write): 파일 변경이 생기면 새로운 파일을 복사해서 쓴다",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-17T21:01:35+09:00'>17 Jun 2022</time><a class='article__image' href='/linux-series1'> <img src='/images/linux_logo.png' alt='Linux Series [Part1]: 리눅스 커널'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux-series1'>Linux Series [Part1]: 리눅스 커널</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part11]: Python API for Kafka",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series11",
      "date"     : "May 11, 2022",
      "content"  : "Table of Contents  Producer          Keyword Arguments      send()      flush()      close()      metrics()      partitions_for()      bootstrap_connected()        Consumer          Parameters      Keyword Arguments      assignment()      bootstrap_connected()      beginning_offsets(partitions)      end_offsets(partitions)        참고Producer  A Kafka client that publishes records to the Kafka cluster.  The producer consists of a pool of buffer space that holds records that haven’t yet been transmitted to the serverKeyword Arguments  bootstrap_servers: host[:port] string (or list of host[:port] strings) that the producer should contact to bootstrap initial cluster metadata. This does not have to be the full node list. It just needs to have at least one broker that will respond to a Metadata API Request. Default port is 9092          default: localhost:9092        key_serializer (callable): used to convert user-supplied keys to bytes If not None, called as f(key), should return bytes.          default: None        value_serializer (callable): used to convert user-supplied message values to bytes. If not None, called as f(value), should return bytes.          default: None      ex. KafkaProducer(value_serializer=lambda m: json.dumps(m).encode(&#39;ascii&#39;))      ex. KafkaProducer(value_serializer=msgpack.dumps)        acks : The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are common:          0: Producer will not wait for any acknowledgment from the server.The message will immediately be added to the socket buffer and considered sent. No guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures). The offset given back for each record will always be set to -1.      1: Wait for leader to write the record to its local log only.Broker will respond without awaiting full acknowledgement from all followers. In this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost.      all(-1): Wait for the full set of in-sync replicas to write the record.This guarantees that the record will not be lost as long as at least one in-sync replica remains alive. This is the strongest available guarantee.      default: 1            linger_ms (int): The producer groups together any records that arrive in between request transmissions into a single batched request. Normally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. This setting accomplishes this by adding a small amount of artificial delay; that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent so that the sends can be batched together. This can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: once we get batch_size worth of records for a partition it will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition we will ‘linger’ for the specified time waiting for more records to show up. This setting defaults to 0 (i.e. no delay). Setting linger_ms=5 would have the effect of reducing the number of requests sent but would add up to 5ms of latency to records sent in the absence of load. Default: 0.    retries (int): Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. Note that this retry is no different than if the client resent the record upon receiving the error. Allowing retries without setting max_in_flight_requests_per_connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, and the first fails and is retried but the second succeeds, then the records in the second batch may appear first.          default: 0        batch_size (int): Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. A small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely).          default: 16384        compression_type (str): The compression type for all data generated by the producer. Valid values are gzip, snappy, lz4, or None. Compression is of full batches of data, so the efficacy of batching will also impact the compression ratio (more batching means better compression).          default: None            partitioner (callable): Callable used to determine which partition each message is assigned to. Called (after key serialization): partitioner(key_bytes, all_partitions, available_partitions). The default partitioner implementation hashes each non-None key using the same murmur2 algorithm as the java client so that messages with the same key are assigned to the same partition. When a key is None, the message is delivered to a random partition (filtered to partitions with available leaders only, if possible).    max_in_flight_requests_per_connection (int): Requests are pipelined to kafka brokers up to this number of maximum requests per broker connection. Note that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).          default: 5        buffer_memory (int): The total bytes of memory the producer should use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will block up to max_block_ms, raising an exception on timeout. In the current implementation, this setting is an approximation.          default: 33554432 (32MB)      send()  send(topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None)  send() is asynchronous      When called, it adds the record to a buffer of pending record sends and immediately returns. This allows the producer to batch together    topic (str): topic where the message will be published  value (optional): message value. Must be type bytes, or be serializable to bytes via configured value_serializer. If value is None, key is required and message acts as a ‘delete’. See kafka compaction documentation for more details: https://kafka.apache.org/documentation.html#compaction (compaction requires kafka &amp;gt;= 0.8.1)  partition (int, optional): optionally specify a partition. If not set, the partition will be selected using the configured ‘partitioner’.  key (optional): a key to associate with the message. Can be used to determine which partition to send the message to. If partition is None (and producer’s partitioner config is left as default), then messages with the same key will be delivered to the same partition (but if key is None, partition is chosen randomly). Must be type bytes, or be serializable to bytes via configured key_serializer.  headers (optional): a list of header key value pairs. List items are tuples of str key and bytes value.  timestamp_ms (int, optional): epoch milliseconds (from Jan 1 1970 UTC) to use as the message timestamp. Defaults to current time.flush()  flush(timeout=None)      Invoking this method makes all buffered records immediately available to send (even if linger_ms is greater than 0) and blocks on the completion of the requests associated with these records.    timeout (float, optional): timeout in seconds to wait for completion.close()  close(timeout=None)      Close this producer    timeout (float, optional): timeout in seconds to wait for completion.metrics()  Get metrics on producer performance.partitions_for()  partitions_for(topic)  Returns set of all known partitions for the topic.bootstrap_connected()  Return True if the bootstrap is connected.Consumer  Consume records from a Kafka cluster.  It also interacts with the assigned kafka Group Coordinator  The consumer is not thread safe and should not be shared across threadsParameters  *topics (str): optional list of topics to subscribe to. If not set, call subscribe() or assign() before consuming records.Keyword Arguments      bootstrap_servers: ‘host[:port]’ string (or list of ‘host[:port]’ strings) that the consumer should contact to bootstrap initial cluster metadata. This does not have to be the full node list. It just needs to have at least one broker that will respond to a Metadata API Request. Default port is 9092. If no servers are specified, will default to localhost:9092.    group_id (str or None): The name of the consumer group to join for dynamic partition assignment (if enabled), and to use for fetching and committing offsets. If None, auto-partition assignment (via group coordinator) and offset commits are disabled.          default: None            key_deserializer (callable): Any callable that takes a raw message key and returns a deserialized key.        value_deserializer (callable): Any callable that takes a raw message value and returns a deserialized value.    auto_offset_reset (str): A policy for resetting offsets on OffsetOutOfRange errors: ‘earliest’ will move to the oldest available message, ‘latest’ will move to the most recent. Any other value will raise the exception.          default: latest        enable_auto_commit (bool): If True , the consumer’s offset will be periodically committed in the background.          Default: True        auto_commit_interval_ms (int): Number of milliseconds between automatic offset commits, if enable_auto_commit is True.          default: 5000        session_timeout_ms (int): The timeout used to detect failures when using Kafka’s group management facilities. The consumer sends periodic heartbeats to indicate its liveness to the broker. If no heartbeats are received by the broker before the expiration of this session timeout, then the broker will remove this consumer from the group and initiate a rebalance. Note that the value must be in the allowable range as configured in the broker configuration by group.min.session.timeout.ms and group.max.session.timeout.ms.          default: 10000        heartbeat_interval_ms (int): The expected time in milliseconds between heartbeats to the consumer coordinator when using Kafka’s group management facilities. Heartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. The value must be set lower than session_timeout_ms, but typically should be set no higher than 1/3 of that value. It can be adjusted even lower to control the expected time for normal rebalances.          default: 3000        partition_assignment_strategy (list): List of objects to use to distribute partition ownership amongst consumer instances when group management is used.          default: [RangePartitionAssignor, RoundRobinPartitionAssignor]      assignment()  Get the TopicPartitions currently assigned to this consumer.bootstrap_connected()  Return True if the bootstrap is connectedbeginning_offsets(partitions)  Get the first offset for the given partitions  This method may block indefinitely if the partition does not existend_offsets(partitions)  Get the last offset for the given partitions. The last offset of a partition is the offset of the upcoming message, i.e. the offset of the last available message + 1.  This method may block indefinitely if the partition does not exist.참고  kafka-python API",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-11T21:01:35+09:00'>11 May 2022</time><a class='article__image' href='/kafka-series11'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part11]: Python API for Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series11'>Kafka Series [Part11]: Python API for Kafka</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (2)",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-oop_2",
      "date"     : "May 1, 2022",
      "content"  : "Table of Contents  캡슐화  상속  다형성  객체지향 프로그래밍을 위해 알아야할 핵심적인 스킬이 있다  캡슐화, 상속, 다형성캡슐화상속다형성",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-01T21:01:35+09:00'>01 May 2022</time><a class='article__image' href='/python-oop_2'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (2)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-oop_2'>Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (2)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (1)",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-oop_1",
      "date"     : "May 1, 2022",
      "content"  : "Table of Contents  객체 지향 프로그래밍  클래스          객체      속성      메서드        친절한 클래스 되기객체 지향 프로그래밍  프로그램은 “여러 독립된 객체들 간의 상호작용” 이라는 관점  객체는 클래스를 통해 정의된다      파이썬의 모든 것은 객체다    객체지향 프로그래밍의 가장 큰 장점은 유지/보수가 쉽고, 확장성이 뛰어나다는 것이다  거대한 하나의 프로그램을 객체지향 방식으로 프로그래밍 하기 위해서는 지켜야할 원칙이 있다. 이를 SOLID 원칙이라고 한다클래스  객체를 정의하는 방법이다  자동차에 비유하면 클래스는 자동차의 설계도에 해당한다# 텅 비어있는 Car 설계도 (클래스)class Car:    pass객체  클래스를 통해 만드는 실체다. 인스턴스라고도 한다  개별 객체는 모두 메모리에 올라간다car_1 = Car()car_2 = Car()속성  클래스에 정의된 변수를 말한다  자동차의 브랜드, 색깔, 가격과 같은 것      정의된 위치에 따라 클래스 변수, 인스턴스 변수가 있다    클래스 변수          모든 인스턴스가 공유하는 속성      클래스 스코프에서 정의한다        인스턴스 변수          각각의 인스턴스가 독립적으로 가지는 속성      __init__이라는 메서드 스코프에서 정의한다      class HyundaiCar:    brand = &quot;Hyundai&quot;    def __init__(self, color):        self.color = color          brand: 현대 회사에서 생산하는 모든 차량의 브랜드는 Hyundai 이므로 클래스 변수로 정의      color: 차량 색깔은 자동차마다 다르므로 인스턴스 변수로 정의. 인스턴스 초기화 메서드인 __init__() 의 인자로 색깔을 넘겨준다    인스턴스(객체)를 생성해보자car_1 = HyundaiCar(&quot;white&quot;)car_2 = HyundaiCar(&quot;black&quot;)print(car_1.brand) # Hyundaiprint(car_2.brand) # Hyundaiprint(car_1.color) # whiteprint(car_2.color) # black메서드  객체에 정의된 함수를 말한다  객체가 가지는 행동을 정의한다  자동차의 시동 걸기, 달리기, 와이퍼로 유리 닦기, 경적 울리기와 같은 것  속성과 마찬가지로 정의된 위치에 따라 클래스 메서드, 인스턴스 메서드가 있다      그 밖에도 파이썬에서 제공하는 __init__, __str__ 과 같은 다양한 매직 메서드가 있다    클래스 메서드          인스턴스 변수를 사용하지 않고, 클래스 변수만 사용하는 경우 클래스 메서드로 정의할 수 있다      @classmethod 라는 데코레이터를 붙여주면 파이썬이 클래스 메서드로 인식한다      변수의 첫 번째 인자로 cls 를 넘겨줘야 한다      클래스 메서드는 객체를 생성하지 않아도 사용할 수 있다는 특징이 있다        정적 메서드          인스턴스 변수, 클래스 변수 모두 사용하지 않는 경우 정적 메서드로 정의할 수 있다      @staticmethod 라는 데코레이터를 붙여주면 파이썬이 정적 메서드로 인식한다        인스턴스 메서드          인스턴스 변수를 사용하는 경우 인스턴스 메서드로 정의해야 한다      변수의 첫 번째 인자로 인스턴스 자기 자신을 의미하는 self 를 넘겨줘야 한다        매직 메서드          파이썬에서 제공하는 특별한 종류의 메서드로 양쪽에 더블 언더바(__) 로 감싸져 있다 (ex. __init__)      __init__: 인스턴스를 생성한 후 초기화 목적으로 가장 먼저 호출된다      __str__: 인스턴스를 print() 함수로 출력할 때 호출된다      import pandas as pdclass HyundaiCar:    brand = &quot;Hyundai&quot;    # 매직 메서드    def __init__(self, color):        self.color = color        # 클래스 메서드    @classmethod    def get_brand(cls):        print(f&quot;차량 브랜드는 {cls.brand} 입니다&quot;)        @staticmethod    def beep():        print(&quot;빵빵&quot;)        # 인스턴스 메서드    def change_color(self, color):        self.color = color        print(f&quot;차량 색깔이 {self.color}로 변경되었습니다.&quot;)친절한 클래스 되기  Docstring  Type hinting",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-01T21:01:35+09:00'>01 May 2022</time><a class='article__image' href='/python-oop_1'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (1)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-oop_1'>Python Advanced Series [Part5]: 파이썬으로 배우는 객체지향 프로그래밍 (1)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 파티션",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-thoery-series6",
      "date"     : "Apr 17, 2022",
      "content"  : "Table of Contents  파티션이란  이용 사례  파티션의 종류  사용시 주의 사항  참고파티션이란  파티션은 테이블을 논리적으로는 하나의 테이블, 물리적으로는 여러 개의 테이블로 분리해 관리하도록 도와주는 기능  파티션 기능을 대용량 테이블에 사용한다고 해서 무조건 성능이 빨라지는 것은 아님 -&amp;gt; 쿼리에 따라 다름  파티션은 실무에서 MySQL의 부하를 줄이기 위해서(성능 향상) 사용할 것을 적극 권장이용 사례  대표적으로 테이블이 너무 커서 인덱스의 크기가 물리적인 메모리보다 훨씬 클 때  데이터 특성상 주기적인 삭제 작업이 필요할 때파티션의 종류  Range: 범위(날짜 등)를 기반으로 파티션을 나눈다. // 가장 흔히 사용  List: 코드나 카테고리 등 특정 값을 기반으로 파티션을 나눈다.  Hash: 설정한 HASH 함수를 기반으로 파티션을 나눈다. // Range, List 사용이 애매할 때 사용  Key: MD5() 함수를 이용한 HASH 값을 기반으로 파티션을 나눈다. // HASH보다 균등-- Range 사용 예시CREATE DATABASE testDB;USE testDB;CREATE TABLE userTable (    userID CHAR(12) NOT NULL    birthYear INT NOT NULL )    PARTITION BY RANGE(birthYear) (PARTITION part1 VALUE LESS THAN (1970),PARTITION part2 VALUE LESS THAN (1980),PARTITION part3 VALUE LESS THAN (1990),PARTITION part4 VALUE LESS THAN MAXVALUE);위와 같이 birthYear을 조건절의 컬럼으로 사용해 쿼리할 때, 파티셔닝을 잘한 경우 일부 파티션만 가져올 수 있도록 해준다.사용시 주의 사항  파티션을 많이 나누는 경우에는 시스템 변수 open_file_limit(동시에 열 수 있는 파일 수의 Max)를 높게 수정  파티션 키는 유니크 인덱스(프라이머리 키 포함) -&amp;gt; 유니크 키 값이 주어졌을 때, 해당 레코드의 파티션이 무엇인지 알 수 있어야함참고  Real MySQL 8.0 (2권) 책  Jae Honey, MySQL - 파티션(Partition), 테이블 분할",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-17T21:01:35+09:00'>17 Apr 2022</time><a class='article__image' href='/mysql-thoery-series6'> <img src='/images/mysql_logo.png' alt='[MySQL] 파티션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-thoery-series6'>[MySQL] 파티션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 데이터베이스 모델링 (이론)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-thoery-series5",
      "date"     : "Apr 16, 2022",
      "content"  : "Table of Contents  데이터베이스 모델링  ERM: Entity Relationship Model  데이터베이스 모델링 단계          비즈니스 룰      Entity, Attribute, Relationship 후보 파악      Entity간 관계 파악      정규화      물리적 모델링(네이밍, 데이터 타입, 제약조건)      ERM으로 표현      데이터베이스 모델링데이터베이스에 데이터를 어떤 식으로 저장할지 계획하는 것은 굉장히 중요합니다. 데이터베이스를 제대로 설계하지 않으면 데이터 삽입, 업데이트, 삭제시 이상 문제가 생길 수 있습니다.삽입: 새로운 데이터를 자연스럽게 저장할 수 없는 문제 (ex. 유저테이블의 주소지 추가)업데이트: 업데이트할 때 정확성을 지키기 어려워지는 문제삭제: 원하는 데이터를 자연스럽게 삭제할 수 없는 문제데이터베이스에 이런 이상 문제가 생기게 되면 새로운 데이터베이스로 데이터를 옮겨야 하는 상황이 발생하게 되고, 이런 문제는 비용적인 부담이 발생하는 일이기 때문에 최대한 사전에 방지하는 것이 좋습니다.데이터베이스 모델링은 크게 세 가지 단계로 구분할 수 있습니다.비즈니스 모델링: 비즈니스 룰을 정의한다 (ex. 유저는 하나의 주문에 하나의 리뷰만 달 수 있다)논리적 모델링: 어떤 것을 테이블로 하고, 어떤 것을 컬럼으로 하고 테이블끼리의 관계를 어떻게 정의할지 정의한다물리적 모델링: 테이블명, 컬럼명, 데이터 타입, 제약조건을 정의한다 (Schema, Table, Index 정의)ERM: Entity Relationship Model데이터베이스를 모델링할 때에는 기존의 익숙한 모델 구조인 관계형 모델(Relational Model) 보다는 개체 관계 모델(ERM)을 주로 사용합니다. ERM의 예시는 아래와 같습니다.ERM은 기존 관계형 모델의 로우(Row)를 엔티티(Entity), 컬럼을 어트리뷰트(Attribute)로 표현하고 테이블간의 관계를 릴레이션쉽(Relationship)으로 나타냅니다.두 엔티티간의 관계를 정의하는 방법에는 다음과 같은 경우가 있습니다.예시는 다음과 같습니다.데이터베이스 모델링 단계데이터베이스를 모델링 하는 단계는 다음과 같습니다.1. 비즈니스 룰을 정한다2. Entity, Attribute, Relationship이 될 수 있는 후보를 정한다3. 비즈니스 룰을 통해 Entity간 관계를 파악한다4. 정규화한다5. 네이밍, 데이터 타입, 제약조건을 정한다6. ERM으로 나타낸다예시를 가지고 데이터베이스를 처음부터 간단하게 모델링 해보도록 하겠습니다.비즈니스 룰당근마켓  유저는 자신의 지역을 등록해야 한다  유저는 자신의 지역 근처에서 물건을 사거나 팔 수 있다  물건에 좋아요 표시를 할 수 있다  물건을 살 때는 채팅창을 열어야 한다  판매자는 물건에 대해 여러 채팅창을 가질 수 있다  구매자는 물건에 대해 한 개의 채팅만 가질 수 있다  물건은 [판매중, 예약중, 거래완료] 중 하나의 상태를 갖는다  구매자와 판매자는 거래를 한 경우 서로에 대해 평가를 할 수 있다Entity, Attribute, Relationship 후보 파악  User: id, name, gender, age, phone_numer, creation_date  Product: id, title, category, seller_id, price, status  Region: id, user_id, region_name  Chatting: id, buyer_id, seller_id, product_id, content  Like: id, buyer_id, product_id  Transactional Information: id, buyer_id, seller_id, product_id, creation_date  Review: id, buyer_id, seller_id, star, contentEntity간 관계 파악  User와 Product          판매자(User)는 여러 개의 물건(Product)을 가질 수 있지만, 물건은 여러 판매자를 가질 수 없다 -&amp;gt; 1:N      판매자는 반드시 물건을 등록할 필요는 없지만, 등록된 물건은 반드시 판매자가 있어야 한다        User와 Region          사용자(User)는 여러 지역을 가질 수 있고, 지역(Region)도 여러 사용자를 포함할 수 있다 -&amp;gt; N:M      사용자는 반드시 지역 정보를 가져야 하고, 지역은 사용자가 없어도 된다        User와 Chatting          판매자와 채팅                  판매자(User)는 여러 개의 채팅(Chatting)을 가질 수 있지만, 채팅은 한 명의 판매자만 가질 수 있다 -&amp;gt; 1:N          판매자는 채팅이 없어도 되지만, 채팅은 반드시 판매자가 있어야 한다                    구매자와 채팅                  구매자(User)는 여러 개의 채팅(Chatting)을 가질 수 있지만, 채팅은 한 명의 구매자만 가질 수 있다 -&amp;gt; 1:N          구매자는 채팅이 없어도 되지만, 채팅은 반드시 구매자가 있어야 한다                      …정규화데이터베이스에서 데이터를 삽입/업데이트/삭제할 때 생길 수 있는 문제를 사전에 방지하기 위해 실시하는 작업1NF: 모든 컬럼 값은 나눌 수 없는 단일값이 되어야 한다2NF: 1NF + 모든 non-prime attribute는 candidate key 전체에 함수 종속성이 있어야 한다    (Non-prime attrbute중 2NF를 만족하지 않는 속성은 테이블에서 분리한다)3NF: 2NF + 모든 attribute는 오직 primary key에 대해서만 함수 종속성을 가져야 한다    (모든 attribute는 직접적으로 테이블 엔티티에 대한 내용이어야 한다)    (이행적 함수종속성을 없애야 한다)  1NF          모든 컬럼 값은 나눌 수 없는 단일값이 되어야 한다      어떤 채용 공고글에서 요구하는 스킬이 리스트 형태([MySQL, Python, Pytorch]로 되어 있으면 skiils를 새로운 테이블로 만들자        2NF          모든 non-prime attribute는 candidate key 전체에 함수 종속성이 있어야 한다      함수 종족성: x, y 속성이 있을 때, y = f(x)라는 관계가 성립하는 경우      Candidate Key: 하나의 로우를 특정 지을 수 있는 속성(attribute)들의 최소 집합      Prime Attribute: Candidate Key에 포함되는 모든 속성        3NF          모든 attribute는 직접적으로 테이블 엔티티에 대한 내용이어야 한다      물리적 모델링(네이밍, 데이터 타입, 제약조건)ERM으로 표현",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-16T21:01:35+09:00'>16 Apr 2022</time><a class='article__image' href='/mysql-thoery-series5'> <img src='/images/mysql_logo.png' alt='[MySQL] 데이터베이스 모델링 (이론)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-thoery-series5'>[MySQL] 데이터베이스 모델링 (이론)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 인덱스 (이론)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-theory-series4",
      "date"     : "Apr 15, 2022",
      "content"  : "Table of Contents  디스크 읽기 방식          HDD와 SSD      랜덤 I/O와 순차 I/O        인덱스란?  B-Tree 인덱스          InnoDB 스토리지 엔진의 데이터 파일      데이터의 Insert, Update, Delete시 인덱스에서의 동작                  인덱스 키 추가          인덱스 키 삭제          인덱스 키 검색                    B-Tree 인덱스 사용에 영향을 미치는 요소                  인덱스 키 값의 크기          B-Tree 깊이          선택도(유니크한 값의 수)          읽어야 하는 레코드의 건수          인덱스 적용 기준                    데이터의 Select시 인덱스에서의 동작                  인덱스 레인지 스캔          인덱스 풀 스캔          테이블 풀 스캔          루스 인덱스 스캔          인덱스 스킵 스캔                      클러스터링 인덱스          클러스터링 인덱스의 장점과 단점      프라이머리 키 사용시 주의 사항      보조 인덱스        참고인덱스는 데이터베이스 쿼리의 성능과 관련해서 빼놓을 수 없는 중요한 부분입니다. 인덱스에 대한 지식은 개발자나 관리자 모두에게 중요한 부분이며, 쿼리 튜닝의 기본이 됩니다.디스크 읽기 방식보통 컴퓨터에서 가장 큰 성능 저하는 디스크 I/O에서 발생합니다. 따라서 데이터베이스의 성능 튜닝은 어떻게 디스크 I/O을 줄이느냐가 관건일 때가 상당히 많습니다.HDD와 SSD데이터베이스 서버에서 순차 I/O 작업보다는 랜덤 I/O이 차지하는 비중이 훨씬 큽니다. 그리고 이러한 랜덤 I/O의 속도를 훨씬 높여준 장치가 바로 SSD입니다. 이러한 이유로 DBMS용 스토리지에 SSD는 최적의 장치라고 할 수 있습니다.랜덤 I/O와 순차 I/O랜덤 I/O은 데이터의 개수만큼 데이터의 위치를 찾아야 하고, 순차 I/O은 한 번만 데이터의 위치를 찾으면 되기 때문에 랜덤 I/O으로 인한 작업 부하가 훨씬 더 크게 발생합니다. HDD는 매번 데이터의 위치를 찾기 위해 디스크 헤드를 움직여야 하기 때문에 랜덤 I/O의 작업 부하는 훨씬 더 커지게 됩니다. SSD는 디스크 원판이 아닌 플래시 메모리를 사용하기 때문에 차이가 없을 것 같지만 마찬가지로 랜덤 I/O에서 성능이 저하됩니다.그래서 일반적으로 쿼리 튜닝의 목적은 랜덤 I/O의 회수를 줄이는 것입니다. 여기서 랜덤 I/O을 줄인다는 것은 쿼리를 처리하는 데 꼭 필요한 데이터만 읽도록 쿼리를 개선하는 것을 의미합니다. 그리고 랜덤 I/O을 줄이기 위해 저희는 인덱스라는 것을 활용할 것입니다.DBMS에서는 랜덤 I/O이 자주 발생하고, 쿼리의 성능을 높이기 위해서는 랜덤 I/O을 줄여야 한다.  이렇게 랜덤 I/O을 줄이기 위해 DBMS에서는 인덱스를 사용한다.인덱스란?보통 인덱스를 설명할 때 책 맨 끝에 있는 색인을 예시로 많이 사용합니다. 예를 들어 책에서 ‘무궁화’라는 단어를 찾고 싶다면 저희는 책 페이지에서 내용을 하나씩 찾아보기 보다는 색인에서 ‘무궁화’라는 단어가 포함된 페이지의 쪽수를 찾게 될 겁니다.DBMS도 데이터베이스 테이블의 모든 데이터를 검색해서 원하는 레코드(Row)를 가져오려면 시간이 오래 걸릴 것입니다. 그래서 컬럼의 값과 그 값을 가지는 레코드가 저장된 주소를 매핑한 인덱스를 만들어 두는 것입니다.책의 색인: 단어 - 책 페이지 매핑DBMS의 인덱스: 컬럼 값 - 값을 가지는 레코드(Row)의 주소 매핑그리고 인덱스의 중요한 특성 중 하나는 키(컬럼 값)가 정렬되어 있다는 것입니다. 예를 들어 테이블의 사람 이름을 나타내는 name이라는 컬럼으로 인덱스를 만들었다고 했을 때, name 값을 정렬하여 각각의 주소를 매핑해 인덱스를 만들게 됩니다.아래는 인덱스 페이지의 예시로 다음과 같이 name이 알파벳 순으로 정렬되어 있습니다.            name      레코드 주소              Alice      14345342              Bob      61345549              Carl      24641345              Doson      41127651      이렇게 인덱스 페이지를 정렬했을 때 장단점이 있습니다.  장점: 정렬되어 있기 때문에 인덱스(컬럼 값)를 빠르게 찾고 결과적으로 데이터를 빠르게 읽어온다  단점: 컬럼 값에 INSERT, UPDATE, DELETE가 발생할 때마다 인덱스 파일을 정렬하기 때문에 저장 속도가 느리다결론적으로 DBMS에서 인덱스는 데이터의 저장(INSERT, UPDATE, DELETE) 성능을 희생하고 데이터의 읽기 속도를 높여주게 됩니다. 그래서 인덱스 파일을 하나 더 추가할지 말지는 데이터의 저장 속도를 얼마만큼 희생하여, 그 결과로 읽기 성능을 얼마나 더 빠르게 만들지에 따라 결정되게 됩니다.위에서 인덱스 페이지가 정렬되어 있어 인덱스 값을 빨리 찾을 수 있다고 했습니다. 정렬되어 있다는 것의 이점은 탐색 알고리즘에서 선형 탐색이 아닌 이진 탐색과 비슷하지만 이보다 더 빠른 탐색 알고리즘을 사용할 수 있다는 것입니다.인덱스 파일은 이진 탐색을 지원하는 이진 트리 자료 구조와 비슷하지만 더 빠른 탐색을 가능하게 하는 Balanced Tree(B-Tree)라는 자료구조로 구현되어 있습니다.이외에도 대표적으로 Hash Table 자료구조를 이용한 방법도 있으며 최근에는 Fractal-Tree, Merge-Tree와 같은 알고리즘을 사용하는 DBMS도 개발되고 있습니다.B-Tree 인덱스  Balanced Tree  가장 일반적으로 사용되는 인덱스 형태  컬럼 값을 변형하지 않고 원래의 값을 이용해 인덱싱  B-Tree를 응용한 많은 자료구조가 등장Hash Table 인덱스  컬럼 값을 해시한 결과를 인덱스로 사용  매우 빠른 검색 지원  해시값을 인덱스로 사용해 컬럼 값의 일부만 검색하거나 범위를 검색할 때는 사용 불가  주로 메모리 기반의 데이터베이스에서 많이 사용B-Tree 인덱스디스크 기반 스토리지는 하드웨어적 특성으로 I/O의 가장 작은 단위가 페이지가 됩니다. 디스크 기반 자료구조는 디스크 접근 횟수를 최소화 하기 위해 데이터의 지역성을 높여야 합니다. 이렇게 지역성을 높이는 방법은 페이지를 만들 때 비슷하게 참조되는 데이터를 페이지로 만드는 것입니다. 비슷하게 참조되는 데이터를 페이지로 만듦으로써 데이터를 조회할 때 페이지를 넘나드는 포인터를 최소화할 수 있습니다.이렇게 만든 페이지가 B-트리에서 노드가 됩니다. 다시 말해 B-트리는 페이지 기반 자료구조입니다.(비슷하게 참조되는 키값은 사실상 정렬했을 때 인근 키값들)이제 키 값을 기준으로 그 키 값을 가지는 실제 파일 주소를 어떻게 찾는지 예시를 들어 설명해보겠습니다.참고로 여기서 설명하는 B-트리는 엄밀히 B+ 트리입니다 B-트리와 B+ 트리의 차이는 B-트리는 루트 노드, 브랜치 노드, 리프 노드 모든 레벨에 값을 저장하는 것이 가능하고, B+ 트리의 경우 브랜치 노드에 리프노드에 저장된 값을 찾는데 필요한 구분(seperator) 키만 저장합니다.MySQL에서는 MyISAM, InnoDB 모두 B+ 트리 형태로 되어 있고, 편의상 그냥 B-트리라고 하기 때문에 B-트리라고 하더라도 마음속으로는 B+ 트리 형태를 떠올리면 되겠습니다.참고로 이러한 B+ 트리는 리프 노드에만 값을 저장하기 때문에 Insert, Update, Delete 연산은 리프노드에만 영향을 미칩니다. 상위 레벨의 노드는 트리 균형을 위해 분할 혹은 병합이 일어날 때만 영향을 받습니다.  인덱스의 리프 노드는 항상 실제 데이터 레코드를 찾아가기 위한 주소값을 가짐  인덱스의 키 값은 모두 정렬돼 있지만, 데이터 파일의 레코드는 정렬돼 있지 않고 임의의 순서로 저장돼 있음 (InnoDB는 예외)  InnoDB엔진의 데이터 파일 레코드는 클러스터되어 디스크에 저장되므로 기본적으로 프라이머리 키 순서로 정렬되어 저장됨  인덱스는 테이블의 키 컬럼만 가지고 있으므로 나머지 컬럼을 읽으려면 데이터 파일에서 해당 레코드를 찾아야 함InnoDB 스토리지 엔진의 데이터 파일InnoDB 스토리지 엔진은 인덱스 뿐만 아니라 데이터 파일도 B-Tree와 같은 형태로 데이터가 정렬되어 저장되어 있습니다. 이를 클러스터링 인덱스라고 합니다. 그래서 MySQL에서 클러스터링 인덱스는 사실 데이터 파일 자체를 의미합니다.  InnoDB 엔진에서는 인덱스를 통해 레코드를 읽을 때 파일을 바로 찾아가지 못함  인덱스에 저장돼 있는 프라이머리 키 값을 이용해 프라이머리 키 인덱스를 한 번 더 검색한 후,  프라이머리 키 인덱스의 리프 페이지에 저장돼 있는 레코드를 읽음  즉 InnoDB 엔진에서는 모든 세컨더리 인덱스 검색에서 데이터 레코드를 읽기 위해서는,  반드시 프라이머리 키를 저장하고 있는 B-Tree를 다시 한번 검색해야 함  InnoDB 엔진에서 데이터 파일은 프라이머리 키 인덱스 자체  세컨더리 인덱스의 리프 노드에 레코드 주소가 아닌 프라이머리 키를 저장하고 있으면, 데이터 삽입으로 페이지 분할이 일어나도, 세컨더리 인덱스의 리프 노드는 메모리 주소가 아니라 프라이머리 키를 가지고 있기 때문에, 세컨더리 인덱스에 값을 변경하지 않아도 된다참고로 MyISAM 스토리지 엔진의 인덱스와 데이터 파일간의 관계는 다음과 같습니다.한 가지 짚고 넘어갈 점은 위와 같이 인덱스 키 하나에 매칭되는 레코드 하나가 매핑되는 경우는 프라이머리 키 인덱스나 유니크 키 인덱스 같은 특별한 경우이고 보통은 인덱스 키 하나에 여러 개의 레코드가 매핑된다.근데 인덱스를 이용해서 레코드를 찾을 때는 테이블에서 그냥 레코드를 찾는 것보다 5배 정도 더 큰 비용이 들어간다. 여기서 5배는 복합적인 요인이 합해져서 5배 정도되는 것 같다.- 우선 인덱스 키를 가지고 해당 리프 노드를 찾아가는데 걸리는 시간- 그 리프 노드에서 해당 키를 찾는데 걸리는 시간 (이 시간은 리프 노드의 사이즈에 따라 또 달라짐)- 그리고 키에 매핑되는 데이터의 실제 주소를 알았을 때 데이터 파일의 해당 주소로 찾아가는데 걸리는 시간. (이 경우 한 건 한건이 모두 랜덤 I/O임)- 그리고 조건절에 사용된 인덱스가 = 과 같은 것이 아니라 범위 조건절이면 리프 노드간 이동도 해야함데이터의 Insert, Update, Delete시 인덱스에서의 동작  테이블의 레코드를 저장하거나 변경하는 경우 인덱스 키 추가나 삭제 작업이 발생인덱스 키 추가  B-Tree에 저장될 때는 저장될 키 값을 이용해 B-Tree상의 적절한 위치를 검색해야 함  위치가 결정되면 레코드의 키 값과 대상 레코드의 주소 정보를 B-Tree의 리프 노드에 저장  리프 노드가 꽉 차서 더 저장할 수 없을 때는 리프 노드가 분리돼야 하는데, 이는 상위 브랜치 노드까지 처리의 범위가 넓어짐  이러한 이유로 B-Tree는 상대적으로 쓰기 작업(새로운 키를 추가하는 작업)에 비용이 많이 들어감인덱스 키 삭제  해당 키 값이 저장된 B-Tree의 리프 노드를 찾아서 그냥 삭제 마크만 하면 작업이 완료됨  인덱스 키 삭제로 인한 마킹 작업 또한 디스크 쓰기가 필요하므로 디스크 I/O이 필요한 작업인덱스 키 검색  인덱스를 구축하는 이유는 빠른 검색  인덱스 트리 탐색은 SELECT문 뿐만 아니라, UPDATE나 DELETE를 처리하기 위해 레코드를 검색하는 경우에도 사용됨  함수나 연산을 수행한 결과로 정렬한다거나 검색하는 작업은 B-Tree의 장점을 이용할 수 없음  InnoDB 엔진의 경우, 검색을 수행한 인덱스를 잠근 후 테이블의 레코드를 잠그는 방식으로 구현돼 있음  따라서 UPDATE나 DELETE문이 실행될 때 테이블에 적절히 사용할 인덱스가 없으면 불필요하게 많은 레코드를 잠금B-Tree 인덱스 사용에 영향을 미치는 요소  B-Tree 인덱스는 인덱스를 구성하는 컬럼의 크기와 레코드의 건수, 그리고 유니크한 인덱스 키 값의 개수 등에 의해 검색이나 변경 작업의 성능이 영향을 받음인덱스 키 값의 크기  InnoDB 스토리지 엔진은 디스크에 데이터를 저장하는 가장 기본 단위를 페이지라고 함  페이지: 디스크의 모든 읽기 및 스기 작업의 최소 작업 단위. 또한 InnoDB 엔진의 버퍼 풀에서 버퍼링하는 기본 단위  인덱스도 결국은 페이지 단위로 관리됨. 루트와 브랜치, 리프 노드를 구분하는 기준이 바로 페이지 단위  B-Tree는 자식 노드를 몇 개까지 가질 수 있을까? -&amp;gt; 인덱스의 페이지 크기와 키 값의 크기에 따라 결정됨  InnoDBB 엔진의 페이지 크기를 innodb_page_size 시스템 변수를 이용해 4KB~64KB로 선택 가능 기본값은 16KB  인덱스의 키를 16B, 값에 해당하는 자식 노드 주소를 12B라 하면, 인덱스 페이지당 16*1024/28 = 585개의 키 저장 가능  최종적으로 이 경우에 자식 노드를 585개 가질 수 있는 B-Tree가 됨  인덱스 키 값이 커지면 자식 노드의 수는 줄어듬 -&amp;gt; B-Tree의 탐색 횟수 증가 -&amp;gt; SELECT 쿼리가 느려짐B-Tree 깊이  위에서 키 값의 크기가 커지면 B-Tree의 깊이가 증가함을 확인  B-Tree의 깊이는 MySQL에서 값을 검색할 때 몇 번이나 랜덤하게 디스크를 읽어야 하는지와 직결되는 문제  실제로는 아무리 대용량 데이터베이스라도 B-Tree의 깊이가 5단계 이상까지 깊어지는 경우는 거의 없음선택도(유니크한 값의 수)  인덱스 컬럼 값중 유니크한 값의 수  인덱스 컬럼이 100개의 값을 가지는데 그중에서 유니크한 값의 수가 10개라면 선택도는 10  인덱스는 선택도가 높을수록 검색 대상이 줄어들고 검색 성능이 빨라짐읽어야 하는 레코드의 건수  인덱스를 통해 테이블의 레코드를 읽는 것은 바로 테이블의 레코드를 읽는 것보다 높은 비용이 드는 작업  인덱스를 이용한 읽기의 손익 분기점이 얼마인지 판단할 필요가 있음  일반적으로 인덱스를 통해 레코드 1건을 읽는 것이 직접 읽는 것보다 4~5배 정도 비용이 더 많이 드는 작업  즉 인덱스를 통해 읽어야 할 레코드의 건수가 전체 테이블 레코드의 20~25%를 넘어서면 테이블을 모두 직접 읽어 필요한 레코드만 가려내는(필터링) 방식으로 처리하는 것이 효율적  전체 100만 건의 레코드 가운데 50만 건을 읽어야 하는 작업은 인덱스를 이용하지 않고 직접 읽어서 처리할 것인덱스 적용 기준  WHERE, JOIN, ORDER BY절에 많이 사용되는 컬럼  카디널리티가 높은 컬럼  INSERT, UPDATE, DELETE가 많이 발생하지 않는 컬럼  테이블 규모가 큰 경우  잘 활용하지 않는 인덱스는 삭제하는 것이 좋음데이터의 Select시 인덱스에서의 동작  스토리지 엔진이 어떻게 인덱스를 이용해 실제 레코드를 읽어내는지 알아보자인덱스 레인지 스캔  인덱스 레인지 스캔은 인덱스의 접근 방법 가운데 가장 대표적인 접근 방식  B-Tree의 필요한 영역을 스캔하는데 어떤 작업이 필요한가  검색해야 할 인덱스의 범위가 결정됐을 때 사용하는 방식  검색하려는 값의 수나 검색 결과 레코드 건수와 관계없이 레인지 스캔이라고 표현  루트 노드에서부터 비교를 시작해 브랜치 노드를 거치고 최종적으로 리프 노드까지 들어가야만 필요한 레코드의 시작 지점을 찾을 수 있음  시작해야 할 위치를 찾으면 그 때부터는 리프 노드의 레코드만 순서대로 읽으면 됨. 이렇게 쭉 읽는 것을 스캔이라고 표현  리프 노드 끝까지 읽으면 노드 간의 링크를 이용해 다음 리프 노드를 찾아 다시 스캔  루트와 브랜치 노드를 이용해 스캔 시작 위치를 검색하고, 그 지점부터 필요한 방향으로 인덱스를 읽어 나감  읽어 나가면서 검색 조건에 일치하는 경우 데이터 파일에서 레코드를 읽어옴  이때 리프 노드에 저장된 레코드 주소로 데이터 파일의 레코드를 읽어오는데 한 건 단위로 랜덤 I/O이 한 번씩 일어남  그래서 인덱스를 통해 데이터 레코드를 읽는 작업은 비용이 많이 드는 작업으로 분류인덱스 풀 스캔  인덱스 레인지 스캔과 달리 인덱스의 처음부터 끝까지 모두 읽는 방식  쿼리의 조건절에 사용된 컬럼이 인덱스의 첫 번째 컬럼이 아닌 경우 인덱스 풀 스캔 방식 사용  예를 들어 인덱스는 (A, B, C) 컬럼의 순서로 만들어져 있지만, 쿼리의 조건절은 B컬럼이나 C컬럼으로 검색하는 경우  일반적으로 인덱스의 크기는 테이블의 크기보다 작으므로 직접 테이블을 읽는 것보다 인덱스만 읽는 것이 효율적  쿼리가 인덱스에 명시된 컬럼만으로 조건을 처리할 수 있는 경우 주로 이 방식이 사용됨테이블 풀 스캔  테이블 자체의 사이즈가 굉장히 작은 경우  조건에 부합하는 레코드가 너무 많은 경우  인덱스를 사용하지 못하는 경우에 해당  WHERE 절이나 ON 절에 인덱스를 이용할 수 있는 적절한 조건이 없는 경우  테이블 전체를 읽어옴루스 인덱스 스캔  오라클의 인덱스 스킵 스캔과 유사한 방식. MySQL에서는 루스(Loose) 인덱스 스캔이라고 함  위에서 봤던 인덱스 레인지 스캔, 인덱스 풀 스캔을 타이트(Tight) 인덱스 스캔으로 분류함  말 그대로 느슨하게 또는 듬성듬성하게 인덱스를 읽는 것을 의미  인덱스 레인지 스캔과 비슷하게 동작하지만 중간에 필요치 않은 인덱스 키 값은 무시(Skip)하고 넘어가는 형태로 처리  일반적으로 GROUP BY 또는 집합 함수 가운데 MAX(), MIN() 함수에 대해 최적화 하는 경우에 사용  아래의 쿼리문이 있을 때, 인덱스가 (dept_no emp_no) 조합으로 정렬되어 있다면, WHERE절을 만족하는 각 dept_no별로 제일 첫 번째 emp_no만 읽어오고 나머지 인덱스는 스킵할 수 있음      SELECT dept_no, MIN(emp_no)  FROM dept_emp  WHERE dept_no BETWEEN &#39;d002&#39; AND &#39;d004&#39;  GROUP BY dept_no;      인덱스 스킵 스캔  데이터베이스에서 인덱스의 핵심은 값이 정렬돼 있다는 사실 -&amp;gt; 이로 인해 인덱스를 구성하는 컬럼의 순서가 매우 중요  (A, B)라는 인덱스가 있을 때 이 인덱스를 사용해 쿼리문의 빠른 처리를 하려면 WHERE 조건절에 A에 관한 조건이 반드시 제일 앞에 등장해야 함. (WHERE A=1000, 또는 WHERE A=1000 AND B=’M’)  나의 쿼리문에 WHERE 조건절에 A랑 B가 있다면 (A, B)라는 인덱스를 생성해주면 쿼리 성능을 향상시킬 수 있음    CREATE INDEX &amp;lt;인덱스명&amp;gt; ON &amp;lt;테이블명&amp;gt; ( 컬럼명1, 컬럼명2, ... );ALTER TABLE &amp;lt;테이블명&amp;gt;ADD INDEX &amp;lt;인덱스명&amp;gt; (컬럼명1, 컬럼명2, ...);CREATE TABLE &amp;lt;테이블명&amp;gt;INDEX &amp;lt;인덱스명&amp;gt; (컬럼명1, 컬럼명2, ...);        MySQL 8.0부터 WHERE B=’M’과 같이 A에 관한 조건절이 없어도 인덱스 (A, B)를 이용해 인덱스 스킵 스캔을 할 수 있도록하는 최적화 기능이 도입됨클러스터링 인덱스  클러스터링: 여러 개를 하나로 묶는다는 의미  InnoDB의 프라이머리 키가 클러스터링 키로 사용되며, 이 값에 의해 레코드의 위치가 결정됨  비슷한 값들을 동시에 조회하는 경우가 많다는 점에 착안한 것  즉 프라이머리 키 값이 비슷한 레코드끼리 묶어서 저장하는 것을 클러스터링 인덱스라고 표현  중요한 것은 프라이머리 키 값에 의해 레코드의 저장 위치가 결정된다는 것  프라이머리 키 값이 변경된다면 그 레코드의 물리적인 저장 위치가 바뀌어야 한다는 것을 의미하기도 함  클러스터링 인덱스로 저장되는 테이블은 프라이머리 키 기반의 검색이 매우 빠름. 대신 저장은 상대적으로 느림  프라이머리 키가 없는 경우 InnoDB 엔진이 다음 우선순위대로 프라이머리 키를 대체할 컬럼을 선택          NOT NULL 옵션의 유니크 인덱스 중에서 첫 번째 인덱스      자동으로 유니크한 값을 가지도록 증가되는 컬럼을 내부적으로 추가한 후 클러스터링 키로 선택                  하지만 이 방법은 프라이머리 키가 사용자에게 노출되지 않으며, 쿼리 문장에서 명시적으로 사용할 수 없음(혜택 없음)                    클러스터링 인덱스의 장점과 단점  장점          프라이머리 키로 검색할 때 처리 성능이 매우 빠름      테이블의 모든 세컨더리 인덱스가 프라이머리 키를 가지고 있기 때문에 인덱스만으로 처리될 수 있는 경우가 많음        단점          테이블의 모든 세컨더리 인덱스가 클러스터링 키를 갖기 대문에 전체적으로 인덱스의 크기가 커짐      세컨더리 인덱스를 통해 검색할 때 프라이머리 키로 다시 한 번 검색해야 하므로 처리 성능이 느림      프라이머리 키 사용시 주의 사항  클러스터링 인덱스 키의 크기를 크게 하지 않도록 해야함  AUTO-INCREMENT보다는 가능한 업무적인 컬럼으로 생성          대부분 검색에서 상당히 빈번하게 사용됨 -&amp;gt; 설령 그 컬럼의 크기가 크더라도 해당 레코드를 대표할 수 있다면 그 컬럼을 프라이머리 키로 사용할 것을 권장        프라이머리 키는 반드시 명시할 것보조 인덱스  세컨더리 인덱스, 논-클러스터링 인덱스라고도 불림  여러 개의 인덱스를 가질 수 있음  리프 노드에는 레코드가 저장되어 있는 주소를 가지고 있음  추가적인 디스크 공간을 요구함CREATE UNIQUE INDEX [인덱스 명]CREATE INDEX [인덱스 명]참고  Real MySQL 8.0 (1권) 책  [자료구조] 그림으로 알아보는 B+Tree  Guru99, Difference between Clustered and Non-clustered index  우아한 Tech Youtube, [10분 테코톡] 라라, 제로의 데이터베이스 인덱스",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-15T21:01:35+09:00'>15 Apr 2022</time><a class='article__image' href='/mysql-theory-series4'> <img src='/images/mysql_logo.png' alt='[MySQL] 인덱스 (이론)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-theory-series4'>[MySQL] 인덱스 (이론)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] MySQL의 로그 (이론)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-theory-series3",
      "date"     : "Apr 14, 2022",
      "content"  : "Table of Contents  Log의 중요성  MySQL의 로그  MySQL 모니터링을 위한 로그          General Query Log      Slow Query Log      Error Log      Binary Log        트랜잭션 처리를 위한 로그          Undo Log      Redo Log      Binary Log        Redo log와 Binary log  Update문 실행 예제  참고Log의 중요성MySQL은 오픈 소스 관계형 데이터베이스이며 효율성과 보안을 향상시키기 위해 MySQL 데이터베이스 로그를 사용하는 방법을 배워야 한다. 장기적으로 MySQL 인스턴스의 성능을 진단하고 모니터링하는 방법을 이해하는 것이 중요하다.MySQL을 서비스 단계에서 사용하다 보면 슬로우 쿼리(slow query), 데드락(deadlock), 커넥션 실패(aborted connection)과 같은 문제를 마주하게 된다. 로깅은 이러한 문제를 진단하는데 필수적이다. 또한 로그는 보안 문제를 진단하는데도 사용된다.MySQL의 로그MySQL에는 6가지의 로그 타입이 있다.  Redo log(WAL)  Undo log  Binary log  Error log  Slow query log  General log여기서 Redo log와 Undo log 그리고 Binary log는 트랜잭션 처리와 밀접한 관련이 있는 로그들이다. 다른 로그들은 MySQL 서버를 모니터링 하는데 도움이 되는 로그들이다.MySQL 모니터링을 위한 로그위에서 MySQL의 로그를 이용해 슬로우 쿼리, 데드락, 연결 실패, 보안과 관련한 문제들을 진단하는데 도움을 준다고 했다. MySQL의 로그를 모니터링함으로써 이러한 문제를 해결하고 성능을 향상시켜 보자.모니터링에 사용되는 로그들은 다음과 같다.- General Query Log- Slow Query Log- Error Log- Binary Log# 설정값 확인하기mysql&amp;gt; show variables;# Enable Logging on MySQLmysql&amp;gt;SET GLOBAL general_log = ‘ON’;# Log가 저장되는 파일 경로mysql&amp;gt;SET GLOBAL general_log_file = ‘path_on_your_system’;# Enable Logging on MySQLmysql&amp;gt;SET GLOBAL slow_query_log = ‘ON’;# Log가 저장되는 파일 경로mysql&amp;gt;SET GLOBAL slow_query_log_file = ‘path_on_your_system’;General Query LogGeneral query log는 서버가 요청받은 명령어들을 기록한다. 예를 들어 클라이언트의 연결/해제 부터해서 클라이언트가 요청한 SQL문도 기록한다. 이 로그를 통해 서버에 에러가 의심될 때 어떤 요청이 들어왔는지 알 수 있다.General query log에 기록되는 순서는 요청이 들어온 순으로 기록된다. 처리가 완료되지 않았더라도 일단 General query log에 기록된다. 이런 점 때문에 MySQL 서버의 장애를 디버깅하기에 좋은 로그가 된다.처리되지 않은 요청도 모두 로그에 기록되기 때문에, 로그 파일의 사이즈가 비교적 빨리 커진다. 그래서 만약 데이터에 변화를 준 요청만 기록된 로그를 보고 싶다면 Binary log를 사용하는 것이 낫다.Slow Query Log서비스의 규모가 커지게 되면, 빨랐던 쿼리들도 느려지는 일이 발생한다. 이렇게 느린 쿼리 문제를 디버깅할 때에 Slow query log를 사용하는 것이 좋은 출발점이 된다.Slow query log는 내가 설정한 기준(threshold) 시간을 넘어가는 쿼리 실행문을 저장한다. 디폴트는 10초이다.# threshold 값 설정하기SET GLOBAL long_query_time = 5.0;아래와 같은 쿼리를 실행해서 내가 설정한 대로 제대로 동작하는지 확인해 볼 수도 있다.SELECT SLEEP(7);대개 인덱스를 사용하지 못하는 쿼리들이 문제의 대상이 된다. 그래서 만약 log_queries_not_using_indexes 시스템 변수를 ON으로 하면 실행 시간에 상관없이 무조건 Slow query log에 인덱스를 사용하지 않는 실행문을 남길 수 있다.SHOW variables LIKE &#39;slow%&#39;;# 슬로우 로그 조회하기SELECT * FROM mysql.slow_log;# 또는SELECT start_time, user_host, query_time, lock_time, rows_sent, rows_examined, db, CONVERT(sql_text USING utf8 ) sql_textFROM mysql.slow_log;Error LogMySQL은 Error log를 사용하여 서버 시작 및 종료 시 그리고 서버가 실행되는 동안 발생하는 진단 메시지, 경고 및 메모를 기록합니다. Error log에는 MySQL 시작 및 종료 시간도 기록됩니다.Error log는 항상 활성화 되어 있습니다. 만약 Error log 파일 경로를 명시하지 않으면 콘솔에라도 출력하게 됩니다.보통 에러로그에 많이 저장되는 에러  Permission errors  Configuration errors  Out of memory errors  Errors with initiation or shutdown of plugins and InnoDBBinary LogBinary log는 데이터 또는 스키마에 변경을 일으킨 실행문만 기록한다. 예를 들어 INSERT, DELETE, UPDATE와 같은 실행문은 기록하고, SELECT, SHOW와 같은 실행문은 로그에 기록하지 않는다.  Binary 로그는 각 실행문이 실행되는데 얼마나 걸렸는지 기록되어 있다.Redo log는 트랜잭션 처리를 위한 InnoDB 엔진만의 특별한 존재이다. 그렇다면 다른 스토리지 엔진을 쓰는 경우는 어떻게 할까?그래서 스토리지 엔진에 상관없이 MySQL Server레벨에서 가지는 로그가 있는데 그게 바로 Binary log이다.Binary log는 General query log와 다르게, 실행문이 만들어낸 변화가 커밋되었을 떄 Binary log에 기록된다.Binary log는 말그대로 바이너리 포맷으로 저장된다, 그래서 Binary log를 읽으려면 mysqlbinlog 유틸리티를 사용해야 한다.아래는 Binary log binlog.0000001을 읽을 수 있는 방법을 소개한다.mysql&amp;gt; mysqlbinlog binlog.0000001MySQL은 Binary log가 로깅될 때 크게 3가지 포맷을 제공한다.  Statement-based logging: In this format, MySQL records the SQL statements that produce data changes. Statement-based logging is useful when many rows are affected by an event because it is more efficient to log a few statements than many rows.  Row-based logging: In this format, changes to individual rows are recorded instead of the SQL statements. This is useful for queries that require a lot of execution time on the source but result in just a few rows being modified.  Mixed logging: This is the recommended logging format. It uses statement-based logging by default but switches to row-based logging when required.런타임이나 복제가 진행 중일 때는 Binary log 포맷을 바꾸지 않는 것이 좋다. 아래는 바꾸는 코드다.SET GLOBAL binlog_format = &#39;STATEMENT&#39;;SET GLOBAL binlog_format = &#39;ROW&#39;;SET GLOBAL binlog_format = &#39;MIXED&#39;;트랜잭션 처리를 위한 로그데이터베이스는 보통 트랜잭션 수행을 위해 로그를 사용한다. 보통 이러한 역할을 수행하는 로그를 트랜잭션 로그라고 하며 MySQL의 경우 Redo Log, Undo Log, Binary Log가 이에 해당한다. Undo Log는 트랜잭션의 Atomicity, Redo Log는 Durability를 제공해준다.MySQL은 디스크의 I/O으로 인한 성능 저하를 줄이기 위해 캐싱 메커니즘을 사용한다.데이터베이스를 수정하는 쿼리가 들어오면, InnoDB는 먼저 메모리에 데이터가 있는지 확인하고, 없으면 디스크에서 불러와 메모리에 올리고 데이터를 수정한다. 이렇게 메모리에서만 계속 읽고 쓰게되면 장애로 서버가 종료될 때 데이터가 날아가게 된다. 이 문제를 해결하기 위해 MySQL에서는 Redo Log를 사용한다. 더 자세한 내용은 밑에서 살펴볼 것이다. 또한 데이터 수정 쿼리가 들어올 때마다 메모리의 데이터를 바로 수정하지는 않고, 이 전 값을 Undo Log에 보관해두고 수정을 가함으로써, 트랜잭션이 실패할 경우 데이터를 롤백(Roll back)할 준비를 한다. 이 내용도 밑에서 더 자세히 살펴보자.(참고로 디스크에 있는 파일중 데이터를 저장하는 파일을 데이터 파일, 로그를 저장하를 파일을 로그 파일이라고 함)(메모리에 있는 버퍼중 데이터 페이지를 캐시해놓는 위치를 데이터 버퍼, 로그를 캐시해놓는 위치를 로그 버퍼라고 함)(InnoDB엔진이 데이터를 읽고 쓸 때는 해당하는 데이터와 인덱스를 메모리 버퍼 풀에 올려둔다.)Memory Buffer pool: occupies the largest block of memory. The cache used to store various data includes index pages, data pages, undo pages, insert buffers, adaptive hash indexes, lock information stored in innodb, data dictionary information, etc. The working method always reads the database file into the buffer pool by page (16k per page), and then retains the cached data in the buffer pool according to the least recently used (lru) algorithm. If the database file needs to be modified, always modify the page in the buffer pool first (dirty page after the modification occurs), and then flush the dirty page of the buffer pool to the file at a certain frequency.Undo LogUndo log is to achieve atomicity of transactions. Undo Log is also used to implement multi-version concurrency control (referred to as: MVCC).  Undo log는 트랜잭션의 Atomicity 특성을 지키기 위한 로그  Undo log는 MVCC(multi-version concurrency control)를 위해서도 사용Undo log는 Undo log file이라는 파일이 따로 없고, Table space라는 공간의 일부인 Undo segment라는 곳에 저장된다.Delete/Update 작업의 내부 메커니즘Undo log의 원리는 굉장히 간단하다. 트랜잭션의 원자적 특성을 지키기 위해, 어떤 작업을 하려는 데이터는 그 전에 먼저 데이터를 Undo log에 백업해둔다. 그리고 난 후 Delete/Update한다. 만약 중간에 장애가 발생하거나, ROLLBACK이 일어나면 Undo log를 이용해 트랜잭션이 발생하기 전 데이터로 돌아갈 수 있다.예를 들어 A=1과 B=2인 데이터가 있을 떄 각각 +2를 하는 작업을 수행한다고 해보자.- Record A=1 to undo log. - Modify A=3. - Record B=2 to undo log. - Modify B=4.---------------------------------- - Write undo log to disk. - Write data to disk. 여기서 중요한 점은,  데이터를 변경하기 전에 먼저 Undo log에 기록해둔다.  데이터를 커밋하기 전에 먼저 Undo log를 Log buffer에서 Undo segment로 flush한다.Redo LogRedo log는 실행된 SQL문을 저장하기 위한 파일이다. 만약 MySQL 서버에 장애가 발생했다면 Redo log에 저장된 SQL문을 재실행 함으로써 원하는 데이터 상태를 다시 얻을 수 있다.The redo log exists as a separate file on the disk. There will be two files by default, named ib_logfile0 and ib_logfile1.  innodb_log_file_size: the size of the redo log  innodb_log_file_in_group specifies: the number of the redo log, and the default is 2  innodb_log_group_home_dir: the path where the redo log is locatedinnodb_additional_mem_pool_size = 100Minnodb_buffer_pool_size = 128Minnodb_data_home_dir =/home/mysql/local/mysql/varinnodb_data_file_path = ibdata1:1G: autoextendinnodb_file_io_threads = 4innodb_thread_concurrency = 16innodb_flush_log_at_trx_commit = 1innodb_log_buffer_size = 8Minnodb_log_file_size = 128Minnodb_log_file_in_group = 2innodb_log_group_home_dir =/home/mysql/local/mysql/varInnoDB는 Undo log의 작업(operation)을 Redo log의 로그로 기록한다. 그러면 Undo log를 자주 flush 하지 않아도 된다. Redo log에 기록되었기 때문에. Redo log만 자주 flush해주면 된다.데이터 변경 작업이 일어날 때 하는 Undo log의 행동(Undo log를 기록한다, 데이터를 변경한다)들을 하나의 실행문으로 생각하고 Redo log에 저장해둔다.Redo log의 I/O 퍼포먼스데이터 작업은 최대한 메모리 위에 있는 페이지에서 하고, 디스크로 flush하는 작업은 DB의 리소스가 여유로울 때 하는 것이 좋다. 그래서 메모리 버퍼 풀을 이용하는 것이다. 근데 그러면 장애 발생으로 서버가 종료될 때 문제가 된다.그래서 복구가 가능하도록 하기 위해 등장한 것이 Redo log이고, 중간중간 Redo log만 Log buffer에서 Log file로 flush함으로써 문제를 해결할 수 있다.여기서 드는 생각은 “성능 저하의 주요 요인이 디스크로 flush하는 작업이었는데, Redo log를 디스크로 flush하면 결국 달라지는 게 없지 않는가?”라는 점이다.하지만 디스크 작업에도 비교적 빠른 작업이 있고, 느린 작업이 있다. 빠른 작업은 디스크의 내용을 순차적으로 읽는 Sequential Access, 느린 작업은 디스크의 블럭을 자주 옮겨 다녀야 하는 Random Access이다.다시 Redo log로 돌아와보면, Redo log는 디스크의 Log file에 단순히 SQL statement를 append하는 형식이다. 그래서 이 작업은 Sequential Access에 해당한다. 하지만 데이터를 디스크로 flush하는 작업은 디스크 내의 데이터 위치를 찾아야 하기 때문에 Random Access가 발생한다. 이것이 바로 Redo log를 사용하는 것이 더 빠른 이유다.트랜잭션 처리시 일어나는 Undo log와 Redo log의 작업1. Transaction start. 2. Record A=1 to undo log. 3. Modify A=3. 4. Record 2, 3 to redo log. 5. Record B=2 to undo log. 6. Modify B=4. 7. Record 5, 6 to redo log. 8. Write redo log to disk. 9. Transaction commit한 가지 알아야 할 점은 Redo log가 있다고 하더라도, 트랜잭션 한 번마다 Redo log가 flush되는게 아니라면 데이터 손실은 피할 수 없다. 그렇기 때문에 은행과 같이 데이터 손실이 하나라도 발생해서는 안되는 곳에서는 트랜잭션 하나를 완료하기 전에 무조건 디스크로 Redo log를 flush하고 커밋을 하게 된다. (트랜잭션 한 개 마다 Redo log flush)the role of redo &amp;amp; undo log  Data persistence          The buffer pool maintains a linked list in the order of dirty page modification, called flush_list. Flush data to persistent storage according to the order of pages in flush_list. The pages are arranged in the order of the earliest modification. Under normal circumstances, when is the dirty page flushed to the disk?                  When the redo space is full, part of the dirty page will be flushed to the disk, and then part of the redo log will be released.          When you need to allocate a page in the Buffer pool, but it is full, you must flush dirty pages to disk. Generally, this situation can be controlled by the startup parameter innodb_max_dirty_pages_pct. When the dirty page in the buffer pool reaches this ratio, the dirty page is flushed to the disk.          When the system is detected to be idle, it will flush.                      Data Recovery          Over time, Redo Log will become very large. If you start to recover from the first record every time, the recovery process will be very slow and cannot be tolerated. In order to reduce the recovery time, the Checkpoint mechanism is introduced. Suppose that at a certain point in time, all dirty pages have been flushed to disk. All Redo Logs before this time point do not need to be redone. The system records the end of the redo log at this point in time as the checkpoint. When restoring, just start from this checkpoint position. The log before the checkpoint point is no longer needed and can be deleted.      Binary Log  Redo log는 트랜잭션 처리를 위한 InnoDB 엔진만의 특별한 존재이다. 그렇다면 다른 스토리지 엔진을 쓰는 경우는 어떻게 할까?  그래서 스토리지 엔진에 상관없이 MySQL Server레벨에서 가지는 로그가 있는데 그게 바로 Binry log이다.  Binary log는 스키마, 테이블에 관한 모든 변경 항을 기록한다. (SELECT와 SHOW 쿼리는 X)  Binary log는 실행문의 실행 시간도 기록해둔다.Redo log와 Binary log  The content is different: redo log is a physical log and the content is based on the Page on disk, bin-log content is binary and depending on the binlog_format parameter, may be based on SQL statements, on the data itself, or a mixture of the two.  Different levels: redo log works with InnoDB and the engine, bin-log is located at the MySQL Server level and is available to all engines.  Different forms of disk storage: redo log writes cyclically, bin-log accumulates, so it can be used for data recovery or primary-secondary synchronization  The timing of writing is different: bin-log are written when a transaction usually commits or when N transactions commit once, redo log are written at a variety of times, either every time a transaction commits, by another threaded transaction, or every second when the disk is flushed. (Note: uncommitted transactions in redo log may also be flushed to disk)  Different roles: redo log is used for crash recovery to ensure that MySQL downtime does not affect persistence; bin-log is used for point-in-time recovery to ensure that the server can recover data based on the point in time, in addition bin-log is also used for primary-secondary replication.Two-phase CommitBecause redo-log is in the InnoDB tier and bin-log is in the Server tier, this introduces a new problem.If the redo log is written successfully and the bin-log crashes before it is written to disk, the transaction has not yet been committed, so the new data written to the redo-log is invalid.Restarting the database for data recovery restores the data in the redo-log to disk, which creates invalid data.In this case, as you wisely know, a two-phase commit is introduced.  In the first stage, the redo-log is written and in the prepared state.  After the Server layer saves the bin-log data and drops it to disk, the transaction commits the redo-log at the same time, so that the redo-log becomes committed, which ensures the consistency of the redo-log data and the bin-log data.Update문 실행 예제With the previous knowledge, you can now explore how the update statement is executed in MySQL.Suppose we now execute the SQL : update table_test set a = a+1 where id = 2;  First, the client connects via the connector and determines the permissions.  After verification, the SQL goes through the parser for lexical and syntax analysis (AST) and if it is an Update statement, MySQL will clear all the query cache for the query table table_test. (As you can see, it is not recommended to turn on the query cache)  The optimizer optimizes the validated SQL, plans to match the id index, and generates an execution plan.  The executor gets the final SQL and calls the interface of the corresponding storage engine to start executing the update SQL.  The InnoDB engine opens a transaction, the execution engine first queries from memory whether there is data with id=2, if it matches then the corresponding data with field+1, and then saves it to memory. If it does not query the data with id=2 then it will go to the disk, the query will read the data into memory in pages, then update it and save it to memory.  The InnoDB engine will then save the data rows to redo-log, which is pre-committed, notifying the Server’s executor that it is ready to commit the transaction.  The executor will generate the corresponding bin-log and write it to disk.  The transaction is committed and the redo-log is then committed.  This is where the execution of a transaction is complete.참고  해커의 개발일기, 데이터베이스의 무결성을 보장해주는 Write-Ahead-Log  alibabacloud, What are the Differences and Functions of the Redo Log, Undo Log, and Binlog in MySQL?  Coralogix, 5 Essential MySQL Database Logs To Keep an Eye On  scaling.dev, Transaction Log. Commit Log. WAL.  PostgreSQL 공식문서, Write-Ahead Logging (WAL)  Dwen, MySQL’s RedoLog and BinLog  developPAPER, You must understand the three MySQL logs – binlog, redo log and undo log  MySQL 공식문서, 14.6.6 Redo Log  Katastros, InnoDB transaction log (redo log and undo log) detailed  heesuk-ahn, [데이터베이스] binary log 란?  The Binary Log  Log-structured storage  ghyeong, 슬로우 쿼리(Slow Query) 조회 쿼리(Oracle, MS-SQL, Mysql, postgreSQL)  MySQL 공식문서, The Slow Query Log",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-14T21:01:35+09:00'>14 Apr 2022</time><a class='article__image' href='/mysql-theory-series3'> <img src='/images/mysql_logo.png' alt='[MySQL] MySQL의 로그 (이론)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-theory-series3'>[MySQL] MySQL의 로그 (이론)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 트랜잭션과 잠금 (이론)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-theory-series2",
      "date"     : "Apr 12, 2022",
      "content"  : "Table of Contents  트랜잭션          ACID        페이지 캐시          캐싱      캐시 만료      페이지 동기화      페이지 고정      페이지 교체 알고리즘        복구          WAL(Write Ahead Log)      버퍼 풀 관리 정책                  스틸(Steal)과 포스(Force) 정책                      동시성 제어          잠금      격리 수준                  READ UNCOMMITTED          READ COMMITTED          REPEATABLE READ                      참고트랜잭션DBMS에서 트랜잭션이란 하나의 논리적 작업 단위를 의미하며, 여러 작업(DB의 읽기, 쓰기)을 한 단계로 표현하는 방법입니다. 트랜잭션은 어플리케이션 개발에서 부분 업데이트 현상으로 인해 데이터의 정합성이 깨지는 문제를 걱정하지 않도록 해줍니다. 트랜잭션을 사용할 때는 가능한 범위를 최소화하는 것이 좋습니다. 특히 네트워크를 통해 발생하는 작업은 트랜잭션 내에서 제거해야 합니다. 네트워크간 데이터 이동은 그 자체로 높은 비용이 발생할 뿐 아니라, 다양한 이유로 장애가 발생할 수 있는 부분이기 때문입니다. 트랜잭션이 종료되지 않고 유지되는 시간이 길어지면 MySQL서버의 전체적인 성능 하락에 주요 이유가 됩니다.ACID이러한 트랜잭션을 정의하기 위해서는 다음의 4가지 속성이 보장되어야 합니다.원자성(Atomicity)  트랜잭션으로 묶인 데이터의 변경 사항은 모두 반영되거나 모두 반영되지 않아야 합니다  All or Nothing  ex) A에서 B로 계좌이체를 할 때, A가 출금이 되면 B도 반드시 입금이 되어야 한다.일관성(Consistency)  트랜잭션이 일어나더라도 데이터베이스의 제약이나 규칙은 그대로 지켜져야 합니다.  사용자가 제어할 수 있는 유일한 속성입니다.  ex) 고객 정보 DB에서 이름을 반드시 입력하도록 제약을 두었다면 트랜잭션 또한 이러한 제약을 가져야 한다.격리성(Isolation)  하나의 트랜잭션은 다른 트랜잭션으로부터 간섭없이 독립적으로 수행되어야 합니다.  동시에 여러 개의 트랜잭션들이 수행될 때, 각 트랜잭션은 순차적으로 실행된 것과 동일한 결과를 나타내야 합니다.  ex) A가 만원이 있는 계좌에서 B에게 3천원을 송금하던 도중 자신의 잔액을 확인할 때는 여전히 만원이 있어야 한다.  많은 데이터베이스는 성능상의 이유로 정의에 비해 약한 격리 수준을 사용합니다. (동시성 제어의 격리 수준 참고)지속성(Durability)  커밋된 데이터는 장애가 발생 하더라도 데이터베이스에 저장되어야 한다.  ex) A에서 B로 송금이 완료되어 커밋을 했다면 시스템 중단, 정전으로 장애가 발생해도 DB에 데이터가 그대로 유지되어야 한다페이지 캐시대부분의 데이터베이스는 상대적으로 속도가 느린 영구 저장소(디스크)에 접근하는 회수를 줄이기 위해 페이지를 메모리에 캐시합니다. 이를 페이지 캐시(page cache)라고 하며 이 때의 메모리 영역을 버퍼 풀(buffer pool)이라고 합니다. 메모리에 있는 페이지에 변경사항이 생겼을 때 아직 디스크로 플러시(flush)되지 않은 페이지를 더티(dirty) 페이지라고 합니다.정리하면 페이지 캐시의 주요 기능은 다음과 같습니다.  페이지를 메모리에 캐시함으로써 빠른 읽기를 지원  쓰기 요청이 발생할때마다 디스크로 플러시하지 않고 버퍼링 후 플러시 할 수 있다 -&amp;gt; 디스크 I/O를 줄임캐싱스토리지 엔진이 특정 페이지를 요청하면 우선 캐시된 버전이 있는지 확인합니다. 페이지가 있다면 반환하고 없다면 페이지 번호를 물리적 주소로 변환해 해당 페이지를 메모리로 복사하고 반환합니다.이때 해당 페이지가 저장된 버퍼는 참조상태라고 표현합니다. 작업이 끝나면 스토리지 엔진은 참조 해제해야 합니다.캐시 만료일반적으로 버퍼 풀은 데이터셋보다 크기가 작기 때문에 새로운 페이지를 추가하기 위해 기존 페이지를 만료시키는 작업도 필요하게 됩니다. 페이지가 동기화됐고 고정 또는 참조 상태가 아니라면 바로 제거할 수 있습니다. 페이지를 제거할 때에는 페이지와 관련된 로그도 WAL에서 삭제합니다.페이지 동기화위에서 버퍼 풀의 메모리 용량을 관리하기 위해서는 캐시가 만료된 페이지는 제거해야 한다고 했습니다. 그리고 이때 페이지를 제거하기 위해서는 우선 페이지가 동기화되어야 한다고 했습니다. 페이지 동기화는 더티페이지를 디스크에 반영(flush)하는 것입니다.이렇게 플러시하는 것은 언제 얼마나 자주하는 것이 좋을까요? 변경 사항이 생길 때마다 플러시하게 되면 데이터 손실 가능성을 줄일 수 있겠지만 결국 잦은 디스크 접근을 유발하기 때문에 트레이드 오프가 있습니다. 그래서 데이터베이스에서는 이러한 플러시를 주기적으로 하게 되며 이 시점을 체크포인트(checkpoint)라고 합니다.체크포인트 시점에 플러시가 일어나는데 이 때 플러시는 디스크에 있는 데이터베이스에 데이터가 저장되는 것을 의미하지는 않습니다. 플러시는 메모리에 있는 페이지에 요청된 작업 명령들을 디스크의 WAL(Write Ahead Log)에 남겨두고 페이지와 싱크를 맞추는 것입니다.정리하면  캐시가 만료된 페이지를 삭제하려면 먼저 페이지를 동기화 해야 한다.  동기화된 시점을 체크포인트라고 한다.  동기화는 플러시하는 것이며 플러시는 페이지의 변경시 요청된 작업 명령을 디스크의 WAL에 기록하는 것이다.  로그를 디스크의 로그 파일에 기록해 둠으로써 장애로 인한 데이터 손실을 대비할 수 있음  버퍼 풀의 더티 페이지가 디스크에 플러시 되는 시점은 보통 MySQL 서버가 idle 상태일 때 백그라운드로 진행됨  로그를 디스크로 동기화 하는 것이 데이터 자체를 디스크로 동기화 하는 것보다 작업이 가볍기 때문에 굳이 로그를 동기화 하는거임  (로그는 단순히 append-only이기 때문에 랜덤 I/O가 발생하지 않지만, 데이터를 디스크에 반영하는 것은 데이터가 여러 페이지에 위치해 있기 때문에 랜덤 I/O이 많이 발생함)페이지 고정가까운 시간 내에 요청될 확률이 높은 페이지는 캐시에 가둬 두는 것이 좋습니다. 이를 페이지 고정(pinning)이라고 합니다. 예를 들어 이진 트리 탐색에서 트리의 상위 노드는 접근될 확률이 높기 때문에 이러한 상위 노드는 고정해두면 성능 향상에 도움이 됩니다.페이지 교체 알고리즘저장 공간이 부족한 캐시에 새로운 페이지를 추가하려면 일부 페이지를 만료시켜야 한다고 했습니다. 하지만 빈번하게 요청될 수 있는 페이지를 만료시키면 같은 페이지를 여러 차례 페이징하는 상황이 발생할 수 있습니다. 페이지 교체 알고리즘은 다시 요청될 확률이 가장 낮은 페이지를 만료시키고 해당 위치에 새로운 페이지를 페이징합니다.하지만 페이지의 요청 순서는 일반적으로 특정 패턴이 없기 때문에 어떤 페이지가 다시 요청될지 정확하게 예측하는 것은 불가능 합니다. 그래서 보통은 그 기준을 최근에 요청되었는지 여부, 요청된 빈도수 등으로 합니다. 관련 알고리즘에는 FIFO(First In First Out), LRU(Least Recently Used), LFU(Least Frequently Used), CLOCK-sweep 알고리즘이 있습니다.복구데이터베이스 시스템은 각자 다른 안정성과 신뢰성 문제를 내재한 하드웨어와 소프트웨어 계층으로 구성됩니다. 따라서 여러 지점에서 장애가 발생할 수 있고, 데이터베이스 개발자는 이러한 장애 시나리오를 고려해 데이터를 저장해야 합니다.WAL(Write Ahead Log)선행 기록 로그(WAL)는 장애 및 트랜잭션 복구를 위해 디스크에 저장하는 추가 자료 구조입니다. WAL은 페이지에 캐시된 데이터가 디스크로 플러시 될 때 까지 관련 작업 이력의 유일한 디스크 기반 복사본입니다. WAL은 리두 로그의 또 다른 말입니다.WAL에 있는 각각의 로그에는 단조 증가하는 고유 로그 시퀀스 번호(LSN: Log Sequence Number)가 있습니다.WAL의 주요 기능은 다음과 같습니다.  장애 발생 시 WAL을 기반으로 마지막 메모리 상태를 재구성한다.  WAL의 로그를 재수행해서 트랜잭션을 커밋한다.버퍼 풀 관리 정책스틸(Steal)과 포스(Force) 정책DBMS는 스틸/노스틸 정책과 포스/노포스 정책을 기반으로 메모리에 캐시된 변경 사항을 디스크로 플러시하는 시점을 결정합니다. 이러한 정책들은 복구 알고리즘 선택에 큰 영향을 미칩니다.스틸(Steal)트랜잭션이 완료되지 않은 상태에서 데이터를 디스크에 기록할 것인가?  STEAL: 기록한다 (수정된 페이지를 언제든지 디스크에 쓸 수 있는 정책) -&amp;gt; Undo 필요  No-STEAL: 기록하지 않는다 (수정된 페이지들을 최소한 트랜잭션 종료 시점(EOT)까지는 버퍼에 유지하는 정책)버퍼 관리자가 트랜잭션 종료 전에는 어떤 경우에도 수정된 페이지들을 디스크에 쓰지 않는다면, Undo 오퍼레이션은 메모리 버퍼에 대해서만 이루어지면 되는 식으로 매우 간단해질 수 있다. 이 부분은 매력적이지만 이 정책은 매우 큰 크기의 메모리 버퍼가 필요하다는 문제점을 가지고 있다. 수정된 페이지를 디스크에 쓰는 시점을 기준으로 다음과 같은 두 개의 정책으로 나누어 볼 수 있다.Steal 정책은 수정된 페이지가 어떠한 시점에도 디스크에 써질 수 있기 때문에 필연적으로 Undo 로깅과 복구를 수반하는데, 거의 모든 DBMS가 채택하는 버퍼 관리 정책이다.포스(Force)트랜잭션이 커밋된 후 바로 데이터를 디스크에 기록할 것인가?  FORCE: 바로 기록한다  No-FORCE: 바로 기록하지 않는다(Redo 필요)성능상의 이유로 때로는 트랜잭션이 완료되기도 전에 디스크에 기록하기도 하고 완료되고 나서도 기록하지 않기도 합니다.정리해보면 DBMS는 버퍼 관리 정책으로 STEAL과 No-FORCE 정책을 채택하고 있어, 이로 인해서 UNDO 복구와 REDO 복구가 모두 필요하게 된다.동시성 제어  잠금과 트랜잭션, 트랜잭션의 격리 수준이 동시성에 영향을 줌  잠금은 동시성을 제어하기 위한 기능  트랜잭션은 데이터의 정합성을 보장하기 위한 개념잠금  하나의 레코드를 여러 커넥션에서 동시에 변경하려고 할 때 잠금이 없다면 레코드의 값은 예측할 수 없는 상태가 됨  잠금은 동시에 여러 요청이 들어와도 순서대로 한 시점에는 하나의 커넥션만 변경할 수 있게 하는 역할  MySQL 엔진 레벨의 잠금에는 테이블락, 메타데이터락, 네임드락이 있음  InnoDB 스토리지 엔진 레벨의 잠금은 (테이블 전체가 아닌) 레코드 기반의 잠금 방식 -&amp;gt; 뛰어난 동시성 처리를 제공  정확히는 레코드를 잠그는 것이 아니라 인덱스를 잠그는 방식(데이터 접근 기준에 해당하는 인덱스 모두 잠금)  만약 테이블에 인덱스가 하나도 없다면 테이블을 모두 스캔하고 모든 레코드에 잠금이 걸림 -&amp;gt; 인덱스 설계가 중요격리 수준  여러 트랜잭션이 동시에 처리될 때 트랜잭션 간의 작업 내용을 어떻게 공유하고 차단할 것인지 결정하는 레벨  격리 수준은 크게 READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE  READ UNCOMMITTED는 DIRTY READ 문제로, SERIALIZABLE은 동시성 처리 성능 저하로 거의 사용하지 않음  오라클 DB는 READ COMMITTED, MySQL은 REATABLE READ를 주로 사용  격리 수준에 따라 DIRTY READ, NON-REPEATABLE READ, PHANTOM READ와 같은 부정합 문제 발생READ UNCOMMITTED  트랜잭션에서의 변경 내용이 COMMIT이 되었는지 ROLLBACK이 되었는지 관계없이 다른 트랜잭션에서 보임  해당 트랜잭션이 COMMIT되면 상관없지만 ROLLBACK된 경우 다른 트랜잭션은 잘못된 데이터를 읽어간 것이 됨READ COMMITTED  오라클 DB를 포함해 많은 온라인 서비스에서 선택하는 격리 수준  트랜잭션이 데이터를 변경하고 COMMIT 하지 않았다면 다른 트랜잭션은 언두 영역에 백업된 레코드를 읽어감  NON-REPEATABLE READ 문제 발생  어떤 트랜잭션내에서 SELECT 요청이 두 번 발생하는 동안 다른 트랜잭션이 데이터 변경 후 커밋하면 하나의 트랜잭션내에서 같은 SELECT 요청에 대해 다른 결과가 나오게 됨REPEATABLE READ  InnoDB  스토리지 엔진에서 기본적으로 사용되는 격리 수준  READ COMMITTED, REPEATABLE READ 모두 트랜잭션이 커밋되지 않은 경우 MVCC를 이용해 이전에 커밋된 데이터를 보여줌  차이는 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전 버전까지 찾아 들어가야 하느냐는 것  모든 트랜잭션은 순차적으로 증가하는 고유한 트랜잭션 번호를 가지며 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션의 번호가 포함돼 있음  트랜잭션은 자신보다 작은 트랜잭션 번호를 가지는 트랜잭션이 변경한 사항만 볼 수 있음참고  Real MySQL 8.0 (1권) 책  데이터베이스 인터널스 책  Naver D2: DBMS는 어떻게 트랜잭션을 관리할까?  온달의 해피클라우드: ACID 이해하기  노력 이기는 재능 없고 노력 외면하는 결과도 없다: [MySQL Internals] FLUSH  MySQL 공식문서: MySQL Glossary  stackoverflow: SQLAlchemy: What’s the difference between flush() and commit()?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-12T21:01:35+09:00'>12 Apr 2022</time><a class='article__image' href='/mysql-theory-series2'> <img src='/images/mysql_logo.png' alt='[MySQL] 트랜잭션과 잠금 (이론)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-theory-series2'>[MySQL] 트랜잭션과 잠금 (이론)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part9]: Kafka Connector",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series9",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  Why Connector?  Kafka Connect          Kafka Connect 구성요소      Connect? Connector?      Standalone과 Distributed Workers        Debezium  도커 컴포즈 파일  kafka 컨테이너에서 워커 실행 모드 설정  kafka 컨테이너에서 커넥터 워커 실행  connect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행  기타 커넥터 관련 REST API  커넥터와 백엔드(Java Spring)의 관계  참고Why Connector?커넥터 없이도 프로듀서 컨슈머 사용 가능하지만 커넥터를 이용하면 카프카를 사용하면서 발생할 수 있는 장애에 대한 복구를 비롯한 필요한 기능들을 따로 개발할 필요없이 사용가능Kafka Connect  Kafka Connect는 다른 데이터 시스템을 Kafka와 통합하는 과정을 표준화한 프레임워크  통합을 위한 Connector 개발, 배포, 관리를 단순화Kafka Connect 구성요소  Connector: Task를 관리하여 데이터 스트리밍을 조정하는 jar파일  Task: 데이터 시스템간의 전송 방법을 구현한 구현체  Worker: Connector와 Task를 실행하는 프로세스  Converter: 데이터 포맷을 변환하는데 사용하는 구성요소  Trasform: 데이터를 변환하는데 사용하는 구성요소Connect? Connector?커넥트는 커넥터를 실행시키기 위한 환경(프레임워크)을 제공해줌. 커넥트 위에서 커넥터 설치하고 커넥터(jar파일) 실행하면 됨커넥트 이미지로 인스턴스 띄우고 거기서 각종 커넥터 다운로드 받아서 커넥터를 몽고db, mysql, s3같은데 RESTapi로 등록Standalone과 Distributed WorkersWorker 프로세스를 한 개만 띄우는 Standalone 모드와 여러개 실행시키는 Distributed 모드가 있다.보통 확장성과 내결함성을 이유로 Distributed 모드를 많이 사용한다.DebeziumDebezium은 변경 데이터 캡처를 위한 오픈 소스 분산 플랫폼이다.Debezium 에서 변경된 데이터 캡쳐를 위해 mysql의 경우 binlog, postgresql의 경우 replica slot(logical)을 이용하여 데이터베이스에 커밋하는 데이터를 감시하여 Kakfa, DB, ElasticSearch 등 미들웨어에 이벤트를 전달한다도커 컴포즈 파일version: &#39;3.2&#39;services:  mongodb:    image: mongo:latest    hostname: mongodb    ports:      - &quot;27017:27017&quot;    environment:      MONGO_INITDB_ROOT_USERNAME: root      MONGO_INITDB_ROOT_PASSWORD: root    tty: true    zookeeper:    image: zookeeper:3.7    hostname: zookeeper    ports:      - &quot;2181:2181&quot;    environment:      ZOO_MY_ID: 1      ZOO_PORT: 2181    volumes:      - ./data/zookeeper/data:/data      - ./data/zookeeper/datalog:/datalogco  kafka:    image: wurstmeister/kafka    hostname: kafka    ports:      - &quot;9092:9092&quot;    environment:      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1    tty: true    volumes:      - ./data/kafka/data:/tmp/kafka-logs    depends_on:      - zookeeper    connect:    image: confluentinc/cp-kafka-connect:latest.arm64    hostname: connect1    depends_on:      - kafka    environment:      CONNECT_BOOTSTRAP_SERVERS: kafka:29092      CONNECT_REST_ADVERTISED_HOST_NAME: connect1      CONNECT_GROUP_ID: connect-cluster      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets      CONNECT_STATUS_STORAGE_TOPIC: connect-status      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1      CONNECT_PLUGIN_PATH: /usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/      CONNECT_REST_PORT: 8083    ports:      - 18083:8083    volumes:      - ./connectors/1:/usr/share/confluent-hub-components    command:      - bash      - -c      - |        confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.7.0        /etc/confluent/docker/run &amp;amp;        sleep infinity  producer:    build:      context: ./      dockerfile: Dockerfile_producer    stdin_open: true    tty: true  consumer:    build:      context: ./      dockerfile: Dockerfile_consumer    stdin_open: true    tty: truevolumes:  mongodb:kafka 컨테이너에서 워커 실행 모드 설정cd opt/kafka/configvi connect-distributed.properties# connect 컨테이너에서 커넥터(jar파일)가 설치되어 있는 경로 설정plugin.path=/usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/# 컨버터 설정key.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter=org.apache.kafka.connect.json.JsonConverterkey.converter.schemas.enable=falsevalue.converter.schemas.enable=falsekafka 컨테이너에서 커넥터 워커 실행./bin/connect-distributed.sh ./config/connect-distributed.propertiesconnect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행curl -X POST -H&#39;Accept:application/json&#39; -H&#39;Content-Type:application/json&#39; http://connect1:8083/connectors   -w &quot;\n&quot;  -d &#39;{&quot;name&quot;: &quot;mongo-sink&quot;,      &quot;config&quot;: {         &quot;connector.class&quot;:&quot;com.mongodb.kafka.connect.MongoSinkConnector&quot;,         &quot;connection.user&quot;: &quot;root&quot;,         &quot;connection.password&quot;: &quot;root&quot;,         &quot;connection.uri&quot;:&quot;mongodb://root:root@mongodb:27017&quot;,         &quot;database&quot;:&quot;quickstart&quot;,         &quot;collection&quot;:&quot;topicData&quot;,         &quot;topics&quot;:&quot;taxi&quot;,        &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,        &quot;value.converter.schemas.enable&quot;: &quot;false&quot;         }     }&#39;기타 커넥터 관련 REST API# 커넥터 상태 확인(커넥터 등록과 태스크 실행이 RUNNING이면 성공)curl -X GET http://connect1:8083/connectors/mongo-sink/status# 커넥터 삭제curl -X DELETE http://connect1:8083/connectors/mongo-sink커넥터와 백엔드(Java Spring)의 관계커넥터가 있으면 알아서 커넥터가 토픽에서 데이터를 가져와 DB로 잘 반영을하는 것 같다.이런거보면 딱히 스프링부트 같은 걸 이용해서 백엔드 프로그램을 개발하지 않아도 되는 것 같아보인다.하지만 만약 내가 스프링부트 같은 거를 엄청 잘 알아서 직접 개발하는데 불편함이 없다면 왠만한 것들은 스프링 부트를 이용하고 부분적으로 특정 프로듀서/컨슈머는 커넥터를 사용하는 것이 아마 가장 좋은 방법이 아닐까 라는 생각이 든다.나는 지금 스프링부트를 모른다. 심지어 자바 언어도 써본 적이 없다. 커넥터는 아예 러닝 커브가 없는 것은 아니지만 훨씬 쉽다.하지만 백엔드의 중요한 철학들을 공부하는 것은 굉장히 중요해보인다.결론은 지금 당장 구현이 필요한 부분들은 커넥터로 구현을 하고, 백엔드 공부는 스프링 부트를 통해서 계속 하자.백엔드 공부를 스프링 부트로 하기로 한 이유는, 내가 사용하고 있는 언어는 파이썬이지만 데이터 엔지니어링 공부에서 자바 언어는 필요해보인다. (데이터 엔지니어링 분야의 관련 오픈 소스들이 자바로 많이 개발됨)파이썬으로 백엔드를 구현하도록 해주는 장고나 플라스크도 있지만, 아직은 스프링 부트를 사용하는 비중이 더 커보이고 뭔가 공부하는 관점에서는 스프링 부트가 더 도움이 많이 될 것 같다.참고  Confluent 공식문서: Kafka Connect Tutorial on Docker  Connect 도커 이미지  Confluent 공식문서: MongoDB 커넥터  MongoDB 공식문서: MongoDB 커넥터를 위한 Configuration  kudl: CDC - debezium 설정  Confluent 공식문서: 커넥터 관련 강의  Confluent 공식문서: Connect 관련 configuration  sup2is: Kafka Connect로 데이터 허브 구축하기  깃허브: mongodb-university/kafka-edu  Kafka Connect Deep Dive – Converters and Serialization Explained  정몽실이: 카프카 커넥트 실행  Stackoverflow: Connector and Spring Kafka",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/kafka-series9'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part9]: Kafka Connector'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series9'>Kafka Series [Part9]: Kafka Connector</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part10]: MySQL Connector",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series10",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents./kafka/bin/zookeeper-server-start.sh [-daemon] ./kafka/config/zookeeper.properties./kafka/bin/kafka-server-start.sh ./kafka/config/server.properties./kafka/bin/connect-distributed.sh ./kafka/config/connect-distributed.properties# 등록 가능한 커넥터 플러그인 목록curl -X GET http://localhost:8083/connector-plugins# connect-distributed.properties# 이렇게 추가하면 FileStreamSinkConnector, FileStreamSourceConnector 커넥터 클래스가 추가됨 (아래와 같은 경로가 없음에도 불구하고.. 그래서 이해가 안됨)plugin.path=/usr/local/share/java,/usr/local/share/kafka/plugins,/opt/connectors# 현재 등록된 커넥터 목록curl -X GET http://localhost:8083/connectors# FileStreamSinkConnector 커넥터 등록 (토픽 메세지를 파일에 쓴다)curl -X POST http://localhost:8083/connectors -H &#39;Content-Type: application/json&#39; -d &#39;{ &quot;name&quot;: &quot;file-sink-test&quot;, &quot;config&quot;: { &quot;topics&quot;: &quot;test&quot;, &quot;connector.class&quot;: &quot;org.apache.kafka.connect.file.FileStreamSinkConnector&quot;, &quot;tasks.max&quot;: 1, &quot;file&quot;: &quot;./connect-test.txt&quot;}}&#39;# 커넥터 상태 조회curl -X GET http://localhost:8083/connectors/file-sink-test/status            요청 메서드      경로      설명                  GET      /connector-plugins      사용 가능한 커넥터 플러그인 목록              GET      /      커넥트 정보              GET      /connectors      커넥터 목록              POST      /connectors      커넥터 등록              GET      /connectors/      커넥터 정보 확인              GET      /connectors//config      커넥터 설정값 확인              PUT      /connectors//config      커넥터 설정값 변경              GET      /connectors//status      커넥터 상태 확인              POST      /connectors//restart      커넥터 재실행              DELETE      /connectors/      커넥터 삭제      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/kafka-series10'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part10]: MySQL Connector'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series10'>Kafka Series [Part10]: MySQL Connector</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 아키텍처 (이론)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-theory-series1",
      "date"     : "Apr 9, 2022",
      "content"  : "Table of Contents  MySQL의 전체 구조  MySQL 엔진 아키텍처  InnoDB 스토리지 엔진 아키텍처          InnoDB 버퍼 풀                  LRU 알고리즘                    체인지 버퍼      로그 버퍼      리두 로그                  버퍼 풀과 리두 로그의 관계          버퍼 풀 플러시                    언두 로그      테이블 스페이스                  The System Tablespace          File-Per-Table Tablespaces          General Tablespaces          Undo Tablespaces          Temporary Tablespaces                    어댑티브 해시 인덱스        참고MySQL의 전체 구조  MySQL 엔진: 요청된 SQL문장을 분석하거나 최적화하는 등 DBMS의 두뇌에 해당하는 처리를 수행  스토리지 엔진: 데이터를 디스크에 저장하거나 디스크로부터 읽어오는 역할MySQL 엔진 아키텍처  커넥션 핸들러: 클라이언트의 접속, 쿼리 요청을 처리  SQL 파서: 실행 이전에 문법성 오류 체크  SQL 옵티마이저: 쿼리의 최적화InnoDB 스토리지 엔진 아키텍처There is a metadata file (ibdata1, which holds, by default, data pages, index pages, table metadata and MVCC information), also known as the InnoDB tablespace file.  You can have more than one ibdata file (see innodb_data_file_path)  There are redo logs (ib_logfile0 and ib_logfile1)  You can have more than two redo logs (see innodb_log_files_in_group)  You can spread data and indexes across multiple ibdata files if innodb_file_per_table is disabled  You can separate data and index pages from ibdata into separate tablespace files (see innodb_file_per_table and StackOverflow Post on how to set this up)InnoDB 버퍼 풀  스토리지 엔진에서 가장 핵심적인 부분  데이터 캐시: 디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간  쓰기 버퍼링: 쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있게 해주는 버퍼 역할  페이지 크기의 조각으로 쪼개어 디스크로부터 읽어온 페이지를 저장  메모리 공간을 관리하기 위해 LRU리스트, 플러시 리스트, 프리 리스트라는 자료구조를 관리LRU 알고리즘  테이블 데이터와 인덱스 데이터를 버퍼 풀에 캐시한다  새로운 위치의 테이블 데이터 또는 인덱스 데이터를 버퍼 풀에 캐시하려는데 메모리에 더 이상 자리가 없다  버퍼 풀에서 한동안 사용되지 않았던 페이지를 버퍼 풀에서 제거한다(만약 더티 페이지라면 플러시 후 제거)  버퍼 풀에 있는 페이지들은 링크드 리스트 구조로 되어 있음  이 링크드 리스트는 크게 두 개의 서브리스트로 구분됨 (New Sublist와 Old Sublist)  (저는 New대신 Young이라고 하겠습니다. 공식 문서에서도 Young이라고도 하는데, 저는 Young이 더 와닿기 때문에)  Young 서브리스트는 링크드 리스트의 HEAD에 가까운 5/8, Old 서브리스트는 링크드 리스트의 TAIL에 가까운 3/8  이 Old 서브리스트가 삭제되는 페이지의 후보들이다(무조건 3/8이 다 삭제되는게 아니라, 필요한만큼 삭제됨)  새로운 페이지는 Old 서브리스트의 HEAD로 감. 이 부분을 Midpoint라고도 함  페이지들은 어떤 작업에 의해 젊어지고 늙어갈까?          유저의 SELECT 연산에 의해 데이터가 읽어지면 young해지고 Young 서브리스트의 제일 머리로 올라간다      InnoDB에 의해 자동적으로 수행되는 Read-ahead 작업으로 데이터가 young해질 수도 있음      SELECT 연산이 WHERE 조건문이 없으면 테이블 풀 스캔 -&amp;gt; young해질 수도 있음        자동적으로 위와 같은 작업에 해당되지 않는 데이터들은 old해짐  하지만 2번과 3번은 정말 의미있는 데이터를 young해지도록 하는 것은 아님 -&amp;gt; 이로 인해 오히려 중요한 데이터가 old해지는 악영향을 초래할 수도 있음          MySQL 공식문서: Making the Buffer Pool Scan Resistant      MySQL 공식문서: Configuring InnoDB Buffer Pool Prefetching (Read-Ahead)      위의 문서를 참고하면, 중요한 데이터를 버퍼 풀에 고정시키는 방법과 같은 것들을 알 수 있음      read-aheadA type of I/O request that prefetches a group of pages (an entire extent) into the buffer pool asynchronously, in case these pages are needed soon. The linear read-ahead technique prefetches all the pages of one extent based on access patterns for pages in the preceding extent. The random read-ahead technique prefetches all the pages for an extent once a certain number of pages from the same extent are in the buffer pool. Random read-ahead is not part of MySQL 5.5, but is re-introduced in MySQL 5.6 under the control of the innodb_random_read_ahead configuration option.체인지 버퍼  The change buffer is a special data structure that caches changes to secondary index pages when those pages are not in the buffer pool. The buffered changes, which may result from INSERT, UPDATE, or DELETE operations (DML), are merged later when the pages are loaded into the buffer pool by other read operations.  세컨더리 인덱스는 보통 유니크하지 않은 경우가 많다  그리고 세컨더리 인덱스에 삽입하는 데이터는 보통 한 곳에서 순차적으로 발생하지 않고, 랜덤한 곳에서 발생한다  INSERT, UPDATE, DELETE와 같은 DML작업이 세컨더리 인덱스(secondary index)를 가지는 컬럼에 발생했을 때,  해당 변경사항을 반영하기 위해 세컨더리 인덱스를 업데이트 해야한다.  이 때 만약 세컨더리 인덱스 페이지가 버퍼 풀에 없다면,  세컨더리 인덱스 페이지를 업데이트 하기 위해 디스크에 접근하게 되면 디스크 I/O로 인해 성능 저하  -&amp;gt; 세컨더리 인덱스 페이지의 변경사항을 체인지 버퍼에 캐시해둔다  나중에 SELECT 연산에 의해 버퍼 풀에 인덱스 페이지가 로드되면, 그 때 체인지 버퍼에 캐시된 변경사항을 버퍼 풀로 병합시킨다  그래서 대량의 DML 작업이 발생할 때, 체인지 버퍼는 많은 도움이 된다  하지만 체인지 버퍼는 버퍼 풀 메모리의 일부를 차지하고 있다  그래서 만약 주로 읽기 작업이 발생하는 OLAP를 위해 사용하고 있거나, 세컨더리 인덱스가 별로 없는 경우에는 체인지 버퍼의 기능을 꺼두는 것이 좋다  MySQL 공식문서 참고. 내용 좋음기본적으로 데이터/인덱스 변경사항은 버퍼 풀에 반영된다근데 그중에서 세컨더리 인덱스에 한해,버퍼 풀에 세컨더리 인덱스 페이지가 없을 때 변경 사항을 체인지 버퍼에 캐시해둔다읽기 작업으로 버퍼 풀에 세컨더리 인덱스 페이지가 생겼으면 버퍼 풀에 있는 세컨더리 인덱스 페이지로 변경 사항을 병합한다-&amp;gt; 체인지 버퍼는 세컨더리 인덱스 페이지가 버퍼 풀에 없는 경우에만 유용하다-&amp;gt; 세컨더리 인덱스 페이지가 버퍼 풀에 있으면 변경 사항은 바로 버퍼 풀에 반영된다로그 버퍼  The log buffer is the memory area that holds data to be written to the log files on disk. Log buffer size is defined by the innodb_log_buffer_size variable. The default size is 16MB. The contents of the log buffer are periodically flushed to disk. A large log buffer enables large transactions to run without the need to write redo log data to disk before the transactions commit. Thus, if you have transactions that update, insert, or delete many rows, increasing the size of the log buffer saves disk I/O.  디스크의 로그 파일에 쓸 데이터를 버퍼링하는 메모리 영역  기본 크기는 innodb_log_buffer_size=16MB  로그 버퍼의 내용은 주기적으로 디스크로 플러시  로그 버퍼 사이즈를 크게하면 대량의 트랜잭션을 커밋하기 전에 리두 로그를 디스크에 쓰지 않아도 된다  만약 많은 레코드에 변경사항이 생기는 트랜잭션을 수행해야 한다면 로그 퍼버 사이즈를 키우는 것이 성능 향상에 도움이 된다# 로그 버퍼와 관련한 몇가지 설정- innodb_log_buffer_size: 로그 버퍼의 크기. 기본값은 16MB- innodb_flush_log_at_trx_commit: 트랜잭션을 얼마나 엄격하게 지킬 것인지    - default 1: 트랜잭션의 ACID 특성을 완전히 준수 -&amp;gt; 트랜잭션이 커밋될 때마다 로그 버퍼상의 로그를 디스크의 로그 파일로 바로 플러시    - 0: 1초마다 로그 버퍼에 로그가 쓰여지고 플러시 된다. 트랜잭션을 수행했지만 그 사이에 충돌이 나서 로그를 쓰지 못하게 될 경우 데이터 손실이 발생할 수 있다    - 2: 로그 버퍼에 로그를 쓰는 시점은 각 트랜잭션이 커밋된 후다. 버퍼에 쓰인 로그가 플러시 되는 주기는 0과 같이 1초다.    - 어떤 모드를 선택하는가에 따라 안전한 트랜잭션 처리 vs 성능 간의 트레이드 오프가 있다- innodb_flush_log_at_timeout: 메모리 상의 로그 버퍼를 디스크에 있는 로그 파일로 플러시하는 주기리두 로그  The redo log is a disk-based data structure used during crash recovery to correct data written by incomplete transactions. During normal operations, the redo log encodes requests to change table data that result from SQL statements or low-level API calls. Modifications that did not finish updating data files before an unexpected shutdown are replayed automatically during initialization and before connections are accepted. The redo log is physically represented on disk by redo log files. Data that is written to redo log files is encoded in terms of records affected, and this data is collectively referred to as redo. The passage of data through redo log files is represented by an ever-increasing LSN value. Redo log data is appended as data modifications occur, and the oldest data is truncated as the checkpoint progresses.  데이터의 변경 내용을 기록하는 디스크 기반 자료구조  ACID의 D(Durable)에 해당하는 영속성과 가장 밀접하게 연관  장애로 데이터 파일에 기록되지 못한 데이터를 잃지 않게 해주는 역할  상대적으로 비용이 높은 쓰기 작업의 성능 향상을 위해 로그 버퍼에 리두 로그를 버퍼링한 후 디스크 영역에 저장  데이터가 데이터 파일에 저장되는 시점보다 리두 로그가 로그 파일에 먼저 저장 -&amp;gt; 리두 로그를 WAL 로그라고도 함  (참고로 트랜잭션 커밋이 플러시를 포함하지는 않음. 커밋은 트랜잭션에서 요구한 데이터 변경사항대로 버퍼 풀에서 데이터에 변경이 이루어졌다는 것을 의미. 만약 autoflush=True라면, 트랜잭션 커밋마다 디스크에 플러시를 수반할 것이다)# 리두 로그와 관련된 몇 가지 옵션- innodb_redo_log_capacity: 리두 로그가 차지하는 디스크의 용량- innodb_log_file_size: 디스크에 있는 로그 파일의 크기- innodb_log_files_in_group: 디스크에 있는 로그 파일의 개수버퍼 풀과 리두 로그의 관계  버퍼 풀과 리두 로그의 관계를 이해하면 버퍼 풀 성능 향상에 도움이 되는 요소를 알 수 있음  데이터 변경이 발생하면 버퍼 풀에는 더티 페이지가 생기고, 로그 버퍼에는 리두 로그 레코드가 버퍼링  이 두가지 요소는 체크포인트마다 디스크로 동기화되어야 함  체크포인트는 장애 발생시 리두 로그의 어느 부분부터 복구를 실행해야 할지 판단하는 기준점버퍼 풀 플러시  버퍼 풀을 플러시하면 버퍼 풀 메모리 공간과 리두 로그 공간을 모두 얻을 수 있음  버퍼 풀을 플러시(더티 페이지들을 디스크에 동기화)하면 오래된 리두 로그 공간을 지울 수 있음  이를 위해 InnoDB 스토리지 엔진은 주기적으로 플러시 리스트 플러시 함수를 호출  플러시 리스트에서 오래 전에 변경된 데이터 페이지 순서대로 디스크에 동기화  또한 사용 빈도가 낮은 데이터 페이지를 제거하기 위해 LRU 리스트 플러시 함수를 호출  이 때 InnoDB 스토리지 엔진은 버퍼 풀을 스캔하며 더티 페이지는 동기화 클린 페이지는 프리 리스트로 옮김언두 로그  DML(INSERT, UPDATE, DELETE)로 변경되기 이전 버전의 백업된 데이터를 기록해두는 디스크 기반 자료구조  트랜잭션 보장: 트랜잭션이 롤백될 경우 원래 데이터로 복구하기 위해 언두 로그에 백업해둔 데이터를 이용  격리수준 보장: 특정 커넥션이 변경 중인 레코드에 다른 커넥션이 접근할 경우 격리수준에 맞게 언두 로그의 이전 데이터 제공  대용량 데이터를 변경하거나, 오랜 시간 유지되는 트랜잭션이 증가할 경우 언두 로그의 크기 급격히 증가  언두 로그의 사이즈가 커지면 쿼리 실행시 스토리지 엔진은 언두 로그를 필요한 만큼 스캔해야해서 쿼리의 성능이 감소테이블 스페이스The System TablespaceThe system tablespace is the storage area for the change buffer. It may also contain table and index data if tables are created in the system tablespace rather than file-per-table or general tablespaces. In previous MySQL versions, the system tablespace contained the InnoDB data dictionary. In MySQL 8.0, InnoDB stores metadata in the MySQL data dictionary. See Chapter 14, MySQL Data Dictionary. In previous MySQL releases, the system tablespace also contained the doublewrite buffer storage area. This storage area resides in separate doublewrite files as of MySQL 8.0.20. See Section 15.6.4, “Doublewrite Buffer”.The system tablespace can have one or more data files. By default, a single system tablespace data file, named ibdata1, is created in the data directory. The size and number of system tablespace data files is defined by the innodb_data_file_path startup option. For configuration information, see System Tablespace Data File Configuration.File-Per-Table TablespacesA file-per-table tablespace contains data and indexes for a single InnoDB table, and is stored on the file system in a single data file.A file-per-table tablespace is created in an .ibd data file in a schema directory under the MySQL data directory. The .ibd file is named for the table (table_name.ibd).General TablespacesA general tablespace is a shared InnoDB tablespace that is created using CREATE TABLESPACE syntax.Undo TablespacesUndo tablespaces contain undo logs, which are collections of records containing information about how to undo the latest change by a transaction to a clustered index record.Temporary TablespacesInnoDB uses session temporary tablespaces and a global temporary tablespace.Session temporary tablespaces store user-created temporary tables and internal temporary tables created by the optimizer when InnoDB is configured as the storage engine for on-disk internal temporary tables. Beginning with MySQL 8.0.16, the storage engine used for on-disk internal temporary tables is InnoDB. (Previously, the storage engine was determined by the value of internal_tmp_disk_storage_engine.)`Session temporary tablespaces are allocated to a session from a pool of temporary tablespaces on the first request to create an on-disk temporary table. A maximum of two tablespaces is allocated to a session, one for user-created temporary tables and the other for internal temporary tables created by the optimizer. The temporary tablespaces allocated to a session are used for all on-disk temporary tables created by the session. When a session disconnects, its temporary tablespaces are truncated and released back to the pool. A pool of 10 temporary tablespaces is created when the server is started. The size of the pool never shrinks and tablespaces are added to the pool automatically as necessary. The pool of temporary tablespaces is removed on normal shutdown or on an aborted initialization. Session temporary tablespace files are five pages in size when created and have an .ibt file name extension.어댑티브 해시 인덱스MySQL과 같은 RDBMS에서 대표적으로 가장 많이 사용되는 자료 구조는 B-Tree입니다. 데이터 사이즈가 아무리 커져도 특정 데이터 접근에 소요되는 비용이 크게 증가되지 않기 때문에 어느정도 예상할 수 있는 퍼포먼스를 제공할 수 있기 때문이죠. 그치만 상황에 따라서, B-Tree 사용에 따른 잠금 현상으로 최대의 퍼포먼스를 발휘하지 못하는 경우도 있습니다.B-Tree를 통하여 굉장히 빈도있게 데이터로 접근한다면, 어떻게 될까요? DB 자체적으로는 꽤 좋은 쿼리 처리량을 보일지는 몰라도, 특정 데이터 노드에 접근하기 위해서 매번 트리의 경로를 쫓아가야하기 때문에, “공유 자원에 대한 잠금”이 발생할 수 밖에 없습니다. 즉, Mutex Lock이 과도하게 잡힐 수 있는데, 이 경우 비록 데이터 셋이 메모리보다 적음에도 불구하고 DB 효율이 굉장히 떨어지게 됩니다.이에 대한 해결책으로 InnoDB에는 Adaptive Hash Index 기능이 있다.“Adpative”라는 말에서 느껴지듯이, 이 특별한 자료구조는 명쾌하게 동작하지는 않고, “자주” 사용되는 데이터 값을 InnoDB 내부적으로 판단하여 상황에 맞게 해시를 생성” 합니다.위 그림에서 자주 사용되는 데이터들이 1,5,13,40이라고 가정할 때 위와 같이 내부적으로 판단하여 트리를 통하지 않고 “직접 원하는 데이터로 접근할 수 있는 해시 인덱스”를 통해 직접 데이터에 접근합니다.참고로, Adative Hash Index에 할당되는 메모리는 전체 Innodb_Buffer_Pool_Size의 1/64만큼으로 초기화됩니다. 단, 최소 메모리 할당은 저렇게 할당되나, 최대 사용되는 메모리 양은 알 수는 없습니다. (경우에 따라 다르지만, Adaptive Hash Index가 사용하는 인덱스 사이즈를 반드시 모니터링해야 합니다.)자주 사용되는 데이터는 해시를 통해서 직접 접근할 수 있기에, Mutex Lock으로 인한 지연은 확연하게 줄어듭니다. 게다가 B-Tree의 데이터 접근 비용(O(LogN))에 비해, 해시 데이터 접근 비용인 O(1)으로 굉장히 빠른 속도로 데이터 처리할 수 있습니다.단, “자주” 사용되는 자원만을 해시로 생성하기 때문에, 단 건 SELECT로 인하여 반드시 해당 자원을 향한 직접적인 해시 값이 만들어지지 않습니다.InnoDB는 Primary Key를 통한 데이터 접근을 제일 선호하기는 하지만, 만약 PK접근일지라도 정말 빈도있게 사용되는 데이터라면 이 역시 Hash Index를 생성합니다.InnoDB Adaptive Hash Index는 B-Tree의 한계를 보완할 수 있는 굉장히 좋은 기능임에는 틀림 없습니다. 특히나 Mutex와 같은 내부적인 잠금으로 인한 퍼포먼스 저하 상황에서는 좋은 튜닝요소가 될 수 있습니다.그러나, “자주” 사용되는 데이터를 옵티마이저가 판단하여 해시 키로 만들기 때문에 제어가 어려우며, 테이블 Drop 시 영향을 줄 수 있습니다. Hash Index 구조가 단일 Mutex로 관리되기 때문에, 수개월간 테이블이 사용되지 않던 상황에서도 문제가 발생할 수 있는 것입니다.굉장한 SELECT를 Adaptive Hash Index로 멋지게 해결하고 있다면, 이에 따른 Side Effect도 반드시 인지하고 잠재적인 장애에 대해서 미리 대비하시기 바래요. ^^gywndi’s database, InnoDB의 Adaptive Hash Index로 쿼리 성능에 날개를 달아보자!! 포스트 참고참고  Real MySQL 8.0 (1권) 책  MySQL 공식문서: InnoDB Buffer Pool  MySQL 공식문서: Change Buffer  MySQL 공식문서: The InnoDB Storage Engine  MyInfraBox: InnoDB Architecture  gywndi’s database, InnoDB의 Adaptive Hash Index로 쿼리 성능에 날개를 달아보자!!  nomadlee, 스토리지 엔진 비교",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-09T21:01:35+09:00'>09 Apr 2022</time><a class='article__image' href='/mysql-theory-series1'> <img src='/images/mysql_logo.png' alt='[MySQL] 아키텍처 (이론)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-theory-series1'>[MySQL] 아키텍처 (이론)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part5]: Code Documenting",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-python-typing",
      "date"     : "Feb 17, 2022",
      "content"  : "Table of Contents  Typing  Docstrings  참고  Documenting code for your team and your future self.We can further organize our code by documenting it to make it easier for others (and our future selves) to easily navigate and extend it. We know our code base best the moment we finish writing it but fortunately documenting it will allow us to quickly get back to that familiar state of mind. Documentation can mean many different things to developers, so let’s define the most common components:  comments: short descriptions as to why a piece of code exists.  typing: specification of a function’s inputs and outputs’ data types, providing information pertaining to what a function consumes and produces.  docstrings: meaningful descriptions for functions and classes that describe overall utility, arguments, returns, etc.  docs: rendered webpage that summarizes all the functions, classes, workflows, examples, etc.TypingIt’s important to be as explicit as possible with our code. We’ve already discussed choosing explicit names for variables, functions, etc. but another way we can be explicit is by defining the types for our function’s inputs and outputs.So far, our functions have looked like this:from typing import Listdef some_function(a: List, b: int = 0) -&amp;gt; np.ndarray:    return cThere are many other data types that we can work with, including List, Set, Dict, Tuple, Sequence and more, as well as included types such as int, float, etc. You can also use types from packages we install (ex. np.ndarray) and even from our own defined classes (ex. LabelEncoder).          Tip              Starting from Python 3.9+, common types are built in so we don&#39;t need to import them with from typing import List, Set, Dict, Tuple, Sequence anymore.    Docstrings  We can make our code even more explicit by adding docstrings to describe overall utility, arguments, returns, exceptions and more. Let’s take a look at an example:from typing import Listdef some_function(a: List, b: int = 0) -&amp;gt; np.ndarray:    &quot;&quot;&quot;Function description.    ```python    c = some_function(a=[], b=0)    print (c)    ```    &amp;lt;pre&amp;gt;    [[1 2]     [3 4]]    &amp;lt;/pre&amp;gt;    Args:        a (List): description of `a`.        b (int, optional): description of `b`. Defaults to 0.    Raises:        ValueError: Input list is not one-dimensional.    Returns:        np.ndarray: Description of `c`.    &quot;&quot;&quot;    return c          Tip              If using Visual Studio Code, be sure to use the Python Docstrings Generator extension so you can type `&quot;&quot;&quot;` under a function and then hit the `Shift` key to generate a template docstring. It will autofill parts of the docstring using the typing information and even exception in your code!    참고  sol-a-qua, 파이썬 Typing 파헤치기 - 기초편  Made With ML, Documenting Code",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-17T21:01:35+09:00'>17 Feb 2022</time><a class='article__image' href='/python-python-typing'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part5]: Code Documenting'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-python-typing'>Python Advanced Series [Part5]: Code Documenting</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part4]: 파이썬 가상환경",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-virtual-environment",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  파이썬 버전 우선순위  파이썬 가상환경 만들기          프로젝트별로 만드는 경우      용도별로 만들고 싶은 경우        명령어 모음  참고  pyenv 와 venv를 활용해 파이썬 가상환경 관리하기파이썬 버전 우선순위- pyenv local: 디렉터리 단위로 지정되는 파이썬 버전- pyenv global: 디렉터리에 지정된 local 버전이 없는 경우 지정되는 파이썬 버전- system python: venv 도구가 관리하지 않는 로컬에 설치된 파이썬 버전  pyenv는 ~/.pyenv 디렉터리에 버전별로 파이썬 패키지를 관리하고 있다  version 파일은 global 버전을 나타낸다  versions 디렉터리에 버전별로 파이썬 패키지가 저장돼있다파이썬 가상환경 만들기  프로젝트별로 만들고 싶은 경우          (ex. 이미지 분류 프로젝트, 음성 인식 프로젝트 등)      프로젝트 디렉토리마다 가상환경을 만든다        용도별로 만들고 싶은 경우          (ex. 딥러닝 목적, 데이터 분석 목적, 웹개발 목적 등)      별도의 디렉터리에 가상환경들을 모아둔다      프로젝트별로 만드는 경우  프로젝트 디렉터리를 만든다  현재 global은 3.8.13 이고 해당 디렉터리에 local은 아직 지정되지 않았다  해당 프로젝트에서 파이썬 버전으로 3.9.1을 쓴다고 해보자    pyenv install 3.9.1        해당 프로젝트 폴더에서 local 로 3.9.1을 지정하자  이제 파이썬 3.9.1 버전을 바탕으로 가상환경을 하나 만들자  (보통 프로젝트별로 가상환경을 만들 때는 가상 환경 이름을 .venv로 만드는게 관례다)    python3 -m venv &amp;lt;원하는 가상환경 이름&amp;gt;            가상환경을 사용하려면 활성화 해야 한다    source ./.venv/bin/activate            가상환경으로 접속됐는지 확인해보자 (activate된 경우 해당 가상환경 폴더 출력. deactivate된 경우 출력 안함)    echo $VIRTUAL_ENV            이제 가상환경에 원하는 라이브러리를 설치해보자    pip3 install pandas            가상환경을 비활성화(deactivate) 시키면 가상환경에서 설치했던 pandas 라이브러리가 다시 안보인다용도별로 만들고 싶은 경우  프로젝트별로 만드는 방법과 똑같다  차이는 용도별로 만들어진 가상환경은 여러 프로젝트에서 쓰일 것이므로, 조금 더 범용적인 위치에 설치해두는 것이 좋다  나는 홈 디렉터리(~)에 가상환경들을 위한 virtual_environments 디렉터리에 가상환경들을 만들어 뒀다  데이터 분석을 위한 용도로 가상환경을 한 번 만들어보자mkdir ~/virtual_environmentscd ~/virtual_environmentsmkdir data-analysiscd data-analysispyenv local 3.8.13python3 -m venv data_analysis_3.8 # 3.8 버전의 데이터 분석을 위한 가상 환경(data_analysis_3.8) 생성source ~/virtual_environments/data_analysis/data_analysis_3.8/bin/activate(# 이렇게 3.8 버전을 만들고 pyenv local 3.8.13 이렇게 하는게 더 정확한 방법인 것 같음mkdir 3.8cd 3.8pyenv local 3.8.13python3 -m venv . # 3.8 버전의 데이터 분석을 위한 가상 환경(3.8) 생성source ~/virtual_environments/data_analysis/3.8/bin/activate)pip3 install scikit-learnpip3 install matplotlibpip3 install pandas명령어 모음# pyenv를 이용해 설치한 파이썬 버전 목록pyenv versions# pyenv로 설치 가능한 파이썬 버전 목록pyenv install --list# 원하는 버전의 파이썬 설치/삭제pyenv install 3.8.13pyenv uninstall 3.8.13# global 또는 local 버전 확인pyenv globalpyenv local# global 또는 local 버전 설정pyenv global 3.8.13pyenv local 3.8.13# local 버전 해제pyenv local --unset# 가상환경 생성python3 -m venv &amp;lt;원하는 가상환경 이름&amp;gt;# 가상환경 활성화source &amp;lt;원하는 가상환경 디렉터리 위치&amp;gt;/bin/activate# 가상환경 비활성화deactivate# 가상환경 확인echo $PYENV_VERSION# 가상환경 삭제rm -rf &amp;lt;삭제하고 싶은 가상환경 디렉터리 위치&amp;gt;참고  SSAMKO의 개발 이야기, [python] pyenv로 원하는 파이썬 버전 설치하기  WINDY BAY, 파이썬 가상환경이 필요한 이유와 사용법 (venv, virtualenv)  아무튼 워라벨, 파이썬 가상환경 venv 사용하기 (패키지 쉽게 관리하기)  donghh0221, [파이썬] venv 가상환경 관리법(파이썬 버젼 다운그레이드하기)  공순이의 블로그, pyenv를 이용한 여러 개의 Python 버전 관리하기 + 가상 환경 만들기(ubuntu)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-virtual-environment'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part4]: 파이썬 가상환경'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-virtual-environment'>Python Advanced Series [Part4]: 파이썬 가상환경</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part3]: 파이썬을 위한 테스트 도구(feat. pytest)",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-pytest",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  TDD  pytest          pytest 맛보기      pytest 디렉토리 구조      pytest fixtures        참고이번 글에서는 pytest를 알아보고자 합니다.pytest란 무엇인가? 정말 이름 그대로 py(thon)을 test 하는 프레임워크를 의미합니다.  pytest is a mature full-featured Python testing tool that helps you write better programs.Python testing tool로써 좋은 프로그램을 작성하도록 도와준다라, 이걸 이해하기 위해서는 왜 테스트가 존재해야 하는가에 대해 생각할 필요가 있습니다TDD최근 많이 주목받고 있는 TDD (Test Driven Development)를 아시나요? 짧게 설명드리면 본격적인 개발에 들어가기 전에 테스트 계획 및 코드를 작성하는 것을 의미합니다. 테스트가 개발을 이끌어 나가는 것이 되는 것이죠.그럼, 왜 TDD 같은 테스트가 우선시 되는 개발이 나오게 되었고, 주목받고 있는 것일까요?자, 개발 중 에러 및 오류가 발생했다고 합시다.이 상황이 정말 작은 소규모 개발 중에 일어난 것이라면 사실 큰 문제가 되지 않습니다. 개발한 모듈간에 연결성도 적을 뿐 아니라, 코드 양이 방대하지 않을 것이기 때문에 바로 찾아 문제를 해결할 수 있을 것입니다.하지만 아주 대규모의 개발 상황이라고 가정해봅시다. 수 많은 모듈, 함수간 종속성과 매우 많은 코드 양이 있기 때문에 오류 및 에러를 잡는데 많은 시간과 인력을 투입하게 될 것입니다. 이러한 상황은 비즈니스적으로도 효율적이지 못하겠죠? 당연히 안정적인 프로그램을 개발해 나가는데도 많은 걸림돌이 될 것입니다.pytest이러한 문제를 해결하기 위해 TDD. 즉, 테스트 주도 개발이 나오게 된 것입니다. 그리고 Python에서 TDD를 하기 위해 나온 프레임워크가 pytest입니다.pytest 맛보기.├── py_test_1.py└── py_test_2.py# py_test_1.py# 테스트를 해보고 싶은 함수def func(x):    return x + 1# 테스트 함수def test_answer():    assert func(3) == 5# py_test_2.pyclass TestClass:    def test_one(self):        x = &quot;Hello, hi&quot;        assert &quot;h&quot; in x    def test_two(self):        x = &quot;what&quot;        assert hasattr(x, &quot;who&quot;)python -m pytest py_test_1.py # 특정 파일을 테스트할 때python -m pytest *.py # 테스트 폴더 안의 모든 파일을 테스트할 때pytest 디렉토리 구조지금까지는 Command line에서 pytest 명령어를 통해 테스트를 실행했고, 또한 한 파일에 일반 함수와 테스트 코드들이 공존했습니다.하지만 실제로 프로젝트에서 활용되는 데 있어서는 테스트 코드를 따로 관리하고, 이에 맞게 끔 구조를 구성해놓는 것이 효율적입니다.그래서 테스트 코드는 프로젝트 코드들과 다르게 tests 라는 디렉토리를 통해서 관리를 합니다.전체적으로 디렉토리 트리를 보면 다음과 같습니다.project/    core_code/        __init__.py        sample_code1.py        sample_code2.py        sample_code3.py    tests/        test_sample1.py        test_sample2.py        test_sample3.pypytest fixturespytest의 특징중 하나인 fixture는 다음과 같은 의미를 가집니다.  시스템의 필수조건을 만족하는 테스팅 프로세스를 설정하는 것 (A software test fixture sets up a system for the software testing process by initializing it, thereby satisfying any preconditions the system may have.)  같은 설정의 테스트를 쉽게 반복적으로 수행할 수 있도록 도와주는 것 (The advantage of a test fixture is that it allows for tests to be repeatable since each test is always starting with the same setup. Test fixtures also ease test code design by allowing the developer to separate methods into different functions and reuse each function for other tests.)간략하게 말하면 수행될 테스팅에 있어 필요한 부분들을 가지고 있는 코드 또는 리소스라고 말할 수 있습니다.import pytestfrom src.calculator import Calculator@pytest.fixturedef calculator():    calculator = Calculator()    return calculatordef test_add(calculator):    assert calculator.add(1, 2) == 3    assert calculator.add(2, 2) == 4def test_subtract(calculator):    assert calculator.subtract(5, 1) == 4    assert calculator.subtract(3, 2) == 1def test_multiply(calculator):    assert calculator.multiply(2, 2) == 4    assert calculator.multiply(5, 6) == 30보시다시피, 먼저 @pytest.fixture를 통해 fixture를 선언합니다. 그리고 fixture function을 정의할 수 있습니다.이렇게 정의된 fixture function를 parameter로 사용하여 테스트를 위한 클래스를 가져올 수 있는 것입니다. 이렇게 되면 중복코드는 물론이고, 계속해서 필요한 모듈, 클래스가 있을 때마다 선언을 하기보다 간단히 parameter를 통해 가져올 수 있습니다.이렇게 보니, 앞서 정의된 test fixture에 대한 정의가 와닿지 않나요?사실 fixture에 대한 것은 이게 끝이 아닙니다.‘대규모의 프로젝트인 경우엔 테스트마다 필요한 모듈, 클래스 등 리소스 및 코드들이 달라 필요한 fixture의 양이 매우 많아질 것입니다.또한, 테스트 코드(py)마다 중복되는 fixture도 있을 겁니다. 예를 들어, A 테스트 코드에서도 계산기 클래스가 필요한데, B 테스트 코드에서도 계산기 클래스가 필요한 경우 말이죠. 지금까지의 경우로 보자면 두 테스트 코드 파일 위에 fixture를 따로 선언한 후 사용했어야 했습니다.이러한 문제를 해결하기 위해 conftest.py를 사용합니다.fixture 코드들은 conftest.py에 선언해두면, 모든 테스트 코드에서는 해당 fixture들을 공유하여 사용할 수 있습니다. 알아서 pytest에서 공유해주는 마법!(fixture들을 모아두는 모듈 -&amp;gt; conftest.py)# Directory treesrc/  __init__.py  calculator.pytests/  conftest.py  test_code1.py  test_code2.py  test_code3.py# conftest.pyimport pytestimport syssys.path.append(&#39;/Users/peter/algo_folder&#39;) # src 폴더가 tests 밖에 있기 때문에, 그냥 이름으로 import 안됨from src.calculator import Calculator@pytest.fixturedef calculator():    calculator = Calculator()    return calculator# test_code.pydef test_add(calculator):    &quot;&quot;&quot;Test functionality of add.&quot;&quot;&quot;    assert calculator.add(1, 2) == 3    assert calculator.add(2, 2) == 4    assert calculator.add(9, 2) == 11def test_subtract(calculator):    &quot;&quot;&quot;Test functionality of subtract.&quot;&quot;&quot;    assert calculator.subtract(5, 1) == 4    assert calculator.subtract(3, 2) == 1    assert calculator.subtract(10, 2) == 8def test_multiply(calculator):    &quot;&quot;&quot;Test functionality of multiply.&quot;&quot;&quot;    assert calculator.multiply(2, 2) == 4    assert calculator.multiply(5, 6) == 30    assert calculator.multiply(9, 3) == 27참고  KimDoubleB, [pytest] python 코드를 테스트 해봅시다  Real Python, Effective Python Testing With Pytest",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-pytest'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part3]: 파이썬을 위한 테스트 도구(feat. pytest)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-pytest'>Python Advanced Series [Part3]: 파이썬을 위한 테스트 도구(feat. pytest)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-memory_allocation",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  메모리 관리          Python Memory Manager      Garbage Collection        메모리 할당          Stack 할당      Heap 할당        참고요즘에는 컴퓨터, 스마트폰을 사용할 때 한가지 프로그램/어플리케이션만 실행하는 사람은 없을 것입니다. 그렇기 때문에 내가 만든 프로그램/어플리케이션이 메모리를 효율적으로 사용하도록 개발하는 것은 중요합니다.메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다. 메모리 할당은 내가 사용하는 프로그래밍 언어, 운영체제, 컴퓨터 아키텍처에 따라 조금씩 다르지만 전체적인 과정은 비슷합니다.파이썬의 메모리 관리는 대부분 Python Memory Manager에 의해 수행되지만, Python Memory Manager를 공부하면 프로그래밍 전반에 대한 이해와 코드 최적화, 디버깅을 더욱 잘 할 수 있게 될 것입니다.메모리 관리파이썬에서의 메모리 관리는 크게 두 가지 레벨로 나누어서 생각할 수 있습니다. 첫 번째로 운영체제 레벨에서는 각 프로세스에 메모리를 얼마나 할당할지를 정하고, 각각의 프로세스가 다른 프로세스에 접근하지 못하도록 관리합니다.두 번째로 파이썬 내에서의 메모리 관리입니다.파이썬에서는 컴파일 단계에서 스택 영역에 메모리를 정적으로 크기를 정하고, 실행 단계에서는 Python memory manager를 이용해 힙 영역에 동적으로 메모리를 할당하고 그 외의 역할(공유, 할당, 제거 등)들을 수행함으로써 메모리를 관리합니다.Python Memory ManagerPython memory manager는 모든 Python objects와 data structures를 포함하는 private 힙을 포함합니다. Python memory manager는 공유(sharing), 분할(segmentation), 사전 할당(preallocation) 또는 캐싱(caching)과 같은 다양한 동적 스토리지 관리 측면을 다루는 다양한 구성 요소를 가지고 있습니다.가장 낮은 수준에서 raw memory allocator는 운영 체제의 메모리 관리자와 상호 작용하여 모든 파이썬 관련 데이터를 저장할 수 있는 충분한 공간을 private 힙에 확보합니다. raw memory allocator 외에도 여러 object-specific allocators가 동일한 힙에서 object의 특성에 맞는 고유한 메모리 관리 정책을 구현합니다.파이썬 힙의 관리는 인터프리터 자체에 의해 수행되며 힙 내부의 메모리 블록에 대한 객체 포인터를 사용자가 직접 제어할 수 없다는 것을 의미합니다. Python objects를 위한 힙 공간 할당은 파이썬/C API 함수를 통해 파이썬 메모리 관리자에 의해 수행됩니다.Garbage Collection가비지 컬렉션은 인터프리터가 프로그램을 사용하지 않을 때 프로그램의 메모리를 비우는 것입니다. 파이썬이 이렇게 할 수 있는 것은 파이썬 개발자들이 백엔드에서 우리를 위해 가비지 컬렉터를 구현했기 때문입니다. 파이썬 가비지 컬렉터는 reference counting 방법으로 객체에 더 이상 참조가 없을 때는 객체가 차지하고 있던 메모리의 할당을 취소하고 메모리를 비우게 됩니다.메모리 할당메모리 할당(memory allocation)은 프로그램이 컴퓨터 메모리의 특정 빈 블록에 할당되거나 할당되는 과정을 의미합니다. 파이썬에서 이 모든 것은 Python memory manager에 의해 수행됩니다.Stack 할당스택 할당은 정적 메모리를 저장하는데, 정적 메모리는 특정 함수나 메서드 호출 내에서만 필요한 메모리입니다. 함수가 호출되면 프로그램의 호출 스택에 추가됩니다. 변수 초기화 같은 특정 함수 내부의 모든 로컬 메모리 할당은 함수 호출 스택에 임시로 저장되며, 함수가 돌아오면 삭제되고 호출 스택이 다음 작업으로 이동합니다. 연속적인 메모리 블록에 대한 이 할당은 미리 정의된 루틴을 사용하여 컴파일러에 의해 처리되기 때문에 개발자들은 이것에 대해 걱정할 필요가 없습니다.(그러면 이 부분은 함수, 메서드, 그리고 함수나 메서드 안에서 사용되는 지역변수들을 컴파일 단계에서 확인하고 이에 알맞은 메모리 크기를 예측해서 정적으로 할당하는 건가?)(그리고 코드 실행 단계에서 함수가 호출될 때마다 스택에 Call stack이 쌓인다?)Heap 할당힙 할당은 프로그램에서 전역 범위로 사용되는 메모리인 동적 메모리를 저장합니다. 이러한 변수들은 특정 메서드나 함수 호출 외부에 필요하거나 전역적으로 여러 함수 내에서 공유됩니다. 스택 할당과 달리 힙 할당은 힙 데이터 구조와 관련이 없습니다. 힙 영역은 단순히 할당하고 어느 정도 임의의 주소에서 자유롭게 사용할 수 있는 큰 메모리 공간이며, 저장되는 객체에 필요한 공간을 기반으로 합니다.(힙 할당은 코드 실행 단계에서 객체의 타입에 맞게 동적으로 메모리 할당된다?)(Python memory manager의 가비지 컬렉션 기능에 의해 더이상 참조되지 않는 객체는 제거되고 메모리 비운다?)참고  How does Memory Allocation work in Python (and other languages)?  Python 공식문서: Memory Management  What’s the difference between a stack and a heap?  RealPython: Memory Management in Python  muchogusto.log: 파이썬 런타임과 메모리 관리  python의 메모리 할당과 관리 (Stack &amp;amp; Heap Memory)  파이썬 메모리 영역",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-memory_allocation'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-memory_allocation'>Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)</a> </h2><p class='article__excerpt'>메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-cpython_pypy",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  파이썬 언어의 특징  파이썬은 인터프린터 언어?  컴파일 언어  인터프리터 언어  파이썬 인터프리터의 종류          CPython      Jython      PyPy        참고파이썬 언어의 특징파이썬(Python)은 1980년대 후반 귀도 반 로섬이 개발하고 1991년에 출시한 high-level 범용 프로그래밍 언어입니다. 동적 타입 언어이므로 변수의 타입을 선언할 필요가 없으며, 코드가 실행되어 메모리 관리가 자동으로 수행됩니다.이번 포스트에서는 파이썬 코드를 실행할 때 어떤 동작이 내부적으로 일어나는지 알아보도록 하겠습니다.저희가 CLI 환경에서 파이썬 코드를 실행하는 경우를 생각해봅시다.python my_code.py여기서 python은 바로 파이썬 인터프리터 프로그램을 의미합니다. 따라서 파이썬 코드는 파이썬 인터프리터를 통해 실행하는 것입니다.  (Python Interpreter - Stanford Computer Science 참고)파이썬은 인터프린터 언어?위에서 파이썬 인터프리터를 통해 파이썬 코드를 실행한다고 했습니다. 그러면 파이썬은 인터프리터 언어일까요? 과거에 C는 컴파일 언어이고, 쉘 프로그래밍 언어는 인터프리터 언어였기 때문에 이 경계가 비교적 명확했지만, 비교적 최근에 나온 언어들은 그 경계가 모호합니다.파이썬 또한 어느 정도의 컴파일 언어적 특성과, 인터프리터 언어의 특성을 모두 가지고 있어서 이분법적으로 나누기가 힘들지만 프로그래밍 언어의 대표인 C언어가 완전한 컴파일 언어이기 때문에 이와 구분하기 위해 그냥 편하게 인터프리터 언어라고 하는 것 같습니다. 정리하면 파이썬은 컴파일 언어이기도 하면서 인터프리터 언어이기도 합니다.컴파일 언어먼저 간단하게 컴파일 언어의 뜻과 컴파일 언어가 코드를 실행하는 과정에 대해 살펴보겠습니다.  컴파일은 소스코드를 다른 타겟 언어(기계어, 자바, C, 파이썬)로 변환하는 과정을 의미  코드를 실행할 때 코드 전체를 인풋으로 사용  코드는 컴파일 단계에서 한 번 기계어로 변환되어 저장되고 나면 언제든 바로 실행가능  컴파일러는 실행하는 역할이 아니고 기계어로 변환하는 역할인터프리터 언어  인터프리터 언어는 소스 코드를 바로바로 실행하게 됩니다.  인터프리터는 코드를 한 줄씩 입력으로 사용해 실행합니다.  인터프리터에는 종류마다 다른 실행 방법이 있습니다.  A. 소스 코드를 파싱해서 설정한 방법에 따라 실행  B. 소스 코드를 먼저 중간 단계의 바이트 언어로 변환하고 A의 과정을 수행  C. 컴파일러에 의해 먼저 변환된 코드를 이용해 1의 과정을 수행파이썬 인터프리터는 이 중 B에 해당합니다.1. 소스 코드를 컴파일러를 이용해 중간 단계의 바이트 언어의 파일(.pyc)로 변환2. Python Virual Machine 위에서 바이트 언어 파일 한 줄씩 실행  파이썬 인터프리터의 종류CPythonJythonPyPyPyPy는 파이썬 언어(RPython: 파이썬의 일부)로 작성된 인터프리터입니다. 디폴트인 CPython과의 호환성을 유지하면서도 CPython 보다 속도가 빠르다고 알려져있습니다.참고  Shalini Ravi, How Does Python Code Run: CPython And Python Difference  elhay efrat, Python » CPython  geeksforgeeks, Difference between various Implementations of Python  파이썬 언어의 특징  파이썬 인터프리터 고르기  Wireframe: 파이썬은 인터프리터언어입니까?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-cpython_pypy'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-cpython_pypy'>Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part6]: 네트워크 프로토콜(3) IP",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series6",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  IP(Internet Protocol)          IP 주소      서브넷 마스크(Subnet Mask)      IP 주소 예시        라우팅          라우팅 테이블      게이트웨이        참고IP(Internet Protocol)IP는 네트워크 상에서 데이터를 목적지로 보내는 역할을 하는 인터넷 계층에서 사용하는 프로토콜IP 주소IP 주소는 인터넷에 연결하고자 하는 디바이스가 가지고 있는 NIC(Network Interface Controller)의 고유한 주소를 뜻합니다. 편지를 주고 받기 위해서는 서로의 주소가 필요한 것처럼 디바이스간 통신을 위해서는 IP주소가 필요합니다. IP주소는 네트워크 번호와 호스트 번호로 이루어진 32비트 숫자입니다.(IPv4 기준)  내부 네트워크에 연결되는 라우터의 포트를 이더넷 인터페이스 (이더넷용 IP 주소는 내부에서 부여받은 IP 주소 중 첫 번째 주소)  외부(인터넷) 쪽으로 연결되는 인터페이스를 시리얼 인터페이스 (시리얼용 IP 주소는 ISP 업체의 라우터가 가지는 시리얼 인터페이스의 IP 주소)  위와 같은 가정에서 우리가 라우터에 부여해야 하는 IP 주소는 두 개가 됨  IP 주소 중 네트워크 부분: 하나의 브로드캐스트 영역          라우터를 거치지 않고 통신이 가능한 영역      라우터가 라우팅할 때 참고하는 부분      라우터는 다른 네트워크로 갈 때만 필요        IP 주소 중 호스트 부분: 각각의 PC 또는 장비  IP 주소의 Class에 따라 어디까지가 네트워크 부분이고, 어디까지가 호스트 부분인지가 나뉨 (네트워크의 크기가 달라짐)          클래스 A는 이진수 중에서 맨 앞쪽 숫자가 항상 0으로 시작되는 것들                  호스트 수가 가장 많은 클래스          앞의 8비트가 네트워크 부분, 나머지 24비트가 호스트 부분          1.0.0.0 ~ 126.0.0.0 까지로 규정 (0 시작과 127 시작은 제외) -&amp;gt; 126개의 네트워크, 각각의 네트워크는 라우터 없이 통신          호스트는 2의 24승 - 2개(모두 0인 경우, 모두 1인 경우) = 16,777,214개 -&amp;gt; 16,777,214개의 호스트가 하나의 네트워크에 연결          따라서 IP 주소를 모두 클래스 A로만 구성한다면 -&amp;gt; 126개의 네트워크 * 16,777,214개의 호스트                    클래스 B는 맨 앞이 10으로 시작                  앞의 16비트가 네트워크 부분          128.0.0.0 ~ 191.255.0.0 까지로 규정                    클래스 C는 맨 앞이 110으로 시작                  앞의 24비트가 네트워크 부분          192.0.0.0 ~ 223.255.255.0 까지로 규정                      기본 게이트웨이(Default Gateway)          내부 네트워크에서는 라우터 없이도 통신이 가능      내부 네트워크에 없는 IP 주소로 갈 때는 이 기본 게이트웨이를 통해 나감      즉 라우터의 이더넷 인터페이스를 의미        라우터는 인터페이스별로 IP 주소 부여. 스위치나 허브는 장비별로 IP 주소 부여서브넷 마스크(Subnet Mask)  주어진 네트워크를 가공할 때 사용  우리가 일단 어떤 IP 주소를 배정받게 되면 보통 이 주소를 그대로 사용하지 않고 서브넷 마스크를 조합하여 사용  우리가 부여받은 net을 여러개의 subnet으로 나눈다는 의미  서브넷마스크를 통해 나누어진 서브넷간의 통신은 라우터를 거쳐야함  모든 IP 주소에는 서브넷 마스크가 따라다님. 쓰지 않더라도. 그래야 지금 IP 주소가 마스킹 된건지 아닌지 알 수 있음  클래스 A의 기본 서브넷 마스크는 255.0.0.0, B는 255.255.0.0, C는 255.255.255.0  서브넷 마스크는 IP주소의 어디까지가 네트워크 부분이고, 어디까지가 호스트 부분인지를 나타내는 역할을 함  서브넷 마스크의 이진수 1은 네트워크 부분, 이진수 0은 호스트 부분을 의미함  즉, 서브넷 마스킹은 기존 IP 주소의 호스트 부분의 일부를 네트워크 부분으로 바꾸는 작업기존 네트워크: 150.150.100.1 =&amp;gt; 1001 0110 1001 0110 0110 0100 0000 0001 =&amp;gt; 클래스 B =&amp;gt; 150.150.0.0이 네트워크를 의미서브넷 마스크: 255.255.255.0 =&amp;gt; 1111 1111 1111 1111 1111 1111 0000 0000 =&amp;gt; 네트워크 자리가 16자리에서 24자리 까지로 늘림 (호스트를 8자리로 줄임)------------------------------------------------------------------------------------------------------------------------------서브넷: 150.150.100.0 =&amp;gt; 1001 01110 1001 01110 01110 0100 0000 0000 =&amp;gt; 최종적으로 서브넷 네트워크가 150.150.100.0가 됨  참고로 호스트 부분이 0인 주소는 호스트 주로로 사용하지 못함. PC에서 사용하는 주소가 아니라 네트워크 자체를 의미  또 호스트 부분이 255인 주소 역시 호스트 주소로 사용할 수 없음. 브로드캐스트 주소 (모든 호스트에게 메시지 보낼 때 사용하는 주소)IP 주소 예시  나의 IP 주소가 150.150.100.1이라고 해보자.  그러면 내가 부여받은 IP 주소의 네트워크 주소는?          이 물음에 대답하려면 먼저 내 IP주소의 클래스가 뭔지 알아야 한다.      150...*는 클래스 B에 속한다 -&amp;gt; 앞의 16자리가 네트워크 주소이다      즉 네트워크 주소는 150.150.0.0이다        만약 내가 AWS의 VPC와 같은 서비스를 이용해 가상 네트워크를 부여 받았는데 그 값이 150.150.0.0이라 해보자  150.150.0.0의 나머지 16비트를 이용해 호스트에게 IP 주소를 부여할 수 있다. -&amp;gt; 2^16개이므로 65,536개. 여기서 2개를 빼면 65,534개를 호스트에 부여할 수 있다  여기서 10비트를 더 사용해 서브네팅해보자.  여기서 IP주소를 하나 가져와보자. 150.150.252.211/26을 예로 들어보자. 이 IP주소를 보고 가질 수 있는 물음은 다음과 같다          이 IP주소의 네트워크 주소는?                  26비트가 네트워크 주소이므로, 24자리 150.150.252까지는 당연하고,          나머지 211을 이진수로 나타내면 1101 0011인데 이중 왼쪽 2자리까지가 네트워크 주소에 포함되므로,          150.150.252.192.0이 서브넷 네트워크 주소가 된다                    이 IP주소의 기본 서브넷 사용시 네트워크 주소는?                  150.150.252.211/26는 클래스 B이므로 기본 서브넷은 16비트일 것이다          그러면 기본 네트워크 주소는 150.150.0.0이 될 것이다                    이 IP주소는 150.150.255.193과 같은 서브넷 안에 있는가? (라우팅 없이 통신이 가능한가?)                  라우팅 없이 통신이 가능한지 보려면 같은 네트워크 주소가 같은지 확인해보면 된다          150.150.252.211/26는 150.150.252.192.0였고,          150.150.255.193는 150.150.255.192이므로 서로 다른 네트워크 상에 있다 -&amp;gt; 라우터가 있어야 한다                    이 IP주소가 속한 네트워크의 브로드캐스트 주소는?                  이를 구하려면 나의 IP주소 중 호스트 부분을 모두 1로 바꾸면 된다          150.150.252.255/26가 될 것이다          주의할 점은 브로드캐스트 주소는 무조건 끝자리가 255로 끝나지는 않는다.                          이를 확인하기 위해 150.150.0.130/26가 있는 네트워크 상의 브로드캐스트 주소를 한 번 구해보자              이는 2진수로 1001 0110. 1001 0110. 0000 0000. 1000 0010인데 여기서 호스트 부분이 오른쪽 6자리를 1로 바꿔보자              1001 0110. 1001 0110. 0000 0000. 1011 1111 -&amp;gt; 150.150.0.191/26이 브로드캐스트 주소가 된다                                          라우팅목적지 IP 주소를 찾아가는 과정라우팅 테이블MAC addresses are determined for a device when it is manufactured. They don’t change wherever you go. So assume MAC addresses as your name(assume it’s unique).Ip addresses are assigned(by your ISP) when your machine connects to the network. It depends on your location. So assume it as your address.If someone needs to send you a post, if they use your unique name, then routing that post to you will be difficult. Now if they use your address, routing becomes easier.That’s why IP addresses are used. Routing a packet to a IP address is easier than routing it to a MAC address.The routing table is used to find out if a packet should be delivered locally, or routed to some network interface using a known gateway address.MAC addresses are layer-2 addresses, IP addresses are layer-3 addresses, and ports are layer-4 addresses.MAC addresses are not in the packet headers, they are in the frame headers. Only layer-3 addresses are in the packet headers. Ports are in the segment headers.MAC addresses are only significant on a LAN. They are in the frame headers, and frames are stripped at layer-3 boundaries (routers). The routers then use the layer-3 headers with the layer-3 address to forward a packet to the next interface, where  서버든 라우터든 패킷을 보내기 위해서는 일단 라우팅 테이블을 참조함  라우팅 테이블을 보니 라우팅 엔트리는 1.1.1.0/24, Gateway는 없음 -&amp;gt; 목적지 주소가 나와 같은 네트워크에 존재함을 확인  이제 Server1은 목적지 주소 1.1.1.20에 대한 MAC 주소를 알기위해 자신의 ARP 테이블을 참조함  그런데 ARP 테이블이 비어있음 -&amp;gt; ARP miss  Server1은 Server2(1.1.1.20)의 MAC 주소를 알아내기 위해 lan1 포트로 ARP request 패킷을 보냄  Switch1은 이 패킷을 수신하고, 수신 패킷의 Source MAC 주소를 배웁니다          (Switch1의 MAC 테이블에 Server1의 MAC주소와 Switch1의 port를 기록)        Switch1은 이 ARP request 패킷을 보고 Destination MAC이 브로드 캐스팅 주소임을 확인  Switch1은 수신포트 fe1을 제외한 나머지 모든 포트로 flooding  라우터 R1은 패킷의 Target IP 주소를 보고 자기 것이 아닌 것을 확인하고 버림  Server2는 자신의 IP 주소임을 확인 -&amp;gt; 자신의 MAC 주소를 필드에 담아 ARP reply 패킷을 lan1 포트로 보냄  이 패킷을 수신한 Switch1은 Source MAC Learning을 하여 MAC 테이블에 기록  Server1은 자신의 ARP 테이블에 MAC 주소를 기록  이제 Server1은 Server2로 IP 패킷을 보냄  Server1은 목적지 주소 1.1.1.20에 대한 MAC 주소를 ARP 테이블에서 얻어오고 이동해야할 포트 번호를 MAC 테이블을 통해 확인  이 패킷은 fe2 포트를 통해 나가고, Server2가 패킷을 수신게이트웨이다른 네트워크로 가기위한 통로A traditional “gateway” can mean a device that (sometimes) is able to route traffic, butwhose primary goal is to translate from one protocol to another. For example- I woulduse a “gateway” if I wanted to send packets from my IPv4 network to/from a IPv6 network,or maybe an AppleTalk network to/from a Token Ring network.게이트웨이는 개념적인 용어, 라우터는 장비네트워크 구성환경에 따라 라우터가 게이트웨이 역할을 할 수도 있고, 다른 장비나 소프트웨어가 게이트웨이 역할을 할 수도 있다.더 깊게 들어가면 L3 장비들인데요.  Layer3 는 IP를 와 연관되어 있습니다.source: 192.168.1.10 / destination : 8.8.8.8source 가 되는 PC에도 routing table 이 있습니다. (cmd - route print)여기에 목적지 정보가 없으면 무조건 gateway 로 패킷을 보냅니다.즉, 어떤 host 에게 Gateway 란건 내 패킷을 무조건 라우팅 시켜줄 통로입니다.gateway 가 되는 장비들이 L3 장비들이고 Router 가 되겠죠.공유기는 간단히 표현하면 Router + DHCP + NAT 역할을 합니다방화벽은 간단히 표현하면 Router + ACL + Firewall 등등의 기능을 합니다.다시 정리하자면.. Gateway 는 개념이지 장비명이 아닙니다.결론적으로 말하자면 cisco에서 얘기하는 게이트웨이 모드는 간단하게 얘기했을 때 우리가 흔히 말하는 공유기처럼 NAT가 활성화된 상태를 말하고, 라우터 모드는 NAT가 비활성화된 상태를 말합니다.사실 게이트웨이라는 것은 라우터와 구분되는 개념이 아니라, 라우터 중에서 관문(gateway) 역할을 하는 라우터를 게이트웨이 라우터라고 하는데 이를 그냥 게이트웨이라고 짧게 부르는 것 뿐입니다.고속도로로 비유한다면 게이트웨이가 아닌 라우터는 JC이고 게이트웨이인 라우터는 IC라고 볼 수 있겠네요. 둘다 도로와 도로를 이어주는 역할을 수행하지만(routing) IC는 특히 일반도로에서 고속도로로 진입하는 관문(gateway) 역할을 하는 점에서 생각해보면 될 것 같습니다.네트워크와 네트워크 사이를 이어주는 역할을 하는 것이 라우터이고, 그 중에서도 가장 끄트머리에 위치한 네트워크(가정이나 회사)를 중앙(인터넷 등)으로 연결해주는 라우터를 그저 좀 특별하게 부르는 것이 게이트웨이인 것입니다.방화벽 그자체의 의미로는 라우팅의 개념은 전혀 없고 단지 조건에 따라 패킷을 걸러주는 역할을 하는 것 뿐입니다. 방화벽 장비 중에 라우팅 역할도 하는 장비가 있을 뿐이지요.참고  Microsoft, Understand TCP/IP addressing and subnetting basics  groom, 서브넷과 Broadcast 주소  스위칭과 라우팅… 참 쉽죠잉~ (1편: Ethernet 스위칭)  스위칭과 라우팅… 참 쉽죠잉~ (2편: IP 라우팅)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/network-series6'> <img src='/images/network_logo.png' alt='Network Series [Part6]: 네트워크 프로토콜(3) IP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series6'>Network Series [Part6]: 네트워크 프로토콜(3) IP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series5",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  TCP          TCP의 특징      3 way handshake        UDP          UDP의 특징        TCP와 UDP 비교          데이터 전송시 발생할 수 있는 오류        Port  TCP와 UDP  TCP와 UDP의 헤더 분석          TCP Header      UDP Header        TCP의 3 way handshake와 4 way handshake  참고TCP와 UDP는 두 대의 컴퓨터를 서로 연결(connection)하는 역할을 하는 전송 계층(Transport layer)에서의 프로토콜TCPTCP의 특징  Transmission Control Protocol  연결 지향 TCP 3 way handshake (논리적 연결)  데이터 전달 보증  가상 회선 패킷 교환 방식 -&amp;gt; 경로가 이미 정해져 있음 -&amp;gt; 순서 보장3 way handshake  클라이언트와 서버를 연결하는 방법UDPUDP의 특징  User Datagram Protocol  기능이 거의 없음 (연결 지향 X, 데이터 전달 보증 X, 순서 보장 X)  단순하고 빠름  애플리케이션에서 추가 작업 필요TCP와 UDP 비교데이터 전송시 발생할 수 있는 오류  패킷의 잘못된 순서  패킷 손실  패킷 내부의 손상된 데이터(헤더, 패킷 내용, IP 주소 등을 기반으로 고정적인 길이의 체크섬 value를 만들어 손상 여부 파악)위 3가지에 대해 TCP는 모두 해결 가능. UDP는 마지막 내부의 손상된 데이터에 대해서만 해결 가능.아래는 TCP와 UDP의 차이를 표로 비교한 것.PortIP 주소를 하나의 공간으로 생각해보자. 여러 서비스에 대해서 구역을 나누지 않고 사용을 하면, 서비스가 겹쳐 Overlap 되면서 서로서로 충돌 및 간섭 현상이 발생할 것이다. 이러한 현상이 발생하면, 통신이 되다가 안되다가를 반복하는 등 트래픽이 불안정해진다. 따라서 이것을 해소하기 위해서는 각각 서비스에 대한 간섭 현상을 없애야 한다. 각 서비스마다 독방 형식의 구역을 나누게 되면 간섭 현상이 사라질 것이다. 따라서 OSI 7계층 중 4계층에서는 하나의 IP 주소를 독방의 개념인 포트(Port)로 나눈다.TCP, UDP는 패킷이 어떤 포트로 이동해야 하는지를 나타낼 수 있습니다. TCP 및 UDP 헤더에는 포트 번호를 표시하는 섹션이 있습니다. 예를 들어 IP(인터넷 프로토콜)와 같은 네트워크 계층 프로토콜은 지정된 네트워크 연결에서 사용 중인 포트를 인식하지 못합니다. 표준 IP 헤더에는 데이터 패킷이 어떤 포트로 이동해야 하는지 나타내는 위치가 없습니다. IP 헤더는 대상 IP 주소만 나타내고 해당 IP 주소의 포트 번호는 표시하지 않습니다.일반적으로 네트워크 계층 프로토콜은 거의 항상 전송 계층 프로토콜과 함께 사용되기 때문에 네트워크 계층에서 포트를 지시할 수 없는 것은 네트워킹 프로세스에 영향을 미치지 않는다.TCP와 UDP  네트워크 계층 중 전송 계층에서 사용하는 프로토콜  TCP(Transmission Control Protocol)            인터넷 상에서 데이터를 메세지의 형태(세그먼트 라는 블록 단위)로 보내기 위해 IP와 함께 사용하는 프로토콜이다.      TCP와 IP를 함께 사용하는데, IP가 데이터의 배달을 처리한다면 TCP는 패킷을 추적 및 관리한다.      연결형 서비스로 가상 회선 방식을 제공한다.                  3-way handshaking과정을 통해 연결을 설정하고, 4-way handshaking을 통해 연결을 해제한다.                    흐름제어 및 혼잡제어를 제공한다.                  흐름제어                          데이터를 송신하는 곳과 수신하는 곳의 데이터 처리 속도를 조절하여 수신자의 버퍼 오버플로우를 방지하는 것              송신하는 곳에서 감당이 안되게 많은 데이터를 빠르게 보내 수신하는 곳에서 문제가 일어나는 것을 막는다.                                혼잡제어                          네트워크 내의 패킷 수가 넘치게 증가하지 않도록 방지하는 것              정보의 소통량이 과다하면 패킷을 조금만 전송하여 혼잡 붕괴 현상이 일어나는 것을 막는다.                                          높은 신뢰성을 보장한다.      UDP보다 속도가 느리다.      전이중(Full-Duplex), 점대점(Point to Point) 방식이다.                  전이중                          전송이 양방향으로 동시에 일어날 수 있다.                                점대점                          각 연결이 정확히 2개의 종단점을 가지고 있다.                                멀티캐스팅이나 브로드캐스팅을 지원하지 않는다.                    연속성보다 신뢰성있는 전송이 중요할 때에 사용된다.        UDP(User Datagram Protocol)            데이터를 데이터그램 단위로 처리하는 프로토콜이다.      비연결형 서비스로 데이터그램 방식을 제공한다.                  연결을 위해 할당되는 논리적인 경로가 없다.          그렇기 때문에 각각의 패킷은 다른 경로로 전송되고, 각각의 패킷은 독립적인 관계를 지니게 된다.          이렇게 데이터를 서로 다른 경로로 독립적으로 처리한다.                    정보를 주고 받을 때 정보를 보내거나 받는다는 신호절차를 거치지 않는다.      UDP헤더의 CheckSum 필드를 통해 최소한의 오류만 검출한다.      신뢰성이 낮다.      TCP보다 속도가 빠르다.      신뢰성보다는 연속성이 중요한 서비스, 예를 들면 실시간 서비스(streaming)에 사용된다.        참고          UDP와 TCP는 각각 별도의 포트 주소 공간을 관리하므로 같은 포트 번호를 사용해도 무방하다. 즉, 두 프로토콜에서 동일한 포트 번호를 할당해도 서로 다른 포트로 간주한다.      또한 같은 모듈(UDP or TCP) 내에서도 클라이언트 프로그램에서 동시에 여러 커넥션을 확립한 경우에는 서로 다른 포트 번호를 동적으로 할당한다. (동적할당에 사용되는 포트번호는 49,152~65,535이다.)      TCP와 UDP의 헤더 분석TCP Header  TCP는 상위계층으로부터 데이터를 받아 헤더를 추가해 IP로 전송            필드      내용      크기(bits)              Source Port, Destination Port      TCP로 연결되는 가상 회선 양단의 송수신 프로세스에 할당되는 포트 주소      16              Sequence Number      송신자가 지정하는 순서 번호, 전송되는 바이트 수 기준으로 증가SYN = 1 : 초기 시퀀스 번호. ACK 번호는 이 값에 + 1      32              Acknowledgment(ACK) Number      수신 프로세스가 제대로 수신한 바이트의 수 응답 용      32              Header Length(Data Offset)      TCP 헤더 길이를 4바이트 단위로 표시(최소 20, 최대 60 바이트)      4              Resv(Reserved)      나중을 위해 0으로 채워진 예약 필드      6              Flag Bit      SYN, ACK, FIN 등 제어 번호(아래 표 참고)      6              Window Size      수신 윈도우의 버퍼 크기 지정(0이면 송신 중지). 상대방의 확인 없이 전송 가능한 최대 바이트 수      16              TCP Checksum      헤더와 데이터의 에러 확인 용도      16              Urgent Pointer(긴급 위치)      현재 순서 번호부터 표시된 바이트까지 긴급한 데이터임을 표시, URG 플래그 비트가 지정된 경우에만 유효      16              Options      추가 옵션 있을 경우 표시      0~40            Flag Bit                            종류          내용                          URG          긴급 위치 필드 유효 여부 설정                          ACK          응답 유효 여부 설정. 최초의 SYN 패킷 이후 모든 패킷은 ACK 플래그 설정 필요. 데이터를 잘 받았으면 긍정 응답으로 ACK(=SYN+1) 전송                          PSH          수신측에 버퍼링된 데이터를 상위 계층에 즉시 전달할 때                          RST          연결 리셋 응답 혹은 유효하지 않은 세그먼트 응답                          SYN          연결 설정 요청. 양쪽이 보낸 최초 패킷에만 SYN 플래그 설정                          FIN          연결 종료 의사 표시                    UDP Header            필드      내용      크기(bits)              Source Port, Destination Port      송수신 애플리케이션의 포트 번호      16              Length      헤더와 데이터 포함 전체 길이      16              Checksum      헤더와 데이터의 에러 확인 용도. UDP는 에러 복구를 위한 필드가 불필요하기 때문에 TCP 헤더에 비해 간단      16      TCP의 3 way handshake와 4 way handshake  TCP는 장치들 사이에 논리적인 접속을 성립(establish)하기 위하여 연결을 설정하여 신뢰성을 보장하는 연결형 서비스 이다.  3-way handshake 란          TCP 통신을 이용하여 데이터를 전송하기 위해 네트워크 연결을 설립(Connection Establish) 하는 과정      양쪽 모두 데이터를 전송할 준비가 되었다는 것을 보장하고, 실제로 데이터 전달이 시작하기 전에 서로 준비되었다는 것을 알 수 있도록 한다.      즉, TCP/IP 프로토콜을 이용해서 통신을 하는 응용 프로그램이 데이터를 전송하기 전에 먼저 정확한 전송을 보장하기 위해 상대방 컴퓨터와 사전에 세션을 수립하는 과정을 의미한다.                  A 프로세스(Client)가 B 프로세스(Server)에 연결을 요청                          A -&amp;gt; B: SYN                                  접속 요청 프로세스 A가 연결 요청 메시지 전송 (SYN)                  송신자가 최초로 데이터를 전송할 때 Sequence Number를 임의의 랜덤 숫자로 지정하고, SYN 플래그 비트를 1로 설정한 세그먼트를 전송한다.                  PORT 상태 - B: LISTEN, A: CLOSED                                            B -&amp;gt; A: SYN + ACK                                  접속 요청을 받은 프로세스 B가 요청을 수락했으며, 접속 요청 프로세스인 A도 포트를 열어 달라는 메시지 전송 (SYN + ACK)                  수신자는 Acknowledgement Number 필드를 (Sequence Number + 1)로 지정하고, SYN과 ACK 플래그 비트를 1로 설정한 세그먼트를 전송한다.                  PORT 상태 - B: SYN_RCV, A: CLOSED                                            A -&amp;gt; B: ACK                                  PORT 상태 - B: SYN_RCV, A: ESTABLISHED                  마지막으로 접속 요청 프로세스 A가 수락 확인을 보내 연결을 맺음 (ACK)                  이때, 전송할 데이터가 있으면 이 단계에서 데이터를 전송할 수 있다.                  PORT 상태 - B: ESTABLISHED, A: ESTABLISHED                                                                          4-way handshake 란          TCP의 연결을 해제(Connection Termination) 하는 과정                  A 프로세스(Client)가 B 프로세스(Server)에 연결 해제를 요청                          A -&amp;gt; B: FIN                                  프로세스 A가 연결을 종료하겠다는 FIN 플래그를 전송                  프로세스 B가 FIN 플래그로 응답하기 전까지 연결을 계속 유지                                            B -&amp;gt; A: ACK                                  프로세스 B는 일단 확인 메시지를 보내고 자신의 통신이 끝날 때까지 기다린다. (이 상태가 TIME_WAIT 상태)                  수신자는 Acknowledgement Number 필드를 (Sequence Number + 1)로 지정하고, ACK 플래그 비트를 1로 설정한 세그먼트를 전송한다.                  그리고 자신이 전송할 데이터가 남아있다면 이어서 계속 전송한다.                                            B -&amp;gt; A: FIN                                  프로세스 B가 통신이 끝났으면 연결 종료 요청에 합의한다는 의미로 프로세스 A에게 FIN 플래그를 전송                                            A -&amp;gt; B: ACK                                  프로세스 A는 확인했다는 메시지를 전송                                                                          참고 - 포트(PORT) 상태 정보          CLOSED: 포트가 닫힌 상태      LISTEN: 포트가 열린 상태로 연결 요청 대기 중      SYN_RCV: SYNC 요청을 받고 상대방의 응답을 기다리는 중      ESTABLISHED: 포트 연결 상태        참고 - 플래그 정보          TCP Header에는 CONTROL BIT(플래그 비트, 6bit)가 존재하며, 각각의 bit는 “URG-ACK-PSH-RST-SYN-FIN”의 의미를 가진다.                  즉, 해당 위치의 bit가 1이면 해당 패킷이 어떠한 내용을 담고 있는 패킷인지를 나타낸다.                    SYN(Synchronize Sequence Number) / 000010                  연결 설정. Sequence Number를 랜덤으로 설정하여 세션을 연결하는 데 사용하며, 초기에 Sequence Number를 전송한다.                    ACK(Acknowledgement) / 010000                  응답 확인. 패킷을 받았다는 것을 의미한다.          Acknowledgement Number 필드가 유효한지를 나타낸다.          양단 프로세스가 쉬지 않고 데이터를 전송한다고 가정하면 최초 연결 설정 과정에서 전송되는 첫 번째 세그먼트를 제외한 모든 세그먼트의 ACK 비트는 1로 지정된다고 생각할 수 있다.                    FIN(Finish) / 000001                  연결 해제. 세션 연결을 종료시킬 때 사용되며, 더 이상 전송할 데이터가 없음을 의미한다.                    ❓TCP 관련 질문 1  Q. TCP의 연결 설정 과정(3단계)과 연결 종료 과정(4단계)이 단계가 차이나는 이유?          A. Client가 데이터 전송을 마쳤다고 하더라도 Server는 아직 보낼 데이터가 남아있을 수 있기 때문에 일단 FIN에 대한 ACK만 보내고, 데이터를 모두 전송한 후에 자신도 FIN 메시지를 보내기 때문이다.      관련 Reference      ❓TCP 관련 질문 2  Q. 만약 Server에서 FIN 플래그를 전송하기 전에 전송한 패킷이 Routing 지연이나 패킷 유실로 인한 재전송 등으로 인해 FIN 패킷보다 늦게 도착하는 상황이 발생하면 어떻게 될까?          A. 이러한 현상에 대비하여 Client는 Server로부터 FIN 플래그를 수신하더라도 일정시간(Default: 240sec)동안 세션을 남겨 놓고 잉여 패킷을 기다리는 과정을 거친다. (TIME_WAIT 과정)      관련 Reference      ❓TCP 관련 질문 3  Q. 초기 Sequence Number인 ISN을 0부터 시작하지 않고 난수를 생성해서 설정하는 이유?          A. Connection을 맺을 때 사용하는 포트(Port)는 유한 범위 내에서 사용하고 시간이 지남에 따라 재사용된다. 따라서 두 통신 호스트가 과거에 사용된 포트 번호 쌍을 사용하는 가능성이 존재한다. 서버 측에서는 패킷의 SYN을 보고 패킷을 구분하게 되는데 난수가 아닌 순처적인 Number가 전송된다면 이전의 Connection으로부터 오는 패킷으로 인식할 수 있다. 이런 문제가 발생할 가능성을 줄이기 위해서 난수로 ISN을 설정한다.      관련 Reference      참고  불곰, TCP, UDP 포트 (Port, OSI 4계층)  What is a computer port?,Ports in networking  곰돌이 놀이터: 소켓 통신이란?  아무거나올리는블로그: Socket Programming - Socket  stackoverflow: difference between socket programming and Http programming",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series5'> <img src='/images/network_logo.png' alt='Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series5'>Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part4]: 네트워크 프로토콜(1) HTTP",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series4",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  HTTP          HTTP 버전 차이      HTTP의 특징        URL  HTTP Method          GET 메서드와 POST 메서드        HTTP 메세지          시작 라인                  요청 메세지          응답 메세지                    HTTP 헤더                  요청 메세지          응답 메세지                    HTTP 바디      예시                  요청 메세지          응답 메세지                      HTTP 상태코드  쿠키, 캐시, 프록시, 세션          쿠키      캐시      쿠키와 세션        웹 브라우저의 동작원리          HTTP 리퀘스트 작성                  URL 입력          HTTP 리퀘스트 작성                    DNS 서버에 웹 서버의 IP주소 조회                  DNS Resolver를 이용해 DNS 서버 조회                    웹 서버와 TCP 연결 시도      클라이언트가 서버에게 요청      서버가 클라이언트에게 데이터 응답      서버 클라이언트 간 연결 종료      웹 브라우저가 웹 문서 출력        HTTPS          HTTPS 프로토콜      HTTPS가 필요한 이유?      HTTPS의 원리      HTTPS의 장단점      HTTPS(SSL) 동작 과정        참고HTTPHTTP(HyperText Transfer Protocol)는 응용 계층(Application layer)에서 압도적으로 많이 사용되는 프로토콜입니다. 요즘에는 모든 데이터(텍스트, 이미지, 음성, 영상, 파일, JSON 등)를 HTTP 메시지에 담아서 전송합니다.HTTP가 하는 역할은 무엇일까요? 저희는 응용 계층에서 데이터를 주고받기 위해(크롬, 사파리와 같은 웹 브라우저에서 뉴스, 사진, 동영상을 보고 물건을 주문하는 것과 같은 행위) 클라이언트는 요청(request), 서버는 응답(response)하는 방식을 사용합니다.  이 때 응용 계층에 있는 단말기(우리의 핸드폰, 노트북 그리고 구글이 가지고 있는 웹 서버와 같은 것들)들이 서로 일관된 방법으로 데이터를 주고받기 위해 규약이 필요했는데 이때 생긴 규약이 바로 HTTP입니다.이 때 클라이언트는 HTTP 메세지를 작성하기 위해 두 가지를 사용합니다. 바로 URL과 HTTP 메소드입니다.HTTP 버전 차이  HTTP/1.1: 1997년에 등장해서 현재까지 가장 많이 사용하는 버전  HTTP/2:  HTTP/3: UDP기반으로 개발됨HTTP의 특징  클라이언트/서버 구조로 동작          클라이언트가 request를 보내면 서버가 response를 돌려주는 구조 -&amp;gt; 단방향 통신        무상태(Stateless) 프로토콜          서버가 클라이언트의 상태를 보존하지 않음 (그래서 클라이언트가 알아서 자신의 상태를 잘 업데이트해서 서버에 전달하게 됨)      장점: 서버 확장성 높음(서버가 중간에 바뀌어도 된다. 어차피 클라이언트가 부담하게 되므로)      장점: 특정 서버에 의존하지 않게 되므로 서버 장애에 강인하다      단점: 클라이언트가 추가 데이터 전송해야함      로그인이 필요한 서비스의 경우 로그인 상태를 서버에 유지해야하므로 이 때는 브라우저의 쿠키와 서버의 세션을 조합해서 보완해야함        비연결성          서버와 클라이언트가 계속 연결을 유지하게 된다면 클라이언트가 늘어날때마다 서버의 리소스 부담 계속 커지게 됨      클라이언트가 request를 보내고 서버가 response를 보낸 후 요청을 끊는다 -&amp;gt; 서버는 최소한의 자원만 사용하게 됨      HTTP는 기본적으로 연결을 유지하지 않는 모델      연결하는데 시간이 별로 소요되지 않나? -&amp;gt; TCP/IP 연결 새로 맺어야함 -&amp;gt; 3-way handshake 시간이 추가된다      그리고 네이버 검색을 예로 들때, 우리가 HTTP 메세지를 보내고 response를 돌려줄 때 검색 결과만 돌려주는게 아니라 그안에 포함된 HTML, CSS, 이미지 등을 함께 돌려줘야 한다 -&amp;gt; 이런 문제를 HTTP Persistent Connection으로 해결      Persistent Connection은 내부 메커니즘에 의해 보통 하나의 웹 페이지를 띄울 동안 연결을 계속 지속시킨다      HTTP/2, HTTP/3 오면서는 HTTP Persistent Connection이 더욱 발전됨        HTTP 메시지URLURL은 Uniform Resource Locator의 약자입니다. URL은 URI(Uniform Resource Identifier)를 표현하기 위한 방법 중 하나입니다. URL말고도 URN이라는 것이 있지만 지금은 거의 URL만 사용하기 때문에 URN은 생략하도록 하겠습니다.인터넷에서 어떤 자원(회원 정보, 주문서, 사진, 동영상 등)을 유일하게 표현하기 위해 URI라는 개념이 등장했고 이를 위한 방법으로 URL을 사용하는 것입니다. URL은 이러한 자원들에게 부여된 고유한 주소를 말합니다.인터넷에서는 모든 자원에 URL이라는 고유한 주소를 부여해 이들을 식별한다URL의 예시를 보겠습니다.https://google.co.kr/search?q=hello&amp;amp;hl=kohttps://order.kyobobook.co.kr/order/orderStepOneURL 문법은 아래와 같습니다.# URL 문법scheme://[userinfo@]host[:port][/path][?query][#fragment]예: https://www.google.com/search?q=hello&amp;amp;hl=ko# scheme예: https- 주로 프로토콜이 사용됩니다.- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 (https, http, ftp)- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트- https는 http에 보안 추가 (HTTP Secure)# host예: www.google.com- 도메인명 또는 IP주소# port예: 8888- 접속 포트# path예: /search- 리소스 경로 (계층적 구조)- 디렉토리명/파일명# query예: ?q=hello&amp;amp;hl=ko- key=value 형태- ?로 시작, &amp;amp;로 추가 가능- query parameter 또는 query string으로 보통 불림# fragment예: #getting-started-introducing-spring-boot- html 내부 북마크 등에 사용- 서버에 전송하는 정보는 아님URL에서 유의할 점은 URL은 자원을 식별하는 용도로만 써야 한다는 것입니다. 예를 들어 어떤 물건을 주문할 때는 주문(order)만을 URL로 표현해야지 주문 확인(order-check), 주문 취소(order-cancel) 이런 행위까지를 포함시키면 안됩니다.HTTP Method이러한 행위를 나타내기 위해 사용하는 것이 바로 HTTP 메소드입니다.인터넷에서 발생하는 행위는 크게 CRUD(Create-Read-Update-Delete)로 나눌 수 있습니다. CRUD를 HTTP에서 제공하는 메소드로 구현할 수 있습니다.            HTTP Method      설명              GET      읽기(리소스 조회)              POST      쓰기(리소스 등록)              PUT      업데이트(리소스 완전 대체)              PATCH      부분 업데이트(리소스 부분 대체)              DELETE      삭제(리소스 삭제)      GET 메서드와 POST 메서드  HTTP 프로토콜을 이용해서 서버에 데이터(요청 정보)를 전달할 때 사용하는 방식  GET 메서드 방식          개념                  정보를 조회하기 위한 메서드          서버에서 어떤 데이터를 가져와서 보여주기 위한 용도의 메서드          가져오는 것(Select)                    사용 방법                  URL의 끝에 ‘?’가 붙고, 요청 정보가 (key=value)형태의 쌍을 이루어 ?뒤에 이어서 붙어 서버로 전송한다.          요청 정보가 여러 개일 경우에는 ‘&amp;amp;’로 구분한다.          Ex) www.urladdress.xyz?name1=value1&amp;amp;name2=value2                    특징                  URL에 요청 정보를 붙여서 전송한다.          URL에 요청 정보가 이어붙기 때문에 길이 제한이 있어서 대용량의 데이터를 전송하기 어렵다.                          한 번 요청 시 전송 데이터(주솟값 + 파라미터)의 양은 255자로 제한된다.(HTTP/1.1은 2048자)                                요청 정보를 사용자가 쉽게 눈으로 확인할 수 있다.                          POST 방식보다 보안상 취약하다.                                HTTP 패킷의 Body는 비어 있는 상태로 전송한다.                          즉, Body의 데이터 타입을 표현하는 ‘Content-Type’ 필드도 HTTP Request Header에 들어가지 않는다.                                POST 방식보다 빠르다.                          GET 방식은 캐싱을 사용할 수 있어, GET 요청과 그에 대한 응답이 브라우저에 의해 캐쉬된다.                                            POST 메서드 방식          개념                  서버의 값이나 상태를 바꾸기 위한 용도의 메서드          수행하는 것(Insert, Update, Delete)                    사용 방법                  요청 정보를 HTTP 패킷의 Body 안에 숨겨서 서버로 전송한다.          Request Header의 Content-Type에 해당 데이터 타입이 표현되며, 전송하고자 하는 데이터 타입을 적어주어야 한다.                          Default: application/octet-stream              단순 txt의 경우: text/plain              파일의 경우: multipart/form-date                                          특징                  Body 안에 숨겨서 요청 정보를 전송하기 때문에 대용량의 데이터를 전송하기에 적합하다.          클라이언트 쪽에서 데이터를 인코딩하여 서버로 전송하고, 이를 받은 서버 쪽이 해당 데이터를 디코딩한다.          GET 방식보다 보안상 안전하다.                      Q. 조회하기 위한 용도 POST가 아닌 GET 방식을 사용하는 이유?          설계 원칙에 따라 GET 방식은 서버에게 여러 번 요청을 하더라도 동일한 응답이 돌아와야 한다. (Idempotent, 멱등)                  GET 방식은 가져오는 것(Select) 으로, 서버의 데이터나 상태를 변경시키지 않아야 한다.                          Ex) 게시판의 리스트, 게시글 보기 기능              예외) 방문자의 로그 남기기, 글을 읽은 횟수 증가 기능                                POST 방식은 수행하는 것 으로, 서버의 값이나 상태를 바꾸기 위한 용도이다.                          Ex) 게시판에 글쓰기 기능                                          웹에서 모든 리소스는 Link할 수 있는 URL을 가지고 있어야 한다.                  어떤 웹페이지를 보고 있을 때 다른 사람한테 그 주소를 주기 위해서 주소창의 URL을 복사해서 줄 수 있어야 한다.          즉, 어떤 웹페이지를 조회할 때 원하는 페이지로 바로 이동하거나 이동시키기 위해서는 해당 링크의 정보가 필요하다.          이때 POST 방식을 사용할 경우에 값(링크의 정보)이 Body에 있기 때문에 URL만 전달할 수 없으므로 GET 방식을 사용해야한다. 그러나 글을 저장하는 경우에는 URL을 제공할 필요가 없기 때문에 POST 방식을 사용한다.                    HTTP 메세지클라이언트와 서버는 URL과 HTTP 메소드를 이용해서 HTTP 메세지를 만들어 통신한다고 했습니다. HTTP 메세지는 바이너리로 표현할 수 있는 모든 데이터를 전송할 수 있습니다. (HTML, TEXT, JSON, XML, 이미지, 영상 파일 등)서버간에 데이터를 주고받을 때에도 대부분 HTTP를 사용한다고 합니다.HTTP 메세지의 구조는 다음과 같습니다.시작 라인요청 메세지  HTTP 메소드          종류: GET, POST, PUT, DELETE …      서버가 수행해야 할 동작 지정                  GET: 리소스 조회          POST: 요청 내역 처리                      요청 대상          절대경로[?쿼리]      절대경로=”/”로 시작하는 경로        HTTP 버전  ex. GET /search?q=hello&amp;amp;hl=ko HTTP/1.1응답 메세지  HTTP 버전  HTTP 상태 코드          200: 성공      400: 클라이언트 요청 오류      500: 서버 내부 오류        ex. HTTP/1.1 200 OKHTTP 헤더  용도          HTTP 전송에 필요한 모든 부가정보      메세지 바디의 내용, 메세지 바디의 크기, 압축, 클라이언트 정보, ..        표현(Representation) 헤더          요청/응답 공통 항목      Content-Type: 표현 데이터의 형식                  리소스를 어떤 형식으로 표현할 것인가          ex. text/html; charset=utf-8, application/json, image/png                    Content-Encoding: 표현 데이터의 압축 방식                  표현 데이터를 압축하기 위해 사용          데이터를 전달하는 곳에서 압축 후 인코딩 헤더 추가          데이터를 읽는 곳에서 헤더 정보를 보고 압축 해제          ex. gzip, deflate, identity                    Content-Language: 표현 데이터의 자연 언어                  ex. ko, en, en-US                    Content-length: 표현 데이터의 길이                  바이트 단위                    Last-Modified: 리소스를 마지막으로 갱신한 일시      Location: 페이지 리다이렉션(redirect)                  리소스가 리다이렉트(redirect)된 때에 이동된 주소, 또는 새로 생성된 리소스 주소          3xx 응답이나 201 Created 응답일 때 어느 페이지로 이동할지를 알려주는 헤더          201: 요청에 의해 생성된 리소스 URL          3xx: 요청을 자동으로 리다이렉션하기 위한 대상 리소스 URL                    요청 메세지  요청(Request) 헤더          Host: 요청하는 호스트에 대한 호스트명 및 포트번호 (필수값)                  하나의 서버가 여러 도메인을 처리해야 할 때          하나의 IP 주소에 여러 도메인이 적용되어 있을 때                    User-Agent: 클라이언트의 웹/애플리케이션 정보                  어떤 종류의 브라우저에서 장애가 발생하는지 파악 가능                    Referer: 현재 요청된 페이지의 이전 페이지 주소                  이를 이용해 유입 경로 분석 가능                    Authorization: 클라이언트 인증 정보를 서버에 전달                  인증 토큰(JWT/Bearer 토큰)을 서버로 보낼 때 사용하는 헤더          토큰의 종류(Basic, Bearer 등) + 실제 토큰 문자를 전송                    Cookie: 쿠키에서 사용할 데이터                  클라이언트가 서버에서 받은 쿠키를 저장하고, HTTP 요청시 서버로 전달          서버에 의해 Set-Cookie로 클라이언트에게 설정된 쿠키 정보                    If-Modified-Since: 캐시 데이터의 유효성을 검증하기 위한 헤더                  이 값보다 더 큰 값을 가지는 데이터만 서버로부터 받아옴          이 값과 같으면 서버 데이터가 캐시와 비교해 업데이트 되지 않았으므로 캐시 데이터 사용                    If-None-Match: ETag 값을 전달해 캐시 데이터를 사용할지, 서버로 부터 새로 받아올지 결정      응답 메세지  응답(Response) 헤더          Server: 요청을 처리하는 Origin 서버의 소프트웨어 정보                  캐시, 프록시 서버 아닌 Origin 서버                    Set-Cookie: 서버에서 클라이언트로 쿠키 전달                  서버측에서 클라이언트에게 세션 쿠키 정보를 설정          max-age, expires와 같은 옵션 있음                    Age: 캐시 응답. max-age 시간 내에서 얼마나 흘렀는지 알려줌(초 단위)      ETag: 리소스에 붙은 고유한 태그                  이를 이용해 캐시 데이터의 유효성을 검사할 수 있음                    HTTP 바디  실제 전송할 데이터  HTML, 이미지, 영상, JSON 등 byte로 표현 가능한 모든 데이터 전송 가능이렇게 HTTP 메세지를 통해서 두 단말기가 응용계층에서 쉽게 통신할 수 있도록 하는 API(Application Program Interface)를 REST(Representational State Transfer) API라고 합니다.예시요청 메세지응답 메세지HTTP 상태코드            상태코드      설명              200 OK      요청이 올바르게 수행되었음(GET, PUT)              201 Created      서버가 새로운 리소스를 생성했음(POST)              204 No Content      응답할 데이터가 없음(HTTP Body가 없음) (DELETE, PUT)              400 Bad Request      요청이 잘못되었음              401 Unauthorized      인증(로그인)이 필요함              403 Forbidden      로그인 되었으나 해당 자원에 대한 권한이 없음              404 Not Found      존재하지 않는 자원에 대해 요청했음 (URI가 잘못된 경우?)              405 Method Not Allowed      자원이 지원하지 않는 메소드임 (Method가 잘못된 경우?)              409 Confilct      비지니스 로직상 요청을 처리하지 못한 경우              429 Too Many Requests      요청을 너무 많이한 경우      쿠키, 캐시, 프록시, 세션쿠키  HTTP는 무상태 프로토콜  클라이언트와 서버가 요청과 응답을 주고 받고나면 연결이 끊어짐  클라이언트가 다시 요청하면 서버는 이전 요청을 기억하지 못함  클라이언트와 서버는 서로 상태를 유지하지 않음  상태가 유지되지 않으면, 매번 페이지를 이동할 때마다 로그인을 다시 하거나, 상품을 선택했는데 구매 페이지에서 선택한 상품의 정보가 없거나 하는 등의 일이 발생할 수 있음  이 때 사용하는 것이 쿠키  쿠키는 사용자 로그인 세션 관리, 광고 정보 트래킹과 같은 곳에 많이 사용  쿠키는 웹 브라우저가 차지하는 클라이언트의 로컬에 저장  쿠키 생명 주기는 Set-Cookie의 expires 또는 max-age 옵션으로 관리캐시  캐시가 없다면, 같은 데이터를 요청하더라도 매번 서버에서 데이터를 응답 받아야 하기 때문에 네트워크 비용이 아깝게 느껴짐  이러한 네트워크 비용을 줄이고자 캐시가 등장  웹 페이지의 HTML, CSS, 이미지, 문서 파일 등 렌더링에 필요한 요소들을 서버단에 있는 캐시 서버에 저장  응답 메세지의 cache-control의 max-age 옵션으로 캐시 생명주기 관리 -&amp;gt; max-age 초과시 캐시만료  (캐시만료는 캐시가 삭제되었다는 뜻이 아니라, 캐시된 데이터가 더 이상 유효하지 않다는 의미)  캐시가 만료되면 다시 전체 데이터를 서버에서 받을 수도 있지만, 데이터에 변경 사항이 있는 경우에만 서버에서 받는 최적화 가능  -&amp;gt; 검증 헤더와 조건부 요청 -&amp;gt; 캐시 데이터와 서버 데이터의 최종 수정일이 같은지  요청 메세지 보낼 때, 헤더에 if-modified-since의 값으로, 캐시에 저장할 때 서버가 보낸 Last-Modified 값을 사용해,  서버에 Last-Modified 값 이후로 데이터가 수정된 적이 있는지 물어봄          변경된 적 없다면(304 Not Modified) -&amp;gt; 응답 메세지에 헤더만 있고 메세지 바디는 x -&amp;gt; 네트워크 비용 감소      데이터가 변경되었다면(200 OK) -&amp;gt; 모든 데이터 전송        Last-Modified와 if-modified-since 조합의 한계점: A-&amp;gt;B-&amp;gt;A로 변경되도 변경된 데이터로 인식  ETag와 If-None-Match 조합 사용          데이터마다 고유한 ETag 부여해서 캐시에 저장      클라이언트가 요청한 데이터가 캐시 만료된 경우 데이터의 ETag 값을 헤더에 담아 전송 (If-None-Match: “ETag값”)      만약 매칭되는 경우, 캐시에서 데이터 가져옴 (캐시만료는 캐시에서 삭제되었다는 의미 아님. 데이터 유효성 검증이 필요하다는 의미)      매칭되는 ETag 값이 없으면 서버는 ETag 값 말고 메세지의 시작 라인의 URL에 기반해 데이터를 클라이언트에 전달      Cache-Control 옵션- max-age: 캐시 유효 시간. 초단위- no-cache: 캐시를 사용하지만, 그 전에 항상 Origin 서버에 검증하고 사용 (ETag, Last-Modified 이런 것들 써서)- no-store: 메모리에서만 사용하고 삭제  클라이언트가 보낸 요청 메세지는 보통 프록시 캐시 서버로 먼저 전달되고 없으면 Origin 서버로 전달쿠키와 세션  HTTP 프로토콜의 특징          비연결 지향(Connectionless)                  클라이언트가 request를 서버에 보내고, 서버가 클라이언트에 요청에 맞는 response를 보내면 바로 연결을 끊는다.                    상태정보 유지 안 함(Stateless)                  연결을 끊는 순간 클라이언트와 서버의 통신은 끝나며 상태 정보를 유지하지 않는다.                      쿠키와 세션의 필요성          HTTP 프로토콜은 위와 같은 특징으로 모든 요청 간 의존관계가 없다.      즉, 현재 접속한 사용자가 이전에 접속했던 사용자와 같은 사용자인지 아닌지 알 수 있는 방법이 없다.      계속해서 연결을 유지하지 않기 때문에 리소스 낭비가 줄어드는 것이 큰 장점이지만, 통신할 때마다 새로 연결하기 때문에 클라이언트는 매 요청마다 인증을 해야 한다는 단점이 있다.      이전 요청과 현재 요청이 같은 사용자의 요청인지 알기 위해서는 상태를 유지해야 한다.      HTTP 프로토콜에서 상태를 유지하기 위한 기술로 쿠키와 세션이 있다.        쿠키(Cookie)란?          개념                  클라이언트 로컬에 저장되는 키와 값이 들어있는 파일이다.          이름, 값, 유효 시간, 경로 등을 포함하고 있다.          클라이언트의 상태 정보를 브라우저에 저장하여 참조한다.                    구성 요소                  쿠키의 이름(name)          쿠키의 값(value)          쿠키의 만료시간(Expires)          쿠키를 전송할 도메인 이름(Domain)          쿠키를 전송할 경로(Path)          보안 연결 여부(Secure)          HttpOnly 여부(HttpOnly)                    동작 방식                    웹브라우저가 서버에 요청          상태를 유지하고 싶은 값을 쿠키(cookie)로 생성          서버가 응답할 때 HTTP 헤더(Set-Cookie)에 쿠키를 포함해서 전송             Set−Cookie: id=doy                                전달받은 쿠키는 웹브라우저에서 관리하고 있다가, 다음 요청 때 쿠키를 HTTP 헤더에 넣어서 전송             cookie: id=doy                                서버에서는 쿠키 정보를 읽어 이전 상태 정보를 확인한 후 응답                    쿠키 사용 예                  아이디, 비밀번호 저장          쇼핑몰 장바구니                      세션(Session)이란?          개념                  일정 시간 동안 같은 브라우저로부터 들어오는 요청을 하나의 상태로 보고 그 상태를 유지하는 기술이다.          즉, 웹 브라우저를 통해 서버에 접속한 이후부터 브라우저를 종료할 때까지 유지되는 상태이다.                    동작 방식                    웹브라우저가 서버에 요청          서버가 해당 웹브라우저(클라이언트)에 유일한 ID(Session ID)를 부여함          서버가 응답할 때 HTTP 헤더(Set-Cookie)에 Session ID를 포함해서 전송  쿠키에 Session ID를 JSESSIONID 라는 이름으로 저장             Set−Cookie: JSESSIONID=xslei13f                                웹브라우저는 이후 웹브라우저를 닫기까지 다음 요청 때 부여된 Session ID가 담겨있는 쿠키를 HTTP 헤더에 넣어서 전송             Cookie: JSESSIONID=xslei13f                                서버는 세션 ID를 확인하고, 해당 세션에 관련된 정보를 확인한 후 응답                    세션 사용 예                  로그인                      세션도 쿠키를 사용하여 값을 주고받으며 클라이언트의 상태 정보를 유지한다.  즉, 상태 정보를 유지하는 수단은 쿠키 이다.  쿠키와 세션의 차이점          저장 위치                  쿠키 : 클라이언트          세션 : 서버                    보안                  쿠키 : 클라이언트에 저장되므로 보안에 취약하다.          세션 : 쿠키를 이용해 Session ID만 저장하고 이 값으로 구분해서 서버에서 처리하므로 비교적 보안성이 좋다.                    라이프사이클                  쿠키 : 만료시간에 따라 브라우저를 종료해도 계속해서 남아 있을 수 있다.          세션 : 만료시간을 정할 수 있지만 브라우저가 종료되면 만료시간에 상관없이 삭제된다.                    속도                  쿠키 : 클라이언트에 저장되어서 서버에 요청 시 빠르다.          세션 : 실제 저장된 정보가 서버에 있으므로 서버의 처리가 필요해 쿠키보다 느리다.                    웹 브라우저의 동작원리우리가 웹 브라우저(크롬, 사파리 등)에서 뉴스 보기를 클릭하거나 유튜브 비디오를 시청할 때 내부적으로 어떤 일들이 일어나는지 한 번 알아보겠습니다.HTTP 리퀘스트 작성  보통 웹 브라우저에서 URL을 입력하거나 어떤 버튼을 클릭하는 식으로 웹 서버와 상호작용  웹 브라우저는 내부에서 HTTP 리퀘스트라는 것을 웹 서버에 전송URL 입력https://www.google.com/search?q=hello&amp;amp;hl=koHTTP 리퀘스트 작성  URL을 입력하고 나면 웹 브라우저는 URL을 바탕으로 HTTP 리퀘스트 메시지를 작성  HTTP 리퀘스트 메시지의 형태DNS 서버에 웹 서버의 IP주소 조회  메시지를 전송하기 전에 먼저 도메인 네임을 IP 주소로 변환  이를 네임 레졸루션(name resolution)이라고 함DNS Resolver를 이용해 DNS 서버 조회  네임 레졸루션을 시행하는 것이 DNS 리졸버(DNS Resolver)웹 서버와 TCP 연결 시도  3way-handshaking클라이언트가 서버에게 요청  HTTP Request Message = Request Header + 빈 줄 + Request Body  Request Header          요청 메소드 + 요청 URI + HTTP 프로토콜 버전      GET /background.png HTTP/1.0 POST / HTTP 1.1      Header 정보(key-value 구조)        빈 줄          요청에 대한 모든 메타 정보가 전송되었음을 알리는 용도        Request Body          GET, HEAD, DELETE, OPTIONS처럼 리소스를 가져오는 요청은 바디 미포함      데이터 업데이트 요청과 관련된 내용 (HTML 폼 콘텐츠 등)      서버가 클라이언트에게 데이터 응답  HTTP Response Message = Response Header + 빈 줄 + Response Body  Response Header          HTTP 프로토콜 버전 + 응답 코드 + 응답 메시지      ex. HTTP/1.1 404 Not Found.      Header 정보(key-value 구조)        빈 줄          요청에 대한 모든 메타 정보가 전송되었음을 알리는 용도        Response Body          응답 리소스 데이터      201, 204 상태 코드는 바디 미포함      서버 클라이언트 간 연결 종료  4way-handshaking웹 브라우저가 웹 문서 출력HTTPS  SSL(Secure Socket Layer)HTTPS 프로토콜  개념          HyperText Transfer Protocol over Secure Socket Layer                  또는 HTTP over TLS, HTTP over SSL, HTTP Secure                    웹 통신 프로토콜인 HTTP의 보안이 강화된 버전의 프로토콜        특징          HTTPS의 기본 TCP/IP 포트로 443번 포트를 사용한다.      HTTPS는 소켓 통신에서 일반 텍스트를 이용하는 대신에, 웹 상에서 정보를 암호화하는 SSL이나 TLS 프로토콜을 통해 세션 데이터를 암호화한다.                  TLS(Transport Layer Security) 프로토콜은 SSL(Secure Socket Layer) 프로토콜에서 발전한 것이다.          두 프로토콜의 주요 목표는 기밀성(사생활 보호), 데이터 무결성, ID 및 디지털 인증서를 사용한 인증을 제공하는 것이다.                    따라서 데이터의 적절한 보호를 보장한다.                  보호의 수준은 웹 브라우저에서의 구현 정확도와 서버 소프트웨어, 지원하는 암호화 알고리즘에 달려있다.                    금융 정보나 메일 등 중요한 정보를 주고받는 것은 HTTPS를, 아무나 봐도 상관 없는 페이지는 HTTP를 사용한다.      HTTPS가 필요한 이유?  클라이언트인 웹브라우저가 서버에 HTTP를 통해 웹 페이지나 이미지 정보를 요청하면 서버는 이 요청에 응답하여 요구하는 정보를 제공하게 된다.  웹 페이지(HTML)는 텍스트이고, HTTP를 통해 이런 텍스트 정보를 교환하는 것이다.  이때 주고받는 텍스트 정보에 주민등록번호나 비밀번호와 같이 민감한 정보가 포함된 상태에서 네트워크 상에서 중간에 제3자가 정보를 가로챈다면 보안상 큰 문제가 발생한다.  즉, 중간에서 정보를 볼 수 없도록 주고받는 정보를 암호화하는 방법인 HTTPS를 사용하는 것이다.HTTPS의 원리  공개키 알고리즘 방식  암호화, 복호화시킬 수 있는 서로 다른 키(공개키, 개인키)를 이용한 암호화 방법          공개키: 모두에게 공개. 공캐키 저장소에 등록      개인키(비공개키): 개인에게만 공개. 클라이언트-서버 구조에서는 서버가 가지고 있는 비공개키        클라이언트 -&amp;gt; 서버          사용자의 데이터를 공개키로 암호화 (공개키를 얻은 인증된 사용자)      서버로 전송 (데이터를 가로채도 개인키가 없으므로 복호화할 수 없음)      서버의 개인키를 통해 복호화하여 요청 처리      HTTPS의 장단점  장점          네트워크 상에서 열람, 수정이 불가능하므로 안전하다.        단점          암호화를 하는 과정이 웹 서버에 부하를 준다.      HTTPS는 설치 및 인증서를 유지하는데 추가 비용이 발생한다.      HTTP에 비해 느리다.      인터넷 연결이 끊긴 경우 재인증 시간이 소요된다.                  HTTP는 비연결형으로 웹 페이지를 보는 중 인터넷 연결이 끊겼다가 다시 연결되어도 페이지를 계속 볼 수 있다.          그러나 HTTPS의 경우에는 소켓(데이터를 주고 받는 경로) 자체에서 인증을 하기 때문에 인터넷 연결이 끊기면 소켓도 끊어져서 다시 HTTPS 인증이 필요하다.                    HTTPS(SSL) 동작 과정  공개키 암호화 방식과 대칭키 암호화 방식의 장점을 활용해 하이브리드 사용                  데이터를 대칭키 방식으로 암복호화하고, 공개키 방식으로 대칭키 전달    1. 클라이언트가 서버 접속하여 Handshaking 과정에서 서로 탐색        1.1. Client Hello                  클라이언트가 서버에게 전송할 데이터                          클라이언트 측에서 생성한 랜덤 데이터              클-서 암호화 방식 통일을 위해 클라이언트가 사용할 수 있는 암호화 방식              이전에 이미 Handshaking 기록이 있다면 자원 절약을 위해 기존 세션을 재활용하기 위한 세션 아이디                                      1.2. Server Hello                  Client Hello에 대한 응답으로 전송할 데이터                          서버 측에서 생성한 랜덤 데이터              서버가 선택한 클라이언트의 암호화 방식              SSL 인증서                                      1.3. Client 인증 확인                  서버로부터 받은 인증서가 CA에 의해 발급되었는지 본인이 가지고 있는 목록에서 확인하고, 목록에 있다면 CA 공개키로 인증서 복호화          클-서 각각의 랜덤 데이터를 조합하여 pre master secret 값 생성(데이터 송수신 시 대칭키 암호화에 사용할 키)          pre master secret 값을 공개키 방식으로 서버 전달(공개키는 서버로부터 받은 인증서에 포함)          일련의 과정을 거쳐 session key 생성                1.4. Server 인증 확인                  서버는 비공개키로 복호화하여 pre master secret 값 취득(대칭키 공유 완료)          일련의 과정을 거쳐 session key 생성                1.5. Handshaking 종료    2. 데이터 전송                  서버와 클라이언트는 session key를 활용해 데이터를 암복호화하여 데이터 송수신                          연결 종료 및 session key 폐기                                          참고  인프런에서 제공하는 이영한님의 모든 개발자를 위한 HTTP 웹 기본 지식 강의  JaeYeopHan/Interview_Question_for_Beginner  WeareSoft/tech-interview",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series4'> <img src='/images/network_logo.png' alt='Network Series [Part4]: 네트워크 프로토콜(1) HTTP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series4'>Network Series [Part4]: 네트워크 프로토콜(1) HTTP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series3",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  인터넷의 계층화  애플리케이션 계층 (L7)  표현 계층  세션 계층  전송 계층 (L4)  네트워크(인터넷) 계층 (L3)  링크 계층  물리 계층  Protocol Data Unit (PDU)  참고인터넷의 계층화인터넷 프로토콜 스위트(suite: 모음, 세트)(인터넷 프로토콜 스택이라고도 함)는 인터넷에서 컴퓨터들이 서로 정보를 주고 받는데 쓰이는 프로토콜의 모음을 뜻합니다. 이 프로토콜의 모음을 프로토콜의 네트워킹 범위에 따라 4개 또는 7개로 계층화한 것을 TCP/IP-4계층 또는 OSI-7계층 이라고 부릅니다. 여기서 각각의 계층은 특정 계층에서 사용하는 프로토콜이 변경되더라도 다른 계층에 영향을 주지 않도록 설계되었습니다.아래 그림은 각각 OSI-7계층과 TCP/IP-4계층을 나타낸 그림입니다.예전에는 인터넷을 계층화하는 모델로 OSI-7계층을 주로 사용했지만, 요즘에는 이를 좀 더 간소화한 TCP/IP-4계층 모델을 더 많이 사용합니다.  국제표준화기구(ISO)에서 개발한 모델로, 컴퓨터 네트워크 프로토콜 디자인과 통신을 계층으로 나누어 설명한 것이다.  이 모델은 프로토콜을 기능별로 나눈 것이다.  각 계층은 하위 계층의 기능만을 이용하고, 상위 계층에게 기능을 제공한다.  프로토콜 스택은 하드웨어나 소프트웨어 혹은 둘의 혼합으로 구현될 수 있다.  일반적으로 하위 계층들은 하드웨어로, 상위 계층들은 소프트웨어로 구현된다.애플리케이션 계층 (L7)  애플리케이션 계층은 FTP, HTTP, SSH, SMTP, DNS 등 응용 프로그램이 사용되는 계층  웹 브라우저와 같은 서비스를 실질적으로 사용자들에게 제공하는 계층  일반적인 응용 서비스는 관련된 응용 프로세스들 사이의 전환을 제공한다.  대표적인 프로토콜: FTP, HTTP, SSH, SMTP, DNS표현 계층  코드 간의 번역을 담당하여 사용자 시스템에서 데이터의 형식상 차이를 다루는 부담을 응용 계층으로부터 덜어 준다.  MIME 인코딩이나 암호화 등의 동작이 이 계층에서 이루어진다.세션 계층  양 끝단의 응용 프로세스가 통신을 관리하기 위한 방법을 제공한다.  동시 송수신 방식(duplex), 반이중 방식(half-duplex), 전이중 방식(Full Duplex)의 통신과 함께, 체크 포인팅과 유휴, 종료, 다시 시작 과정 등을 수행한다.  이 계층은 TCP/IP 세션을 만들고 없애는 책임을 진다.전송 계층 (L4)  전송 계층은 송신자와 수신자를 서로 연결(connection)하는 계층  양 끝단(End to end)의 사용자들이 신뢰성있는 데이터를 주고 받을 수 있도록 해 주어, 상위 계층들이 데이터 전달의 유효성이나 효율성을 생각하지 않도록 해준다.  시퀀스 넘버 기반의 오류 제어 방식을 사용한다.  전송 계층은 특정 연결의 유효성을 제어하고, 일부 프로토콜은 상태 개념이 있고(stateful), 연결 기반(connection oriented)이다. (이는 전송 계층이 패킷들의 전송이 유효한지 확인하고 전송 실패한 패킷들을 다시 전송한다는 것을 뜻한다.)  가장 잘 알려진 전송 계층의 예는 TCP이다.  데이터 전송 단위는 Segment(TCP), Datagram(UDP)이다.  대표적인 프로토콜: TCP, UDP네트워크(인터넷) 계층 (L3)  네트워크 패킷을 IP주소를 이용해 목적지로 전송하기 위해 사용되는 계층  네트워크 계층은 라우팅, 흐름 제어, 세그멘테이션(segmentation/desegmentation), 오류 제어, 인터네트워킹(Internetworking) 등을 수행한다.  상대방이 데이터를 제대로 수신했는지에 대해 보장하지 않는 비연결형적인 특징을 가짐  데이터 전송 단위는 Packet (또는 Datagram)이다.  대표적인 프토로콜: IP, ARP링크 계층  링크 계층은 전선, 광섬유, 무선으로 네트워크 장비(스위치, 라우터)를 연결해 실질적으로 데이터를 전송하는 계층  주소 값은 물리적으로 할당 받는데, 이는 네트워크 카드가 만들어질 때부터 맥 주소(MAC address)가 정해져 있다는 뜻이다.  데이터 전송 단위는 Frame이다.  대표적인 프로토콜: Ethernet, Token-Ring물리 계층  네트워크의 기본 네트워크 하드웨어 전송 기술을 이룬다.  네트워크의 높은 수준의 기능의 논리 데이터 구조를 기초로 하는 필수 계층이다.  전송 단위는 Bit이다.Protocol Data Unit (PDU)  Encapsulation:          데이터를 출발지에서 도착지로 가는 동안 필요한 정보를 덧붙여서 전송한다      도착지에서 캡슐화된 헤더를 하나씩 확인하며 제거한다        Segment:          전송 계층에서 TCP를 사용한 경우 전송 단위를 세그먼트라고 한다        Packet:          패킷이라는 용어의 의미는 한 번에 전송할 수 없는 크기의 데이터를 여러 개로 쪼갠 각각의 조각을 의미      보통 전송 계층에서 TCP를 사용했을 때, 네트워크 계층에서의 전송 단위를 패킷이라고 한다      다른 용어에 비해 비교적 혼용되어 사용되는 것 같다        Datagram:          전송 계층에서 UDP를 사용한 경우, 전송 단위를 데이터그램이라고 한다      전송 계층에서 전송 단위가 데이터그램인 경우,                  네트워크 계층에서 쪼개진 각각의 조각을 패킷이라고 표현하는 곳도 있고, 데이터그램이라고 표현하는 곳도 있다                    내 생각에는 1번이나 2번 정도가 맞는 것 같다전송 계층에서는 확실히 TCP를 사용했는지 UDP를 사용했는지에 따라 세그먼트/데이터그램으로 분류되고,네트워크 계층에서는 TCP, UDP에 따라 패킷/데이터그램으로 분류하기도 하고, 그냥 모두 패킷으로 표현하는 것 같다참고  Quora, What is the exact difference between packets and datagrams  badldung, Definition of Network Units: Packet, Fragment, Frame, Datagram, and Segment  coengoedegebure, The OSI Model",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series3'> <img src='/images/network_logo.png' alt='Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series3'>Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part2]: 네트워크 장비",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series2",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  랜카드  허브  스위치  라우터랜카드랜카드는 유저의 데이터를 케이블에 실어서 허브나 스위치 혹은 라우터 등으로 전달해주고, 자신에게 온 데이터를 CPU에게 전달해주는 역할을 합니다. 랜카드는 어떤 환경에서 사용하는가에 따라 이더넷용 랜카드, 토큰링용 랜카드, FDDI, ATM용 랜카드 등으로 구분하지만 요즘은 90% 이상 이더넷용 랜카드를 사용합니다. 랜카드는 데스크탑의 메인보드에 기본적으로 붙어있고, 추가적으로 랜카드를 부착할 수도 있습니다.허브허브는 한마디로 정의하면 멀티포트(Multiport) 리피터(Repeater)라고 할 수 있습니다. 멀티포트는 포트가 많이 붙어있다는 뜻이고, 리피터는 들어온 데이터를 그대로 재전송한다는 의미입니다. 따라서 허브는 특정 포트에서 들어온 데이터를 나머지 포트로 데이터를 뿌려주는 역할을 합니다.허브를 이용한 네트워크의 예시를 하나 들어보겠습니다. 허브에도 이더넷 방식의 허브와 토큰링 방식의 허브가 있는데, 보통 이더넷 방식이 더 빠르기 때문에 이더넷 허브를 기준으로 얘기하도록 하겠습니다.허브로 연결된 컴퓨터 5대가 있을 때, 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하려고 합니다. 데이터를 허브로 보내주게 되면 허브는 일단 이 데이터를 허브내의 나머지 컴퓨터에 모두 전송합니다. 그러면 각각의 컴퓨터는 랜카드를 통해 들어온 데이터가 자신의 데이터가 맞는지 확인하고 자기의 것이 아니면 버리게 됩니다.여기서 중요한 점은 위에서 구성한 네트워크가 이더넷 네트워크이기 때문에 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하는 동안 다른 컴퓨터들은 데이터를 주고 받을 수 없다는 점입니다. 따라서 이렇게 같은 허브에 연결된 컴퓨터들은 모두 같은 콜리전 도메인에 있다고 말합니다. 그래서 만약 허브만으로 약 100대의 컴퓨터를 연결할 수 있는 네트워크를 연결했다면 1대의 컴퓨터가 통신하는 동안 나머지 99대의 컴퓨터들은 모두 기다리고 있어야 합니다. 이는 네트워크의 속도를 저하시키는 치명적인 원인이 됩니다.스위치위에서 살펴본 허브의 단점은 아주 명확합니다. 네트워크의 규모를 확장하기 위해 아무리 허브의 개수를 늘려도 하나의 콜리전 도메인이기 때문에 네트워크의 속도를 느리게 만든다는 점입니다. 이를 해결하기 위해서는 네트워크의 규모를 확장시켜도 콜리전 도메인이 커지지 않아야 하는데 이 때 등장한 것이 바로 스위치입니다. 사실 스위치 이전에 브릿지라는 것이 있었지만 요즘은 브릿지를 스위치가 대체하였기 때문에 스위치에 대해서 알아보도록 하겠습니다.스위치는 콜리전 도메인을 나눠준다고 했습니다. 예시를 보도록 하겠습니다.보시다시피 콜리전 도메인은 스위치에 의해 분리되었습니다. 이렇게 되면 왼쪽 콜리전 도메인 내의 컴퓨터들이 통신하는 동안 오른쪽에서도 이와 상관없이 통신이 가능합니다. (왼쪽 콜리전 도메인 내의 컴퓨터와 오른쪽 콜리전 도메인에 있는 컴퓨터가 통신하는 경우는 제외. 이런 경우에 해당하는지는 스위치에서 저장하고 있는 맥 주소 테이블을 바탕으로 판단합니다)라우터라우터가 필요한 이유는 또 스위치로는 해결하지 못하는 문제가 있기 때문입니다. 바로 브로드캐스트 도메인 분할 문제입니다. 브로드캐스트 도메인은 무엇이고 콜리전 도메인이랑 차이는 무엇인지 보도록 하겠습니다. 콜리전 도메인 영역은 A라는 컴퓨터가 B라는 컴퓨터에 데이터를 보내고 싶은데 어디로 보내야 할지 몰라 콜리전 도메인 영역 내의 모든 컴퓨터에 데이터를 보내는 영역입니다. 이 때는 B 컴퓨터를 제외한 나머지 컴퓨터는 데이터를 받아도 본인 것이 아니기 때문에 랜카드가 자신의 CPU까지 데이터를 보내지 않기 때문에 컴퓨터 성능에 영향을 주지 않습니다.콜리전 도메인은 컴퓨팅 성능에는 영향을 주지 않는다. 다만 네트워크 속도를 저하시킬 뿐이다. 브로드캐스트 도메인은 컴퓨팅 성능에도 영향을 주는 영역입니다. A라는 컴퓨터가 만약 어떤 데이터를 브로드캐스팅하면 브로드캐스트 도메인 내의 모든 컴퓨터는 이 데이터의 목적지가 됩니다. 그렇기 때문에 이 데이터는 브로드캐스트 도메인 내의 모든 컴퓨터에 도착하고 랜카드는 이 데이터를 CPU에 전달해주게 됩니다. 이런 일이 자주 발생하게 되면 모든 컴퓨터의 컴퓨팅 성능에 영향을 미칠 것입니다.라우터는 브로드캐스트 도메인을 분할해줍니다. 이 말은 네트워크를 분리해 준다는 말입니다. 브로드캐스팅은 하나의 네트워크 내에서만 일어납니다. 그래서 라우터를 이용하면 네트워크를 분할할 수 있고 브로드캐스트 도메인 영역을 분할할 수 있습니다.라우터는 네트워크를 분할한다.  🦊🐱 VLAN(Virtual LAN)위에서 라우터를 이용해 네트워크를 분할했습니다. 이런 역할을 스위치가 할 수도 있습니다. 스위치의 VLAN 기능을 이용하면 네트워크 영역을 분할할 수 있습니다. 하지만 딱 그 뿐입니다. 두 네트워크 간의 통신은 스위치로 할 수 없습니다. 두 네트워크가 통신하기 위해서는 라우터가 필요합니다.스위치의 VLAN을 이용하면 네트워크를 분할할 수 있다.라우터는 네트워크를 분할하고 두 네트워크 간의 통신을 가능하게 해준다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series2'> <img src='/images/network_logo.png' alt='Network Series [Part2]: 네트워크 장비'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series2'>Network Series [Part2]: 네트워크 장비</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part1]: 네트워크 용어",
      "category" : "CS",
      "tags"     : "network",
      "url"      : "/network-series1",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  네트워크  인터넷  LAN  이더넷  참고네트워크네트워크는 서로를 연결시켜 놓은 망이라고 할 수 있습니다. 그렇다고 연결만 해놓고 끝내면 되는 것은 아니고, 연결된 장비들끼리 대화(통신)를 할 수 있어야 합니다. 왜 이런 개념이 등장했을까요? 그 이유는 바로 자원을 공유하고 싶어서 였습니다.지금과 같이 개인마다 컴퓨터가 보급되기 이전에는 보통 터미널이라고 불리는 단말기 여러 대를 호스트 컴퓨터에 붙여서 사용했습니다. 그러다가 프린트기를 공유하기도 하고, 나중에는 하나의 호스트를 공유하는 것이 아니라 여러 대의 호스트를 공유하고자 하는 요구가 생겨나면서 지금의 네트워크로 발전하게 되었습니다.인터넷그럼 인터넷은 무엇일까요? 인터넷의 인터(Inter)는 연결을 의미합니다. 따라서 인터넷이란, 여러 개의 네트워크를 연결한 것이라는 의미를 가지고 있습니다.위의 네트워크에서 설명했듯이, 네트워크는 자원의 공유를 위해 등장하게 되었습니다. 이 당시에는 물론 그 범위가 회사 또는 특정 단체 내에서의 자원 공유였을 겁니다. 그러다가 사람들이 이제 다른 네트워크와도 정보를 공유하고 싶어졌습니다. 이 때 등장하게 된 개념이 바로 인터넷인 것입니다.인터넷의 특징은 하나의 프로토콜만을 사용한다는 것입니다. 프로토콜은 쉽게 말해 무엇인가를 하기 위해 정해놓은 규약을 의미합니다. 인터넷은 네트워크간의 통신을 의미하기 때문에, 인터넷에서 사용하는 프로토콜은 네트워크간의 통신을 위한 규약이라고 할 수 있습니다. 전세계에서 수많은 네트워크가 생성될텐데 이들간의 통신을 위한 규약을 정해놓지 않는다면, 이들을 연결하기란 불가능할 것입니다. 이를 위해 인터넷에서 모든 네트워크가 사용하는 프로토콜이 있는데 이를 TCP/IP라고 합니다.LANLAN은 Local Area Network의 약자로, 즉 한정된 공간에서 구성한 네트워크를 의미합니다.보통 집, 사무실, 학교, 회사 내에서 사용하는 네트워크를 LAN이라고 합니다. 반면 멀리 떨어진 지역과도 통신할 수 있도록 구성한 네트워크를 WAN(Wide Area Network)라고 합니다. 보통 네트워크내에서 인터넷을 사용할 수 있다면 이러한 네트워크를 WAN이라고 할 수 있습니다.이더넷네트워크를 공부하다 보면 이더넷(Ethernet)이라는 용어를 자주 듣게 됩니다. 이더넷은 네트워크에서 통신을 하는 방식 중 하나라고 생각하면 될 것 같습니다. 대표적으로 토큰링 방식과 이더넷 방식이 있는데 요즘에는 이더넷 방식이 성능적으로 훨씬 우수하기 때문에 대부분의 네트워크는 이더넷 방식을 사용하고 있다고 보면 됩니다.  토큰링: 네트워크내의 장비들 중 토큰을 가지는 장비만 데이터 전송, 전송 후 토큰 넘기는 방식  이더넷: 정해진 순서없이 모든 장비들이 데이터 전송, 데이터 간 충돌 발생하면 기다렸다 또 전송위에서 설명했듯이 이더넷은 네트워크 통신 방식의 일종으로 네트워크 내 장비들이 자유롭게 데이터를 전송하고, 충돌이 발생하면 랜덤한 시간을 기다린 후 다시 전송하는 방식으로 이를 CSMA/CD (Carrier Sense Multiple Access / Collision Detection)라고 합니다. 초기에는 이러한 방식보다 토큰링이 더 안정된 방식이라는 인식이 있었지만, 이더넷과 관련한 기술들이 계속 발전을 거듭하게 되면서 이제는 대부분 이더넷 방식으로 네트워크를 구성합니다.참고  성공과 실패를 결정하는 1%의 네트워크 원리 책  양햄찌가 만드는 세상: 맥 어드레스란 무엇인가?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series1'> <img src='/images/network_logo.png' alt='Network Series [Part1]: 네트워크 용어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series1'>Network Series [Part1]: 네트워크 용어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part18]: Kafka on Kubernetes",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series18",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고  데브원영, 카프카를 쿠버네티스 위에 올리는게 좋은 선택일까?  Alvin Lee, How To Deploy Apache Kafka With Kubernetes  Sona, Kubernetes &amp;amp; Kafka  learnk8s, Designing and testing a highly available Kafka cluster on Kubernetes  FRANK’S BLOG, 헬름으로 Kafka 설치하기    ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series18'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part18]: Kafka on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series18'>Kubernetes Series [Part18]: Kafka on Kubernetes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part17]: Spark on Kubernetes",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series17",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고  mightytedkim, Kubernetes) k8s와 Airflow 이용한 spark작업  Ramiro Alvarez Fernandez, Running Apache Spark on Kubernetes  Shubhomoy Biswas, Running Apache Spark with HDFS on Kubernetes cluster  Jason Heo’s Blog, Spark on Kubernetes 사용법 및 secure HDFS에 접근하기  이웅규, Kubernetes를 이용한 효율적인 데이터 엔지니어링",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series17'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part17]: Spark on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series17'>Kubernetes Series [Part17]: Spark on Kubernetes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part16]: MongoDB on Kubernetes",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series16",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고  zzsza, Kubernetes StatefulSet를 사용해 MongoDB 실행하기  LINE Engineering, Kafka와 MongoDB, Kubernetes로 유연하고 확장 가능한 LINE 쇼핑 플랫폼 구축하기  Percona, Run MongoDB in Kubernetes: Solutions, Pros and Cons  Opcito, Ways to host a MongoDB cluster on Kubernetes",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series16'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part16]: MongoDB on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series16'>Kubernetes Series [Part16]: MongoDB on Kubernetes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part15]: Django on Kubernetes",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series15",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series15'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part15]: Django on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series15'>Kubernetes Series [Part15]: Django on Kubernetes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part14]: Ubuntu on Kubernetes",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series14",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series14'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part14]: Ubuntu on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series14'>Kubernetes Series [Part14]: Ubuntu on Kubernetes</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part13]: Helm",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series13",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  Intuition          헬름 설치      헬름 기본 사용법        Helm Chart          변수 주입      사용자 정의 변수      로직 적용      다양한 함수들        참고Intuition  쿠버네티스 오브젝트의 매니페스트 파일은 yaml,json 과 같은 정적인 형태이다  만약 A 어플리케이션에도 MySQL 이 필요하고, B 어플리케이션에도 MySQL이 필요하다면, 각 MySQL 오브젝트의 형태는 거의 유사하고, 레이블과 같은 요소들만 바뀔 것이다  이를 위해 MySQL 오브젝트 파일을 각각 따로 관리하는 것은 쿠버네티스의 매우 불편한 요소이다      그래서 yaml 파일을 동적으로 만들고 싶다 (매니페스트 파일 안의 값들을 변수 처리하고, 동적으로 값을 할당한다)    Helm은 하나의 Template을 통해 yaml 파일을 동적으로 생성하게 해주는 툴  그리고 하나의 오픈 소스를 위해 필요한 Template을 모아 Chart라고 함  이러한 Chart를 개발자들이 퍼블릭 저장소에 올려두고 서로 공유함 =&amp;gt; 필요한 오픈소스의 템플릿을 다운받아 변수만 할당해주면 빠르게 사용할 수 있음      그래서 Helm을 apt, pip와 같은 패키지 매니저 툴이라고도 함    Helm Hub (Artifact Hub): 원하는 Helm Chart를 검색 =&amp;gt; 해당 Chart를 다운 받을 수 있는 Helm Chart Repository 로 안내함  Helm Chart Repository (Bitnami): 실제 Helm Chart를 저장하고 있는 저장소 (Grafana에서 Grafana Chart도 제공하지만 Bitnami라는 곳에서도 Grafana Chart 제공)(네이버 쇼핑에서 ‘자켓’ 이라고 검색하면 다양한 쇼핑몰에서 제공하는 ‘자켓’ 목록이 보이는 것과 같다. 네이버 쇼핑이 Artifact Hub, 쇼핑몰이 Helm Chart Repository)헬름 설치brew install helmhelm version헬름 기본 사용법  Helm Chart를 이용하려면 우선 Chart를 가진 레포지토리가 나의 로컬 Helm에 등록돼 있어야 한다# 레포지토리 나의 Helm 에 등록# 레포지토리명과 URL을 매핑해둔다helm repo add [레포지토리명] [url]# 차트 배포 (쿠버네티스의 kubectl apply와 유사)# 레포지토리명 -&amp;gt; URL -&amp;gt; 해당 URL 저장소에서 다운로드helm install [배포명(optional)] [레포지토리명/차트명] [flags(optional)]ex. helm install my-tomcat bitnami/tomcat --version 7.1.2# 배포명은 따로 명시 안하면 차트에서 제공하는 배포명 사용함helm install bitnami/tomcat --version 7.1.2 --generate-name# 그 밖에도 다양한 형식으로 설치할 수 있음helm install foo foo-0.1.1.tgz # 압축 파일을 이용helm install foo path/to/foo # 디렉터리를 이용helm install foo https://example.com/charts/foo-1.2.3.tgz # URL을 이용# 설정값 오버라이딩 (--set이 가장 우선순위 높음)helm install stable/mariadb -f config.yamlhelm install stable/mariadb --set a=b# 차트 압축파일 다운# 파일을 내가 커스텀해서 배포할 수 있음helm pull [레포지토리명/차트명] [flags(optional)]# 배포된 차트 관련 명령어helm list helm status [배포명]helm uninstall [배포명]Helm Chart  차트는 쿠버네티스 리소스와 관련된 셋을 설명하는 파일의 모음  차트는 특정한 디렉터리 구조를 가진 파일들로 생성wordpress/            # 차트 디렉터리  Chart.yaml          # 차트에 대한 정보를 가진 YAML 파일  LICENSE             # 옵션: 차트의 라이센스 정보를 가진 텍스트 파일  README.md           # 옵션: README 파일  values.yaml         # 차트에 대한 기본 환경설정 값들  values.schema.json  # 옵션: values.yaml 파일의 구조를 제약하는 JSON 파일  charts/             # 이 차트에 종속된 차트들을 포함하는 디렉터리  crds/               # 커스텀 자원에 대한 정의  templates/          # values와 결합될 때, 유효한 쿠버네티스 manifest 파일들이 생성될 템플릿들의 디렉터리  templates/NOTES.txt # 옵션: 간단한 사용법을 포함하는 텍스트 파일helm create [차트명]helm show values . # values.yamlhelm show chart . # Chart.yamlhelm show readme . # README.mdhelm show all . # All# 템플릿에 변수가 적용된 모습을 확인할 수 있음# 배포하기 전에 확인하는 용도로 많이 사용helm template [차트명] .helm get manifest [차트명] # 배포된 매니페스트 파일 보여줌helm get values [차트명] # -f 또는 --set 옵션으로 적용한 값들을 보여줌helm get all [차트명] # 배포된 차트의 모든 것을 보여줌변수 주입  values.yaml, Chart.yaml, 템플릿 정보, 릴리즈 정보를 템플릿에 있는 변수에 할당할 수 있다  values.yaml은 scope만 대문자, 나머지는 정의된대로 읽어오면 되고, 나머지는 항상 대문자로 시작한다# values.yamlreplicaCount: 1image:   repository: nginxserivce:  type: ClusterIP# Chart.yamlname: mycharttype: applicationversion: 0.1.0# deployment.yamlkind: Deploymentmetadata:spec:  replicas: {{ .Values.replicaCount }}  containers:  - name: {{ .Chart.Name }}# 그 밖에도 이런식으로 사용할 수 있음{{ .Template.BasePath }}{{ .Template.Name }}{{ .Release.Name }}{{ .Release.Namespace }}사용자 정의 변수  _helpers.tpl: templates 폴더에서 사용할 변수들을 정의할 수 있는 곳 (templates 폴더 안에 있는 template들이 언제든지 참조해서 사용할 수 있다)  (_helpers.tpl 안에서 define 키워드를 사용해 선언하고, 템플릿 안에서는 template 또는 include 키워드를 이용해 참조한다)  (template은 불러온 값을 파이프 라인으로 전달할 수 없지만 include는 불러온 값을 파이프라인으로 전달이 가능하다. 따라서 include는 불러온 값의 가공이 가능하기 때문에 더 사용할 것이 권장된다. )# _helpers.tpl{{- define &quot;mychart.name&quot; -}}MyChart{{- end}}# deployment.yamlapiVersion:kind:metadata:  name: {{ include &quot;mychart.name&quot; . }}로직 적용# 함수{{ [함수명] 인자1 인자2 ... }}ex. {{ quote .Values.name }} # (quote 함수에 .Values.name 을 인자로 넣는다 =&amp;gt; 양쪽에 쌍따옴표가 붙은 결과물 출력)# 파이프라인 =&amp;gt; 여러 함수를 쓸 때는 파이프라인 방식을 써야된다# 함수 하나를 쓰더라도 그냥 파이프라인으로 많이 쓴다{{ [인자] | [함수명] }}ex {{ .Values.name | quote }} # .Values.name을 quote 함수의 인자로 넣는다# if문# false가 되는 예시 0, &quot;&quot;, [], {}, Null, false# eq(=), ne(!=), not(!), ge(&amp;gt;=), gt(&amp;gt;), le(&amp;lt;=), lt(&amp;lt;), and(&amp;amp;&amp;amp;), or(||), default, empty{{- if eq .Values.dev.env &quot;dev&quot; }}  log: debug{{- else if .Values.dev.env }}  log: {{ .Vlaues.dev.log }}{{- else }}  log: error{{- end }}{{- with .Values.dev }}  env: {{ .env }} # {{ .Values.dev.env }}  log: {{ .log }} # {{ .Values.dev.log }}{{- end }}{{- range .Values.list }} # [a, b, c]- {{ . }} # a -&amp;gt; b -&amp;gt; c{{- end }}다양한 함수들  {{- .Values.data | toYaml | nindent 4 }}: 리스트 요소를 YAML 형태로 열거하고 싶을 때# values.yamldata:  - a  - b  - c  위의 데이터를 아래와 같이 템플릿에 주입하고 싶다# deployment.yamlfoo:  bar:    - a    - b    - cfoo:  bar:    {{ .Values.data | toYaml }} # toYaml 함수에 넣으면 리스트형 데이터를 YAML에 맞게 형변환 시켜준다----------------------------------------------------# 하지만 결과는 이상하게 나온다 (첫 번째 요소에만 4칸이 띄어지고, 나머지는 indent가 없다)foo:  bar:     - a- b- cfoo:  bar:{{ .Values.data | toYaml | indent 4 }} # indent 4를 넣어 각 요소에 indent 4를 넣어준다# 결과는 올바르지만 정작 템플릿은 indent가 제대로 적용이 안되기 때문에 가독성이 떨어진다foo:  bar:  {{- .Values.data | toYaml | indent 4 }} # -를 추가하면 앞에 공백문자를 없애준다---------------------------------------------------------------------# 앞에 공백문자 2칸 뿐만 아니라 줄바꿈도 없애버린다foo:  bar: - a    - b    - cfoo:  bar:  {{- .Values.data | toYaml | nindent 4 }} # 앞에 한 번 줄바꿈을 실행하고 각 요소에 indent 4를 넣어준다------------------------------------------------# 템플릿 indent도 보기좋게 유지하면서 원하는 결과를 얻었다foo:  bar:    - a    - b    - c      {{- .Values.data | default 5 }}: .Values.data가 false이면 default 값 5가 나온다    {{ trim &quot;  hello &quot; }}: 좌우 공백문자를 제거 (“hello” 리턴)  {{ trimPrefix &quot;-&quot; &quot;-hello-&quot; }}: 왼쪽에 - 문자 제거 (“hello-“ 리턴)      {{ trimSuffix &quot;-&quot; &quot;-hello-&quot; }}: 오른쪽에 - 문자 제거 (“-hello” 리턴)    참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series13'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part13]: Helm'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series13'>Kubernetes Series [Part13]: Helm</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part12]: 오토 스케일링",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series12",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series12'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part12]: 오토 스케일링'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series12'>Kubernetes Series [Part12]: 오토 스케일링</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part11]: 헬스체크",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series11",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series11'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part11]: 헬스체크'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series11'>Kubernetes Series [Part11]: 헬스체크</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part10]: Service로 배우는 kubectl 명령어",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series10",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of ContentsapiVersion: v1kind: Servicemetadata:  name: order # &amp;lt;서비스명&amp;gt;  namespace: snackbar   labels:    service: order    project: snackbarspec:  type: ClusterIP  selector: # &amp;lt;Deployment 오브젝트 매핑&amp;gt;    service: order    project: snackbar  ports:  - port: 80 # &amp;lt;서비스 오브젝트의 포트&amp;gt;    targetPort: 8080 # &amp;lt;Deployment 오브젝트에서 원하는 컨테이너의 포트&amp;gt;---apiVersion: v1kind: Servicemetadata:  name: payment  namespace: snackbar  labels:    service: payment    project: snackbarspec:  type: ClusterIP  selector:    service: payment    project: snackbar  ports:  - port: 80    targetPort: 8080---apiVersion: apps/v1kind: Deploymentmetadata:  name: order  namespace: snackbar  labels: # &amp;lt;서비스가 매핑하기 위해 찾는 부분&amp;gt;    service: order    project: snackbarspec:  replicas: 2  selector: # &amp;lt;복제할 파드 매핑&amp;gt;    matchLabels:      service: order      project: snackbar  template:    metadata:      labels:        service: order        project: snackbar    spec:      containers:      - name: order        image: yoonjeong/order:1.0        ports:        - containerPort: 8080 # &amp;lt;서비스 오브젝트의 targetPort와 일치시키는 부분&amp;gt;        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;--- apiVersion: apps/v1kind: Deploymentmetadata:  name: payment  namespace: snackbar  labels:    service: payment    project: snackbarspec:  replicas: 2  selector:    matchLabels:      service: payment      project: snackbar  template:    metadata:      labels:        service: payment        project: snackbar    spec:      containers:      - name: payment        image: yoonjeong/payment:1.0        ports:        - containerPort: 8080        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;kubectl create namespace snackbarkubectl apply -f ./snackbar/service-deployment.yamlkubectl get all -n snackbarkubectl get svc -o wide -n snackbarkubectl get endpoints -n snackbar  서비스 오브젝트를 만들면 컨테이너에 엔드포인트 오브젝트 자동 생성kubectl exec -it order-5d45bf5796-j4g7w -c order -n snackbar -- /bin/shcat /etc/resolv.conf서비스 관련 환경변수는 모든 오브젝트에 저장됨order 컨테이너에 payment 서비스 관련 환경변수도 저장되어 있음 -&amp;gt; 환경 변수로 통신할 수 있게함curl ${PAYMENT_SERVICE_HOST}:${PAYMENT_SERVICE_PORT}payment 서비스를 이용해서 통신하는 방법# 여기서 payment는 Service 오브젝트의 metadata.name에 해당한다curl payment:80만약 다른 네임스페이스에 있는 파드인 경우 . 으로 하면된다.curl payment.snackbar:80",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/kubernetes-series10'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part10]: Service로 배우는 kubectl 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series10'>Kubernetes Series [Part10]: Service로 배우는 kubectl 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series8",
      "date"     : "Jan 31, 2022",
      "content"  : "Table of Contents  Connection Client To Broker          Scenario 0: Client and Kafka running on the same local machine      Scenario 1: Client and Kafka running on the different machines      Scenario 2: Kafka and client running in Docker      Scenario 3: Kafka in Docker container with a client running locally                  Adding a new listener to the broker                    Scenario 4: Kafka running locally with a client in Docker container      원문: Confluent블로그Connection Client To Broker클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.  브로커와의 초기 연결. 연결이 되면 브로커는 클라이언트에게 연결 가능(resolvable and accessible from client machine)한 브로커의 엔드포인트 제공(advertised.listeners)  클라이언트와 연결 가능한 브로커와의 연결초기 연결은 producer = KafkaProducer(bootstrap_servers=[&quot;localhost:9092&quot;]) 와 같이 bootstrap_servers 중 하나의 서버와 초기 연결된다. 그러면 연결된 서버는 클라이언트에게 advertised.listeners를 노출해 연결되도록 한다.예시로 클라이언트와 카프카가 서로 다른 머신에 있는 경우를 보자.연결이 성공되는 경우는 다음과 같다.연결이 실패되는 경우는 다음과 같다.이러한 경우에는 advertised.listeners를 localhost:9092로 설정하면 안된다.Scenario 0: Client and Kafka running on the same local machinebootstrap_servers = &#39;localhost:9092&#39;advertised_listeners = &#39;localhost:9092&#39;  잘 동작한다.클라이언트에 전달되는 메타데이터는 192.168.10.83이다. 이 값은 로컬 머신의 IP 주소이다.Scenario 1: Client and Kafka running on the different machines카프카 브로커가 다른 머신에서 동작하는 경우를 살펴보자. 예를 들면 AWS, GCP와 같은 클라우드에서 생성한 머신여기 예제에서 클라이언트는 나의 노트북이고 카프카 브로커가 동작하고 있는 머신의 LAN은 asgard03이라고 해보자.초기 연결은 성공한다. 하지만 메타데이터에서 돌려주는 노출된 리스너는 localhost이다. 하지만 클라이언트의 localhost에는 카프카 브로커가 없으므로 연결은 실패한다.이 문제를 해결하기 위해서는 server.properties에서 advertised.listeners 값을 수정해 클라이언트에서 접근 가능한 올바른 호스트네임과 포트를 제공해주어야 한다.# advertised.listeners 수정 전advertised.listeners=PLAINTEXT://localhost:9092listeners=PLAINTEXT://0.0.0.0:9092# advertised.listeners 수정 후advertised.listeners=PLAINTEXT://asgard03.moffatt.me:9092listeners=PLAINTEXT://0.0.0.0:9092Scenario 2: Kafka and client running in Docker도커를 이용할 때 기억해야할 점은 도커는 컨테이너를 통해 그들만의 작은 세상을 만든다는 것이다. 컨테이너는 자체적인 호스트네임, 네트워크 주소, 파일 시스템을 가지고 있다. 따라서 컨테이너를 기준으로 localhost는 더이상 나의 노트북이 아니다. 도커 컨테이너에서 localhost는 컨테이너 자기 자신이다.여기서는 카프카와 클라이언트를 모두 각각 도커 호스트 위에 컨테이너로 만들어 본다.클라이언트를 컨테이너로 만들어주는 Dockerfile이다.FROM python:3# We&#39;ll add netcat cos it&#39;s a really useful# network troubleshooting toolRUN apt-get updateRUN apt-get install -y netcat# Install the Confluent Kafka python libraryRUN pip install confluent_kafka# Add our scriptADD python_kafka_test_client.py /ENTRYPOINT [ &quot;python&quot;, &quot;/python_kafka_test_client.py&quot;]위의 메니페스트를 이용해 클라이언트 이미지를 만든다.docker build -t python_kafka_test_client .카프카 브로커를 생성하자.docker network create rmoff_kafkadocker run --network=rmoff_kafka --rm --detach --name zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:5.5.0docker run --network=rmoff_kafka --rm --detach --name broker            -p 9092:9092           -e KAFKA_BROKER_ID=1            -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181            -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092            -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1           confluentinc/cp-kafka:5.5.0쥬키퍼와 카프카 브로커가 컨테이너로 돌아가고 있다.$ docker psIMAGE                              STATUS              PORTS                          NAMESconfluentinc/cp-kafka:5.5.0        Up 32 seconds       0.0.0.0:9092-&amp;gt;9092/tcp         brokerconfluentinc/cp-zookeeper:5.5.0    Up 33 seconds       2181/tcp, 2888/tcp, 3888/tcp   zookeeper위에서 우리는 우리만의 도커 네트워크를 만들었고 이제 이 네트워크를 통해 클라이언트와 브로커가 통신하도록 해보자$ docker run --network=rmoff_kafka --rm --name python_kafka_test_client        --tty python_kafka_test_client broker:9092결과를 보면 초기 연결은 성공하지만, 메타데이터로 localhost를 돌려주기 때문에 프로듀서와 클라이언트의 연결은 실패된다.이를 해결하려면 advertise.listeners의 호스트네임을 컨테이너 이름으로 바꿔줘야 한다.# 수정 전-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092# 수정 후 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092최종적으로 브로커 설정을 다음과 같이 고칠 수 있다.docker stop brokerdocker run --network=rmoff_kafka --rm --detach --name broker            -p 9092:9092            -e KAFKA_BROKER_ID=1            -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181            -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092            -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1            confluentinc/cp-kafka:5.5.0Scenario 3: Kafka in Docker container with a client running locally위의 Scenario 2와 비교하여 클라이언트가 컨테이너화 되어 있다가 여기서는 따로 컨테이너화 되지 않고 로컬 머신 위에 있다.로컬에 실행하는 클라이언트는 따로 네트워크가 구성되어 있지 않다. 그렇기 때문에 따로 특정 트래픽을 받기 위해서는 로컬의 포트를 열어 이를 통해 통신해야 한다. 아래 그림과 같이 9092:9092 포트를 열었다고 해보자. 클라이언트가 로컬의 9092포트 엔드포인트로 접근하기 위해서는 bootstrap_servers=&#39;localhost:9092&#39;로 해야 한다. advertised.listeners는 broker:9092로 해야 한다(클라이언트와 localhost관계가 아니므로).문제는 클라이언트 입장에서 broker:9092는 resolvable하지 않다.Adding a new listener to the broker이 문제를 해결하는 방법은 다수의 리스너를 만드는 것이다....    ports:      - &quot;19092:19092&quot;    environment:      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT...Scenario 4: Kafka running locally with a client in Docker container이런 상황이 잘 있지는 않지만, 어쨋든 이런 경우에 대한 해결책은 있다. 다만 좀 임시방편적일 뿐이다.만약 맥에서 도커가 동작하고 있다면, host.docker.internal을 이용할 수 있다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-31T21:01:35+09:00'>31 Jan 2022</time><a class='article__image' href='/kafka-series8'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series8'>Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]</a> </h2><p class='article__excerpt'>클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part7]: Kafka Listeners – Explained[번역]",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series7",
      "date"     : "Jan 30, 2022",
      "content"  : "Table of Contents  Kafka Listeners  Why can I connect to the broker, but the client still fails?  HOW TO: Connecting to Kafka on Docker  HOW TO: Connecting to Kafka on IaaS/Cloud          Option 1: External address is resolvable locally      Option 2: External address is NOT resolvable locally        Exploring listeners with Docker원문: Confluent블로그읽어보면 좋은 포스트Kafka Listeners카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners(또는 도커 이미지를 사용할 경우 KAFKA_ADVERTISED_LISTENERS)를 external IP 주소로 설정해야 합니다.아파치 카프카는 분산 시스템입니다. 데이터는 리더 파티션으로부터 쓰고 읽어지며 리더 파티션은 어떤 브로커에도 있을 수 있습니다. 그래서 클라이언트가 카프카에 연결되기 위해서는 해당 리더 파티션을 가지고 있는 브로커가 누구인지에 대한 메타데이터를 요청합니다. 이 메타데이터에는 리더 파티션을 가지는 브로커의 엔드포인트 정보를 포함하고 있으며 클라이언트는 이 정보를 이용해 카프카와 연결될 것입니다.만약 카프카가 도커와 같은 가상머신이 아닌 bare metal 위에서 동작한다면 이 엔드포인트는 그저 hostname이나 localhost 정도가 될 것입니다. 하지만 조금 더 복잡한 네트워크 환경 또는 멀티 노드 환경으로 오게 되면 조금 더 주의가 필요하게 됩니다.초기에 브로커가 연결되면 실제로 리더 파티션을 가지는 브로커의 host와 IP의 정보를 돌려줍니다. 이러한 과정은 단일 노드 환경에서도 마찬가지입니다.KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXTKAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOBserver.properties  KAFKA_LISTENERS: 카프카가 리스닝하기 위해 노출하는 host/IP와 port  KAFKA_ADVERTISED_LISTENERS: 클라이언트에게 알려주는 리스너의 host/IP와 port 리스트  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 각 리스너들이 사용하는 security protocol  KAFKA_INTER_BROKER_LISTENER_NAME: 브로커들 간의 통신을 위해 사용하는 리스너브로커에 연결되면 연결된 리스너가 반환됩니다. kafkacat은 이러한 정보를 알아보는 유용한 툴입니다. -L을 이용하면 연결된 리스너에 관한 메타데이터를 얻을 수 있습니다.# 9092포트로 연결시, localhost:9092 리스너가 반환$ kafkacat -b kafka0:9092           -LMetadata for all topics (from broker -1: kafka0:9092/bootstrap):1 brokers:  broker 0 at localhost:9092# 29092포트로 연결시, kafka0:29092 리스너가 반환$ kafkacat -b kafka0:29092           -LMetadata for all topics (from broker 0: kafka0:29092/0):1 brokers:  broker 0 at kafka0:29092Why can I connect to the broker, but the client still fails?초기 브로커 연결에 성공했다고 하더라도, 브로커가 반환하는 메타데이터 안에 있는 주소로 여전히 클라이언트가 접근하지 못하는 경우가 있습니다.      AWS EC2 인스턴스에 브로커를 만들어 로컬 머신에서 EC2에 있는 브로커로 메세지를 보내보려고 합니다. external hostname은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com입니다. 로컬 머신과 EC2가 포트포워딩을 통해 연결되었는지 확인해보겠습니다.        우리의 로컬 머신은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com을 54.191.84.122으로 성공적으로 리졸브(resolve) 합니다.  $ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -LMetadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):1 brokers:  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092      hostname이 ip-172-31-18-160.us-west-2.compute.internal인 리스너를 반환합니다.        하지만 인터넷을 통해 ip-172-31-18-160.us-west-2.compute.internal은 not resolvable해서 클라이언트는 브로커에 메세지 전송을 실패합니다.  $ echo &quot;test&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time  브로커가 설치된 서버의 클라이언트로는 문제없이 동작한다.$ echo &quot;foo&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginningfoo이러한 일이 발생하는 이유는 9092포트로 연결하는 리스너가 내부 리스너이기 때문이라고 한다. 그래서 브로커가 설치된 서버의 내부에서만 resolvable한 hostname인 ip-172-31-18-160.us-west-2.compute.internal을 리턴한다.HOW TO: Connecting to Kafka on Docker도커에서 동작하기 위해서는 카프카의 두 개의 listener를 지정해야 한다.      도커 네트워크 내에서의 통신: 이것은 브로커간의 통신 또는 도커 안의 다른 컴포넌트와의 통신을 의미한다. 이를 위해서는 도커 네트워크 안에 있는 컨테이너의 호스트네임을 사용해야 한다. 각각의 브로커는 컨테이너의 호스트네임을 통해 서로 통신하게 될 것이다.        도커가 아닌 네트워크로부터의 트래픽: 이것은 도커를 실행하는 서버에서 로컬로 동작하는 클라이언트가 될 수 있다. 이러한 경우 도커를 실행하는 서버(localhost)에서 컨테이너의 포트에 연결할 수 있다. 아래의 도커 컴포즈 스니펫을 한 번 보자.  […]kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      […]      # For more details see See https://rmoff.net/2018/08/02/kafka-listeners-explained/      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB[…]      도커 네트워크 내에 클라이언트가 있다면 클라이언트는 호스트네임 kafka0, 29092 포트를 이용한 BOB 리스너를 통해 브로커와 통신할 것입니다. 각각의 컨테이너(클라이언트, 브로커)는 kafka0를 도커 내부 네트워크를 통해 resolve합니다.        도커를 실행하는 호스트 머신(VM)에 있는 외부 클라이언트의 경우, 호스트 네임 localhost, 9092 포트를 이용한 FRED 리스너를 통해 브로커와 통신한다.        도커를 실행하는 호스트 머신(VM) 밖에 있는 외부 클라이언트는 위의 리스너를 통해 통신할 수 없다. 왜냐하면 kafka0도 localhost도 모두 resolvable하지 않기 때문이다.  HOW TO: Connecting to Kafka on IaaS/Cloud도커와의 차이점은, 도커에서 외부의 연결은 단순히 localhost에서 이루어진 반면, 클라우드 호스트 기반의 카프카는 클라이언트가 localhost에 존재하지 않는다는 것이다.더 복잡한 것은 도커 네트워크가 호스트의 네트워크와는 크게 분리되어 있지만 IaaS에서는 외부 호스트 이름이 내부적으로 확인 가능한 경우가 많기 때문에 이러한 문제가 실제로 발생할 경우 호스트 이름이 잘못될 수 있다.브로커에 연결할 외부 주소가 브로커에게 로컬로 확인할 수 있는지 여부에 따라 두 가지 방법이 있다.Option 1: External address is resolvable locallyEC2 인스턴스의 IP 주소는 기본적으로 External IP. 만약 local에서 resolvable하다면, 로컬 내의 클라이언트, 외부 클라이언트 모두 이를 통해 통신 가능. 다만 외부 클라이언트는 밑의 설정만 추가해주면 된다.advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092Option 2: External address is NOT resolvable locally만약 로컬 내에서 resolvable하지 않다면, 두 가지 리스너가 필요하다.  VPC 내에서의 통신을 위해 local에서 resolvable한 Internal IP를 통해 내부에서 리슨한다  VPC 밖, 예를 들어 나의 노트북에서 접속하려는 경우 인스턴스의 External IP가 필요하다listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTadvertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092inter.broker.listener.name=INTERNALExploring listeners with Docker  Listener BOB (port 29092) for internal traffic on the Docker network  Listener FRED (port 9092) for traffic from the Docker host machine (localhost)  Listener ALICE (port 29094) for traffic from outside, reaching the Docker host on the DNS name never-gonna-give-you-up---version: &#39;2&#39;services:  zookeeper:    image: &quot;confluentinc/cp-zookeeper:5.2.1&quot;    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000  kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      KAFKA_BROKER_ID: 0      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot;      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100  kafkacat:    image: confluentinc/cp-kafkacat    command: sleep infinity",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-30T21:01:35+09:00'>30 Jan 2022</time><a class='article__image' href='/kafka-series7'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part7]: Kafka Listeners – Explained[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series7'>Kafka Series [Part7]: Kafka Listeners – Explained[번역]</a> </h2><p class='article__excerpt'>카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners를 external IP 주소로 설정해야 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part9]: Deployment로 배우는 kubectl 명령어",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series9",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  배포 전략apiVersion: apps/v1kind: Deploymentmetadata:  name: my-app  labels:    app: my-appspec:  replicas: 3  selector:    matchLabels:      app: my-app  template:    metadata:      labels:        app: my-app        project: fastcampus        env: production    spec:      containers:      - name: my-app        image: yoonjeong/my-app:1.0        ports:        - containerPort: 8080        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;kubectl apply -f ./my-app/deployment.yamlkubectl get pods -L app,env,projectapiVersion: apps/v1kind: Deploymentmetadata:  name: my-app  labels:    app: my-appspec:  replicas: 3  selector:    matchLabels:      app: my-app  template:    metadata:      labels:        app: my-app        project: fastcampus        env: development # 레이블 변경    spec:      containers:      - name: my-app        image: yoonjeong/my-app:1.0        ports:        - containerPort: 8080        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;kubectl apply -f ./my-app/deployment.yamlkubectl get pods -L app,env,project위 과정을 -w (Watch 모드)를 사용해서 실시간으로 변화를 관찰할 수도 있음kubectl get pods -L app,env,project -w배포 전략  spec의 strategy에 설정  Recreate          기존 버전 모두 삭제한 후, 새로운 버전으로 모두 새로 띄움      중간에 서비스가 중단되는 시점이 생김 -&amp;gt; 개발 단계에서 주로 사용        RollingUpdate          기존 버전을 단계적으로 줄이고, 새로운 버전을 단계적으로 늘려나감      무중단 업데이트 가능 -&amp;gt; 서비스 단계에서 주로 사용      RollingUpdate에는 두 가지 옵션이 있다 (리소스 사용량을 조절하기 위해)      maxUnavailable: 업데이트를 위해 한 번에 얼마나 기존 파드를 종료할 것인지 (개수 또는 퍼센트로 표현)      maxSurge: 업데이트를 위해 리소스를 기존에 비해 얼마나 초과할 수 있는지 (개수 또는 퍼센트로 표현)        strategy:type: RollingUpdaterollingUpdate:  maxUnavailable: 2  maxSurge: 1                    kind: Deploymentmetadata:  name: my-app  labels:    app: my-appspec:  replicas: 3  selector:    matchLabels:      app: my-app  strategy:    type: Recreate  template:    metadata:      labels:        app: my-app        project: fastcampus        env: production        version: v1    spec:      containers:      - name: my-app        image: yoonjeong/my-app:1.0        ports:        - containerPort: 8080        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;kubectl get pods -L app,versionversion 레이블을 v2로 바꾸고 다시 적용(apply) 해보면,변경이 잘 적용되었다. 정말 기존 버전 파드가 모두 삭제되고 새로운 버전이 생긴건지 확인해보자.kubectl get rw -wstrategy 를 RollingUpdate 했을 떄의 과정은 다음과 같다.",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series9'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part9]: Deployment로 배우는 kubectl 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series9'>Kubernetes Series [Part9]: Deployment로 배우는 kubectl 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part8]: Replicaset로 배우는 kubectl 명령어",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series8",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  오브젝트 정보 확인  쿠버네티스 명령 수행 순서 확인하기  Replicaset 오브젝트 삭제하기  Scale up/down  Replicaset 템플릿 변경후 다시 적용하면  spec에 selector와 replicas가 추가          selector: Replicaset 오브젝트가 담당할 파드 매칭      replicas: 복제할 파드의 개수      apiVersion: apps/v1kind: ReplicaSetmetadata:  name: blue-replicasetspec:  selector:    matchLabels:      app: blue-app  replicas: 3  template:    metadata:      labels:        app: blue-app    spec:      containers:      - name: blue-app        image: yoonjeong/blue-app:1.0        ports:        - containerPort: 8080        env:        - name: NODE_NAME          valueFrom:            fieldRef:              fieldPath: spec.nodeName        resources:          limits:            memory: &quot;64Mi&quot;            cpu: &quot;50m&quot;kubectl get rs blue-replicaset -o widekubectl apply -f ./blue-app/replicaset.yaml 오브젝트 정보 확인kubectl describe rs blue-replicasetkubectl get rs blue-replicaset -o json쿠버네티스 명령 수행 순서 확인하기kubectl get events --sort-by=.metadata.creationTimestampReplicaset 오브젝트 삭제하기kubectl delete rs blue-replicaset(파드부터 삭제하면 Replicaset 오브젝트가 계속 새로 생성함 -&amp;gt; 삭제할 때는 Replicaset 오브젝트)Scale up/downkubectl scale rs blue-replicaset --replicas=1Replicaset 템플릿 변경후 다시 적용하면  Replicaset 생성 후, 템플릿 변경 후 다시 apply 적용하면?  이미 배포된 파드에는 변경이 적용 안됨 (이미 replicas 수만큼 배포돼 있으므로)  만약 의도적으로 파드 한 개를 삭제하면, 그 다음부터 새로 띄어지는 파드는 템플릿 변경 후의 파드  이를 자동으로 버전 롤업/롤백해주는 오브젝트가 Deployment",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series8'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part8]: Replicaset로 배우는 kubectl 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series8'>Kubernetes Series [Part8]: Replicaset로 배우는 kubectl 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part7]: Pod로 배우는 kubectl 명령어",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series7",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  환경변수  네트워크          파드내에 컨테이너끼리 통신      서로 다른 파드의 컨테이너간 통신        라벨링  배포할 노드 지정  파드 삭제하기환경변수apiVersion: v1kind: Podmetadata:  name: hello-appspec:  containers:  - name: hello-app-con    image: yoonjeong/hello-app:1.0    ports:    - containerPort: 8080    resources:      limits:        memory: &quot;128Mi&quot;        cpu: &quot;100m&quot;kubectl apply -f ./hello-app/pod.yamlkubectl get pods -o widekubectl exec -it hello-app -- /bin/shenvapiVersion: v1kind: Podmetadata:  name: hello-appspec:  containers:  - name: hello-app-con    image: yoonjeong/hello-app:1.0    ports:    - containerPort: 8080    env:      - name: MY_NAME        value: &quot;Jay&quot;    resources:      limits:        memory: &quot;128Mi&quot;        cpu: &quot;100m&quot;envkubectl get pod hello-app -o json  크게 metadata, spec, status 항목이 있음apiVersion: v1kind: Podmetadata:  name: hello-appspec:  containers:  - name: hello-app-con    image: yoonjeong/hello-app:1.0    ports:    - containerPort: 8080    env:      - name: MY_NAME        value: &quot;Jay&quot;      - name: POD_IP        valueFrom:          fieldRef:            fieldPath: status.podIP      - name: NODE_IP        valueFrom:          fieldRef:            fieldPath: status.hostIP      - name: NODE_NAME        valueFrom:          fieldRef:            fieldPath: spec.nodeName    resources:      limits:        memory: &quot;128Mi&quot;        cpu: &quot;100m&quot;env네트워크파드내에 컨테이너끼리 통신apiVersion: v1kind: Podmetadata:  name: blue-green-app # Pod의 호스트명spec:  containers:  - name: blue-app    image: yoonjeong/blue-app:1.0    ports:    - containerPort: 8080    resources:      limits:        memory: &quot;64Mi&quot;        cpu: &quot;100m&quot;kubectl apply -f blue-green-app/pod.yamlkubectl logs blue-green-app -c blue-appkubectl exec -it blue-green-app -c blue-app -- /bin/sh# 블루 앱에서 그린 앱에 localhost로 통신curl -vs localhost:8081/tree서로 다른 파드의 컨테이너간 통신kubectl apply -f red-app/pod.yamlkubectl get pods -o wideexport RED_POD_IP=$(kubectl get pod red-app -o jsonpath=&quot;{.status.podIP}&quot;)echo $RED_POD_IP----------------------10.100.1.3kubectl exec blue-green-app -c blue-app -- curl -vs $RED_POD_IP:8080/rosekubectl port-forward blue-green-app 8080:8080URL: localhost:8080/sky라벨링apiVersion: v1kind: Podmetadata:  name: red-app  labels:    category: nature    app: rosespec:  containers:  - name: red-app    image: yoonjeong/red-app:1.0    ports:    - containerPort: 8080    resources:      limits:        memory: &quot;64Mi&quot;        cpu: &quot;100m&quot;kubectl get pods --show-labels# (라벨간에 띄어쓰기 허용 x)kubectl get pod -L app,category# --selector 대신 -l 써도됨kubectl get pod --selector app=rose -L appkubectl get pod -l &#39;app in (rose,sky-and-tree)&#39; -L app배포할 노드 지정# 노드 목록과 레이블 확인kubectl get nodes# 노드에 Label 추가# -- 첫번째, 세번째 노드에 soil=moist# -- 두번째 노드에 soil=drykubectl label node &amp;lt;1번째 노드&amp;gt; &amp;lt;3번째 노드&amp;gt; soil=moistkubectl label node &amp;lt;2번째 노드&amp;gt; soil=dry# soil 노드 레이블 확인kubectl get node -L soilapiVersion: v1kind: Podmetadata:  name: red-appspec:  nodeSelector:    soil: moist  containers:  - name: red-app    image: yoonjeong/red-app:1.0    ports:    - containerPort: 8080    resources:      limits:        memory: &quot;64Mi&quot;        cpu: &quot;100m&quot;파드 삭제하기kubectl delete pod --allkubectl delete pod -l app=rose",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series7'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part7]: Pod로 배우는 kubectl 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series7'>Kubernetes Series [Part7]: Pod로 배우는 kubectl 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part6]: Volume",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents  Volume          emptyDir      hostPath        Persistent Volume          매니페스트 파일        PersistentVolumeClaim          매니페스트 파일        StorageClass          매니페스트 파일        스테이트풀셋에서 영구볼륨클레임 사용          매니페스트 파일      Volume  파드에 정적으로 볼륨을 지정하는 형태  ex. emptyDir, hostPath 등emptyDir  파드용 임시 디스크 영역 (파드가 종료되면 삭제)  파드안에 직접 지정한다  호스트의 임시영역을 마운트 할 수도 없고, 호스트에 있는 파일을 참조할 수도 없다  쿠버네티스 노드의 디스크 영역이 할당된다apiVersion: v1kind: Podmetadata:  name: # 파드명spec:  containers:  - image: # 사용할 이미지    name: # 컨테이너명    volumeMounts:    - name: # 사용할 볼륨명      mountPath: # 컨테이너 안에서 마운트할 경로  volumes:  - name: # 볼륨명    emptyDir:   #  medium: memory # 메모리 영역을 사용할 수도 있다      sizeLimit: # 사용할 리소스 용량hostPath  마운트가 가능하다 (그래서 호스트 영역 지정해야함)apiVersion: v1kind: Podmetadata:  name: # 파드명spec:  containers:  - image: # 사용할 이미지    name: # 컨테이너명    volumeMounts:    - name: # 사용할 볼륨명      mountPath:  volumes:  - name: # 볼륨명    hostPath:      path: # 마운트할 호스트 경로      type: # Directory / DirectoryOrCreate / FilePersistent Volume  볼륨(Volume)은 파드 안에 직접 지정하는 형태로 연결한다  영구볼륨은 매니페스트를 통해 개별 리소스를 생성한다  네트워크를 통해 디스크를 어태치하는 디스크 타입이다  (미리 영구 디스크를 생성한 후에 적용해야 한다)  ex. GCE Persistent Disk, AWS Elastic Block Store 등매니페스트 파일apiVersion: v1kind: PersistentVolumemetadata:  name:  labels: # 볼륨에 레이블링 해두면 나중에 파드에서 요청할 때 알맞은 영구 볼륨 요청할 수 있음spec:  capacity:    storage:  accessModes:  - # 접근 모드: ReadWriteOnce / ReadWriteMany / ReadOnlyMany              # ReadWriteOnce: 단일 노드에서 읽기와 쓰기 가능              # ReadWriteMany: 여러 노드에서 읽기와 쓰기 가능              # ReadOnlyMany: 여러 노드에서 읽기 가능  persistentVolumneReclaimPolicy: # 영구 볼륨을 사용한 후 처리방법: Delete / Retain / Recycle  gcePersistentDisk: # GCE Perstent Disk 사용하는 경우    pdName:PersistentVolumeClaim  영구볼륨을 요청하는 리소스  영구볼륨은 영구볼륨클레임을 통해 사용한다  영구볼륨클레임에서 지정된 조건을 기반으로 영구볼륨을 요청하면, 스케줄러는 현재 가지고 있는 영구볼륨중에서 적당한 볼륨을 할당한다매니페스트 파일apiVersion: v1kind: PersistentVolumeClaimmetadata:  name:spec:  selector:    matchLabels:    matchExpressions:    - key:      operator:      values:  resources:    requests:      storage:  accessModes:  - ReadWriteOnce  storageClassName:# 파드에서 영구볼륨클레임 사용 예시apiVersion: v1kind: Podmetadata:  name:spec:  containers:  - image:    name:    volumeMounts:    - mountPath:      name:  volumes:    - name:    persistentVolumeClaim:      claimName: # 영구볼륨클레임명1. 저장소 생성(Local, GCP, AWS 등)2. PersistentVolume 생성3. PersistentVolumeClaim 생성4. 워크로드 오브젝트에서 사용5. 정적으로 PersistentVolume 할당 (자원 낭비)StorageClass  위의 방법은 사전에 영구 볼륨을 생성해야 하는 번거로움  용량을 동적으로 확보할 수 없어서 리소스를 낭비할 수 있다  StorageClass로 어디서(Local, GCP, AWS 등) 저장소를 사용할지만 정의하고, 볼륨의 크기는 PersistentVolumeClaim 에서 동적으로 프로비저닝 하자  동적 프로비저닝을 이용하면, 영구볼륨클레임이 생성될때 동적으로 영구볼륨을 생성하고 할당한다  동적 프로비저닝을 사용하려면, 사전에 어떤 영구 볼륨을 생성할지 정의하는 스토리지클래스를 정의해야 한다1. 저장소 생성(Local, GCP, AWS 등)2. StorageClass 생성3. PersistentVolumeClaim 생성4. 워크로드 오브젝트에서 사용5. 동적으로 PersistentVolume 생성 및 할당매니페스트 파일apiVersion: storage.k8s.io/v1kind: StorageClassmetadata:  name: # 스토리지클래스명 parameters:provisioner: kubernetes.io/gce-pdreclaimPolicy:apiVersion: v1kind: PersistentVolumeClaimmetadata:  name:spec:  storageClassName: # 스토리지클래스명  accessModes:  - ReadWriteOnce  resources:    requests:      storage: 3Gi스테이트풀셋에서 영구볼륨클레임 사용  스테이트풀셋에서는 영구 데이터 영역을 사용하는 경우가 많다  spec.volumeClaimTemplate 항목을 통해 영구볼륨클레임을 별도로 정의하지 않고도, 자동으로 영구볼륨클레임을 생성할 수 있다 (스토리지클래스 + 스테이트풀셋)1. 저장소 생성(Local, GCP, AWS 등)2. StorageClass 생성3. 워크로드 오브젝트에서 spec.volumeClaimTemplate을 이용해 PersistentVolumeClaim 생성4. 동적으로 PersistentVolume 생성 및 할당매니페스트 파일apiVersion: apps/v1kind: StatefulSet...spec:  template:    ...    spec:      containers:      - name:        image:        volumeMounts:        - name: pvc-template-volume          mountPath: /tmp  volumeClaimTemplates:  - metadata:      name: pvc-template-volume    spec:      storageClassName:      accessModes:      - ReadWriteOnce      resources:        requests:          storage: 10Gi",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kubernetes-series6'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part6]: Volume'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series6'>Kubernetes Series [Part6]: Volume</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part5]: ConfigMap과 Secret",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series5",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents  기본 방법  ConfigMap          컨피그맵 리소스 만드는 방법      컨피그맵 리소스 사용하는 방법      컨피그맵 업데이트        Secret  참고  컨테이너에 대한 설정파일  패스워드와 같은 기밀 정보  환경 변수기본 방법  개별 컨테이너의 설정 내용은 환경 변수나 파일이 저장되어 있는 영역을 마운트하여 전달하는 것이 일반적이다  쿠버네티스에서 환경 변수를 전달할 때는 파드 템플릿에 env 또는 envForm을 지정한다  command나 args에 환경 변수 전달할 때는 $() 표기법으로 나타낸다 (또한 매니페스트에서 정의한 환경 변수만 사용할 수 있다)apiVersion: v1kind: Podmetadata:  name: sample-env  labels:    app: sample-appspec:  containers:    - name: nginx-container      image: nginx:1.16      command: [&quot;echo&quot;]      # K8S_NODE는 환경 변수로 인식, HOSTNAME은 그냥 문자열로 출력 (매니페스트에서 정의한 환경 변수만 사용 가능)      args: [&quot;$(K8S_NODE)&quot;, &quot;$(HOSTNAME)&quot;]      env:        # 단순 키,벨류 형태        - name: MAX_CONNECTION          value: &quot;100&quot;        # 파드 정보를 참조할 때        - name: K8S_NODE          valueFrom:            fieldRef:              fieldPath: spec.nodeName        # 컨테이너 리소스 정보        - name: CPU_LIMITS          valueFrom:            resourceFieldRef:              containerName: nginx-container              resource: limits.cpuConfigMap  위의 방법처럼 매번 파드에 환경 변수를 설정해도 되지만,  여러 파드에 적용해야할 경우 컨피그맵을 사용해 한 번에 적용/관리할 수 있다  컨피그맵은 설정파일을 외부화할 수 있게 도와준다  키-벨류 형식 외에도 .conf 같은 설정 파일 자체도 저장할 수 있다컨피그맵 리소스 만드는 방법  보통 리소스는 YAML 형태의 매니페스트 파일을 통해 만들지만, 컨피그맵은 여러 방법으로 리소스를 만들 수 있다  파일에서 값을 참조해 생성하는 방법 (--from-file)  직접 값을 전달해서 생성하는 방법 (--from-literal)  매니페스트로 생성하는 방법(-f)# 파일명이 키(key)가되고, 파일안의 내용이 값(value)가 된다kubectl create configmap &amp;lt;컨피그 리소스명&amp;gt; --from-file &amp;lt;파일명&amp;gt;(ex. kubectl create configmap my-config --from-file NAME =&amp;gt; key는 NAME, value는 &#39;mike&#39;)(만약 NAME.txt 처럼 확장자가 포함된 파일의 경우, --from-file NAME.txt=NAME 이런식으로 적어주면 된다)# 폴더명을 전달하면 폴더안의 파일과 내용이 각각 키와 값이 된다kubectl create configmap &amp;lt;컨피그 리소스명&amp;gt; --from-file &amp;lt;폴더명&amp;gt;# .conf 와 같은 파일을 전달할 수도 있다kubectl create configmap &amp;lt;컨피그 리소스명&amp;gt; --from-file ./server.confapiVersion: v1kind: ConfigMapmetadata:  name: sample-configmapdata:  thread: &quot;16&quot;  nginx.conf: |    user nginx;    worker_processes auto;    pid /run/nginx.pid;  test.sh: |    #!/bin/bash    echo &quot;hello&quot;    sleep infinitykubectl create configmap &amp;lt;컨피그 리소스명&amp;gt; -f myconfig.yaml컨피그맵 리소스 사용하는 방법  환경 변수를 전달하는 방법apiVersion: v1kind: Podmetadata:  name: sample-podspec:  containers:    - name: configmap-container      image: nginx:1.16      env:        - name: CONNECTION_MAX          valueFrom:            configMapKeyRef: # 특정 키만              name: &amp;lt;컨피그 리소스명&amp;gt;              key: connection.max      envFrom:        - configMapRef: # 컨피그맵에 정의된 전체 키 (키 값이 매니페스트 파일에 직접 명시되지 않아 가독성 조금 떨어짐)            name: &amp;lt;컨피그 리소스명&amp;gt;  볼륨으로 마운트하는 방법apiVersion: v1kind: Podmetadata:  name: sample-podspec:  containers:  - name: my-container    image: nginx:1.16    volumeMounts:    - name: &amp;lt;볼륨명&amp;gt;      mountPath: &amp;lt;마운트할 컨테이너 경로&amp;gt;  volumes:  - name: &amp;lt;볼륨명&amp;gt;    configMap:      name: &amp;lt;컨피그 리소스명&amp;gt;      items: # 특정 키만      - key: nginx.conf        path: &amp;lt;컨테이너에 저장될 파일명&amp;gt; (보통 key와 똑같이 함)        # 최종적으로 컨테이너에 mountPath/path 로 저장됨    configMap:      name: &amp;lt;컨피그 리소스명&amp;gt; # 컨피그 맵에 정의된 전체 키컨피그맵 업데이트  환경 변수로 전달했으면, 업데이트 하기 위해 파드를 재기동 해야 한다          kubectl rollout restart        볼륨 마운트로 전달했으면 60초마다 자동 업데이트 된다Secret  기밀 정보를 취급하기 위한 리소스  시크릿 데이터는 etcd에 저장된다  시크릿을 사용하는 파드가 있는 경우에만 etcd에서 쿠버네티스 노드에 데이터를 보낸다  이 때 노드상에 영구적으로 데이터가 남지 않도록 tmpfs(메모리상에 구축된 임시 파일시스템)에 저장된다  시크릿이 안전한 또 다른 이유는 kubectl 명령어로 표시했을 때 값이 보기 어렵게 되어있다는 점이다참고  kubernetes 공식문서, ConfigMaps",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kubernetes-series5'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part5]: ConfigMap과 Secret'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series5'>Kubernetes Series [Part5]: ConfigMap과 Secret</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents  Kafka as a Datastore  Kafka as a Data Loss Prevention Tool  참고Kafka as a DatastoreKafka can be used for storing data. You may be wondering whether Kafka is a relational or NoSQL database. The answer is that it is neither one nor the other.Kafka, as an event streaming platform, works with streaming data. At the same time, Kafka can store data for some time before removing it. This means that Kafka is different from traditional message queues that drop messages as soon as they are read by the consumer. The period during which the data is stored by Kafka is called retention. Theoretically, you can set this period to forever. Kafka also can store data on persistent storage, and replicates data over the brokers inside a cluster. This is just another trait that makes Kafka look like a database.Why then isn’t Kafka used widely as a database, and why aren’t we addressing the idea that this might be a data storage solution? The simplest reason for this is because Kafka has some peculiarities that are not typical for general databases. For example, Kafka also doesn’t provide arbitrary access lookup to the data. This means that there is no query API that can be used to fetch columns, filter them, join them with other tables, and so on. Actually, there is a Kafka StreamsAPI and even an ksqlDB. They support queries and strongly resemble traditional databases. But they are like scaffolds around Kafka. They act as consumers that process data for you after it’s consumed. So, when we talk about Kafka in general and not its extensions, it’s because there isn’t a query language like SQL available within Kafka to help you access data. By the way, modern data lake engines like Dremio can solve this issue. Dremio supports interactions using SQL with data sources that don’t support SQL natively. So, for example, you can persist data from Kafka streams in AWS S3, and then access it using Dremio AWS edition.Kafka is also focused on the paradigm of working with streams. Kafka is designed to act as the core of applications and solutions that are based on streams of data. In short, it can be seen as a brain that processes signals from different parts of the body and allows an organ to work by interpreting those signals. The aim of Kafka is not to replace more traditional databases. Kafka lives in a different domain, and it can interact with databases, but it is not a replacement for databases. Kafka can be easily integrated with databases and cloud data lake storage such as Amazon S3 and Microsoft ADLS with the help of Dremio.Keep in mind that Kafka has the ability to store data and the data storage mechanism is quite easy to understand. Kafka stores the log of records (messages) from the first message up till now. Consumers fetch data starting from the specified offset in this log of records. This is the simplified explanation of what it looks like:The offset can be moved back in history which will force the consumer to read past data again.Because Kafka is different from traditional databases, the situations where it can be used as a data store are also somewhat specific. Here are some of them:  To repeat the processing of the data from the beginning when the logic of processing changes;  When a new system is included in the processing pipeline, and it needs to process all previous records from the very beginning or from some point in time. This features helps avoid copying the full dump of one database to another;  When consumers transform data and save the results somewhere, but for some reason, you need to store the log of data changes over time.Later in this article, we will look at an example of how Kafka can be used as a data store in a use case similar to the first one described above.Kafka as a Data Loss Prevention ToolA lot of developers choose Kafka for their projects because it provides a high level of durability and fault-tolerance. These features are achieved by saving records on disk and replicating data. Replication means that the same copies of your data are located on several servers (Kafka brokers) within the cluster. Because the data is saved on disk, the data is still there even if the Kafka cluster becomes inactive for some period of time. Thanks to the replication, the data stays protected even when one or several of the clusters inside the broker are damaged.After data is consumed, it is often transformed and/or saved somewhere. Sometimes data can become corrupt or lost during data processing. In such cases, Kafka can help restore the data. If needed, Kafka can provide a way to execute operations from the beginning of the data stream.You should be aware that the two main parameters used to control the data loss prevention policy are the replication factor and the retention period. The replication factor shows how many redundant copies of data for the given topic are created in the Kafka cluster. To support fault-tolerance you should set the replication factor to a value greater than one. In general, the recommended value is three. The greater the replication factor, the more stable the Kafka cluster. You can also use this feature to place Kafka brokers closer to the data consumers while having replicas on geographically remote brokers at the same time.The retention period is the time during which Kafka saves the data. It is obvious that the longer the period, the more data you will save, and the more data you will be able to restore in case something bad happens (for example, the consumer goes down due to power failure, or the database loses all data as the result of an accidental wrong database query or hacker attack, etc.).참고  Towards Data Science, Using Kafka as a Temporary Data Store and Data-loss Prevention Tool in The Data Lake",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kafka-series6'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series6'>Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series5",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  카프카 CLI          토픽      프로듀서      컨슈머        브로커 주요 옵션  프로듀서 주요 옵션  컨슈머 주요 옵션  처리량을 높이고 싶은 경우 (배치전송)  지연율을 낮추고 싶은 경우  순서 보장, 정확히 한 번 전송  토픽 내 데이터를 주기적으로 삭제  참고카프카 CLI토픽# 토픽 리스트 확인${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list# 주키퍼 --zookeeper zookeeper:2181 명시해줘도 되지만 DEPRECATED 예정, --bootstrap-server 옵션 권장# 토픽 생성# bootstrap-server로 설정하면 --partition, --replication-factor 설정 안해도# server.properties의 num.partitions=1, offsets.topic.replication.factor=1 디폴트값 가진다${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --create# 주키퍼로 나타내면 --partition, --replication-factor 반드시 명시해야함${KAFKA_HOME}/bin/kafka-topics.sh --zookeeper zookeeper:2181 --topic myKafkaTopic --create--partitions 1 --replication-factor 1# 토픽 정보${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic myKafkaTopic1 --describe# 토픽 삭제${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --delete# 토픽 파티션 변경${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --alter --partitions 3# 토픽별로 설정 변경${KAFKA_HOME}/bin/kafka-configs.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --alter --add-config min.insync.replicas=2# 설정값은 가장 넓은 범위로 브로커에 기본값을 설정할 수 있고, ${KAFKA_HOME}/bin/kafka-server-start.sh ${KAFKA_HOME}/config/server.properties# 각 토픽마다 오버라이딩하여 독립적으로 설정할 수 있다 (토픽 레벨의 설정값만)# https://kafka.apache.org/081/documentation.html#topic-config# 토픽 생성할 때,${KAFKA_HOME}/bin/kafka-topics.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --create--config min.insync.replicas=2 --config min.cleanable.dirty.ratio=0.5# 토픽 생성 후,${KAFKA_HOME}/bin/kafka-configs.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --alter --add-config min.insync.replicas=2# 브로커에 설정된 각종 기본값 조회${KAFKA_HOME}/bin/kafka-configs.sh --bootstrap-server localhost:9092 --broker 1 --all --describe프로듀서# key없이 value만 전송${KAFKA_HOME}/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic# key와 value를 콤마(,)로 구분${KAFKA_HOME}/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic--property &quot;key.separator=,&quot;# key값을 이용해 파티셔닝 하겠지만 consumer에게는 value만 전달한다${KAFKA_HOME}/bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic--property &quot;key.separator=,&quot; --property &quot;parse.key=true&quot;컨슈머${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic# 토픽에 쌓여있던 모든 데이터를 처음부터 읽어온다 (replay)${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --from-beginning# 파티션 1번 데이터만 컨슘 (같은 key를 가지는 데이터는 같은 파티션으로 간다. 그래서 특정 파티션만 컨슘한다는 의미는 특정 키를 가지는 데이터만 컨슘하겠다는 의미이다)${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --from-beginning --partition 1# 컨슈머에게 그룹을 할당해줄 수 있다# 컨슈머 그룹간에는 같은 파티션을 할당받을 수 없다${KAFKA_HOME}/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic myKafkaTopic --group groupA브로커 주요 옵션- broker.id- zookeeper.connect- log.dirs- advertised.listeners- listeners- auto.create.topics.enable- delete.topic.enable- log.flush.interval.messages- log.flush.interval.ms- log.flush.scheduler.interval.ms- log.retention.bytes- log.retention.hours- log.retention.minutes- log.roll.hours- log.roll.ms- log.segment.bytes- log.segment.delete.delay.ms- message.max.bytes- min.insync.replicas- offsets.topic.replication.factor- offsets.topic.segment.bytes- inter.broker.listener.name프로듀서 주요 옵션- acks: 프로듀서가 전송한 데이터가 브로커들에 정상적으로 저장되었는지 전송 성공 여부를 확인    - 1(default): 리더 파티션을 가지는 브로커만 확인    - 0: 브로커가 받았는지 전혀 신경쓰지 않는다 -&amp;gt; 가장 빠름, 불안전 -&amp;gt; GPS 데이터    - -1(all): 리더와 팔로워 파티션을 가지는 모든 브로커에게 저장되었는지 확인 -&amp;gt; 가장 느림, 안전- linger.ms: 배치를 전송하기 전까지 기다리는 최소 시간. 기본값은 0 -&amp;gt; 기본적으로 배치 전송 안함- batch.size: 배치 크기- buffer.memory: 브로커로 보내기 전에 버퍼링할 수 있는 메모리의 크기- compression.type: 압축 타입- retries: 브로커로부터 에러를 받고 난 뒤 재전송을 시도하는 횟수. 기본값은 2147483647- max.in.flight.requests.per.connection: 한 번에 요청하는 최대 커넥션 개수 기본값은 5 -&amp;gt; sender의 스레드 개수- partitioner.class: 파티셔너 종류. 기본값은 org.apache.kafka.clients.producer.internals.DefaultPartitioner- transactional.id: 트랜잭션 단위로 전송할지 여부. 기본값은 null- min.insync.replicas: ISR 그룹에 복제되어야 하는 최소 파티션 개수- enable.idempotence: 중복 저장 여부. 카프카 3.0.0 부터 기본값은 trueacks=-1, min.insync.replicas=2 조합이 성능도 얻으면서, 신뢰성도 얻는 가장 좋은 옵션컨슈머 주요 옵션- group.id: 컨슈머 그룹 아이디를 지정한다- auto.offset.reset: 컨슈머 오프셋이 없을 때, 어느 오프셋부터 읽을지 선택하는 옵션. 기본값은 latest- enable.auto.commit: 오토 커밋 여부. 기본값은 true- auto.commit.interval.ms: 오토 커밋할 때 커밋의 시간 사이즈. 기본값은 5000(5초)- heartbeat.interval.ms: 하트비트를 전송하는 간격. 기본값은 3000(3초)- session.timeout.ms: 하트비트가 안와서 컨슈머가 죽었다 판단하기 까지 기다리는 시간. 기본값은 10000(10초)처리량을 높이고 싶은 경우 (배치전송)  linger.ms  batch.size  buffer.memory지연율을 낮추고 싶은 경우  linger.ms=0순서 보장, 정확히 한 번 전송  순서를 보장하기 위해서는 partition 개수를 1개로 해야함 (파티션 1개 내에서는 순서보장 되지만 2개 이상에서는 순서 보장 안됨)  정확히 한 번 전송 (한 번 저장)을 위해서는 enable.idempotence=true토픽 내 데이터를 주기적으로 삭제  log.roll.ms, log.roll.hours  log.retention.ms, log.retention.minutes, or log.retention.hours참고  intrepidgeeks, 카프카 기본 개념과 구조/프로듀서 옵션/컨슈머 옵션  kafka-python API",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/kafka-series5'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series5'>Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Database]",
      "category" : "CS",
      "tags"     : "database",
      "url"      : "/mysql-ex-seriesasdfasdf",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/mysql-ex-seriesasdfasdf'> <img src='/images/db_logo.png' alt='[Database]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-ex-seriesasdfasdf'>[Database]</a> </h2><p class='article__excerpt'>운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Fault tolerance in Kafka",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series3",
      "date"     : "Jan 24, 2022",
      "content"  : "Table of Contents  Fault tolerance in Kafka          카프카 리플리케이션(Replication)      리더(Leader)와 팔로워(Follower)      컨트롤러(Controller)      리플리케이션 과정        참고자료Fault tolerance in Kafka카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.카프카 리플리케이션(Replication)카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, 다른 브로커에 파티션을 복제해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 리더와 팔로워가 있습니다.(shwitha B G 블로그 참고)리더(Leader)와 팔로워(Follower)카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.컨트롤러(Controller)리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 컨트롤러라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.(shwitha B G 블로그 참고)컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. 주키퍼(Zookeeper) 개념이 잠깐 등장합니다.(Zookeeper is the centralized service for storing metadata of topic, partition, and broker)  주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.  브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다  컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.  컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.리플리케이션 과정마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 하이워터마크(high water mark)라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 Message3을 읽어갔지만, 이제는 더이상 Message3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.참고자료  Hackernoon 블로그  Ashwitha B G 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-24T21:01:35+09:00'>24 Jan 2022</time><a class='article__image' href='/kafka-series3'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part3]: Fault tolerance in Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series3'>Kafka Series [Part3]: Fault tolerance in Kafka</a> </h2><p class='article__excerpt'>이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part4]: Kubernetes Service Object",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series4",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Network  Network 오브젝트를 사용하는 이유          외부 클라이언트와의 통신      파드 집합에 대한 엔드포인트 제공      파드 엔드포인트 추상화      kube-dns        ClusterIP          매니페스트 파일        NodePort          매니페스트 파일        LoacBalancer          매니페스트 파일        Ingress          클러스터 외부의 로드 밸런서만을 이용한 Ingress      클러스터 내부의 Ingress 파드를 곁들인 Ingress      매니페스트 파일        Headless          매니페스트 파일      헤드리스 서비스로 파드명 디스커버리        참고자료Kubernetes Network  In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 localhost로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 파드의 IP 주소로 통신하면 됩니다.Service는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod와 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.  ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할  NodePort: 외부에서 접근가능하도록 하는 포트 역할  LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.Network 오브젝트를 사용하는 이유이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러면 Network 오브젝트를 굳이 사용하는 이유는 뭘까요?외부 클라이언트와의 통신  파드가 가지고 있는 IP는 클러스터 내부에서 유효한 IP 주소  클러스터 외부에 있는 클라이언트와 통신하기 위해서는 외부로 노출시킬 IP 주소가 필요하다  이를 위해 쿠버네티스에서는 LoadBalancer, Ingress를 제공한다파드 집합에 대한 엔드포인트 제공  파드 IP 주소로 통신한다는 말은 특정 파드 한 개 하고만 통신을 한다는 뜻이다  이렇게 되면, 해당 파드가 위치한 노드가 장애가 난 경우, 대처가 불가능하다  트래픽도 모두 그 파드 한 곳에만 모두 몰리게 될 것이다  이를 해결하기 위해서 쿠버네티스 서비스 오브젝트는 파드 집합에 대한 단일 엔드포인트를 제공한다파드 엔드포인트 추상화  파드는 중간에 장애가 날 경우, 해당 파드를 종료하고 새로운 파드를 실행한다  또한 중간에 업데이트가 발생하면, 또 새로운 파드를 실행한다  이렇게 파드는 계속 변경되기 때문에 파드 IP 주소를 직접 사용할 경우 계속 수동으로 업데이트 해줘야 한다kube-dns  쿠버네티스는 서비스명으로 통신을 제공하기 위해 마스터 노드에 kube-dns라는 서버를 가지고 있다  쿠버네티스는 서비스 오브젝트 정보를 kube-dns에 저장하고 있다  워커 노드에 실행되고 있는 컨테이너들은 /etc/resolv.conf 파일에 이 kube-dns 네임서버 정보를 가지고 있다  컨테이너는 서비스 디스커버리가 필요하면 /etc/resolv.conf 파일에 있는 네임서버(kube-dns)로 이동하고, 네임서버에 저장되어 있는 서비스를 디스커버리한다ClusterIP  ClusterIP는 서비스의 기본 타입  ClusterIP 서비스를 생성하면 클러스터 내부에서만 통신 가능한 가상 IP가 할당  kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송  (위의 장점중 파드 집합에 대한 엔드포인트, 엔드포인트 추상화 이점을 제공)(개발 단계에서는 kubectl port-forward service/&amp;lt;서비스명&amp;gt; &amp;lt;target port&amp;gt;:&amp;lt;service port&amp;gt;로 포트 포워딩해서 localhost:&amp;lt;target port&amp;gt; 로 접근 가능)매니페스트 파일# ClusterIP# redis라는 Deployment 오브젝트에 IP할당apiVersion: v1kind: Servicemetadata:  name: redisspec:  ports:    - port: 6379 # clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)      protocol: TCP  selector: # 어떤pod로 트래픽을 전달할지 결정    app: counter    tier: dbNodePort  외부에서 접근할 수 있도록 모든 노드에 node port 포트를 열어둔다  클러스터 외부의 클라이언트는 &amp;lt;임의의 노드 IP&amp;gt;:node port 로 클러스터와 통신  해당 트래픽은 ClusterIP 서비스 오브젝트에게 전달되고, ClusterIP 서비스 오브젝트를 이를 파드 집합으로 포워딩  노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)이 된다  NodePort는 쿠버네티스에서 지정한 범위(30000~32767) 안에서만 지정 가능  노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양매니페스트 파일# NodePortapiVersion: v1kind: Servicemetadata:  name: counter-npspec:  type: NodePort  ports:    - port: 3000 # ClusterIP, Pod IP의 포트      protocol: TCP      nodePort: 31000 # Node IP의 포트  selector:    app: counter    tier: appLoacBalancer  외부에서 접근 가능한 External IP를 할당 받는다  LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 생성 (Interal IP)매니페스트 파일# LoadBalancerapiVersion: v1kind: Servicemetadata:  name: counter-lbspec:  type: LoadBalancer  ports:    - port: 30000      targetPort: 3000      protocol: TCP  selector:    app: counter    tier: appIngress인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, kind: Ingress타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.Ingress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 nginx, haproxy, traefik, alb등이 있습니다.클러스터 외부의 로드 밸런서만을 이용한 Ingress  GKE 인그레스외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.클러스터 내부의 Ingress 파드를 곁들인 Ingress  Nginx 인그레스클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.매니페스트 파일apiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: echo-v1spec:  rules:    - host: v1.echo.192.168.64.5.sslip.io      http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: echo-v1                port:                  number: 3000# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라spec에는 rules, defaultBackend(어느 rule에도 속하지 않을 경우) 등이 있습니다.(Ingress 공식문서 참고)Headless  헤드리스 서비스는 파드의 집합에 대한 엔드포인트인 ClusterIP가 없다  개별 파드에 직접 접근한다  워크로드 오브젝트가 스테이트풀셋인 경우에만 특별히 파드명으로 IP 주소를 디스커버리할 수 있다  (다른 오브젝트는 파드명에 해시값이 붙지만, 스테이트풀셋의 경우에는 특별히 파드명 뒤에 인덱스 번호만 붙어 있다)매니페스트 파일apiVersion: v1kind: Servicemetadata:  name: sample-headlessspec:  type: CLusterIP  clusterIP: None  ports:  - name: &quot;http-port&quot;    protocol: &quot;TCP&quot;    port: 80    targetPort: 80  selector:    app: sample-appapiVersion: apps/v1kind: StatefulSetmetadata:  name: sample-statefulsetspec:  serviceName: sample-headless  replicas: 3  selector:    matchLabels:      app: sample-app  template:    metadata:      labels:        app: sample-app    spec:      containers:      - name: nginx-container        image: amsy810/echo-nginx:v2.0헤드리스 서비스로 파드명 디스커버리  &amp;lt;파드명&amp;gt;.&amp;lt;서비스명&amp;gt;.&amp;lt;네임스페이스명&amp;gt;.svc.cluster.local  (ex. sample-statefulset-0.sample-headless.default.svc.cluster.local)참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서  NodePort vs LoadBalancer stackoverflow  Google Kubernetes Engine 가이드  Confluent 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series4'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part4]: Kubernetes Service Object'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series4'>Kubernetes Series [Part4]: Kubernetes Service Object</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part4]: 카프카의 데이터 저장",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series4",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  카프카의 데이터 저장 방식          Partition      Segment        저장된 데이터의 포맷(Kafka messages are just bytes)  성능 향상을 위한 파티션 수  장애 복구를 위한 복제  로그 설정을 통해 효율적으로 보관하기(Log Retention)          Role of Indexing within the Partition      Rolling segments      Impact of increasing/decreasing the segment size      Log retention - The records may persist longer than the retention time      Conclusion        참고카프카의 데이터 저장 방식Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a Distributed, Replicated Commit Log. This, I think, clearly represents what Kafka does, as all of us understand how logs are written to disk. And in this case, it is the messages pushed into Kafka that are stored to disk.  Kafka는 커밋 로그를 분산 복제하는 시스템  여기서 로그는 우리가 디스크에 저장한 메세지를 의미  (우리의 메세지를 로그로 표현하려고 하는 이유는 아마 메세지 안에 보통 데이터 뿐만 아니라 다른 메타데이터도 들어 있어서?)카프카의 데이터는 다음과 같은 구조로 이루어져 있다.  Topic: namespace처럼 논리적으로 구분하는 기준. 데이터를 구분하는 가장 큰 구분 기준  Partition: 실제로 컨슈머가 담당하는 작업 단위(컨슈머 그룹내에서 파티션은 하나의 컨슈머에게만 할당 가능). 폴더로 구분  Segment: 여러 메세지를 묶어놓은 하나의 파일. 파티션 한 개에 여러 개의 세그먼트가 저장되어 있음.  Message: 우리가 실제로 보내는 데이터 + 생성된 타임스탬프 + 프로듀서 ID + …로 이루어져 있음Partition3개의 파티션을 가지는 토픽을 우선 한 개 만들어보자.kafka-topics.sh --create --topic freblogg --partitions 3 --replication-factor 1 --zookeeper localhost:2181파티션이 저장되는 위치로 이동해 토픽 이름으로 시작하는 파티션을 검색해보면 3개의 폴더가 보인다.$ tree freblogg*freblogg-0|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpointfreblogg-1|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpointfreblogg-2|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpoint다음과 같은 명령어를 실행해 브로커로 메세지를 보내보자.kafka-console-producer.sh --topic freblogg --broker-list localhost:9092$ ls -lh freblogg*freblogg-0:total 20M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpointfreblogg-1:total 21M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpointfreblogg-2:total 21M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint두 개의 메세지를 보냈다. 결과를 확인해보면 두 개의 파티션이 가지는 00000000000000000000.log 라는 세그먼트 파일의 용량이 증가했다. 파일을 열어보면 다음과 같은 내용이 적혀있다.$ cat freblogg-2/*.log@^@^BÂ°Â£Ã¦Ãƒ^@^K^XÃ¿Ã¿Ã¿Ã¿Ã¿Ã¿^@^@^@^A&quot;^@^@^A^VHello World^@브로커에 저장된 메세지는 바이트 형태로 저장되기 때문에 제대로 디코딩하지 않으면 이상하게 읽힌다. 하지만 Hello World라고 적힌 것을 보아 .log라는 파일에 우리가 보낸 메세지가 저장된다는 것을 알 수 있다.메세지가 파티션에 하나씩 저장된 이유는 라운드 로빈 방식으로 메세지를 파티션에 할당하기 때문이다. 메세지 할당 방식은 카프카에서 제공하는 다른 방식을 사용할 수도 있고, 만약 메세지에 키를 설정해줬다면 키마다 파티션을 다르게 할당하도록 커스터마이징할 수도 있다.세그먼트는 여러 메세지를 하나로 묶어 저장하고 있고, 각각의 메세지는 1씩 증가하는 offset을 가진다. 각 세그먼트는 자신이 가지고 있는 메세지의 가장 처음 오프셋을 이름으로 한다.위와 같은 랜덤한 문자열들을 읽고 싶으면 Kafka 툴을 사용할 수 있다.kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files logs/freblogg-2/00000000000000000000.logThis gives the outputDumping logs/freblogg-2/00000000000000000000.logStarting offset: 0offset: 0 position: 0 CreateTime: 1533443377944 isvalid: true keysize: -1 valuesize: 11 producerId: -1 headerKeys: [] payload: Hello Worldoffset: 1 position: 79 CreateTime: 1533462689974 isvalid: true keysize: -1 valuesize: 6 producerId: -1 headerKeys: [] payload: amazonCreateTime과 같은 값은 컨슈머로 가져와서 사용할 수 있는 값이 아니다. 카프카 내부적으로 가지고 있는 메타데이터이다. 그렇기 때문에 데이터의 타임스탬프가 필요하다면, 데이터를 생성할 때 내부적으로 메세지에 명시적으로 담아서 브로커에 담아야 한다.You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.Segment위에서 봤던 .log, .index, .timeindex을 모두 세그먼트 파일이라고 한다. 세그먼트 파일을 하나로 하지 않고, 나누어 저장하는 이유는 여러가지가 있다.그중에서도 데이터를 삭제할 때 이점이 있다는 것이다. Kafka는 구조적 특성으로 메세지마다 데이터를 삭제하는 것이 불가능하다. 유일하게 메세지를 삭제하는 방법은 바로 세그먼트 파일을 삭제하는 것이다. 보통 세그먼트 파일 삭제는 카프카 configuration을 통해 삭제하는 Retention policy 방법을 사용한다. (정책을 통해 주기적으로 삭제)세그먼트 파일의 의미는 다음과 같다.  .index file: This contains the mapping of message offset to its physical position in .log file.  .log file: This file contains the actual records and maintains the records up to a specific offset. The name of the file depicts the starting offset added to this file.  .index file: This file has an index that maps a record offset to the byte offset of the record within the** .log **file. This mapping is used to read the record from any specific offset.  .timeindex file: This file contains the mapping of the timestamp to record offset, which internally maps to the byte offset of the record using the .index file. This helps in accessing the records from the specific timestamp.  .snapshot file: contains a snapshot of the producer state regarding sequence IDs used to avoid duplicate records. It is used when, after a new leader is elected, the preferred one comes back and needs such a state to become a leader again. This is only available for the active segment (log file)  .leader-epoch-checkpoint: It refers to the number of leaders previously assigned by the controller. The replicas use the leader epoch as a means of verifying the current leader. The leader-epoch-checkpoint file contains two columns: epochs and offsets. Each row is a checkpoint for the latest recorded leader epoch and the leader’s latest offset upon becoming leaderAn index file for the log file I’ve showed in the ‘Quick detour’ above would look something like this:If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.저장된 데이터의 포맷(Kafka messages are just bytes)Kafka messages are just bytes. Kafka messages are organized into topics. Each message is a key/value, but that is all that Kafka requires. Both key and value are just bytes when they are stored in Kafka. This makes Kafka applicable to a wide range of use cases, but it also means that developers have the responsibility of deciding how to serialize the data.There are various serialization formats with common ones including:  JSON  Avro  Protobuf  String delimited (e.g., CSVThere are advantages and disadvantages to each of these—well, except delimited, in which case it’s only disadvantages 😉Choosing a serialization format  Schema: A lot of the time your data will have a schema to it. You may not like the fact, but it’s your responsibility as a developer to preserve and propagate this schema. The schema provides the contract between your services. Some message formats (such as Avro and Protobuf) have strong schema support, whilst others have lesser support (JSON) or none at all (delimited string).  Ecosystem compatibility: Avro, Protobuf, and JSON are first-class citizens in the Confluent Platform, with native support from the  Confluent Schema Registry, Kafka Connect, ksqlDB, and more.  Message size: Whilst JSON is plain text and relies on any compression configured in Kafka itself, Avro and Protobuf are both binary formats and thus provide smaller message sizes.  Language support: For example, support for Avro is strong in the Java space, whilst if you’re using Go, chances are you’ll be expecting to use Protobuf.데이터를 브로커에 저장할 때는 전송된 데이터의 포맷과는 상관없이 원하는 포맷으로 브로커에 저장할 수 있다. 예를 들어 프로듀서가 JSON으로 보냈다고 하더라도 브로커에 저장할 때 포맷은 Avro, Parquet, String 뭘 하든 상관없다. 다만 중요한 것은 Serializer로 Avro를 선택했다면, Deserializer도 반드시 Avro를 선택해야 한다. 그러고 나면 컨슈머에서 전달 받는 데이터의 포맷은 자연스럽게 다시 JSON 형태를 얻게 된다.Remember, Kafka messages are just pairs of key/value bytes, and you need to specify the converter for both keys and value, using the key.converter and value.converter configuration setting. In some situations, you may use different converters for the key and the value.Here’s an example of using the String converter. Since it’s just a string, there’s no schema to the data, and thus it’s not so useful to use for the value:&quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,Some converters have additional configuration. For Avro, you need to specify the Schema Registry. For JSON, you need to specify if you want Kafka Connect to embed the schema in the JSON itself. When you specify converter-specific configurations, always use the key.converter. or value.converter. prefix. For example, to use Avro for the message payload, you’d specify the following:&quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,Common converters include:# Avroio.confluent.connect.avro.AvroConverter# Protobufio.confluent.connect.protobuf.ProtobufConverter# Stringorg.apache.kafka.connect.storage.StringConverter# JSONorg.apache.kafka.connect.json.JsonConverter# JSON schemaio.confluent.connect.json.JsonSchemaConverter# ByteArrayorg.apache.kafka.connect.converters.ByteArrayConverterJSON의 경우 스키마가 설정을 안하는 것이 디폴트다. 하지만 스키마를 고정하고 싶은 경우 두 가지 방법을 사용할 수 있다.  JSON schema io.confluent.connect.json.JsonSchemaConverter를 쓴다 (with 스키마 레지스트리)    &quot;value.converter&quot;: &quot;io.confluent.connect.json.JsonSchemaConverter&quot;,&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,        비효율적이지만 매번 메시지에 스키마를 담아서 전송/저장한다.    value.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=true      2번 방식을 사용하면 메세지가 다음과 같이 schema 부분과, payload 부분이 함께 저장된다.{  &quot;schema&quot;: {    &quot;type&quot;: &quot;struct&quot;,    &quot;fields&quot;: [      {        &quot;type&quot;: &quot;int64&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;registertime&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;userid&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;regionid&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;gender&quot;      }    ],    &quot;optional&quot;: false,    &quot;name&quot;: &quot;ksql.users&quot;  },  &quot;payload&quot;: &quot;Hello World&quot;}이렇게 하면 메세지 사이즈가 커지기 때문에 비효율적이다. 그래서 스키마가 필요한 경우에는 스키마 레지스트리를 사용하는 것이 효율적이다.만약 컨버터에 JSON serializer를 사용했고 스키마를 따로 설정하지 않을거라면,value.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=false이렇게 schema를 찾을 필요 없다고 명시해주자. (디폴트가 false인데 왜 해줘야하는거지..?)아래 표는 serializer와 deserializer의 싱크를 어떻게 맞춰야 에러가 안나는지 알려준다. 기본적으로 serializer는 메세지나 상황에 맞게 원하는 것을 선택하고, deserializer는 serializer와 같은 포맷을 사용하도록 하면 된다.성능 향상을 위한 파티션 수To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.파티션 내에서는 메세지의 순서가 지켜진다. 그래서 토픽을 이루는 파티션이 1개라면 메세지의 순서를 걱정할 필요가 없다. 하지만 파티션의 개수를 2개 이상으로 하면 메세지의 순서가 보장되지 않는다.병렬 처리를 통해 성능을 높이고자 할 때, 파티션의 개수와 컨슈머의 개수를 늘려준다.  파티션의 수 &amp;gt;= 컨슈머 수  병렬 정도 = MIN(파티션의 수, 컨슈머 수)  파티션의 개수는 늘릴수만 있고 줄일 수는 없음장애 복구를 위한 복제복제는 특정 브로커 서버에 장애가 났을 경우를 대비하기 위한 용도다. 만약 브로커가 1대라면 복제는 아무 의미가 없다. 복제는 브로커의 개수만큼 설정하면 된다. 더 크게 더 적게 해도 되지만, 같게 하는 것이 제일 합당한 선택이다.복제수는 토픽마다 다르게 설정할 수 있다. 복제 수는 늘리는 만큼 성능이 약간 떨어진다. 그래서 토픽의 중요도에 따라 다르게 설정하는 것이 좋다.복제에 관해 이해하려면 리더/팔로워, 커밋과 같은 것들을 배워야 한다. 컨슈머는 리더 파티션만 가져갈 수 있다. 복제는 리더가 장애가 났을 경우를 대비하기 위한 용도다.Say for the freblogg topic that we’ve been using so far, we’ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this.Even when you have a replicated partition on a different broker, Kafka wouldn’t let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition.A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.로그 설정을 통해 효율적으로 보관하기(Log Retention)Apache Kafka is a commit-log system. The records are appended at the end of each Partition, and each Partition is also split into segments. Segments help delete older records through Compaction, improve performance, and much more.(아파치 카프카는 커밋-로그 시스템이다. 레코드는 파티션의 끝에 append되고, 파티션의 세그먼트 파일로 분리되어 저장된다. 세그먼트 파일을 통해 카프카는 오래된 레코드를 관리할 수 있다.)Kafka allows us to optimize the log-related configurations, we can control the rolling of segments, log retention, etc. These configurations determine how long the record will be stored and we’ll see how it impacts the broker’s performance, especially when the cleanup policy is set to Delete.(카프카에는 로그 저장/삭제와 관련된 설정을 통해 로그를 얼마나 오랫동안 저장하고 있을지 설정할 수 있다)For better performance and maintainability, multiple segments get created, and rather than reading from one huge Partition, Consumers can now read faster from a smaller segment file. A directory with the partition name gets created and maintains all the segments for that partition as various files.The active segment is the only file available for reading and writing while consumers can use other log segments (non-active) to read data. When the active segment becomes full (configured by log.segment.bytes, default 1 GB) or the configured time (log.roll.hours or log.roll.ms, default 7 days) passes, the segment gets rolled. This means that the active segment gets closed and re-opens with read-only mode and a new segment file (active segment) will be created in read-write mode.(세그먼트 파일은 보통 읽기만 가능하고, active인 세그먼트 파일에만 쓰는 것도 가능하다. active 세그먼트 파일은 log.segment.bytes 의 크기를 초과하거나, log.roll.hours 시간을 초과하는 경우 활성화가 종료되고, 새로운 active 세그먼트 파일을 생성한다.)Role of Indexing within the PartitionIndexing helps consumers to read data starting from any specific offset or using any time range. As mentioned previously, the .index file contains an index that maps the logical offset to the byte offset of the record within the .log file. You might expect that this mapping is available for each record, but it doesn’t work this way.How these entries are added inside the index file is defined by the log.index.interval.bytes parameter, which is 4096 bytes by default. This means that after every 4096 bytes added to the .log file, an entry gets added to the .index file. Suppose the producer is sending records of 100 bytes each to a Kafka topic. In this case, a new index entry will be added to the .index file after every 41 records (41*100 = 4100 bytes) appended to the log file.(log.index.interval.bytes 크기만큼의 레코드가 보내진 후 다음 레코드의 메모리 주소가 인덱싱 된다)(컨슈머는 먼저 .index 파일의 인덱싱 정보를 통해 읽어들일 메모리의 처음 위치를 선택한다)If a consumer wants to read starting at a specific offset, a search for the record is made as follows:  Search for the .index file based on its name. For e.g. If the offset is 1191, the index file will be searched whose name has a value less than 1191. The naming convention for the index file is the same as that of the log file  Search for an entry in the .index file where the requested offset falls.  Use the mapped byte offset to access the .log file and start consuming the records from that byte offset.As we mentioned, consumers may also want to read the records from a specific timestamp. This is where the .timeindex file comes into the picture. It maintains a timestamp and offset mapping (which maps to the corresponding entry in the .index file), which maps to the actual byte offset in the .log file. (특정 타임스탬프로 레코드 읽는 방법: .timeindex -&amp;gt; .index -&amp;gt; .log)Rolling segmentsAs discussed in the above sections, the active segment gets rolled once any of these conditions are met-  Maximum segment size - configured by log.segment.bytes, defaults to 1 Gb  Rolling segment time - configured by log.roll.ms or log.roll.hours, defaults to 7 days  Index/timeindex is full - The index and timeindex share the same maximum size, which is defined by the log.index.size.max.bytes, defaults to 10 MB(보통 1번 크기를 늘리면, 3번 크기도 늘려야 한다)Impact of increasing/decreasing the segment sizeGenerally you don’t want to increase/decrease the log.segment.bytes and keep it as default. But let’s discuss the impact of changing this value so that you can make an informed decision if there’s a need.Log retention - The records may persist longer than the retention timeKafka, with its feature of retaining the log for a longer duration rather than deleting it like traditional messaging queues once consumed, provides many added advantages. Multiple consumers can read the same data, apart from reading the data it can also be sent to data warehouses for further analytics.How long is the data retained in Kafka? This is configurable using the maximum number of bytes to retain by using the log.retention.bytes parameter. If you want to set a retention period, you can use the log.retention.ms, log.retention.minutes, or log.retention.hours (7 days by default) parameters.The following things may impact when the records get deleted-  If the producer is slow and the maximum size of 16 Kb is not reached within 10 minutes, older records won’t be deleted. In this case, the log retention would be higher than 10 mins.  If the active segment is filled quickly, it will be closed but only get deleted once the last inserted record persists for 10 mins. So in this case as well, the latest inserted record would be persisted for more than 10 mins. - Suppose the segment is getting filled in 7 mins and getting closed, the last inserted record will stay for 10 mins so the actual retention time for the first record inserted into the segment would be 17 mins.  The log can be persisted for an even longer duration than the last added record in the segment. How? Because the thread which gets executed and checks which log segments need to be deleted runs every 5 mins. This is configurable using log.retention.check.interval.ms configurations. - Depending on the last added record to the segment, this cleanup thread can miss the 10 min retention deadline. So in our example above instead of persisting the segment for 17 mins, it could be persisted for 22 mins.  Do you think that this would be the maximum time the record is persisted in Kafka? No, the cleaner thread checks and just marks the segment to be deleted. The log.segment.delete.delay.ms broker parameter defines when the file will actually be removed from the file system when it’s marked as “deleted” (default, 1 min) - Going back to our example the log is still available even after 23 mins, which is way longer than the retention time of 10 mins.So The usual retention limits are set by using log.retention.ms defines a kind of minimum time the record will be persisted in the file system.Consumers get records from closed segments but not from deleted ones, even if they are just marked as “deleted” but not actually removed from the file system.Conclusion참고  Data types for Kafka connector  Kafka Connect Deep Dive – Converters and Serialization Explained  dol9, Kafka 스키마 관리, Schema Registry  A Practical Introduction to Kafka Storage Internals  Here’s what makes Apache Kafka so fast  stackoverflow: Which directory does apache kafka store the data in broker nodes  Abhishek Sharma, How kafka stores data  Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals  towardsdatascience, Log Compacted Topics in Apache Kafka  conduktor, Understanding Kafka’s Internal Storage and Log Retention  What is a commit log and why should you care?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kafka-series4'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part4]: 카프카의 데이터 저장'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series4'>Kafka Series [Part4]: 카프카의 데이터 저장</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part2]: Main elements of Kafka",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series2",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kafka의 주요 구성요소  Topic, Partition, Segment  Producer          레코드 전송과정      레코드 파티셔닝 전략                  라운드 로빈(Round-Robbin) 방식          스티키 파티셔닝(Sticky Partitioning) 방식                    적어도 한 번 전송      정확히 한 번 전송                  중복 없는 전송          트랜잭션 API                      Broker          컨트롤러      데이터의 저장      데이터의 삭제      데이터 복제        Consumer          컨슈머 오프셋 관리      그룹 코디네이터      파티션 리밸런싱                  라운드 로빈 파티션 할당 전략          스티키 파티션 할당 전략          협력적 스티키 파티션 할당 전략                      마치며  참고자료Kafka의 주요 구성요소Kafka는 크게 3가지로 이루어 있습니다.  Producer: Kafka로 메시지를 보내는 모든 클라이언트  Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버  Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트(참고: cloudkarafka)Topic, Partition, SegmentKafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.  Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)  Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)  Segment: 메시지가 실제로 저장되는 파일. 기본적으로 1GB를 넘을 때마다 파일이 새로 생성(참고: cloudkarafka)카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.Producer프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.(Dzone 블로그 참고)레코드 전송과정프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 ProducerRecord라고 하는 형태로 전송되며, Topic과 Value는 필수값이며, Partition과 Key는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은  프로듀서에서 send() 메소드 호출  Serializer는 JSON, String, Avro 등의 object를 bytes로 변환  ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달  Partition이 지정되지 않았을 때, Key값이 지정되었다면 Partitioner가 Key값을 바탕으로 해당 파티션에 전달  Partition, Key값이 모두 없으면 라운드 로빈(Round-Robbin)방식 또는 스티키 파티셔닝(Sticky Partitioning) 방식으로 메세지를 파티션에 할당  파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 배치로 전송할 수도 있음레코드 파티셔닝 전략라운드 로빈(Round-Robbin) 방식프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.스티키 파티셔닝(Sticky Partitioning) 방식라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)적어도 한 번 전송카프카에서 적어도 한 번 전송은 프로듀서와 브로커간 주고 받는 ACK로 구현됩니다. 프로듀서는 메시지를 브로커에게 전송하고 브로커로부터 메시지를 잘 받았다는 ACK를 받으면 다음 메시지를 전송합니다.만약 브로커가 메시지를 못 받았다면 프로듀서는 ACK를 못 받게 되고 프로듀서는 다시 메시지를 전송합니다. 또한 브로커가 메시지를 받았다고 하더라도 네트워크 장애로 ACK를 브로커에게 돌려 주지 못하면 프로듀서는 브로커가 메시지를 못 받은 것으로 간주하고 다시 같은 메시지를 보내게 됩니다. 이러한 특징으로 인해 이를 적어도 한 번 전송이라고 합니다.카프카는 기본적으로 이와 같은 적어도 한 번 전송 방식을 기반으로 동작합니다.정확히 한 번 전송데이터 처리나 가공 작업을 하는 대부분의 사람들은 데이터 파이프라인에서 메세지 손실 뿐만 아니라 중복도 발생하지 않기를 원할겁니다. 이러한 방식을 정확히 한 번 전송 방식이라고 합니다.하지만 카프카에서 정확히 한 번 전송 방식은 기준이 조금 더 엄격합니다. 카프카에서 정확히 한 번 전송은 트랜잭션과 같은 전체적인 프로세스 처리를 의미하며, 중복 없는 전송은 정확히 한 번 전송의 일부 기능이라 할 수 있습니다.중복 없는 전송중복 없는 전송은 브로커가 중복된 메세지를 받을 경우 중복 저장하지 않고, 메세지를 받았다는 ACK만 돌려보내는 방식입니다. 프로듀서는 메세지를 보낼 때 누가(Producer ID), 어떤 메세지(시퀀스 번호)를 보내는지 정보를 추가해서 브로커에게 보내고, 브로커는 이 정보를 메모리와 리플리케이션 로그에 저장해 놓습니다.(kafka-logs/토픽-파티션 폴더/세그먼트 시작 오프셋.snapshot) 그리고 프로듀서가 보낸 메세지의 시퀀스 번호와 비교해 브로커가 자신이 저장해 놓은 시퀀스 번호보다 정확하게 하나 큰 경우에만 메세지를 저장합니다.트랜잭션 API프로듀서가 카프카로 정확히 한 번 방식으로 메세지를 전송할 때, 프로듀서가 보내는 메세지들은 원자적으로 처리되어 전송에 성공하거나 실패하게 됩니다. 이렇게 메세지를 관리하며 커밋 또는 중단 등을 표시하는 것은 트랜잭션 코디네이터에 의해 수행됩니다.정확히 한 번 전송을 위해서는 트랜잭션 API를 이용해 다음과 같은 동작을 단계별로 수행합니다.      트랜잭션 코디네이터 찿기    트랜잭션 코디네이터는 __transaction_state 토픽의 리더 파티션을 가지고 있는 브로커입니다. 트랜잭션 코디네이터의 주 역할은 PID와 transactional.id를 매핑하고 해당 트랜잭션 전체를 관리하는 것입니다.        프로듀서 초기화    프로듀서가 InitPidRequest(프로듀서 초기화 요청)를 트랜잭션 코디네이터로 보내면 트랜잭션 코디네이터는 TID와 PID를 매핑하고 해당 정보를 트랜잭션 로그에 기록합니다.        메시지 전송    프로듀서는 토픽의 파티션으로 메세지를 전송합니다.        트랜잭션 종료 요청    메세지 전송을 완료한 프로듀서는 commitTransaction()메소드 또는 abortTransaction()메소드 중 하나를 호출해 트랜잭션이 완료되었음을 트랜잭션 코디네이터에게 알립니다.        사용자 토픽에 표시    메세지를 전송한 토픽의 파티션에 트랜잭션 커밋 표시를 기록합니다. 이렇게 파티션에 기록한 커밋 표시를 ‘컨트롤 메세지’라고 합니다. 이 메세지는 해당 메세지가 제대로 전송됐는지 여부를 컨슈머에게 나타내는 용도로, 커밋되지 않은 트랜잭션에 포함된 메세지는 컨슈머에게 반환하지 않게 됩니다.        트랜잭션 완료    마지막으로 트랜잭션 코디네이터는 트랜잭션 로그에 완료됨(Committed)이라고 기록합니다.  Broker브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.컨트롤러클러스터의 다수 브로커중 한 대가 컨트롤러의 역할을 합니다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가 장애로 클러스터에서 빠지는 경우 해당 브로커에 존재하던 리더 파티션을 다른 브로커로 재분배합니다. 컨트롤러 역할을 하는 브로커에 장애가 생기면 다른 브로커가 컨트롤러 역할을 합니다.데이터의 저장카프카의 데이터는 config/server.properties의 log.dir 옵션에 정의한 디렉토리(보통 /tmp/kafka-logs)에 저장됩니다. 데이터는 토픽 이름과 파티션 번호의 조합으로 디렉토리(토픽명-파티션 번호)를 생성하여 데이터를 저장합니다.디렉토리(토픽명-파티션 번호) 아래를 확인하면 .index, .log, .timeindex 파일이 존재합니다.000000000.index: 메시지의 오프셋을 인덱싱한 정보를 담은 파일000000000.log: 메시지와 메타데이터를 저장한 파일(대표적으로 offset, key, value 저장)000000000.timeindex: 메시지에 포함된 타임스탬프 값을 기준으로 인덱싱한 정보를 담은 파일(생성 시간 또는 적재된 시간)(000000000은 각 세그먼트가 가지는 레코드중 시작 오프셋)데이터의 삭제카프카는 다른 메세징 플랫폼과 다르게 컨슈머가 데이터를 가져가더라도 토픽의 데이터는 삭제되지 않습니다. 또한 컨슈머나 프로듀서가 데이터 삭제를 요청할 수 없으며 오직 브로커만이 데이터를 삭제할 수 있습니다. 하지만 카프카에 저장한 데이터는 다른 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없습니다. 카프카에서 데이터를 삭제하는 단위는 로그 세그먼트 파일 단위입니다. 참고로 수정 또한 불가능하기 때문에 프로듀서는 데이터를 브로커에 전송하기 전에 검증하는 것이 좋습니다.데이터 복제Consumer컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.  파티션 할당 전략  프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가  컨슈머의 개수가 파티션보다 많지는 않은가  컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가컨슈머 오프셋 관리컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 __consumer_offsets라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, 컨슈머가 다음으로 읽어야 할 위치를 말합니다.참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.그룹 코디네이터컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.이렇게 컨슈머 그룹내의 변화를 감지하기 위해 트래킹하는 것이 바로 그룹 코디네이터입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.파티션 리밸런싱이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.라운드 로빈 파티션 할당 전략라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.스티키 파티션 할당 전략스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.협력적 스티키 파티션 할당 전략이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.(컨플루언트 블로그 참고)마치며이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 장애 발생에 대처 가능한 안정적인 서비스를 제공해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 높은 처리량, 데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활하게 해주는 것이 좋습니다.참고자료  실전 카프카 개발부터 운영까지 책  Dzone 블로그  CodeX 블로그",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kafka-series2'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part2]: Main elements of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series2'>Kafka Series [Part2]: Main elements of Kafka</a> </h2><p class='article__excerpt'>프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part1]: What is Kafka",
      "category" : "data_engineering",
      "tags"     : "kafka",
      "url"      : "/kafka-series1",
      "date"     : "Jan 17, 2022",
      "content"  : "Table of Contents  카프카의 탄생  카프카의 구조  카프카의 장점          높은 처리량      낮은 지연      확장성      영속성      고가용성        카프카의 핵심 기능          순서 보장      적어도 한 번 전송 방식      강력한 파티셔닝      비동기 방식        정리  카프카 생태계  참고자료카프카의 탄생2011년 링크드인에서는 파편화된 데이터 수집 및 분배를 위한 데이터 파이프라인을 운영하는 데에 어려움을 겪었습니다.초기 운영시에는 데이터를 생성하고 적재하는데 큰 어려움이 없었지만, 서비스 사용자가 증가함에 따라 데이터량이 폭발적으로 증가하기 시작했고 아키텍처도 점점 복잡해지기 시작했습니다.이를 해결하기 위해 기존에 나와있던 데이터 프레임워크와 오픈소스를 활용해 파이프라인의 파편화를 개선하려고 노력했지만 결국 한계를 느끼고 링크드인의 데이터팀은 데이터 파이프라인을 위한 새로운 시스템을 만들기로 결정했습니다.그 결과물로 등장한 것이 카프카(Kafka)입니다. 카프카는 이후 아파치 재단의 최상위 프로젝트로 등록되었습니다. 그리고 2014년 카프카 개발의 핵심 멤버였던 제이 크랩스(Jay Kreps)는 컨플루언트(Confluent)라는 회사를 창업해 카프카를 더욱 편리하게 운영할 수 있도록 몇 가지 추가적인 기능을 추가해 현재 카프카 관련 다양한 서비스를 제공하고 있습니다.카프카의 구조카프카는 더이상 각각의 애플리케이션을 서로 연결해서 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙 집중화 하였습니다.기존에 있던 데이터 파이프라인은 소스와 타겟의 직접적인 연결로 한쪽의 이슈가 다른 쪽의 애플리케이션에 영향을 미치곤 했지만 카프카는 이러한 높은 의존성 문제를 해결했습니다. 이제 소스 애플리케이션은 어느 타겟 애플리케이션으로 보낼지 고민하지 않고 일단 카프카로 넣으면 됩니다.카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐 자료구조와 유사합니다. 카프카에서는 이러한 큐를 브로커(broker), 소스 애플리케이션을 프로듀서(producer), 타겟 애플리케이션을 컨슈머(consumer)라고 합니다.카프카를 소개하는 좋은 문장이 있어 가져와봤습니다.  Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.  실시간 데이터를 스트리밍하는 분산환경의 publish-subscribe 메시지 플랫폼  복잡한 데이터 파이프라인 구조를 간단하게 해주며 파이프라인의 확장성을 높여준다  Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.  데이터를 소스에서 목적지로 직접 전달하는 것이 아니다  프로듀서가 데이터를 생성(발행)해서 브로커(잡지)에 저장한다  컨슈머는 브로커(잡지)에 저장된 데이터를 읽어간다(구독)카프카의 장점높은 처리량  배치 처리 제공 -&amp;gt; 네트워크 비용 낮춤 -&amp;gt; 처리량 증가  데이터를 파티션 단위로 나눔 -&amp;gt; 컨슈머 개수를 파티션 수 만큼 늘림 -&amp;gt; 컨슈머 수만큼 처리량 증가  데이터를 압축하여 브로커에 전송 가능 -&amp;gt; 처리량 증가낮은 지연  높은 처리량에 초점이 맞춰져 있지만 설정값을 통해 낮은 지연율을 얻을 수 있음  (카프카에 관한 글을 읽으면 카프카가 제공하는 큰 장점 중 하나가 낮은 지연율이라고 생각이 들지만 직접 써보게 되면 낮은 지연율 보다는 높은 처리량에 초점이 맞춰져있다. 하지만 설정값을 바꿔줌으로써 낮은 지연율을 얻을 수 있다)확장성  카프카는 분산 시스템 -&amp;gt; 트래픽량에 따라 스케일 아웃 가능 -&amp;gt; 높은 확장성영속성  다른 메세징 플랫폼과 다르게 데이터를 파일 시스템에 저장 -&amp;gt; 종료되더라도 데이터가 남아있다 -&amp;gt; 영속성  (또한 페이지 캐시 영역을 메모리에 따로 생성해 데이터 읽는 속도도 느려지지 않음)고가용성  여러 대의 브로커에 데이터 복제 -&amp;gt; 브로커가 고장나도 장애 복구 가능 -&amp;gt; 높은 가용성카프카의 핵심 기능순서 보장이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.적어도 한 번 전송 방식분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(idempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다. 최근에는 ‘정확히 한 번’ 전송 방식이 도입되어 카프카내에서 중복성을 제거하는 방법이 많이 사용되고 있습니다.Idempotent: Characteristic that we can apply operation multiple times without changing the result beyond the initial application.카프카에서 정확히 한 번 전송 방식을 지원하는데 멱등적인 방법은 아님. De-duplication 방식임.In the de-duplication approach, we give every message a unique identifier, and every retried message contains the same identifier as the original. In this way, the recipient can remember the set of identifiers it received and executed already. It will also avoid executing operations that are executed.It is important to note that in order to do this, we must have control on both sides of the system: sender and receiver. This is because the ID generation occurs on the sender side, but the de-duplication process occurs on the receiver side.it’s impossible to have exactly-once delivery in a distributed system. However, it’s still sometimes possible to have exactly-once processing.(정확히 한 번 전송은 사실 Producer는 중복 전송하더라도, 브로커에서 저장하는 과정을 한 번만 수행함으로서 보장된다)강력한 파티셔닝파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.비동기 방식데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)정리  Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.  데이터를 분산처리하여 높은 처리량을 제공하고, 선택적으로 낮은 지연율을 얻을 수 있다  심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.카프카 생태계참고자료  실전 카프카 개발부터 운영까지 책  CodeX 블로그  How Does Kafka Perform When You Need Low Latency?  Here’s what makes Apache Kafka so fast  Apache Kafka Architecture: What You Need to Know",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-17T21:01:35+09:00'>17 Jan 2022</time><a class='article__image' href='/kafka-series1'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part1]: What is Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series1'>Kafka Series [Part1]: What is Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part3]: Kubernetes Workload Object",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series3",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  쿠버네티스의 오브젝트  Workload Object  Pod          매니페스트 파일      파드 디자인 패턴      파드명 제한      명령어 실행      기타 설정      파드 실습        ReplicaSet          매니페스트 파일      레플리카셋 실습        Deployment          매니페스트 파일      디플로이먼트 업데이트 전략                  디플로이먼트 실습                      DaemonSet          매니페스트 파일      업데이트 전략      데몬셋 실습        StatefulSet          매니페스트 파일      스테이트풀셋 실습        참고자료쿠버네티스의 오브젝트  애플리케이션을 쿠버네티스 클러스터에 배포할 때 사용할 수 있는 리소스 단위  프로젝트를 구성하는 워크로드, 네트워크(서비스), 스토리지(볼륨), 설정 파일 등  어떤 애플리케이션을 얼마나 어디에 어떤 방식으로 배포할지에 관한 것들을 Manifest 파일로 정의할 수 있음  사용자의 의도를 YAML 형식으로 정의  REST API로 마스터 노드에 전달  오브젝트에는 크게 Workload 관련 오브젝트와 Network 관련 오브젝트가 있음Manifest 파일# 간단한 예시apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1  쿠버네티스 오브젝트를 명시적으로 관리하도록 해주는 YAML 파일  오브젝트의 목적에 따라 kind를 달리해줌          kind에는 크게 Pod, Replicaset, Deployment, Service, Ingress        우리가 원하는 애플리케이션의 상태를 spec에 정의함          쿠버네티스 컨트롤러는 YAML 파일에 정의된 spec과 자신의 오브젝트 status를 비교      차이점이 발견되면 status를 spec(desired state)에 맞도록 업데이트 후 etcd에 저장      Workload Object  Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.(Rancher문서 참고)Pod  Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다.  Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다.  (IP 주소는 파드 단위로 할당 =&amp;gt; 컨테이너들은 서로 같은 IP 주소를 가진다 =&amp;gt; 포트 번호로 구분)      컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.    Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 파드를 할당.  그리고 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.매니페스트 파일apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1Pod의 spec에는 containers, volumes, restartPolicy, hostname, hostNetwork 등이 있습니다.(Pod공식문서 참고)파드 디자인 패턴  대부분의 경우 하나의 파드에 하나의 컨테이너를 가진다  하지만 메인 컨테이너를 지원하는 서브 컨테이너를 가지도록 파드를 디자인할 수도 있다 =&amp;gt; 이를 사이드카 패턴이라고 한다  (서브 컨테이너의 예: 프록시 역할, 설정값을 동적으로 변경시키는 역할, 로컬 캐시 역할 등)파드명 제한  이용 가능한 문자: ‘영어 소문자’, ‘숫자’, ‘-‘, ‘.’ (언더바(_)는 안됨)  시작과 끝은 ‘영문 소문자’명령어 실행  spec.containers.command 와 spec.containers.args 로 나타낼 수 있다  (command는 기본 명령어, args는 command에 전달할 인자로 생각하면 된다)  (여기에 명령어를 명시하면, 도커 이미지의 ENTRYPOINT와 CMD를 덮어 쓴다)  kubectl exec -it &amp;lt;파드명&amp;gt; -- &amp;lt;전달할 명령어&amp;gt; 명령어로 실행중인 컨테이너에 명령어를 실행할 수도 있다  특정 컨테이너에 명령어를 전달하고 싶으면 kubectl exec -it &amp;lt;파드명&amp;gt; -c &amp;lt;컨테이너명&amp;gt; -- &amp;lt;전달할 명령어&amp;gt; 이렇게 쓴다기타 설정  spec.hostNetwork          호스트(노드) IP 주소를 파드의 IP 주소로 설정할 수 있다      모든 파드가 같은 IP 주소 가지게 되므로 포트 충돌 주의        spec.dnsPolicy          어디있는 DNS를 이용해서 서비스를 디스커버리할 것인지 정의할 수 있다      기본값은 ClusterFIrst로 클러스터 내부 DNS를 이용한다      그 외에 Default 값을 이용하면 각 노드에 정의된 /etc/resolv.conf를 상속받는다        spec.hostAliases          파드 내부 모든 컨테이너의 /etc/hosts를 변경할 수 있다        spec.containers.workingDir          작업 디렉터리를 설정할 수 있다      쿠버네티스에서 특정 스크립트를 실행하는 명령어를 실행할 때, 해당 디렉터리에서 스크립트를 실행한다      파드 실습ReplicaSet  ReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트이다  단일 노드 환경이면 Pod는 모두 단일 노드에서 생성되고, 여러개의 노드를 가지고 있는 상황이면, Pod는 노드에 각각 분산되어 배포된다  (Pod를 어떤 노드에 배치할지는 스케줄러가 결정한다)  Replicaset을 사용하는 이유는 노드 장애 대비(High Availability), 트래픽 분산(Load Balancing) 때문이다  보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많다매니페스트 파일apiVersion: apps/v1kind: ReplicaSetmetadata:  name: echo-rsspec:  replicas: 3  selector: # 어떤 파드의 복제를 관리할 것인가    matchLabels: # app: echo이고 tier: app인 label을 가지는 파드를 관리      app: echo      tier: app  template: # replicaset이 만드는 pod의 템플릿    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v1ReplicaSet의 spec에는 replicas, selector, template, minReadySeconds가 있습니다.(ReplicaSet 공식문서 참고)레플리카셋 실습Deployment  Deployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트중 하나  Pod를 중단없이 업데이트하거나 특정 버전으로 롤백할 수 있다 (무중단 배포)  쿠버네티스에서는 컨테이너 하나를 가동하더라도 디플로이먼트 사용을 권장한다  파드에 장애가 발생했을 때 자동으로 파드가 다시 생성된다  Deployment 오브젝트가 Pod의 버전을 관리하는 과정매니페스트 파일apiVersion: apps/v1kind: Deploymentmetadata:  name: echo-deployspec:  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 0  replicas: 4  selector:    matchLabels:      app: echo      tier: app  template:    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v2spec에는 replicas, selector, template, strategy  등이 있습니다.(Deployment 공식문서 참고)디플로이먼트 업데이트 전략      기본값은 RollingUpdate 이고 그 밖에 Recreate가 있다    Recreate          모든 파드를 한 번 삭제하고 다시 파드를 생성 =&amp;gt; 다운타임이 발생하지만 추가 리로스를 사용하지 않고 전환이 빠른 장점이 있다      (기존 레플리카셋의 레플리카 수를 0으로 한다. 이 후 신규 레플리카셋을 생성해서 파드수를 늘린다      (딱 레플리카 수만큼의 컴퓨팅 리소스만 필요)      개발 단계에서 유용        RollingUpdate          새로운 버전을 배포하면서 이전 버전을 종료한다      레플리카 수 이상의 컴퓨팅 리소스가 일시적으로 필요하게 된다      RollingUpdate 옵션: maxUnavailable, maxSurge                  maxUnavailable: 업데이트 중에 한 번에 정지 가능한 최대 파드 수 (또는 비율)          maxSurge: 업데이트 중에 가질 수 있는 최대 파드 수 (또는 비율)                    디플로이먼트 실습DaemonSet  데몬셋은 레플리카셋의 특수한 형태  데몬셋은 파드를 각 노드에 한 개씩 배치하는 리소스  노드를 늘리면 파드도 자동으로 늘어남  모든 노드에서 반드시 동작해야 하는 프로세스에 유용하게 사용된다  (ex. 로그를 호스트 단위로 수집하는 Fluentd, 파드 리소스 사용 현황 및 노드 상태를 모니터링하는 Datadog)매니페스트 파일apiVersion: apps/v1kind: DaemonSetmetadata:  name: sample-dsspec:  updateStrategy:    type: OnDelete  # type: RollingUpdate  # rollingUpdate:  #   maxUnavailable: 2  selector:    matchLabels:      app: sample-app  template:    metadata:      labels:        app: sample-app    spec:      containers:      - name: nginx-container        image: nginx:1.16업데이트 전략  OnDelete: 노드가 정지된 경우에만 다시 실행할 때 업데이트 된다 -&amp;gt; 운영상 이유로 가급적 정지되면 안되는 경우 사용  RollingUpdate: 즉시 파드를 업데이트한다 -&amp;gt; 버전 업데이트가 바로바로 필요한 경우 사용데몬셋 실습kubectl apply -f sample-ds.yamlkubectl get pods -o wideStatefulSet  레플리카셋의 특수한 형태      데이터베이스 등과 같은 스테이트풀한 워크로드에 사용하기 위한 리소스    생성되는 파드명의 접미사는 숫자 인덱스가 부여된 것  파드명이 바뀌지 않는다      데이터를 영구적으로 저장하기 위한 구조로 되어있다    스테이트풀셋에서는 spec.volumeClaimTemplates를 설정함으로써, 각 파드에 영구 볼륨 클레임을 설정할 수 있다      영구 볼륨 클레임을 사용하면 클러스터 외부의 영구 볼륨을 파드에 연결할 수 있기 때문에 언제든 데이터를 보유한 상태로 컨테이너가 생성된다. 영구 볼륨은 하나의 파드가 소유할 수도 있고, 여러 파드에서 공유할 수도 있다    레플리카셋은 파드를 삭제하면 무작위로 삭제된다  스테이트풀셋은 0번째 파드가 항상 가장 먼저 생성되고, 가장 늦게 삭제된다  만약 마스터-슬레이브 구조로 파드를 구성한다면 스테이트풀셋의 0번째 파드를 마스터로 사용하면 된다  스테이트풀셋은 서비스 오브젝트의 Headless, 볼륨 오브젝트의 StorageClass 부분에서 더 다뤄볼 것이다매니페스트 파일apiVersion: apps/v1kind: StatefulSetmetadata:  name: sample-statefulsetspec:  updateStrategy:    type: OnDelete  serviceName: sample-statefulset  replicas: 3  selector:    matchLabels:      app: sample-app  templates:    metadata:      labels:        app: sample-app    spec:      containers:      - name: nginx-container        image: nginx:1.16        volumeMounts:        - name: www          mountPath: /usr/share/nginx/html      volumeClaimTemplates:      - metadata:          name: www        spec:          accessModes:          - ReadWriteOnce          resources:            requests:              storage: 1G스테이트풀셋 실습kubectl apply -f sample-statefulset.yamlkubectl get statefulsetskubectl get pods -o widekubectl get persistentvolumeclaimskubectl get persistentvolumeskubectl scale statefulset sample-statefulset --replicas=5참고자료  subicura님의 kubenetes안내서  하나씩 점을 찍어나가며 블로그  Kubernetes 공식문서  Rancher 공식문서",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/kubernetes-series3'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part3]: Kubernetes Workload Object'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series3'>Kubernetes Series [Part3]: Kubernetes Workload Object</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[Docker] 가상화 기술의 역사와 컨테이너의 등장",
      "category" : "devops",
      "tags"     : "docker",
      "url"      : "/container-series1",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  물리서버(Bare Metal)          물리서버의 장점      물리서버의 단점        가상머신(Virtual Machine)          호스트 가상화      하이퍼바이저 가상화        컨테이너(Container)  컨테이너 기술의 특징  컨테이너 기술의 역사  참고물리서버(Bare Metal)  물리서버는 서버가 가지고 있는 컴퓨팅 자원을 오직 하나의 사용자(여기서 사용자는 서비스 사용자가 아니라 자원을 사용하는 사람)에게 할당하는 것을 말한다  그렇기 때문에 물리서버에서는 자원을 전혀 분리해서 사용하지 않는다물리서버의 장점  하드웨어간 네트워크 이동이 발생하지 않기 때문에 네트워크 지연율을 최소화할 수 있고, 보안적인 측면에서도 훨씬 안전하다  또한 별도의 하드웨어 에뮬레이션이 없기 때문에 하드웨어에 루트 레벨의 접근이 가능하고 이를 통해 뛰어난 커스터마이징을 할 수 있다  그래서 보안을 최우선으로 생각하는 뱅킹시스템이나 데이터베이스 시스템은 이러한 Bare Metal 형태로 컴퓨팅 자원을 사용한다물리서버의 단점  단점으로는 서비스 개발에 있어 여러 개의 애플리케이션을 독립적으로 개발하기가 힘들다  그 이유는 자원이 분리되어 있지 않고 그로 인해 각각의 애플리케이션에 필요한 라이브러리 종속성도 해결하기가 어려워지기 때문이다  또한 사용자가 가지고 있는 Bare Metal 서버에 최적화된 방식으로 서비스를 개발하다보니 다른 서버에 배포가 힘들어진다  이러한 단점은 Scale Out 방식이 아닌 Scale Up으로 밖에 확장성을 가지지 못한다는 점으로 이어진다가상머신(Virtual Machine)  하드웨어의 기술이 급격하게 발전하는 동안, 소프트웨어의 발전은 한참 뒤쳐져 있었다  이로 인해 애플리케이션을 실행하는데 있어, 하드웨어의 자원의 낭비가 커지게 되었다  하드웨어 자원을 최대한 활용하고자 애플리케이션 크기만큼만 하드웨어를 사용하여 애플리케이션마다 독립적인 환경을 제공하는 가상화 기술에 대한 요구가 등장했다호스트 가상화  처음 시중에 등장한 가상화 기술은 호스트 가상화(Type2 Hypervisor)이다  호스트가상화는 Host OS위에 컴퓨팅 자원이 격리된 가상 머신을 띄우고 그 안에 Guest OS가 구동되는 방식이다  종류로는 VM Workstation, VMware Server, VMware Player, MS Virtual Sever, Virtual Box 등이 있다  호스트 가상화는 간단하지만 하드웨어 에뮬레이팅을 위한 하이퍼바이저(Hypervisor)와 Host OS라는 두 개의 소프트웨어를 추가로 실행시켜야 하는 오버헤드가 있다하이퍼바이저 가상화  그다음 등장한 것이 하이퍼바이저 가상화(Type1 Hypervisor)이다  현재 서버 가상화 기술에서는 주류 방식으로 사용되고 있다  종류로는 Xen, KVM 등이 있다. 이러한 방식의 가상화는 Host OS 없이 사용하는 가상화 방식이기 때문에 불필요한 오버헤드를 줄여준다  아마존 AWS와 같은 클라우드의 컴퓨팅 서비스가 대표적으로 이러한 방식의 가상화 기술을 사용한다  하지만 Host OS가 없다는 사실에서 생기는 문제는 각각 다른 Guest OS가 하드웨어에 접근할 수 있어야 한다는 것이다이러한 문제를 해결하는데는 두 가지 방법이 있다    첫 번째는 하이퍼바이저가 구동될 때 DOM0라고 하는 관리용 가상머신을 하나 실행시켜 DOM0가 중개하는 전가상화 방식입니다. DOM0의 역할은 각각의 Guest OS에서 발생하는 요청을 하이퍼바이저가 해석할 수 있도록 컴파일해서 하이퍼바이저에 전달하는 것입니다. 이 방법은 호스트 OS보다는 가벼운 DOM0를 실행한다는 점에서 오버헤드가 줄게되지만 여전히 성능상의 단점이 있습니다.        두 번째는 반가상화 방식(Bare Metal Hypervisor)입니다. 반가상화는 DOM0를 없애고, 각각의 Guest OS가 하이퍼바이저에게 직접 요청(Hypercall)할 수 있도록 Guest OS의 커널을 수정하는 방법입니다. 이 방법은 별도의 레이어가 필요없기 때문에 가장 오버헤드가 적게 발생합니다. 하지만 이 방법은 OS의 커널을 수정해야하기 때문에 오픈 소스의 OS에서만 가능하고 macOS나 windows같은 운영체제에서는 불가능합니다.    하지만, 어디까지나 분류는 분류일 뿐 Type 1 하이퍼바이저들이 모두 전가상화에만 속하거나 반가상화에만 속하는 것은 아닙니다. 최근에는 하이퍼바이저에서 전가상화와 반가상화의 경계가 별 의미가 없어졌습니다.          VMware나 KVM이 대표적인 전가상화 제품에 속하고, Xen이 대표적인 반가상화 제품에 속했다(과거형).      전가상화는 모든 CPU 명령어를 가상화(애뮬레이션)하므로 아키텍처에 제한을 받지 않지만 느리다.      반가상화는 꼭 필요한 CPU 명령어만 가상화한다.                  꼭 필요한 명령어만을 가상화 요청(Hyper Call)하도록 커널 수정 필요                    Xen은 반 가상화 하이퍼바이저로 등장했지만 오래 전부터 전 가상화도 지원한다.      VMWare이나 KVM도 전 가상화 하이퍼바이저이지만 반 가상화 기능을 제공한다.      전 가상화와 반 가상화 하이퍼바이저의 경계가 거의 없어짐.      컨테이너(Container)  컨테이너 기술은 가상화의 꽃이라고 할 수 있다  컨테이너는 애플리케이션 가상화로, VM과 달리 OS를 포함하지 않는다. 즉, 하드웨어와 호스트 OS는 그대로 둔 채 애플리케이션 영역만 캡슐화하여 격리하는 방식이다  VM에 비해 가볍고, 배포가 빠르며, 자원을 효율적으로 사용할 수 있다는 장점이 있어 최근에 많이 활용되고 있다  2013년 도커의 등장으로 컨테이너의 기술이 대중화 되었다컨테이너 기술의 특징  컨테이너 기술에서의 Host OS는 보통 OS가 가지는 모든 기능들을 제공하지는 않고, 리눅스 커널 기술까지만 제공한다  그 외에 컴포넌트(파일 시스템, 패키지 매니저 등)들은 컨테이너가 만들어진 이미지에 종속적이다.  그래서 도커에서 이미지들을 보게 되면, Ubuntu 기반, Centos 기반 등 다양한 배포판들이 있는 것이다컨테이너 기술의 역사  참고로 도커 이전에도 컨테이너 기반의 가상화는 있었다  컨테이너의 역사에 대해 간략히 살펴보면 아래와 같다  2000년, Unix OS 인 FreeBSD 에서 OS 가상화 기능인 FreeBSD Jail를 발표합니다.  2001년, Linux에서 커널에 Linux-Vserver 라는 기능을 추가하여 OS 가상화 환경을 이용할 수 있게 되었습니다.  2006년, Google은 cgroup는 프로세스 자원 이용량을 제어하는 기능을 발표합니다.  2008년, Red Hat 에서 논리적으로 시스템 자원을 분할하는 Namespace를 발표합니다.  비슷한 시기에 IBM에서 LXC (LinuX Containers)를 발표합니다.  LXC가 cgroup 과 Namespace를 사용하여 구현한 최초의 Linux 컨테이너 엔진입니다.  2013년, 도커라는 회사에서 LXC를 아주 잘 활용할 수 있도록 도커( Docker) 라는 기술을 오픈소스로 발표합니다.  도커는 Dockerfile이란 메니페스트를 만들고, Container Hub를 만들면서, Container기술은 급속히 발전하게 됩니다.  2015년, Google에서 컨테이너를 통합하여 오케스트레이션하는 쿠버네티스라는 프로젝트를 오픈소스로 발표합니다.  2016년, 구글이 쿠버네티스를 CNCF 재단에 기증하면서 클라우드네이티브 시대의 서막을 알리게 됩니다.  이후 Containerd 와 CRI-O 그리고 PODMAN 등 컨테이너는 표준기술 중심으로 발전하고 있습니다.  이외에도 rht, OCI, CRI-O 등 표준 기술들이 발전하였고, 레드햇은 Kubernetes 기반으로 OpenShift를 개발했습니다.참고  KT Cloud: Cloud 인프라 Intro - 물리서버와 가상서버  opennaru: 물리서버 , 가상화 , 컨테이너 기술 진화의 역사  phoenixnap: The Definitive Guide to Bare Metal Servers for 2021  phoenixnap: Bare Metal Vs VM: What Performs Better  IT Opening: Xen Kvm 반가상화 전가상화 차이 비교 분석  NDS: [소개] 가상화의 종류3가지  하드웨어 가상화(Virtualization) 뜻, 가상화 기술 종류 4가지, 가상머신(Virtual Machine)의 단점 3가지  Rain.i: 하이퍼바이저(Hypervisor)의 종류  openmaru: 컨테이너 기술의 발전과 역사",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/container-series1'> <img src='/images/container_5.png' alt='[Docker] 가상화 기술의 역사와 컨테이너의 등장'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series1'>[Docker] 가상화 기술의 역사와 컨테이너의 등장</a> </h2><p class='article__excerpt'>가상화 기술의 역사와 컨테이너 기술의 등장 배경에 대해 배운다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part2]: Kubernetes 기본 명령어",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series2",
      "date"     : "Jan 1, 2022",
      "content"  : "Table of Contents  리소스 생성/삭제/갱신  파드 재기동  실행 순서를 지켜야 하는 리소스의 경우  여러 매니페스트 파일 적용  레이블  리소스 조회: get  리소스 상세 정보: describe  실제 리소스 사용량 확인: top  컨테이너에 명령어 전달: exec  포트 포워딩: port-forward  컨테이너 로그 확인: logs  컨테이너와 로컬간의 파일 복사: cp  쿠버네티스에서 클러스터 조작은 모두 마스터 API 를 통해 이루어진다  직접 API에 요청을 보내거나 클라이언트 라이브러리를 사용해 클러스터를 조작할 수도 있지만,  수동으로 조작하는 경우에는 CLI 도구인 kubectl을 사용하는 것이 일반적이다  kubectl은 kubeconfig ( ~/.kube/config)에 쓰여있는 정보를 사용하여 접속한다  kubectl 명령어 실행은 바로 전달되지만, 바로 처리가 되지는 않는다  (실제 리소스 처리는 비동기로 실행되기 때문에 바로 완료되지 않을 수 있다)리소스 생성/삭제/갱신# 리소스 생성# 리소스가 있을 경우 에러 발생kubectl create -f sample-pod.yaml# 리소스 조회kubectl get pods# 리소스 삭제# 리소스가 없을 경우 에러 발생kubectl delete -f sample-pod.yaml# 파일을 사용하지 않고 리소스 종류와 이름으로 지정해 삭제할 수도 있다kubectl delete pod sample-pod# 특정 리소스 종류를 모두 삭제kubectl delete pod --all# 리소스 처리를 완료하고 명령어 실행을 종료kubectl delete -f sample-pod.yaml --wait# 리소스 즉시 강제 삭제kubectl delete -f sample-pod.yaml --force# 리소스 수정# 변경 부분이 있으면 적용하고, 없으면 적용하지 않는다# 리소스가 없으면 create 명령어와 동일하게 리소스를 새로 생성한다# 일반적으로 create 보다 apply를 사용하는 것이 편하다kubectl apply -f sample-pod.yaml파드 재기동# 디플로이먼트와 같은 리소스에 연결되어 있는 모든 파드를 재기동할 수 있다# 재실행하고 싶거나, 시크릿 리소스에서 참조되는 환경변수를 변경하고 싶을 때 사용하면 좋다kubectl rollout restart deployment sample-deployment실행 순서를 지켜야 하는 리소스의 경우  한 개의 매니페스트에 여러 리소스를 정의할 수 있다  매니페스트에 작성된 순서대로 리소스가 적용된다  중간에 리소스에 에러가 발생하면, 이후 정의된 리소스는 적용되지 않는다  리소스간 구분은 ---으로 한다여러 매니페스트 파일 적용  디렉터리를 경로로 해서 실행하면 디렉터리 안에 리소스들이 함께 적용된다  파일명 순으로 매니페스트 파일이 적용되기 때문에 순서를 제어하고 싶을 때는 파일명 앞에 인덱스 번호를 지정하면 된다  디렉터리 안에 디렉터리가 있는 경우에는 -R을 사용하면 된다kubectl apply -f ./dirkubectl apply -f ./dir -R레이블  레이블은 리소스를 구분하기 위한 정보apiVersion: v1kind: Podmetadata:  name: sample-pod  labels:    app: A    env: dev  레이블은 수많은 리소스에 대해 동일한 레이블로 그룹핑하여 처리하거나, 어떤 처리에 대한 조건으로 사용된다  레플리카 리소스는 레이블을 이용해 파드의 수를 일정하게 유지한다  서비스 리소스는 레이블을 긱준으로 목적지 파드를 결정한다# app 레이블이 A이고 env 레이블을 가진 파드 조회kubectl get pods -l app=A, env# 조회할 때 app 레이블 컬럼으로 표시kubectl get pods -L app-------------------------------------NAME READY STATUS RESTARTS AGE APP# -l -L 옵션 함께 사용 가능kubectl get pods -l app=A, env -L env# 모든 레이블을 LABELS 컬럼에 표시kubectl get pods --show-labels리소스 조회: get# 레이블이 label1: val인 파드들의 레이블을 LABELS 컬럼에 표시kubectl get pods -l label1=val1 --show-labels# 파드 목록 상세 표시kubectl get pods -o wide# 파드 목록 yaml 형태로 표시kubectl get pods -o yaml# 파드의 특정 항목 표시kubectl get pods sample-pod -o jsonpath=&quot;{.metadata.name}&quot;# 배열 데이터의 일부 항목 표시# 배열[] 안에 ?(@.field == value) 형식으로 지정kubectl get pod sample-pod -o jsonpath=&quot;{.spec.containers[?(@.name == &#39;nginx-container&#39;)].image}&quot;# 노드 목록 표시kubectl get nodes# 모든 종류의 리소스 표시kubectl get all# 리소스 상태 변화를 실시간으로 출력kubectl get pods --watch리소스 상세 정보: describekubectl describe pod sample-podkubectl describe node gke-k8s-default실제 리소스 사용량 확인: top# describe에 나타난 리소스는 사용량이 아니라 확보한 용량# 노드의 리소스 사용량 조회kubectl top node# 파드별 리소스 사용량 조회kubectl -n default top pod# 컨테이너별 리소스 사용량 조회kubectl -n default top pod --containers컨테이너에 명령어 전달: exec# -i: 표준 입출력을 패스스루# -t: 가상 터미널 생성# -- 다음에 전달할 명령어kubectl exec -it sample-pod -- /bin/ls# 특정 컨테이너에 명령어 실행kubectl exec -it sample-pod -c nginx-container -- /bin/bash포트 포워딩: port-forward# 로컬 머신에서 파드로 포트 포워딩kubectl port-forward sample-pod 8888:80# 파드가 아닌 디플로이먼트 리소스나 서비스 리소스에 연결되는 파드에도 포트 포워딩을 할 수 있다kubectl port-forward deployment/sample-deployment 8888:80kubectl port-forward service/sample-service 8888:80컨테이너 로그 확인: logskubectl logs sample-pod# 특정 컨테이너 로그 출력kubectl logs sample-pod -c nginx-container# 실시간 로그 출력kubectl logs -f sample-pod컨테이너와 로컬간의 파일 복사: cp# 파드의 파일을 로컬 머신에 복사kubectl cp sample-pod:etc/sample.txt ./sample.txt# 로컬 파일을 컨테이너로 복사kubectl cp ./sample.txt sample-pod:/tmp/",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-01T21:01:35+09:00'>01 Jan 2022</time><a class='article__image' href='/kubernetes-series2'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part2]: Kubernetes 기본 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series2'>Kubernetes Series [Part2]: Kubernetes 기본 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part1]: Kubernetes 설치",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series1",
      "date"     : "Jan 1, 2022",
      "content"  : "Table of Contents  로컬환경          미니큐브(minikube)      Docker Desktop      kind(Kubernetes in Docker)        클라우드환경          GKE(Google Kubernetes Engine)      EKS(Elastic Kubernetes Service)        참고자료로컬환경쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 테스트, 개발 환경에 적합합니다.미니큐브(minikube)미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)kind(Kubernetes in Docker)minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)brew install kindkind version--------------------kind v0.11.1 go1.17.2 darwin/arm64잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)# kind로 클러스터 구축을 위한 kind.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦 ✓ Configuring the external load balancer ⚖️ ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✗ Joining worker nodes 🚜 실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드환경GKE(Google Kubernetes Engine)  클러스터 생성하기  gcloud (Google SDK)설치 프로그램 다운로드          https://cloud.google.com/sdk/docs/install?hl=ko 접속      tar -xzf google-cloud-cli-402.0.0-darwin-arm.tar.gz      ./google-cloud-sdk/install.sh      ./google-cloud-sdk/bin/gcloud init      source ~/.zshrc        나의 GKE에 관한 정보를 ~/.kube/config 파일에 저장 =&amp;gt; kubectl 명령어를 전달할 쿠버네티스 클러스터 설정          gcloud components install gke-gcloud-auth-plugin        클러스터에 연결          gcloud container clusters get-credentials my-cluster --zone us-central1-a --project second-lodge-364202        연결 테스트          kubectl get pods        노드 수 줄이기 (사실상 GKE 종료하는 방법)          gcloud container clusters resize my-cluster --num-nodes=0 --zone us-central1-a      EKS(Elastic Kubernetes Service)참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-01T21:01:35+09:00'>01 Jan 2022</time><a class='article__image' href='/kubernetes-series1'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part1]: Kubernetes 설치'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series1'>Kubernetes Series [Part1]: Kubernetes 설치</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part0]: Kubernetes란 무엇인가",
      "category" : "devops",
      "tags"     : "kubernetes",
      "url"      : "/kubernetes-series0",
      "date"     : "Jan 1, 2022",
      "content"  : "Table of Contents  쿠버네티스란 무엇인가  쿠버네티스의 역사  쿠버네티스 등장 배경  쿠버네티스의 특징  쿠버네티스가 제공하는 것들  쿠버네티스 아키텍처          마스터 노드      워커 노드        Desired State  쿠버네티스가 명령을 수행하는 과정  쿠버네티스가 통신하는 과정  마치며  참고자료쿠버네티스란 무엇인가  여러 개의 컨테이너화된 애플리케이션을 여러 서버(쿠버네티스 클러스터)에 자동으로 배포, 스케일링 및 관리해주는 오픈소스 플랫폼쿠버네티스의 역사쿠버네티스는 구글이 내부적으로 사용하던 컨테이너 클러스터 관리 도구 Borg에서 아이디어를 얻어 만들어진 오픈 소스 소프트웨어다. 2014년 6월에 공개되어, 2015년 7월에 클라우드 네이티브 컴퓨팅 파운데이션(CNCF)으로 이관되었다.CNCF는 쿠버네티스 외에도 많은 프로젝트를 호스트하고 있으며, 프로젝트별로 성숙도가 정의되어있다. 성숙도는 높은 순으로 Graduated, Incubating, Sandbox 중 하나로 분류된다. 쿠버네티스는 Graduated로 분류되어 성숙도를 인정받았다.쿠버네티스는 구글의 GCP 에서 GKE 라는 서비스로, AWS에서 EKS 라는 서비스로 쿠버네티스를 제공한다.쿠버네티스 등장 배경  도커의 등장과 함께 컨테이너 기술이 개발자들에게 널리 이용되었다  동시에 서비스의 규모가 점차 커지게 되면서, 모놀리틱 아키텍처가 아닌 마이크로 서비스 아키텍처가 각광을 받았다  클라우드 컴퓨팅, 분산 시스템의 등장으로 여러 서버를 클러스터로 구성하여 서비스를 운영하게 되었다  여러 컨테이너를 여러 서버에 배포하는데 이 때 컨테이너 크기와 서버의 리소스를 고려해 배포해주어야 한다  이 때 서비스에 요구되는 스케일 확장, 장애 대응, 버전 롤백과 같은 것들을 쉽게 해줄 도구가 필요했다  이를 해결해주기 위해 등장한 것이 쿠버네티스이다쿠버네티스의 특징  쿠버네티스(Kubernetes)는 컨테이너로 향상된 리소스 활용의 이점을 누리면서도 복잡한 분산 시스템을 쉽게 배포하고 관리할 수 있도록 만들어준다.  쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 한다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작한다.  쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화한다.쿠버네티스가 제공하는 것들  선언적 코드를 사용한 관리 (IaC)          YAML 형식이나 JSON 형식으로 작성한 선언적 코드(매니페스트)를 통해 컨테이너 배포할 수 있다        (오토)-스케일링          부하에 따라서 레플리카 수를 자동으로 늘리거나 줄일 수 있다        스케줄링          컨테이너를 노드에 배포할 때, 노드의 성능을 기준으로 스케줄링할 수 있다        자동화된 복구          프로세스 모니터링, 헬스 체크 등을 이용해 컨테이너를 자동으로 재배포할 수 있다        서비스 디스커버리          마이크로서비스 아키텍처에서  서로의 마이크로서비스를 참조할 수 있는 서비스 디스커버리 기능을 제공한다        로드 밸런싱          로드 밸런서 주소를 엔드포인트로 할당하고, 트래픽을 여러 대의 서버로 분산시킬 수 있다        데이터 관리          etcd를 사용해 데이터를 이중화된 상태로 관리할 수 있다      쿠버네티스 아키텍처마스터 노드  Control Plane (클러스터 기능을 제어하고 전체 클러스터가 동작하게 만드는 역할)  전체 클러스터를 관리하는 서버쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.  API 서버: 클러스터 상태 조회, 변경을 위한 API 인터페이스 제공. 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행  컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 Desired 상태를 유지, API서버 요청 처리  스케줄러: 배포할 Pod(컨테이너와 비슷)가 있는지 계속 체크, 필요한 경우 가장 최적의 노드를 선택  etcd: 클러스터에 배포된 애플리케이션 실행 정보를 저장. 고가용성을 제공하는 키-밸류(key-value) 저장소워커 노드      컨테이너가 배포되고 실제로 실행되는 서버    kubelet: 클러스터내의 모든 노드에서 실행되는 에이전트. 파드내의 컨테이너들이 실행되는걸 직접적으로 관리하는 역할  kube-proxy: 쿠버네티스는 클러스터 내부에 별도의 가상 네트워크를 설정하고 관리. kube-proxy는 이런 가상 네트워크가 동작할 수 있게 하는 실질적인 역할을 하는 프로세스. 호스트의 네트워크 규칙을 관리하거나 커넥션 포워딩을 하기도함.  container runtime: 컨테이너 런타임은 실제로 컨테이너를 실행시키는 역할. 가장 많이 알려진 런타임으로는 도커(Docner)가 있고, 그외 rkt, runc같은 런타임도 지원. 그외에도 컨테이너에 관한 표준을 제정하는 역할을 하는 OCI의 런타임 규격을 구현하고 있는 컨테이너 런타임이라면 쿠버네티스에서 사용가능Desired State쿠버네티스에서 가장 중요한 것은 desired state(원하는 상태)라는 개념이다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말한다.쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있다.이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청한다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 된다.쿠버네티스가 명령을 수행하는 과정쿠버네티스가 통신하는 과정마치며지금까지는 쿠버네티스가 어떻게 명령을 수행하는지 알아봤다. 다음 포스트에서는 클라이언트가 무엇을 이용해 어떤식으로 쿠버네티스에 명령을 내리는지 알아보자.참고자료  subicura님의 kubenetes안내서  OSS, 쿠버네티스(kubernetes) 구성요소  패스트캠퍼스, Kubernetes와 Docker로 한 번에 끝내는 컨테이너 기반 MSA",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-01T21:01:35+09:00'>01 Jan 2022</time><a class='article__image' href='/kubernetes-series0'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part0]: Kubernetes란 무엇인가'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series0'>Kubernetes Series [Part0]: Kubernetes란 무엇인가</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-component-heapq",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/python-component-heapq'> <img src='/images/python_logo.png' alt='Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-heapq'>Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 내장 함수 모음",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series12",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  샘플 데이터  문자열 함수  날짜 함수  수학 함수  참고샘플 데이터+----+------------+-----------+----------------------------+------+--------+------------+--------------------------------+-----------------------+---------------+---------------------+| id | first_name | last_name | email                      | age  | gender | state      | street_address                 | city                  | country       | created_at          |+----+------------+-----------+----------------------------+------+--------+------------+--------------------------------+-----------------------+---------------+---------------------+|  1 | Justin     | Lee       | justinlee@example.org      |   48 | M      | Cataluña   | 856 Morgan Highway Apt. 238    | Sant Adrià de Besòs   | Spain         | 2019-02-10 04:40:00 ||  2 | Jennifer   | Garcia    | jennifergarcia@example.net |   28 | F      | Chongqing  | 96683 Cunningham Ports Apt. 20 | Putian                | China         | 2022-06-22 00:41:00 ||  3 | Renee      | Ramirez   | reneeramirez@example.net   |   15 | F      | Bahia      | 5102 Melanie Turnpike          | Campo Formoso         | Brasil        | 2020-08-24 06:12:00 ||  4 | Brent      | Webster   | brentwebster@example.com   |   57 | M      | Arizona    | 9041 Kirby Lights Apt. 193     | Phoenix               | United States | 2022-08-21 14:42:00 ||  5 | Mark       | Bradley   | markbradley@example.com    |   30 | M      | São Paulo  | 3700 Tiffany Radial            | Jardinópolis          | Brasil        | 2019-12-18 12:20:00 |+----+------------+-----------+----------------------------+------+--------+------------+--------------------------------+-----------------------+---------------+---------------------+문자열 함수-- CHAR_LENGTH: Returns the length of a string (in characters)SELECT last_name, CHAR_LENGTH(last_name) AS lengthFROM ecommerce.USERLIMIT 5;------------------------------------------------------+-----------+------------------------+| last_name | CHAR_LENGTH(last_name) |+-----------+------------------------+| Lee       |                      3 || Garcia    |                      6 || Ramirez   |                      7 || Webster   |                      7 || Bradley   |                      7 |+-----------+------------------------+-- CONCAT: Adds two or more expressions togetherSELECT CONCAT(state, city, country) AS addressFROM ecommerce.USERLIMIT 5------------------------------------------------------+-------------------------------------+| address                             |+-------------------------------------+| CataluñaSant Adrià de BesòsSpain    || ChongqingPutianChina                || BahiaCampo FormosoBrasil            || ArizonaPhoenixUnited States         || São PauloJardinópolisBrasil         |+-------------------------------------+SELECT CONCAT(state, &quot;, &quot;, city, &quot;, &quot;, country) AS address FROM ecommerce.USER LIMIT 5;------------------------------------------------------+-----------------------------------------+| address                                 |+-----------------------------------------+| Cataluña, Sant Adrià de Besòs, Spain    || Chongqing, Putian, China                || Bahia, Campo Formoso, Brasil            || Arizona, Phoenix, United States         || São Paulo, Jardinópolis, Brasil         |+-----------------------------------------+-- CONCAT_WS: Adds two or more expressions together with a separatorSELECT CONCAT_WS(&quot; + &quot;, state, city, country) AS address FROM ecommerce.USER LIMIT 5;------------------------------------------------------+-------------------------------------------+| address                                   |+-------------------------------------------+| Cataluña + Sant Adrià de Besòs + Spain    || Chongqing + Putian + China                || Bahia + Campo Formoso + Brasil            || Arizona + Phoenix + United States         || São Paulo + Jardinópolis + Brasil         |+-------------------------------------------+-- FORMAT: ,와 . 으로 숫자를 보기좋게 포맷팅 (뒤에 숫자는 반올림하고자 하는 소수점 자리)SELECT FORMAT(250500.5664, 2);------------------------------------------+------------------------+| FORMAT(250500.5664, 2) |+------------------------+| 250,500.57             |+------------------------+-- INSERT: 특정 위치에 있는 문자열 대신 다른 문자열을 삽입SELECT INSERT(&quot;Apple.com&quot;, 1, 5, &quot;Samsung&quot;);------------------------------------------+--------------------------------------+| INSERT(&quot;Apple.com&quot;, 1, 5, &quot;Samsung&quot;) |+--------------------------------------+| Samsung.com                          |+--------------------------------------+-- INSTR: 특정 문자열을 찾아 첫 번째로 발견된 곳의 위치를 리턴한다SELECT INSTR(&quot;Apple&quot;, &quot;p&quot;);------------------------------------------+---------------------+| INSTR(&quot;Apple&quot;, &quot;p&quot;) |+---------------------+|                   2 |+---------------------+-- LCASE: 문자열 전체를 소문자로 변환한다-- LOWER: LCASE와 같다SELECT LCASE(&quot;SQL Tutorial is FUN!&quot;);------------------------------------------+-------------------------------+| LCASE(&quot;SQL Tutorial is FUN!&quot;) |+-------------------------------+| sql tutorial is fun!          |+-------------------------------+-- LEFT: 왼쪽 문자열 일부를 추출한다SELECT LEFT(&quot;SQL Tutorial&quot;, 3) AS ExtractString;------------------------------------------+---------------+| ExtractString |+---------------+| SQL           |+---------------+-- LENGTH: Returns the length of a string (in bytes)-- LENGTH 함수는 문자의 BYTE 길이를 가져오기 때문에 한글 같은 문자는 정확한 길이를 할 수 없는데 이 때 사용하는 것이 CHAR_LENGTH 함수이다SELECT LENGTH(&quot;SQL Tutorial&quot;) AS LengthOfString;------------------------------------------+----------------+| LengthOfString |+----------------+|             12 |+----------------+-- LOCATE: Returns the position of the first occurrence of a substring in a stringSELECT LOCATE(&quot;3S&quot;, &quot;W3Schools.com&quot;) AS MatchPosition;------------------------------------------+---------------+| MatchPosition |+---------------+|             2 |+---------------+SELECT LOCATE(&quot;3a&quot;, &quot;W3Schools.com&quot;) AS MatchPosition;------------------------------------------+---------------+| MatchPosition |+---------------+|             0 |+---------------+-- LPADSELECT LPAD(&quot;SQL Tutorial&quot;, 20, &quot;abc&quot;);------------------------------------------+---------------------------------+| LPAD(&quot;SQL Tutorial&quot;, 20, &quot;abc&quot;) |+---------------------------------+| abcabcabSQL Tutorial            |+---------------------------------+-- LTRIMSELECT LTRIM(&quot;     SQL Tutorial&quot;) AS LeftTrimmedString;------------------------------------------+-------------------+| LeftTrimmedString |+-------------------+| SQL Tutorial      |+-------------------+-- REPEATSELECT REPEAT(&quot;ABC &quot;, 3);------------------------------------------+-------------------+| REPEAT(&quot;ABC &quot;, 3) |+-------------------+| ABC ABC ABC       |+-------------------+-- REPLACESELECT REPLACE(&quot;Apple in Apple&quot;, &quot;Apple&quot;, &quot;Samsung&quot;);------------------------------------------+-----------------------------------------------+| REPLACE(&quot;Apple in Apple&quot;, &quot;Apple&quot;, &quot;Samsung&quot;) |+-----------------------------------------------+| Samsung in Samsung                            |+-----------------------------------------------+-- REVERSESELECT REVERSE(&quot;SQL Tutorial&quot;);------------------------------------------+-------------------------+| REVERSE(&quot;SQL Tutorial&quot;) |+-------------------------+| lairotuT LQS            |+-------------------------+-- RIGHTSELECT RIGHT(&quot;SQL Tutorial is cool&quot;, 4) AS ExtractString;------------------------------------------+---------------+| ExtractString |+---------------+| cool          |+---------------+-- RPADSELECT RPAD(&quot;SQL Tutorial&quot;, 20, &quot;+++&quot;);------------------------------------------+---------------------------------+| RPAD(&quot;SQL Tutorial&quot;, 20, &quot;+++&quot;) |+---------------------------------+| SQL Tutorial++++++++            |+---------------------------------+-- RTRIMSELECT RTRIM(&quot;SQL Tutorial     &quot;) AS RightTrimmedString;------------------------------------------+--------------------+| RightTrimmedString |+--------------------+| SQL Tutorial       |+--------------------+-- SUBSTR, SUBSTRINGSELECT SUBSTR(&quot;SQL Tutorial&quot;, 5, 3) AS ExtractString;------------------------------------------+---------------+| ExtractString |+---------------+| Tut           |+---------------+-- SUBSTRING_INDEXSELECT SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 1);+------------------------------------------+| SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 1) |+------------------------------------------+| www                                      |+------------------------------------------+SELECT SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 2);+------------------------------------------+| SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 1) |+------------------------------------------+| www.naver                                |+------------------------------------------+SELECT SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 3);+------------------------------------------+| SUBSTRING_INDEX(&quot;www.naver.com&quot;, &quot;.&quot;, 3) |+------------------------------------------+| www.naver.com                            |+------------------------------------------+-- TRIMSELECT TRIM(&#39;    SQL Tutorial    &#39;) AS TrimmedString;------------------------------------------+---------------+| TrimmedString |+---------------+| SQL Tutorial  |+---------------+-- UCASE, UPPERSELECT UCASE(&quot;SQL Tutorial is FUN!&quot;);------------------------------------------+-------------------------------+| UCASE(&quot;SQL Tutorial is FUN!&quot;) |+-------------------------------+| SQL TUTORIAL IS FUN!          |+-------------------------------+날짜 함수-- ADDDATE(date, INTERVAL value addunit), DATE_ADD(date, INTERVAL value addunit)-- (addunit: MICROSECOND, SECOND, MINUTE, HOUR, DAY, WEEK, MONTH, QUARTER, YEAR 등)-- or ADDDATE(date, days)SELECT ADDDATE(&quot;2017-06-15&quot;, INTERVAL 10 DAY);-----------------------------------------------+----------------------------------------+| ADDDATE(&quot;2017-06-15&quot;, INTERVAL 10 DAY) |+----------------------------------------+| 2017-06-25                             |+----------------------------------------+SELECT ADDDATE(&quot;2017-06-15 09:34:21&quot;, INTERVAL -3 HOUR);----------------------------------------------------------+--------------------------------------------------+| ADDDATE(&quot;2017-06-15 09:34:21&quot;, INTERVAL -3 HOUR) |+--------------------------------------------------+| 2017-06-15 06:34:21                              |+--------------------------------------------------+-- CURDATE, CURRENT_DATESELECT CURDATE();------------------------------------------+------------+| CURDATE()  |+------------+| 2023-01-06 |+------------+-- CURRENT_TIMESTAMP, SYSDATE, NOWSELECT CURRENT_TIMESTAMP();------------------------------------------+---------------------+| CURRENT_TIMESTAMP() |+---------------------+| 2023-01-06 05:38:06 |+---------------------+SELECT CURRENT_TIMESTAMP() + 1;------------------------------------------+-------------------------+| CURRENT_TIMESTAMP() + 1 |+-------------------------+|          20230106061229 |+-------------------------+-- DATESELECT DATE(CURRENT_TIMESTAMP());------------------------------------------+---------------------------+| DATE(CURRENT_TIMESTAMP()) |+---------------------------+| 2023-01-06                |+---------------------------+-- DATEDIFFSELECT DATEDIFF(&quot;2017-06-25&quot;, &quot;2017-06-15&quot;);------------------------------------------+--------------------------------------+| DATEDIFF(&quot;2017-06-25&quot;, &quot;2017-06-15&quot;) |+--------------------------------------+|                                   10 |+--------------------------------------+-- DATE_FORMATSELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%Y %y&quot;);------------------------------------------+------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%Y %y&quot;) |+------------------------------------+| 2017 17                            |+------------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%D %d&quot;);+------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%D %d&quot;) |+------------------------------------+| 15th 15                            |+------------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%H %h&quot;);+------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%H %h&quot;) |+------------------------------------+| 00 12                              |+------------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%M %m&quot;);+------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%M %m&quot;) |+------------------------------------+| June 06                            |+------------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%i&quot;);+---------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%i&quot;) |+---------------------------------+| 00                              |+---------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%j&quot;);+---------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%j&quot;) |+---------------------------------+| 166                             |+---------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%S %s&quot;);+------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%S %s&quot;) |+------------------------------------+| 00 00                              |+------------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%T&quot;);+---------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%T&quot;) |+---------------------------------+| 00:00:00                        |+---------------------------------+SELECT DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%a %b %c&quot;);+---------------------------------------+| DATE_FORMAT(&quot;2017-06-15&quot;, &quot;%a %b %c&quot;) |+---------------------------------------+| Thu Jun 6                             |+---------------------------------------+            Format      Description                  %a      Abbreviated weekday name (Sun to Sat)              %b      Abbreviated month name (Jan to Dec)              %c      Numeric month name (0 to 12)              %D      Day of the month as a numeric value, followed by suffix (1st, 2nd, 3rd, …)              %d      Day of the month as a numeric value (01 to 31)              %e      Day of the month as a numeric value (0 to 31)              %f      Microseconds (000000 to 999999)              %H      Hour (00 to 23)              %h      Hour (00 to 12)              %I      Hour (00 to 12)              %i      Minutes (00 to 59)              %j      Day of the year (001 to 366)              %k      Hour (0 to 23)              %l      Hour (1 to 12)              %M      Month name in full (January to December)              %m      Month name as a numeric value (00 to 12)              %p      AM or PM              %r      Time in 12 hour AM or PM format (hh:mm:ss AM/PM)              %S      Seconds (00 to 59)              %s      Seconds (00 to 59)              %T      Time in 24 hour format (hh:mm:ss)              %U      Week where Sunday is the first day of the week (00 to 53)              %u      Week where Monday is the first day of the week (00 to 53)              %V      Week where Sunday is the first day of the week (01 to 53). Used with %X              %v      Week where Monday is the first day of the week (01 to 53). Used with %x              %W      Weekday name in full (Sunday to Saturday)              %w      Day of the week where Sunday=0 and Saturday=6              %X      Year for the week where Sunday is the first day of the week. Used with %V              %x      Year for the week where Monday is the first day of the week. Used with %v              %Y      Year as a numeric, 4-digit value              %y      Year as a numeric, 2-digit value      -- DAYOFMONTH, DAY-- DAYOFWEEK-- DAYOFYEAR, YEAR-- SECOND, MINUTE, HOUR, MONTHSELECT DAYOFMONTH(&quot;2017-06-15&quot;);------------------------------------------+--------------------------+| DAYOFMONTH(&quot;2017-06-15&quot;) |+--------------------------+|                       15 |+--------------------------+수학 함수-- COSSELECT COS(3.141592)------------------------------+---------------------+| COS(3.141592)       |+---------------------+| -0.9999999999997864 |+---------------------+-- CEIL, CEILING-- FLOOR-- ROUND-- TRUNCATESELECT CEIL(25.75);------------------------------+-------------+| CEIL(25.75) |+-------------+|          26 |+-------------+SELECT ROUND(135.375, 2);------------------------------+-------------------+| ROUND(135.375, 2) |+-------------------+|            135.38 |+-------------------+SELECT FLOOR(25.75);------------------------------+--------------+| FLOOR(25.75) |+--------------+|           25 |+--------------+SELECT TRUNCATE(135.375, 2);------------------------------+----------------------+| TRUNCATE(135.375, 2) |+----------------------+|               135.37 |+----------------------+SELECT TRUNCATE(135.375, 1);------------------------------+----------------------+| TRUNCATE(135.375, 1) |+----------------------+|                135.3 |+----------------------+SELECT TRUNCATE(135.375, 0);------------------------------+----------------------+| TRUNCATE(135.375, 0) |+----------------------+|                  135 |+----------------------+SELECT TRUNCATE(135.375, -1);------------------------------+-----------------------+| TRUNCATE(135.375, -1) |+-----------------------+|                   130 |+-----------------------+-- GREATEST-- LEASTSELECT GREATEST(3, 12, 34, 8, 25);------------------------------+----------------------------+| GREATEST(3, 12, 34, 8, 25) |+----------------------------+|                         34 |+----------------------------+SELECT LEAST(3, 12, 34, 8, 25);------------------------------+-------------------------+| LEAST(3, 12, 34, 8, 25) |+-------------------------+|                       3 |+-------------------------+-- COUNT-- MAX-- MIN-- SUM-- AVGSELECT COUNT(id) FROM ecommerce.USER WHERE country=&quot;Spain&quot;;------------------------------+-----------+| COUNT(id) |+-----------+|      3941 |+-----------+SELECT MAX(age) FROM ecommerce.USER;------------------------------+----------+| MAX(age) |+----------+|       70 |+----------+SELECT MIN(age) FROM ecommerce.USER;------------------------------+----------+| MIN(age) |+----------+|       12 |+----------+SELECT SUM(age) FROM ecommerce.USER;------------------------------+----------+| SUM(age) |+----------+|  4103528 |+----------+SELECT AVG(age) FROM ecommerce.USER------------------------------+----------+| AVG(age) |+----------+|  41.0353 |+----------+-- MOD: Return the remainder-- POW, POWERSELECT MOD(18, 4);------------------------------+------------+| MOD(18, 4) |+------------+|          2 |+------------+SELECT POW(4, 2);------------------------------+-----------+| POW(4, 2) |+-----------+|        16 |+-----------+-- SIGN: 양수면 1, 음수면 -1, 0이면 0-- ABS-- SQRTSELECT SIGN(255.5);------------------------------+-------------+| SIGN(255.5) |+-------------+|           1 |+-------------+SELECT ABS(-243.5);------------------------------+-------------+| ABS(-243.5) |+-------------+|       243.5 |+-------------+SELECT SQRT(64);------------------------------+----------+| SQRT(64) |+----------+|        8 |+----------+-- RAND: 0 &amp;lt;= x &amp;lt; 1 의 랜덤 값SELECT RAND();------------------------------+---------------------+| RAND()              |+---------------------+| 0.10379043789933476 |+---------------------+참고  MySQL 공식문서, 12.1 Built-In Function and Operator Reference  W3School, MySQL Functions  ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/mysql-series12'> <img src='/images/mysql_logo.png' alt='[MySQL] 내장 함수 모음'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series12'>[MySQL] 내장 함수 모음</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 데이터 타입",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series11",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/mysql-series11'> <img src='/images/mysql_logo.png' alt='[MySQL] 데이터 타입'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series11'>[MySQL] 데이터 타입</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-component-itertools",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/python-component-itertools'> <img src='/images/python_logo.png' alt='Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-itertools'>Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-component-collections",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/python-component-collections'> <img src='/images/python_logo.png' alt='Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-collections'>Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 백업, 복제 명령어",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series10",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/mysql-series10'> <img src='/images/mysql_logo.png' alt='[MySQL] 백업, 복제 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series10'>[MySQL] 백업, 복제 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 인덱스 명령어",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series9",
      "date"     : "Mar 29, 2021",
      "content"  : "Table of Contents  인덱스 확인  인덱스 통계 정보 확인  인덱스 생성          Create Table 내      Create Index 문      ALTER TABLE ADD INDEX 문        인덱스 삭제          보조인덱스 삭제      클러스터 인덱스 삭제        Execution Plan  Sargable Query  Full-Text Index  참고인덱스 확인SHOW INDEXFROM &amp;lt;테이블&amp;gt;  Table: 테이블의 이름을 표시함.  Non_unique: 인덱스가 중복된 값을 저장할 수 있으면 1, 저장할 수 없으면 0을 표시함.  Key_name: 인덱스의 이름을 표시하며, 인덱스가 해당 테이블의 기본 키라면 PRIMARY로 표시함.  Seq_in_index: 인덱스에서의 해당 필드의 순서를 표시함.  Column_name: 해당 필드의 이름을 표시함.  Collation: 인덱스에서 해당 필드가 정렬되는 방법을 표시함.  Cardinality: 인덱스에 저장된 유일한 값들의 수를 표시함.  Sub_part: 인덱스 접두어를 표시함.  Packed: 키가 압축되는(packed) 방법을 표시함.  Null: 해당 필드가 NULL을 저장할 수 있으면 YES를 표시하고, 저장할 수 없으면 ‘‘를 표시함.  Index_type: 인덱스에 사용되는 메소드(method)를 표시함.  Comment: 해당 필드를 설명하는 것이 아닌 인덱스에 관한 기타 정보를 표시함.  Index_comment: 인덱스에 관한 모든 기타 정보를 표시함.인덱스 통계 정보 확인SHOW TABLE STATUS LIKE &amp;lt;테이블&amp;gt;인덱스 생성Create Table 내CREATE TABLE books (  -- 같이 지정  id varchar(5) primary key, -- 기본키 지정 (클러스터 인덱스)  name varchar(20) unique, -- 인덱스 생성 (유니크 보조 인덱스)  writer varchar(20) NOT NULL,    INDEX idx_test (writer asc) -- 인덱스 생성 (보조 인덱스));Create Index 문CREATE INDEX 인덱스명 ON 테이블명 (컬럼명); -- 보조 인덱스 생성 (중복 허용) CREATE UNIQUE INDEX 인덱스명 ON 테이블명 (컬럼명); -- 유니크 보조 인덱스 생성 (중복 비허용) CREATE FULLTEXT INDEX 인덱스명 ON 테이블명 (컬럼명); -- 클러스터 인덱스 생성 CREATE UNIQUE INDEX 인덱스명 ON 테이블명 (컬럼명1, 컬러명2); -- 다중 컬럼 인덱스 생성 ANALYZE TABLE 테이블명; -- !! 생성한 인덱스 적용 !!ALTER TABLE ADD INDEX 문ALTER TABLE 테이블이름ADD INDEX 인덱스이름 (필드이름)-- 중복을 허용하는 인덱스.-- 보조 인덱스.-- 가장 느리지만 인덱스 안한 컬럼 조회하는 것보다 인덱스 붙인 컬럼 조회하는게 더 빠르다. 여러개 노멀키 를 지정할수 있다.  ALTER TABLE 테이블이름ADD UNIQUE INDEX 인덱스이름 (필드이름)-- 중복을 허용하지 않는 유일한 키. null 허용. -- 보조 인덱스.-- 고속으로 조회 가능  ALTER TABLE 테이블이름ADD PRIMARY KEY INDEX 인덱스이름 (필드이름)-- 중복되지 않은 유일한 키. null 비허용. -- 클러스터 인덱스-- where로 데이터를 조회할때 가장 고속으로 조회  ALTER TABLE 테이블이름ADD FULLTEXT INDEX 인덱스이름 (필드이름)-- 풀텍스트 인덱스-- 긴 문자열 데이터를 인덱스로 검색할 때 사용.인덱스 삭제보조인덱스 삭제ALTER TABLE 테이블이름DROP INDEX 인덱스이름클러스터 인덱스 삭제ALTER TABLE 테이블이름DROP PRIMARY KEY; -- 만일 외래키와 연결이 되어있을 경우 제약조건에 의해 삭제가 안될수 있음Execution PlanEXPLAIN SELECT ...            구분      설명              id      select 아이디로 SELECT를 구분하는 번호              select_type      select에 대한 타입              table      참조하는 테이블              type      조인 혹은 조회 타입              possible_keys      데이터를 조회할 때 DB에서 사용할 수 있는 인덱스 리스트              key      실제로 사용할 인덱스              key_len      실제로 사용할 인덱스의 길이              ref      Key 안의 인덱스와 비교하는 컬럼(상수)              rows      쿼리 실행 시 조회하는 행 (통계에 기반한 추정)              filtered      조회되지 않은 행 (통계에 기반한 추정)              extra      추가 정보      Sargable Query  인덱스를 효율적으로 사용할 수 없는 경우: 인덱스 풀 스캔하는 경우      인덱스를 효율적으로 사용하는 경우: 인덱스 레인지 스캔, 루스 인덱스 스캔을 사용 -&amp;gt; 사거블(Sargable) 하다고 함    where, order by, group by 등에는 가능한 index가 걸린 컬럼 사용.  where 절에 함수, 연산, Like(시작 부분 %)문은 사거블하지 않다!  between, like, 대소비교(&amp;gt;, &amp;lt; 등)는 범위가 크면 사거블하지 않다.  or 연산자는 필터링의 반대 개념(로우수를 늘려가는)이므로 사거블이 아니다.  offset이 길어지면 사거블하지 않는다.  범위 보다는 in 절을 사용하는 게 좋고, in 보다는 exists가 더 좋다.  꼭 필요한 경우가 아니라면 서브 쿼리보다는 조인(Join)을 사용하자.Full-Text Index  contents 컬럼에서 ‘무궁화’라는 단어를 가지는 레코드를 찾고 싶을 때  SELECT * FROM table WHERE contents LIKE &#39;%무궁화%&#39;; 라고 하면 인덱스 없이 풀 테이블 스캔 하게된다  Full-Text 인덱스를 만들면 contents 컬럼의 문자열을 파싱해서 인덱스로 저장하고, 그 인덱스를 이용해 찾아준다  여러 컬럼을 이용해 인덱스를 만들 수도 있다  인덱스를 이용해 SELECT 할 때는 MATCH와 AGAINST를 사용하면 된다SHOW INDEX FROM Product;풀 텍스트 인덱스를 하나 만들어보자.CREATE FULLTEXT INDEX &amp;lt;index-name&amp;gt;ON &amp;lt;table&amp;gt;(col1, col2, ..)CREATE FULLTEXT INDEX ft_idx_Product_name_categoryON Product(name, category);SELECT * FROM Product WHERE MATCH(name, category) AGAINST(&#39;아이폰&#39;);IN BOOLEAN MODE를 사용하면 조건을 더 디테일하게 걸 수 있다.*: partial search ex. 아이폰* -&amp;gt; 아이폰, 아이폰은, 아이폰을, .. 다 매치해준다+: required search ex. +스페이스 -&amp;gt; 아이폰 13 스페이스 그레이와 같이 스페이스가 무조건 포함되는 레코드만 매치해준다-: excluded search ex. -스페이스 -&amp;gt; 아이폰 13 그린라이트와 같이 스페이스가 포함된 레코드를 제외시켜준다SELECT * FROM Product WHERE MATCH(name, category) AGAINST(&#39;아이폰 -스페이스&#39; IN BOOLEAN MODE);단어가 어떤식으로 인덱싱되어 있는지 보기 위해 테이블을 하나 만들 수도 있다.SET GLOBAL innodb_ft_aux_table = &#39;carrot/Product&#39;; -- 스키마/테이블SET GLOBAL innodb_optimize_fulltext_only = ON;OPTIMIZE TABLE Product;SET GLOBAL innodb_optimize_fulltext_only = OFF;SELECT * FROM information_schema.innodb_ft_index_table;풀텍스트 관련 서버 설정값은 다음으로 확인할 수 있다.SHOW VARIABLES LIKE &#39;innodb_ft%&#39;;참고  인파, [MYSQL] 📚 인덱스(index) 핵심 설계 &amp;amp; 사용 문법 💯 총정리",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-29T21:01:35+09:00'>29 Mar 2021</time><a class='article__image' href='/mysql-series9'> <img src='/images/mysql_logo.png' alt='[MySQL] 인덱스 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series9'>[MySQL] 인덱스 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] 유틸리티(View, CTE, Trigger, Function, Procedure, Cursor)",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series8",
      "date"     : "Mar 28, 2021",
      "content"  : "Table of Contents  View          장점      단점      생성      수정      삭제      정보 확인        CTE(Common Table Expression)          RECURSIVE CTE      Table vs View      View vs CTE      CTE vs Subquery        Trigger          DELETE 예시      UPDATE 예시        Variables          사용자 정의 변수      지역 변수      서버 시스템 변수        Stored Function  Stored Procedure  Cursor  Script          IF      CASE      WHILE        참고View  뷰(view)는 데이터베이스에 존재하는 일종의 가상 테이블로, 실제로 데이터를 저장하고 있지는 않음  테이블은 실제 데이터를 저장, 뷰는 그저 SELECT문이 실행되고 난 후의 이미지 같은 느낌  때에 따라 서브쿼리가 이중 중첩, 삼중 중첩되는 경우, 이 때 생기는 SELECT문의 복잡성을 줄이고자 뷰를 사용  특정 역할을 하는 SELECT문들을 뷰로 저장해서, 코드 스니펫처럼 필요할 때마다 가져와서 사용  뷰는 백엔드 개발자들의 자산장점  특정 사용자에게 테이블 전체가 아닌 필요한 필드만을 보여줄 수 있음  쿼리의 재사용성  이미 실행된 서브쿼리라는 점에서 더 빠르다고 할 수 있음단점  뷰는 수정할 수 없는 경우가 많음          SUM, AVG와 같은 집계 함수가 있는 경우, UNION ALL, DISTINCT, GROUP BY가 포함된 경우        삽입, 삭제, 갱신 작업에 제한 사항이 많음생성CREATE VIEW &amp;lt;뷰 이름&amp;gt; AS  SELECT 필드1, 필드2, ...  FROM 테이블  WHERE 조건수정ALTER VIEW &amp;lt;뷰 이름&amp;gt; AS  SELECT 필드1, 필드2, ...  FROM 테이블삭제DROP VIEW &amp;lt;뷰 이름&amp;gt;정보 확인SHOW TABLESSHOW CREATE VIEW &amp;lt;뷰 이름&amp;gt;DESC &amp;lt;뷰 이름&amp;gt;SELECT * FROM information_schema.views WHERE table_schema = &amp;lt;DB&amp;gt;CTE(Common Table Expression)  메모리에 임시 결과로 올려놓고 재사용  쿼리가 실행중인 동안에만 데이터가 메모리에 올라와 있음  순차적으로 쿼리 작성 가능-- CTE 한 개 생성WITH &amp;lt;CTE 테이블명&amp;gt; AS (SELECT ...) -- CTE 여러 개 생성WITH &amp;lt;CTE 테이블명1&amp;gt; AS (SELECT ...),&amp;lt;CTE 테이블명2&amp;gt; AS (SELECT ...),RECURSIVE CTE  스스로 추가적인 Record를 생성할 수 있음  그래서 반드시 UNION 사용해야함Table vs View  테이블은 데이터와 RDBMS에 관한 정보를 영구적으로 저장하는 저장소  뷰는 어떤 쿼리에 의해 생성된 가상의 테이블. 인덱싱 해놓지 않으면 데이터베이스에 별도로 저장되지 않음View vs CTE  뷰는 데이터베이스에 존재하는 일종의 오브젝트(Object)          다른 곳에서도 쓰일 일이 있는 쿼리라면 뷰      다른 사용자들에게 데이터의 일부만 제공하고자 하는 경우 뷰        CTE는 쿼리가 실행되는 동안에만 존재하는 임시 테이블          Ad-hoc하게 사용하려는 경우 CTE      CTE vs Subquery  CTE와 서브쿼리는 성능이나 결과적인 측면에서 다른 점이 없다  차이점은 CTE가 가독성이 더 좋다는 것, CTE는 재귀적으로 호출해 완전히 새로운 테이블을 만들 수 있다TriggerCREATE Trigger &amp;lt;trigger-name&amp;gt; { BEFORE | AFTER } { INSERT | UPDATE | DELETE } { PRECEDES | FOLLOWS } &amp;lt;other-trigger-name&amp;gt; ON &amp;lt;table-name&amp;gt; FOR EACH ROWBEGIN  OLD.&amp;lt;col&amp;gt; ... -- OLD: UPDATE 이거나 DELETE일 때 적용된 레코드  NEW.&amp;lt;col&amp;gt; ... -- NEW: UPDATE 이거나 INSERT일 때 새로 추가된 레코드ENDDELETE 예시  Emp 테이블에서 직원 한 명이 나갔을 때  해당 직원 부서의 인원을 1 감소시킨다CREATE Trigger Emp_AFTER_DELETE AFTER DELETE ON Emp FOR EACH ROWBEGIN  UPDATE Dept SET empcnt = empcnt - 1  WHERE id = OLD.dept;ENDUPDATE 예시  Emp 테이블에서 개발부서 직원이 마케팅부서로 옮겼을 때  (직원의 연봉이 업데이트된 경우에는 트리거 되면 안된다 -&amp;gt; IF OLD.dept != NEW.dept THEN 추가)  개발 부서의 직원 수는 1 감소, 마케팅 부서의 직원 수는 1 증가DELIMITER $$CREATE Trigger Emp_AFTER_UPDATE AFTER UPDATE ON Emp FOR EACH ROWBEGIN  IF OLD.dept != NEW.dept THEN        UPDATE Dept SET empcnt = empcnt - 1    WHERE id = OLD.dept;    UPDATE Dept SET empcnt = empcnt + 1    WHERE id = NEW.dept;  END IF;END $$Variables  변수는 기본적으로 세션 단위로 실행사용자 정의 변수  세션 내에 있는 여러 쿼리에서 사용 가능  @ 접두사 붙여야함  변수는 크게 SET 또는 SELECT 로 선언 가능SET @x = 5; -- @x는 5로 초기화SELECT @x; -- @x는 NULL로 초기화SELECT @x := 5; -- @x는 5로 초기화지역 변수  사용자 정의 변수보다 더 범위가 좁은 변수  지역의 범위는 BEGIN 과 END 사이를 의미  (BEGIN 과 END는 프로시저나 함수 또는 트리거 안에 여러 문(Statement)을 작성하기 위한 용도로 사용된다)  DECLARE로 선언 -- 프로시저나, 함수, 트리거와 같은 스토어드 프로그램은 여러 실행문이 ;로 끝나지만 하나로 묶어줘야 한다.  -- 그래서 구분자를 //로 임시 변경DELIMITER //CREATE PROCEDURE sp_test()BEGIN    DECLARE start INT; -- 선언만 할 수도 있다    DECLARE end INT DEFAULT 10; -- 기본값을 설정할 수 있다    -- 변수에 값을 할당하는 두 가지 방법    SET start = 5; -- 선언된 지역 변수에 값을 할당    SELECT num INTO start FROM num_table WHERE id = 1; -- 이런식으로 SELECT문으로 가져온 값을 넣을 수도 있다END //서버 시스템 변수  서버에 이미 저장된 시스템 변수  @@ 접두사를 통해 값에 접근 가능  SELECT를 통해 값을 읽을 수도 있고, SET을 통해 값을 새로 할당할 수도 있다SELECT @@sort_buffer_size; -- 이미 서버에 저장된 변수기 때문에 할당없이 바로 읽어올 수 있다SET @@sort_buffer_size=1000000; -- SET을 통해 값을 새로 할당할 수도 있다Stored Function  일련의 문을 실행한 뒤 값을 리턴하고 싶은 경우  사용할 때는 내장함수 처럼 SELECT function-name(col1) FROM table-name; 이렇게 사용하면 됌DELIMITER $$CREATE FUNCTION &amp;lt;function-name&amp;gt;(&amp;lt;param&amp;gt; &amp;lt;type&amp;gt;, ..)RETURNS &amp;lt;return-type&amp;gt;BEGIN  ...  RETURN &amp;lt;return-value&amp;gt;;END $$DELIMITER $$CREATE FUNCTION ts_to_dt(_ts TIMESTAMP)RETURNS VARCHAR(31)BEGIN  RETURN DATE_FORMAT(_ts, &#39;%m/%d %H:%i&#39;);END $$ts_to_dt(CURRENT_TIMESTAMP);Stored Procedure  프로시저는 함수와 비슷  차이점은 함수는 뭔가를 변환하고 리턴하는 것에 초점  프로시저는 그냥 실행하는 것에 초점  실행 중간에 멈추고 싶으면 LEAVE 사용DROP Procedure IF EXISTS &amp;lt;procedure-name&amp;gt;;DELIMITER $$CREATE Procedure &amp;lt;procedure-name&amp;gt;([IN | OUT | INOUT] &amp;lt;param&amp;gt; &amp;lt;type&amp;gt;, ..)BEGIN  ...END $$CALL &amp;lt;procedure-name&amp;gt;([IN parameters, OUT variables]);DELIMITER $$CREATE Procedure plus_ten_procedure(IN i_1 INT, OUT o_1 INT)BEGIN  SET o_1 = i_1 + 10END $$-- CALL 을 통해 프로시저 호출CALL plus_ten_procedure(5, @a); -- @a 변수에 값이 저장됨SET @b = @a; -- 이런식으로 써도 되고,SELECT @a, @b; -- 이런식으로 써도 되고,CALL plus_ten_procedure(@a); -- 이런식으로 써도 된다CREATE Procedure &amp;lt;procedure-name&amp;gt;()stepA:BEGIN -- BEGIN ~ END 블록을 stepA 이런식으로 명명할 수 있음  ...  IF &amp;lt;condition&amp;gt; THEN    LEAVE stepA; -- LEAVE를 통해 stepA 를 종료시킬 수 있음. stepA 안에 포함된 쿼리문의 부하가 굉장히 큰 경우 유용하다  END IF;  ...ENDCursor  SELECT문으로 뽑아온 테이블의 ROW에 한 개씩 접근하고 싶을 때ScriptIFIF ... THEN  ...ELSE IF ... THEN  ...ELSE  ...END IF;CASECASE   WHEN ... THEN ...;  WHEN ... THEN ...;  ...  ELSE ...END CASE;WHILEWHILE (...) DO  ...END WHILE;참고  stackoverflow, Difference between View and table in sql  stackoverflow, CTE vs View Performance in SQL Server  LearnSQL, What’s the Difference Between SQL CTEs and Views?  stackoverflow, Is there any performance difference btw using CTE, view and subquery?  인파, [MYSQL] 📚 WITH (임시 테이블 생성)  inyong_pang, [MySQL] MySQL Variables(변수) 만들기",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-28T21:01:35+09:00'>28 Mar 2021</time><a class='article__image' href='/mysql-series8'> <img src='/images/mysql_logo.png' alt='[MySQL] 유틸리티(View, CTE, Trigger, Function, Procedure, Cursor)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series8'>[MySQL] 유틸리티(View, CTE, Trigger, Function, Procedure, Cursor)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] TCL: COMMIT, ROLLBACK, SAVEPOINT",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series7",
      "date"     : "Mar 27, 2021",
      "content"  : "Table of Contents  오토커밋 확인하기  트랜잭션 기능 활성화  트랜잭션 처리  상태 저장  트랜잭션은 Session 단위로 제어됨  DDL 작업에 대해서는 ROLLBACK 적용 안됨 (DDL은 AutoCommit)오토커밋 확인하기  AutoCommit은 DML 실행문이 자동으로 커밋되는 것을 의미  해제해야 트랜잭션 처리 가능SHOW VARIABLES LIKE &#39;%commit%&#39;트랜잭션 기능 활성화START TRANSACTIONSET AUTOCOMMIT = FALSE트랜잭션 처리-- DML 작업을 한 후UPDATE &amp;lt;테이블&amp;gt; SET &amp;lt;컬럼명&amp;gt; = &amp;lt;값&amp;gt; WHERE &amp;lt;조건&amp;gt;-- 커밋하고 싶은 경우COMMIT-- 롤백하고 싶은 경우ROLLBACK상태 저장-- DML 작업을 한 후UPDATE &amp;lt;테이블&amp;gt; SET &amp;lt;컬럼명&amp;gt; = &amp;lt;값&amp;gt; WHERE &amp;lt;조건&amp;gt;-- 이전 까지의 상태를 x로 저장SAVEPOINT x-- x 상태로 롤백ROLLBACK TO SAVEPOINT x",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-27T21:01:35+09:00'>27 Mar 2021</time><a class='article__image' href='/mysql-series7'> <img src='/images/mysql_logo.png' alt='[MySQL] TCL: COMMIT, ROLLBACK, SAVEPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series7'>[MySQL] TCL: COMMIT, ROLLBACK, SAVEPOINT</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DCL: GRANT, REVOKE",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series6",
      "date"     : "Mar 26, 2021",
      "content"  : "Table of Contents  유저 목록  유저 생성하기  권한 부여  권한 적용  권한 확인  권한 삭제  유저 삭제  현재 서버 사용중인 유저  root 비밀번호 분실시  MySQL 서버 실행유저 목록USE mysql;SELECT user FROM user;유저 생성하기-- 유저명, 호스트, 비밀번호는 따옴표로 감싸줘도 된다-- 호스트는 보통 localhost 또는 % 또는 IP주소를 사용한다. %는 모든 IP주소를 허용한다는 의미CREATE USER &amp;lt;유저명&amp;gt;@&amp;lt;호스트&amp;gt; IDENTIFIED BY &amp;lt;비밀번호&amp;gt;권한 부여-- 모든DB, 모든 테이블은 각각 *(asterisk)로 표현 가능하다-- *.*: 모든 DB의 모든 테이블에 대해 권한을 준다GRANT ALL PRIVILEGES ON &amp;lt;DB명&amp;gt;.&amp;lt;테이블명&amp;gt; TO &amp;lt;유저명&amp;gt;@&amp;lt;호스트&amp;gt;권한 적용-- 권한 적용하기FLUSH PRIVILEGES권한 확인SHOW GRANTS FOR &amp;lt;유저명&amp;gt;&amp;gt;@&amp;lt;호스트&amp;gt;권한 삭제REVOKE ALL PRIVILEGES ON &amp;lt;DB&amp;gt;.&amp;lt;테이블&amp;gt; FROM &amp;lt;유저&amp;gt;@&amp;lt;호스트&amp;gt;유저 삭제DROP USER &amp;lt;유저&amp;gt;@&amp;lt;호스트&amp;gt;현재 서버 사용중인 유저SELECT CURRENT_USER()root 비밀번호 분실시https://harrydony.tistory.com/873 참고# 안전모드로 데몬 실행 (비밀번호 없이 mysql 접속 가능)mysqld_safe --skip-grant-tables &amp;amp;mysql -u root mysqlUPDATE user SET authentication_string=null WHERE user=&#39;root&#39;;flush privileges;exit;mysql -u rootALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH caching_sha2_password BY &#39;변경할 패스워드&#39;;ps aux | grep mysqlkill -9 &#39;안전모드로 켜져있는 mysql의 PID&#39;MySQL 서버 실행https://velog.io/@taelee/mysql-%EC%84%9C%EB%B2%84-%EC%8B%9C%EC%9E%91-%EB%98%90%EB%8A%94-%EC%9E%AC%EC%8B%9C%EC%9E%91-%ED%95%98%EA%B8%B0MACMAC- mysql 서버 시작    mysql.server start- mysql 서버 중지    mysql.server stop- mysql 서버 재시작    mysql.server restartLinux- mysql 서버 시작    service mysql start    systemctl start mysql.service- mysql 서버 재시작    service mysql restart    systemctl restart mysql.service",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-26T21:01:35+09:00'>26 Mar 2021</time><a class='article__image' href='/mysql-series6'> <img src='/images/mysql_logo.png' alt='[MySQL] DCL: GRANT, REVOKE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series6'>[MySQL] DCL: GRANT, REVOKE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part8]: 파이썬의 컨텍스트 매니저",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-context-manager",
      "date"     : "Mar 23, 2021",
      "content"  : "Table of Contents  Context Manager  with statement  open()  참고Context Manager  __enter__()와 __exit__() 메서드를 구현한 객체          __enter__(): 획득한 자원에 대한 디스크립터를 리턴하는 역할      __exit__(): 획득한 자원에 대한 락을 풀어주는 역할      디스크립터(descriptor): 요청된 자원에 대해 접근하기 위해 필요한 OS에서 제공해주는 핸들        자원 관리, 예외 처리와 같은 것들이 필요한 코드를 더 깔끔하고, 버그 발생 가능성을 줄이기 위해 사용  __enter__(), __exit__()를 정의만 하는 것은 반쪽짜리 결과. 이것들이 자동적으로 호출되어야 진정한 의미가 있음 -&amp;gt; with문class MyContextManager:    def __enter__(self):        print(&quot;컨텍스트 매니저가 만들어졌습니다&quot;)        self.x = &quot;자원 관리를 필요로 하는 디스크립터&quot;        return self.x        def __exit__(self, exc_type, ex_value, ex_traceback):        print(&quot;컨텍스트 매니저를 삭제합니다&quot;)cm = MyContextManager()# 이것만으로는 아무런 의미가 없음# with문으로 자동적으로 호출되도록 해야 의미가 있음with statement  컨텍스트 매니저의 __enter__()와 __exit__()를 자동으로 호출  컨텍스트 매니저가 자신의 역할을 하기 위해 반드시 사용해야 하는 문class MyContextManager:    def __enter__(self):        print(&quot;컨텍스트 매니저가 만들어졌습니다&quot;)        self.x = &quot;자원 관리를 필요로 하는 디스크립터&quot;        return self.x        def __exit__(self, exc_type, ex_value, ex_traceback):        print(&quot;컨텍스트 매니저를 삭제합니다&quot;)with MyContextManager() as f:    print(f)--------------------------------------------------------------------컨텍스트 매니저가 만들어졌습니다자원 관리를 필요로 하는 디스크립터컨텍스트 매니저를 삭제합니다open()  open() 함수는 컨텍스트 매니저가 될 수 있는 파일 디스크립터를 리턴  파일을 다 사용하고 난 후에는 반드시 닫아주는 것이 좋음          이를 위해 직접 f.close() 하지말고 with문으로 알아서 열고 종료하도록 권장하기 위해 __enter__()와 __exit__()를 구현해놓음      with open(&#39;python_test/test.txt&#39;, &#39;w&#39;) as f:    f.write(&#39;Hello world!&#39;)# test.txtHello world!참고  Jay’s Blog, Context manager  GeeksforGeeks, with statement in Python",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-23T21:01:35+09:00'>23 Mar 2021</time><a class='article__image' href='/python-context-manager'> <img src='/images/python_logo.png' alt='Python Basic Series [Part8]: 파이썬의 컨텍스트 매니저'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-context-manager'>Python Basic Series [Part8]: 파이썬의 컨텍스트 매니저</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series5",
      "date"     : "Mar 23, 2021",
      "content"  : "Table of Contents  데이터베이스 생성  테이블 정보 확인  테이블 생성  테이블 설정 추가          테이블 이름 변경      컬럼 추가, 이름 변경      컬럼 삭제      컬럼 타입 변경      컬럼 속성 변경      테이블에 제약 사항 걸기      테이블의 제약 사항 삭제      컬럼 순서 앞으로 당기기      컬럼 순서 정하기      컬럼명 속성 동시에 바꾸기      외래키 설정      외래키 정책      외래키 삭제      외래키 파악        참고데이터베이스 생성--데이터베이스 생성CREATE DATABASE &amp;lt;DB이름&amp;gt;CREATE DATABASE IF NOT EXISTS &amp;lt;DB이름&amp;gt;--데이터베이스 지정USE &amp;lt;DB이름&amp;gt;--데이터베이스 삭제DROP DATABASE &amp;lt;DB이름&amp;gt;테이블 정보 확인DESCRIBE &amp;lt;테이블명&amp;gt;SHOW CREATE TABLE &amp;lt;테이블명&amp;gt;테이블 생성공식문서에서 [table_options]와 [partition_options] 부분만 제외하고 (create_definition)만 가지고 와봤다. 공식문서는 아래 참고란에 주석을 달아놓았다. 대략적인 공식문서를 해석하는 방법은 다음과 같다.  (): 반드시 필요한 설정  []: optional한 설정  {}: | 기호와 함께 사용되어 {A|B|C}인 경우 A 또는 B또는 C중 하나를 반드시 선택해야 한다는 의미공식문서 참고    CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    (create_definition,...)    [table_options]    [partition_options]CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    [(create_definition,...)]    [table_options]    [partition_options]    [IGNORE | REPLACE]    [AS] query_expressionCREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    { LIKE old_tbl_name | (LIKE old_tbl_name) }create_definition: {    col_name column_definition  | {INDEX | KEY} [index_name] [index_type] (key_part,...)      [index_option] ...  | {FULLTEXT | SPATIAL} [INDEX | KEY] [index_name] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] PRIMARY KEY      [index_type] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] UNIQUE [INDEX | KEY]      [index_name] [index_type] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] FOREIGN KEY      [index_name] (col_name,...)      reference_definition  | check_constraint_definition}column_definition: {    data_type [NOT NULL | NULL] [DEFAULT {literal | (expr)} ]      [VISIBLE | INVISIBLE]      [AUTO_INCREMENT] [UNIQUE [KEY]] [[PRIMARY] KEY]      [COMMENT &#39;string&#39;]      [COLLATE collation_name]      [COLUMN_FORMAT {FIXED | DYNAMIC | DEFAULT}]      [ENGINE_ATTRIBUTE [=] &#39;string&#39;]      [SECONDARY_ENGINE_ATTRIBUTE [=] &#39;string&#39;]      [STORAGE {DISK | MEMORY}]      [reference_definition]      [check_constraint_definition]  | data_type      [COLLATE collation_name]      [GENERATED ALWAYS] AS (expr)      [VIRTUAL | STORED] [NOT NULL | NULL]      [VISIBLE | INVISIBLE]      [UNIQUE [KEY]] [[PRIMARY] KEY]      [COMMENT &#39;string&#39;]      [reference_definition]      [check_constraint_definition]}data_type:    (see Chapter 11, Data Types)key_part: {col_name [(length)] | (expr)} [ASC | DESC]index_type:    USING {BTREE | HASH}index_option: {    KEY_BLOCK_SIZE [=] value  | index_type  | WITH PARSER parser_name  | COMMENT &#39;string&#39;  | {VISIBLE | INVISIBLE}  |ENGINE_ATTRIBUTE [=] &#39;string&#39;  |SECONDARY_ENGINE_ATTRIBUTE [=] &#39;string&#39;}check_constraint_definition:    [CONSTRAINT [symbol]] CHECK (expr) [[NOT] ENFORCED]reference_definition:    REFERENCES tbl_name (key_part,...)      [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]      [ON DELETE reference_option]      [ON UPDATE reference_option]reference_option:    RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT      내가 생각했을 때 자주 사용할만한 문법들을 종합했을 때 다음과 같다.CREATE TABLE [IF NOT EXISTS] &amp;lt;테이블명&amp;gt; (    &amp;lt;컬럼명&amp;gt; &amp;lt;컬럼타입&amp;gt; [UNSIGNED] [NOT NULL] [DEFAULT &amp;lt;디폴트값&amp;gt;] [AUTO_INCREMENT] [COMMENT &amp;lt;코멘트&amp;gt;],    ...    [PRIMARY KEY &amp;lt;(프라이머리 키 컬럼명)&amp;gt;,]    [FOREIGN KEY &amp;lt;(외래 키 컬럼명)&amp;gt; REFERENCES &amp;lt;부모테이블&amp;gt; &amp;lt;(부모 컬럼명)&amp;gt; [ON DELETE &amp;lt;DELETE 정책&amp;gt;] [ON UPDATE &amp;lt;UPDATE 정책&amp;gt;],])하지만 보통 테이블 생성을 위해 처음부터 이렇게 스키마를 다 정해서 만들기는 힘들고, 이렇게 꼭 할 필요도 없다.왜냐하면 뒤에서 배울 ALTER라는 것이 새로운 컬럼을 추가하거나, 외래키를 설정하거나 하는 모든 추가적인 적용을 가능하게 하기 때문이다.또한 기존 테이블의 테이블 구조로 새로운 테이블을 만들 수도 있다.CREATE TABLE &amp;lt;테이블명&amp;gt; LIKE &amp;lt;기존 테이블명&amp;gt;기존 테이블 구조에 데이터까지 가져와 새로운 테이블로 만들 수도 있다.(인덱스는 복사 안됨. 그래서 프라이머리 키, 외래키와 같은 설정은 다시 직접 해줘야함)CREATE TABLE &amp;lt;테이블명&amp;gt; AS SELECT * FROM &amp;lt;기존 테이블명&amp;gt;테이블 설정 추가테이블 이름 변경ALTER TABLE &amp;lt;기존 테이블명&amp;gt; RENAME &amp;lt;새로운 테이블명&amp;gt;컬럼 추가, 이름 변경ALTER TABLE &amp;lt;테이블 이름&amp;gt;         ADD &amp;lt;추가할 컬럼&amp;gt; CHAR(10) NULL;  ALTER TABLE &amp;lt;테이블 이름&amp;gt;RENAME COLUMN &amp;lt;원래 컬럼명&amp;gt; TO &amp;lt;바꿀 컬럼명&amp;gt;;컬럼 삭제ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP COLUMN &amp;lt;삭제할 컬럼명&amp;gt;;컬럼 타입 변경ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT;컬럼 속성 변경-- NOT NULL 속성ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT NOT NULL;-- DEFAULT 속성ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT NOT NULL DEFAULT &amp;lt;주고 싶은 default값&amp;gt;;-- DATETIME, TIMESTAMP 타입에 줄 수 있는 특별한 속성-- DEFAULT CURRENT_TIMESTAMP: 값 입력 안되면 default로 현재 시간 입력ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; DATETIME DEFAULT CURRENT_TIMESTAMP;-- 처음 default로 현재 시간 넣어주고, 데이터 갱신될 때 마다 갱신된 시간 넣어줌  ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;-- UNIQUE 속성-- UNIQUE는 PRIMARY KEY와 다르게 NULL 허용ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT UNIQUE;테이블에 제약 사항 걸기   ALTER TABLE &amp;lt;테이블 이름&amp;gt;ADD CONSTRAINT &amp;lt;제약 사항 네이밍&amp;gt; CHECK &amp;lt;제약 사항(ex. age &amp;lt; 100)&amp;gt;;테이블의 제약 사항 삭제    ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP CONSTRAINT &amp;lt;제약 사항 이름&amp;gt;;컬럼 순서 앞으로 당기기ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;컬럼명&amp;gt; INT FIRST;컬럼 순서 정하기ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;뒤에 올 컬럼명] INT AFTER &amp;lt;앞에 있는 컬럼명&amp;gt;;컬럼명 속성 동시에 바꾸기 ALTER TALBE &amp;lt;테이블 이름&amp;gt;CHANGE &amp;lt;원래 컬럼명&amp;gt; &amp;lt;바꿀 컬럼명&amp;gt; VARCHAR(10) NOT NULL;외래키 설정외래키(Foreign Key)란 한 테이블의 컬럼 중에서 다른 테이블의 특정 컬럼을 식별할 수 있는 컬럼을 말합니다. 그리고 외래키에 의해 참조당하는 테이블을 부모 테이블(parent table), 참조당하는 테이블(referenced table)이라고 합니다. 외래키를 이용하면 테이블간의 참조 무결성을 지킬 수 있습니다. 참조 무결성이란 아래 그림과 같이 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미합니다.예를 들어, 강의 평가인 review 테이블에는 ‘컴퓨터 개론’에 관한 평가 데이터가 남아있지만, 강의 목록을 나타내는 course 테이블에는 ‘컴퓨터 개론’ 과목이 삭제된다면 이상한 상황이 벌어질 것입니다. 이 때 외래키를 통해 지정해 놓으면 이런 상황을 해결할 수 있습니다.   ALTER TABLE &amp;lt;테이블 이름&amp;gt;ADD CONSTRAINT &amp;lt;제약 사항 네이밍&amp;gt;   FOREIGN KEY (자식테이블의 컬럼)    REFERENCES 부모테이블 (부모테이블의 컬럼)     ON DELETE &amp;lt;DELETE정책&amp;gt;     ON UPDATE &amp;lt;UPDATE정책&amp;gt;;외래키 정책  RESTRICT: 자식 테이블에서 삭제/갱신해야만 부모 테이블에서도 삭제/갱신 가능  CASCADE: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터도 같이 삭제/갱신  SET NULL: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터의 컬럼에 NULL 지정외래키 삭제     ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP FOREIGN KEY &amp;lt;제약 사항이 걸린 테이블&amp;gt;;외래키 파악SELECT          i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME,          k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAME     FROM information_schema.TABLE_CONSTRAINTS iLEFT JOIN information_schema.KEY_COLUMN_USAGE k    USING (CONSTRAINT_NAME)    WHERE i.CONSTRAINT_TYPE = &#39;FOREIGN KEY&#39;;참고  MySQL 공식문서13.1.20 CREATE TABLE Statement",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-23T21:01:35+09:00'>23 Mar 2021</time><a class='article__image' href='/mysql-series5'> <img src='/images/mysql_logo.png' alt='[MySQL] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series5'>[MySQL] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part7]: 파이썬의 네임스페이스와 스코프",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-namespace",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  파이썬의 네임스페이스          Built-In      Global      Enclosing과 Local        파이썬 스코프  다양한 사용 예시  참고파이썬의 네임스페이스  Python uses namespaces to implement scoping.A namespace is a mapping from names to objects파이썬의 모든 것은 객체다. 파이썬에서는 객체에 접근하기 위해 객체를 참조하는 참조 변수를 사용한다. a = [1, 2, 3]에서 a를 참조 변수라고 한다.이러한 참조 변수와 그 변수가 참조하는 객체를 매핑해놓은 일종의 딕셔너리이다.(내가 생각하기에 파이썬은 컴파일 단계에서 코드를 보고 네임스페이스를 파이썬의 딕셔너리 객체로 만든 후 런타임 단계에서 계속 업데이트 하는 것 같다)  (파이썬 공식문서: A namespace is a mapping from names to objects. Most namespaces are currently implemented as Python dictionaries)(RealPython: These have differing lifetimes. As Python executes a program, it creates namespaces as necessary and deletes them when they’re no longer needed)네임스페이스는 크게 두 가지 목적 때문에 필요하다.  독립된 공간에서는 변수명이 중복 사용될 수 있어야 한다. 변수명이 프로그램 전체에서 유일해야 한다면 변수명 짓는 것 부터 너무 어려워 질 것이다.  코드 위치마다 정의된 변수의 수명이 달라야 한다. 잠깐 사용할 목적의 변수명은 사용 후 삭제되도록 하는 것이 좋다.파이썬에는 이러한 네임스페이스를 크게 4가지 종류로 분류한다.  Built-In  Global  Enclosing  LocalBuilt-In  파이썬에서 미리 찜해놓은 키워드(return, def, class 등), 내장 함수(abs())와 같은 것들을 의미한다  파이썬 코드 어디서든 적용된다 -&amp;gt; 가장 적용되는 범위가 넓다  Built-In 네임스페이스는 인터프리터가 시작하는 시점에 만들고 종료될 때 까지 계속 가지고 있는다print(dir(__builtins__))----------------------------------------------------------------------------------------------------[&#39;ArithmeticError&#39;, &#39;AssertionError&#39;, &#39;AttributeError&#39;, ..., &#39;__build_class__&#39;, &#39;__debug__&#39;, &#39;__doc__&#39;, &#39;__import__&#39;, &#39;__loader__&#39;, &#39;__name__&#39;, &#39;__package__&#39;, &#39;__spec__&#39;, &#39;abs&#39;, &#39;all&#39;, &#39;any&#39;, &#39;ascii&#39;, &#39;bin&#39;, &#39;bool&#39;, ..., &#39;str&#39;, &#39;sum&#39;, &#39;super&#39;, &#39;tuple&#39;, &#39;type&#39;, &#39;vars&#39;, &#39;zip&#39;]Global  파이썬의 메인에서 정의된 식별자(identifier 또는 name으로 변수명, 함수명, 클래스명 등을 뜻한다)  코드 가장 바깥 부분에서 정의된 것들을 뜻함  그 밖에 import해온 모듈, 함수도 포함print(globals())----------------------------------------------------------------------------------------------------{&#39;__name__&#39;: &#39;__main__&#39;, &#39;__doc__&#39;: None, &#39;__package__&#39;: None, &#39;__loader__&#39;: &amp;lt;_frozen_importlib_external.SourceFileLoader object at 0x1051b13a0&amp;gt;, &#39;__spec__&#39;: None, &#39;__annotations__&#39;: {}, &#39;__builtins__&#39;: &amp;lt;module &#39;builtins&#39; (built-in)&amp;gt;, &#39;__file__&#39;: &#39;/Users/peter/algo_folder/python_test/test_scope.py&#39;, &#39;__cached__&#39;: None}Enclosing과 Local  Local은 블록된 코드 내에서 정의된 식별자. 블록은 클래스, 함수 등이 될 수 있음class Test:    a = 1        def __init__(self) -&amp;gt; None:        self.a = 10        print(locals())    print(locals())t = Test()----------------------------------------------------------------------------------------------------{&#39;__module__&#39;: &#39;__main__&#39;, &#39;__qualname__&#39;: &#39;Test&#39;, &#39;a&#39;: 1, &#39;__init__&#39;: &amp;lt;function Test.__init__ at 0x104da6310&amp;gt;}{&#39;self&#39;: &amp;lt;__main__.Test object at 0x104d90fd0&amp;gt;} # self.a라고 따로 안되고, self라고만 뜨네.. self안에 내포되서 안보이는 건가?  Enclosing은 Local을 감싸는 Local 네임스페이스를 의미한다 (함수 안에 정의된 함수)  (메서드를 감싸는 클래스는 Enclosing 취급을 받지 못한다. 이유는 잘 모르겠다..)class Test:    a = 1        def __init__(self) -&amp;gt; None:        nonlocal a # SyntaxError: no binding for nonlocal &#39;a&#39; found        a = 100t = Test()------------SyntaxError: no binding for nonlocal &#39;a&#39; founds = &quot;Global&quot;def outer():    s = &quot;Outer&quot;    def inner():        nonlocal s        tmp = s        s = &quot;Change to Inner&quot;        print(f&quot;{tmp} -&amp;gt; {s}&quot;)    inner()outer()------------------------Outer -&amp;gt; Change to Inners = &quot;Global&quot;def outer():    s = &quot;Outer&quot;    def inner():        global s        tmp = s        s = &quot;Change to Inner&quot;        print(f&quot;{tmp} -&amp;gt; {s}&quot;)    inner()outer()--------------------------------Global -&amp;gt; Change to Inner  함수가 한겹으로만 되어 있으면 바로 위의 Global 네임스페이스를 nonlocal로 가져올 수 있을까? -&amp;gt; 안된다  파이썬의 객체지향적 특성을 최대한 지키기 위해 파이썬은 함수 내부에서 Global 네임스페이스의 영역을 침범하는 것을 굉장히 꺼린다  그래서 nonlocal이라는 것을 python3에서 부터 지원하기 시작한 것이고,  nonlocal을 사용하는 것은 Global 네임스페이스가 침범되는 염려로부터 해방시켜 준다s = &quot;Global&quot;def func():    nonlocal s # SyntaxError: no binding for nonlocal &#39;s&#39; found    s = &quot;Func&quot;func()------------------------------------------------SyntaxError: no binding for nonlocal &#39;s&#39; found파이썬 스코프  scope of identifier is the region of a program in which that identifier has meaning. The interpreter determines this at runtime based on where the identifier definition occurs and where in the code the identifier is referenced  스코프는 어떤 식별자가 의미를 가지는 영역을 뜻한다  런타임 시점에 파이썬 인터프리터는 이 식별자가 어디서 정의되고, 어디서 참조되었는지 판단한다  인터프리터는 실행 도중 식별자를 만나면, Local -&amp;gt; Enclosing -&amp;gt; Global -&amp;gt; Built-In 순으로 네임스페이스를 탐색한다다양한 사용 예시  함수 내부에서도 외부 스코프의 네임스페이스에 정의된 참조 변수를 이용해 객체를 읽을 수 있다def func1():    print(x)x = &quot;Global variable&quot;func1()print(x)----------------------Global variableGlobal variable  불변 객체는 내부 스코프에서 외부 스코프의 객체를 수정할 수 없다  불변 객체는 항상 수정할 때 할당(Assignment)이라는 과정을 거치게 되는데, 내부 스코프에서 할당하는 식(x = “Local variable”)을 쓰면, 파이썬은 컴파일 단계에서 이를 내부 스코프의 네임스페이스에 x라는 참조 변수를 새로 만든다def func1():    x = &quot;Local variable&quot;    print(x)x = &quot;Global variable&quot;func1()print(x)----------------------Local variableGlobal variable  가변 객체는 global이나 nonlocal같은 키워드 없이도 외부 스코프의 객체를 수정할 수 있다def func1():    x.append(4)    print(x)x = [1, 2, 3]func1()print(x)----------------------[1, 2, 3, 4][1, 2, 3, 4]  외부 스코프의 불변 객체를 함수 내부에서 수정하려면 global 또는 nonlocal과 같은 키워드를 사용한다  global x라고 쓰게되면 그 다음에 나오는 할당하는 코드(x = &quot;Local variable&quot;)가 내부 스코프의 네임스페이스에 있는 참조변수 x가 아니라, 글로벌 네임스페이스(Global namespace)에 있는 참조변수 x라는 것을 파이썬 인터프리터가 알게된다def func1():    global x    x = &quot;Local variable&quot;    print(x)x = &quot;Global variable&quot;func1()print(x)----------------------Local variableLocal variableglobal nonlocal 키워드는 글로벌 네임스페이스 또는 외부 스코프의 네임스페이스의 참조 변수라는 것을 인터프리터에 알린다그러면 인터프리터는 컴파일 단계에서 내부 스코프의 네임스페이스에 새로운 참조 변수를 생성하는 것이 아니라, 외부 스코프의 네임스페이스에 있는 참조 변수로 인식한다def func1(x):    x.append(4)    print(x)x = [1, 2, 3]func1(x)print(x)----------------[1, 2, 3, 4][1, 2, 3, 4]def func1(k):    k.append(4)    print(k)x = [1, 2, 3]func1(x)print(x)----------------[1, 2, 3, 4][1, 2, 3, 4]def func1(k):    print(k)x = &quot;Global variable&quot;func1(x)print(x)---------------------Global variableGlobal variabledef func1(k):    k = &quot;Local variable&quot;    print(k)x = &quot;Global variable&quot;func1(x)print(x)---------------------Local variableGlobal variable  함수에 매개변수(Parameter)로 k를 정의하고, 함수에 인자(Argument)로 x를 넣어주면,  함수 안에서 k = &quot;Global variable&quot;를 할당하는 것과 같다def func1(k):    print(k)    k = &quot;Local variable&quot;    print(k)x = &quot;Global variable&quot;func1(x)print(x)---------------------Global variableLocal variableGlobal variable  만약 함수에서 x를 매개변수로 정의하고, global x를 선언하면, SyntaxError: name &#39;x&#39; is assigned to before global declaration라는 컴파일 에러가 뜬다def func1(x):    global x # SyntaxError: name &#39;x&#39; is assigned to before global declaration    print(x)    x = &quot;Local variable&quot;    print(x)x = &quot;Global variable&quot;func1(x)print(x)  이 문제를 해결하려면 매개변수를 x가 아닌 다른 문자로 사용하면 된다.def func1(k):    print(k)    global x     x = &quot;Local variable&quot;    print(x)x = &quot;Global variable&quot;func1(x)print(x)---------------------Global variableLocal variableLocal variable함수의 매개변수에 인자를 전달하는 것은 함수 내부에서 매개변수를 선언하는 것이다참고  Real Python, Namespaces and Scope in Python  홍러닝, 파이썬의 Namespace와 Scope",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/python-namespace'> <img src='/images/python_logo.png' alt='Python Basic Series [Part7]: 파이썬의 네임스페이스와 스코프'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-namespace'>Python Basic Series [Part7]: 파이썬의 네임스페이스와 스코프</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 파이썬의 이터레이터와 제너레이터",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-iter-gene",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  이터레이터(Iterator)          이터레이터 객체 생성      반복문        제너레이터          yield      제너레이터 코드        참고이터레이터(Iterator)  이터레이터 객체는 컨테이너 객체가 가지고 있는 원소들을 순회(iterate)하도록 해준다.  이터레이터 객체는 __iter__ 메서드와, __next__ 메서드를 가지고 있다  이터레이터 객체는 보통 이터러블(iterable)한 객체가 __iter__ 메서드를 호출함으로써 생성된다이터러블 객체와 이터레이터 객체를 정리하면 다음과 같다  iterable: an object that has the __iter__ method defined  iterator: an object that has both __iter__ and __next__ defined where __iter__ will return the iterator object and __next__ will return the next element in the iteration.참고로, 보통 많이 사용하는 list, tuple, str, dict, set 등 대부분이 __contains__와, __iter__를 가지는 이터러블 객체이다.# Container 객체Containers are any object that holds an arbitrary number of other objects. Generally, containers provide a way to access the contained objects and to iterate over them.Examples of containers include tuple, list, set, dict; these are the built-in containers. More container types are available in the collections module.그러면 이제 이터레이터 객체를 생성하고, 원소들을 순회해 보자.이터레이터 객체 생성  이터레이터 객체는 크게 두 가지 방법으로 생성한다            이터레이터 객체가 __iter__ 메서드를 호출해 자기 자신(self)을 리턴한다                  이터러블 객체가 __iter__ 메서드를 호출해 별도로 정의된 이터레이터 객체를 리턴한다.        보통 직접 클래스를 만들 때는 1번 방법을 많이 사용하고, 내장된 데이터 타입(tuple, list, str 등)은 2번을 사용한다# 직접 이터레이터 클래스 정의class SequenceOfNumbers:    def __init__(self, start=0):        self.current = start    def __iter__(self):        return self    def __next__(self):        current = self.current        self.current += 1        return currentseq = SequenceOfNumbers()seq_iterator = seq.__iter__() # 또는 iter(seq)print(seq_iterator.__next__()) # 또는 next(seq_iterator)print(seq_iterator.__next__())print(seq_iterator.__next__())---------------------------------012# 내장된 이터러블 객체x = [1, 2, 3]print(next(x)) # 참고로 이렇게 이터러블 객체 자체는 next메서드가 없기 때문에 이런식으로 순회(iterate)가 안된다--------------------------------------------TypeError: &#39;list&#39; object is not an iteratorx_iterator = x.__iter__() # 또는 iter(x)print(x_iterator.__next__()) # 또는 next(x_iterator)print(x_iterator.__next__())print(x_iterator.__next__())---------------------------------------------------123iterable객체와 iterator객체의 관계: iter(iterable객체) -&amp;gt; iterator객체반복문  반복문은 알아서 __next__함수를 호출해준다  만약 이터러블 객체(list, tuple 등)라면 알아서 __iter__함수를 호출해 이터레이터 객체로 만들어준다  순회(iterate)가 끝나면 알아서 StopIteration 예외를 발생시키고 순회를 종료해준다  참고로 반복문은 꼭 __next__만으로 순회를 하지는 않는다. __getitem__과 __len__으로도 순회할 수 있다제너레이터  제너레이터도 일종의 이터레이터  함수로 구현한다 -&amp;gt; 이터레이터보다 구현이 간단하다 (이터레이터는 클래스 정의, __iter__, __next__ 메서드를 정의해줬어야 했다)  함수가 호출되면 함수는 제너레이터(이터레이터) 객체를 반환하고, 제너레이터 객체 내부에 __iter__, __next__가 구현되어 있다  코드의 원래 흐름에서 제너레이터 객체의 __next__ 함수를 호출하면, 제너레이터 함수가 제어권을 가지고 함수 내부의 코드를 실행한다  실행 도중 yield문을 만나면 yield에 정의된 값을 __next__ 함수를 호출한 곳에 값을 반환하고, 일시적으로 제어권을 넘겨준다  yield문이 제너레이터의 핵심이다  yield문은 처음부터 모든 데이터를 메모리에 올려두지 않아도 된다 -&amp;gt; 그때 그때 필요한 값만 돌려주면 된다 -&amp;gt; 메모리 절약의 이점이 있다  하지만 어떤 시퀀스의 임의의 지점에 값을 가져오려면 시간 복잡도가 O(n)이다  제너레이터는 코루틴을 구현할 때 항상 사용된다yield  yield문이 포함된 함수는 return문을 가지는 일반적인 함수와 다른 점이 있다  일반적인 함수는 호출되면 자신의 함수에 정의된 코드를 처음부터 끝까지 실행하고, return문을 통해 값을 리턴한 후 함수의 실행을 아예 종료한다  yield문이 포함된 함수는 호출되면 함수 내부의 코드를 처음부터 실행하다가 yield문을 만나면 yield문을 통해 값을 리턴하고, 종료하지 않고 대기한다  다시 말해, 제너레이터는 실행부터 종료가 한 번에 이루어지는 것이 아니라, 계속 일시적으로 제어의 흐름이 왔다 갔다 하는 것이다제너레이터 코드def doubler_generator():    number = 2    while True:        yield number        number *= numberdoubler = doubler_generator()print (next(doubler))print (next(doubler))print (next(doubler))print (type(doubler))------------------------------------2416&amp;lt;class &#39;generator&#39;&amp;gt;def silly_generator():    yield &quot;Python&quot;    yield &quot;Rocks&quot;    yield &quot;So do you!&quot;gen = silly_generator()print (next(gen))print (next(gen))print (next(gen))print (next(gen))-----------------------------------PythonRocksSo do you!Traceback (most recent call last):  File &quot;main.py&quot;, line 15, in &amp;lt;module&amp;gt;    print (next(gen))StopIteration참고  Jay’s Blog, Iterable, Iterator, Generator, 그리고 for문  stackoverflow, What exactly are “containers” in python? (And what are all the python container types?)",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/python-iter-gene'> <img src='/images/python_logo.png' alt='Python Basic Series [Part6]: 파이썬의 이터레이터와 제너레이터'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-iter-gene'>Python Basic Series [Part6]: 파이썬의 이터레이터와 제너레이터</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-component-file_directory",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-component-file_directory'> <img src='/images/python_logo.png' alt='Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-file_directory'>Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DML(4): INSERT, UPDATE, DELETE",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series4",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents  INSERT  UPDATE  DELETE  SQL문 데이터 타입INSERT-- 데이터 추가INSERT INTO &amp;lt;테이블명&amp;gt; (col1, col2, col3, ...)     VALUES (val1, val2, val3, ...);-- 특정 col에만 데이터 넣을 수도 있다INSERT INTO &amp;lt;테이블명&amp;gt; (col1, col3)     VALUES (val1, val3);-- SET을 이용한 방법INSERT INTO &amp;lt;테이블명&amp;gt;         SET col1=val1, col2=val2;UPDATE-- 데이터 갱신UPDATE &amp;lt;테이블명&amp;gt;   SET col1 = &amp;lt;갱신 데이터&amp;gt; WHERE &amp;lt;조건&amp;gt;; -- 기존 값을 기준으로 갱신UPDATE &amp;lt;테이블명&amp;gt;   SET col1 = &amp;lt;col1 + 3&amp;gt; WHERE &amp;lt;조건&amp;gt;; DELETE-- 테이블을 사용했던 흔적이 남는다 -- (AUTO_INCREMENT된 프라이머리키가 15에서 모두 삭제돼도 다음 삽입되는 프라이머리 키가 1이 아니라 16이 됨)DELETE FROM &amp;lt;테이블명&amp;gt;      WHERE &amp;lt;조건&amp;gt;-- 테이블을 사용했던 흔적을 아예 없앤다TRUNCATE &amp;lt;테이블명&amp;gt;   WHERE &amp;lt;조건&amp;gt;SQL문 데이터 타입            종류      타입              정수형      TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT              실수형      DECIMAL, FLOAT, DOUBLE              날짜 및 시간      DATE, TIME, DATETIME, TIMESTAMP              문자열      CHAR, VARCHAR, TEXT            TINYINTsigned: -128 ~ 127unsigned: 0 ~ 255        INTsigned: -2147483648 ~ 2147483647unsigned: 0 ~ 4294967295        DECIMALDECIMAL(M, D): M은 전체 숫자의 최대 자리수, D는 소수점 자리 숫자의 최대 자리수DECIMAL(5, 2): -999.99 ~ 999.99M은 최대 65까지 가능, D는 최대 30까지 가능        FLOAT-3.4 * 10^38 ~ 3.4 * 10^38        DOUBLE-1.7 * 10^308 ~ 1.7 * 10^308FLOAT와 비교해 범위도 더 넓고, 정밀도 또한 더 높음(더 많은 소수점 자리 수 지원)        DATE날짜를 저장하는 데이터 타입’2021-03-21’ 이런 형식의 연, 월, 일 순        TIME시간을 저장하는 데이터 타입’09:27:31’ 이런 형식의 시, 분, 초        DATETIME날짜와 시간을 저장하는 데이터 타입’2021-03-21 09:30:27’ 이런 식으로 연, 월, 일, 시, 분, 초        TIMESTAMPDATETIME과 같다차이점은 TIMESTAMP는 타임 존 정보도 포함        CHARCHAR(30): 최대 30자의 문자열을 저장 (0~255까지 가능)차지하는 용량이 항상 숫자값에 고정됨데이터의 길이가 크게 변하지 않는 상황에 적합        VARCHARVARCHAR(30): 최대 30자의 문자열을 저장 (0~65536까지 가능)차지하는 용량이 가변적. 30이어도 그 이하의 길이면 용량도 적게 차지함해당 값의 사이즈를 나타내는 부분(1byte 또는 2byte)이 저장 용량에 추가데이터 길이가 크게 들쑥날쑥해지는 경우에 적합        TEXT문자열이 아주 긴 상황에 적합  ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/mysql-series4'> <img src='/images/mysql_logo.png' alt='[MySQL] DML(4): INSERT, UPDATE, DELETE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series4'>[MySQL] DML(4): INSERT, UPDATE, DELETE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-data-type-dictionary",
      "date"     : "Mar 16, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-16T21:01:35+09:00'>16 Mar 2021</time><a class='article__image' href='/python-data-type-dictionary'> <img src='/images/python_logo.png' alt='Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-dictionary'>Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part3]: 파이썬 리스트 자료형",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-data-type-list",
      "date"     : "Mar 15, 2021",
      "content"  : "Table of Contents  리스트          1. 리스트 자료형의 특징      2. 리스트의 장점      3. 리스트 생성      4. 인덱싱, 슬라이싱      5. 리스트 메소드                  5-1 .append(), .extend(), .insert(), .copy()          5-2 .pop(), .remove(), .clear()          5-3 .sort(), .reverse()          5-4 .count(), .index()                    6. 리스트에서 주목할 만한 것들                  6-1 List &amp;amp; Range          6-2 리스트 표현식 (List comprehension)          6-3 리스트와 문자열 넘나들기          6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)                    7. 리스트 주요 연산들의 시간 복잡도      리스트파이썬 언어는 개발자의 편의성, 생산성, 가독성에 가장 초점을 맞춘 언어입니다. 그래서 파이썬에서는 기존 언어에서 동적 배열이라는 기초 자료형의 불편함을 개선해 리스트라는 파이썬만의 강력한 자료형을 제공합니다.1. 리스트 자료형의 특징  파이썬에서 가장 자주 사용하는 자료형  원소들의 순서가 있는 시퀀스  원소들의 변경이 가능 (Mutable)  다양한 타입의 원소 저장 가능  동적배열로 구현됨2. 리스트의 장점  임의의 원소에 O(1) 접근 가능: 이것은 기존 동적배열이 제공해주는 기능입니다  다양한 타입의 원소 저장 가능: 리스트가 값이 아닌 값을 가진 객체의 주소를 동적배열로 저장하고 있기 때문입니다.  왠만한 추상 자료형은 리스트로 구현 가능: 리스트 자료형이 가지고 있는 많은 메소드로 스택, 큐, 트리, 그래프 등 거의 모든 추상 자료형을 구현할 수 있습니다.3. 리스트 생성리스트는 여러 가지 자료형을 가질 수 있는 시퀀스형 자료형입니다.또한 값을 변경할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = [3.1, 2.5, 7]&amp;gt;&amp;gt;&amp;gt; c = [&quot;Hello&quot;, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; d = [1, 4.5, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; a[0] = 100&amp;gt;&amp;gt;&amp;gt;a[100, 2, 3, 4]🔔 리스트를 곱하거나 더하면 값이 반복되거나 추가됩니다&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a + [5][1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a + 5 -&amp;gt; Error&amp;gt;&amp;gt;&amp;gt; a * 2[1, 2, 3, 4, 1, 2, 3, 4]4. 인덱싱, 슬라이싱이번에는 위에서 만들어진 리스트 데이터를 가지고 원하는 부분만 가져올 수 있도록 해주는 인덱싱, 슬라이싱에 대해 알아보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&quot;&quot;&quot;인덱싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0]1&amp;gt;&amp;gt;&amp;gt; a[2]3&amp;gt;&amp;gt;&amp;gt; a[4] = 10 -&amp;gt; Error  (a[50] = 10 이런식으로 하면 그 사이의 인덱스에 값을 표시할 수 없어서 무조건 차례대로 값을 채워넣어야 함 -&amp;gt; 더하기 또는 append 메소드)&quot;&quot;&quot;슬라이싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0:3] # 0에서 부터 3앞까지 -&amp;gt; 인덱스 0~2[1, 2, 3]&amp;gt;&amp;gt;&amp;gt; a[:][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::-1] #처음부터 끝까지 거꾸로 슬라이싱 (중요)[4, 3, 2, 1]5. 리스트 메소드리스트 데이터는 프로그래밍을 하다보면 정말 자주 만나게 되는 자료형 중에 하나입니다.그렇기 때문에 문자열 객체의 메소드를 잘 활용할 줄 아는 것이 굉장히 중요합니다.먼저 어떤 메소드가 있는지 확인해 보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__add__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;,  &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__imul__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;,   &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;,    &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__rmul__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;,    &#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;]실제로 코딩을 하실 때는 기억이 안나면 그 때마다 dir() 함수를 사용해 어떤게 있는지 살펴보면 됩니다.5-1 .append(), .extend(), .insert(), .copy().append()리스트 맨 끝에 인자로 넣어준 값 하나를 추가해준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.append(100)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100]여러 개를 추가하고 싶어서 인자로 값을 여러 개 준다면? -&amp;gt; 에러가 난다그래서 [100, 101, 102] 이런 식으로 추가하면? -&amp;gt; 에러는 안나지만 리스트가 추가되어 원하는 모습과는 다르다..extend()iterable한 객체를 인자로 넣어주면 그 안의 원소들이 모두 차례대로 리스트에 추가된다.&amp;gt;&amp;gt;&amp;gt; a.extend([101, 102, 103]) # 리스트와 같은 iterable한 객체를 인자로 주어야 한다.&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100, 101, 102, 103]이제는 맨 뒤가 아니라 원하는 인덱스에 값을 추가(교체)하고 싶다..insert()인자로 인덱스와 값을 넣어주면 인덱스에 값을 넣어준다.인덱스에 이미 값이 있으면 바꿔주고 리스트 길이보다 인덱스 값이 크거나 같으면 리스트 맨 뒤에 값을 넣어준다.-&amp;gt; 길이 신경쓰지 않고 해줘도 오류는 안난다. (내가 원하는 인덱스에 값이 들어가지 않을 수도 있지만)&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1, 10)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1000, 7)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4, 7].copy()객체와 똑같은 값을 가지는 리스트를 복사한다. 변수를 지정해주면 새로운 메모리에 저장된다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = a.copy()&amp;gt;&amp;gt;&amp;gt; b.append(5)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b[1, 2, 3, 4, 5]5-2 .pop(), .remove(), .clear().pop()리스트의 가장 끝에 있는 원소를 뽑아 리턴해준다.&amp;gt;&amp;gt;&amp;gt; a = [&#39;banana&#39;, &#39;lemon&#39;, &#39;apple&#39;]&amp;gt;&amp;gt;&amp;gt; a.pop()&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;banana&#39;, &#39;lemon&#39;].remove()인자로 받은 값은 값을 제거해준다.&amp;gt;&amp;gt;&amp;gt; a.remove(2)&amp;gt;&amp;gt;&amp;gt; a[1, 3, 4].clear()리스트를 싹 비운다.&amp;gt;&amp;gt;&amp;gt; a.clear()&amp;gt;&amp;gt;&amp;gt; a[]5-3 .sort(), .reverse().sort()  리스트를 작은 값부터 순서대로 정렬해준다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a = [&#39;안녕&#39;, &#39;Hello&#39;, &#39;Hi&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a.sort(key=len)&amp;gt;&amp;gt;&amp;gt; a[&#39;안녕&#39;, &#39;Hi&#39;, &#39;Hello&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: x**2)&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]# 같은 제곱값에 대해 양수가 먼저 나오게 하려면 양수가 논리연산 시 False가 되면 되므로 기준을 0보다 작은지로 하면 된다 &amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: (x**2, x&amp;lt;=0))&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]&amp;gt;&amp;gt;&amp;gt; a =[False, True, False, True, True, False]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[False, False, False, True, True, True]🔔 sorted() 함수  sorted() 함수는 정렬된 값을 리턴해줄 뿐 인자로 받은 리스트를 정렬하지는 않는다.  또 한가지 중요한 특징은 sorted()함수는 리스트 뿐 아니라 모든 iterable한 값들을 정렬시켜 준다는 것입니다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted(a)[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a[3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted({1: &#39;D&#39;, 2: &#39;B&#39;, 3: &#39;B&#39;, 4: &#39;E&#39;, 5: &#39;A&#39;})[1, 2, 3, 4, 5]🔔 .sort()와 sorted() 모두 key, reverse 인자를 갖는다  key: 정렬을 목적으로 하는 함수를 값으로 넣는다. lambda를 이용할 수 있다. key 매개 변수의 값은 단일 인자를 취하고 정렬 목적으로 사용할 키를 반환하는 함수(또는 다른 콜러블)여야 합니다.  reverse: bool값을 넣는다. 기본값은 reverse=False(오름차순)이다..reverse()리스트의 원소의 순서를 뒤집어준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.reverse()&amp;gt;&amp;gt;&amp;gt; a[4, 3, 2, 1]🔔 reversed() 함수뒤집은 값을 리턴해줄 뿐 인자로 받은 리스트는 그대로다.🔔 문자열을 뒤집는 방법a.reverse()a = list(reversed(a))a = a[::-1]5-4 .count(), .index().count()인자로 받은 값이 등장하는 횟수를 리턴해준다.a = [1, 1, 1, 2, 3, 4]a.count(1)---------------------3.index()인자로 받은 값의 인덱스를 리턴해준다.a = [1, 3, 5, 7]a.index(7)--------------------36. 리스트에서 주목할 만한 것들6-1 List &amp;amp; Range1부터 1000까지 값을 하나씩 출력하는 코드를 짠다고 할 때for i in [1, 2, 3, 4, 5, 6, 7, 8, ..., 1000]:  print(i)로 하게 되면 위의 코드를 실행하기 위해 1000개의 요소를 적어서 리스트를 만드는 것은 너무 비효율적입니다.이를 개선시키는 방법으로for i in range(1000):  print(i)이렇게 해주면 훨씬 짧고 간결한 코드를 작성할 수 있습니다.range(start, end, step)range(1000) =&amp;gt; 0, 1, 2, 3, ..., 999range(1, 1000) =&amp;gt; 1, 2, 3, ..., 999range(1, 1000, 2) =&amp;gt; 1, 3, 5, 7, ..., 9996-2 리스트 표현식 (List comprehension)&amp;gt;&amp;gt;&amp;gt; a = []&amp;gt;&amp;gt;&amp;gt; for i in range(100):        if i % 3 == 0 and i % 5 == 0:          a.append(i)&amp;gt;&amp;gt;&amp;gt; [i for i in range(100) if i % 3 == 0 and i % 5 == 0]6-3 리스트와 문자열 넘나들기문자열을 리스트로 바꿔야 하는 경우문자열은 값을 바꿀 수가 없기 때문에 예를 들어 스펠링을 고치기 위해서는리스트로 바꿔서 고친 후 다시 문자열로 변환해줘야 한다.&amp;gt;&amp;gt;&amp;gt; name = &#39;kinziont&#39;&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39; -&amp;gt; 에러&amp;gt;&amp;gt;&amp;gt; name = list(name)&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39;&amp;gt;&amp;gt;&amp;gt; name[&#39;k&#39;, &#39;i&#39;, &#39;m&#39;, &#39;z&#39;, &#39;i&#39;, &#39;o&#39;, &#39;n&#39;, &#39;t&#39;]&amp;gt;&amp;gt;&amp;gt; name = str(name)&amp;gt;&amp;gt;&amp;gt; name&#39;kimziont&#39;문자열 데이터를 단어 단위 또는 문장 단위로 토크나이징하기 위해 문자열 메소드인 .split()을 쓰면자동으로 리스트로 변환된다.6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)a = [1, 2, 3, 4] # 1*4 vectorb = [[1, 2], [3, 4]] # 2*2 matrixc = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] # 2*2*2 tensora[0] -&amp;gt; 1b[0] -&amp;gt; [1, 2]c[0] -&amp;gt; [[1, 2], [3, 4]]c[0][1] -&amp;gt; [3, 4]c[0][1][0] -&amp;gt; 37. 리스트 주요 연산들의 시간 복잡도            연산      시간 복잡도      설명              len(a)      O(1)      전체 요소의 개수를 리턴              a[i]      O(1)      인덱스 i의 요소를 가져온다              a[i:j]      O(k)      객체 k개에 대한 조회가 필요하므로 O(k)이다              x in a      O(n)      정렬되어 있지 않은 a 이므로 순차 탐색              a.append(x)      O(1)      동적배열의 특징              a.pop(x)      O(1)      동적배열의 특징              a.pop(0)      O(n)      배열의 특성상 앞의 원소가 추가/삭제 되면 그 뒤의 모든 원소들의 이동이 발생              del a[i]      O(n)      i에 따라 다르다. 최악의 경우 O(n)이다              a.sort()      O(nlogn)      파이썬에서는 팀소트(Timsort)를 사용              min(a), max(a)      O(n)      최소, 최대값 찾기 위해서는 선형 탐색 해야함              a.reverse()      O(n)      선형 이동하면서 처음과 끝 원소 바꾼다      ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-15T21:01:35+09:00'>15 Mar 2021</time><a class='article__image' href='/python-data-type-list'> <img src='/images/python_logo.png' alt='Python Basic Series [Part3]: 파이썬 리스트 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-list'>Python Basic Series [Part3]: 파이썬 리스트 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part2]: 파이썬 숫자 자료형",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-data-type-number",
      "date"     : "Mar 13, 2021",
      "content"  : "Table of Contents  1. 숫자 자료형의 종류  2. 파이썬의 특별한 점  3. 2진법, 8진법, 16진법  4. 부동 소수점 연산 오류  5. 숫자 자료형 관련 메소드1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-13T21:01:35+09:00'>13 Mar 2021</time><a class='article__image' href='/python-data-type-number'> <img src='/images/python_logo.png' alt='Python Basic Series [Part2]: 파이썬 숫자 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-number'>Python Basic Series [Part2]: 파이썬 숫자 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DML(3): SELECT 중급 WINDOW",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series3",
      "date"     : "Mar 13, 2021",
      "content"  : "Table of Contents  윈도우 함수  OVER절  윈도우 함수 예시          ROW_NUMBER()      RANK()      DENSE_RANK()      LEAD()      LAG()      윈도우 함수  특정 범위마다 함수를 적용하는 것을 윈도우 함수라고 함  MySQL에서 제공하는 윈도우 함수라고 따로 정의해둔 윈도우 함수 묶음이 있음  집계 함수도 OVER절을 이용해 범위를 정의하면 윈도우 함수로 사용할 수 있음(Most aggregate functions also can be used as window functions, MySQL 공식문서)  사용 방법: [윈도우 함수] + OVER or [집계 함수] + OVER  범위마다 함수를 적용한다는 점에서 GROUP BY와 비슷하게 느껴지지만, GROUP BY는 집계된 결과를 테이블로 보여주는 반면, 윈도우 함수는 집계된 결과를 기존 테이블에 하나의 열로 추가하여 결과를 볼 수 있음OVER절  윈도우 함수는 항상 OVER절과 함께 사용됨  2018년에 MySQL에 처음으로 윈도우 함수가 도입되었고, 윈도우 함수는 OVER절을 통해 접근할 수 있음  윈도우 함수는 set of rows에 특별한 함수 또는 계산을 위한 용도. 이러한 set of rows를 window라고 함  이러한 윈도우가 OVER절에 의해 정의됨  OVER clause which has three possible elements: partition definition, order definition, and frame definition.    {window_function(expression)] | [aggregation_function(expression)} OVER (  [partition_defintion] [order_definition] [frame_definition])        PARTITION BY: 윈도우 범위 결정  ORDER BY: 정렬하여 계산윈도우 함수 예시ROW_NUMBER()  행 번호를 매길 수 있음RANK()  윈도우 내에서 순위를 매길 수 있음  공동 2등이 2명이면 다음은 4등 -&amp;gt; 1, 2, 2, 4DENSE_RANK()  윈도우 내에서 순위를 매길 수 있음  공동 2등이 2명 있더라도 다음은 3등 -&amp;gt; 1, 2, 2, 3LEAD()  현재 행에서 다음에 있는 행들과 관계를 맺을 수 있음LAG()  현재 행 앞에 있는 행들과 관계를 맺을 수 있음",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-13T21:01:35+09:00'>13 Mar 2021</time><a class='article__image' href='/mysql-series3'> <img src='/images/mysql_logo.png' alt='[MySQL] DML(3): SELECT 중급 WINDOW'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series3'>[MySQL] DML(3): SELECT 중급 WINDOW</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part1]: 파이썬에서 데이터의 특성",
      "category" : "language",
      "tags"     : "python",
      "url"      : "/python-data-type-intro",
      "date"     : "Mar 12, 2021",
      "content"  : "Table of Contents  파이썬 데이터는 객체다          원시타입과 객체        타입  가변성  참조  복사          Alias      Shallow Copy      Deep Copy        참고파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.원시타입과 객체C나 자바 같은 언어는 기본적으로 원시 타입을 제공합니다. 원시 타입은 메모리에 정확하게 타입 크기만큼의 공간을 할당하고 그 공간을 오로지 값으로 채워넣습니다. 배열이라면 요소들이 연속된 순서로 메모리에 배치될 것입니다.객체는 단순히 값 뿐만 아니라 여러 가지 정보를 함께 저장하고, 이를 이용해 여러 가지 작업(비트조작, 시프팅 등)을 수행할 수 있게됩니다. 하지만 이로 인해 메모리 점유율이 늘어나게 되고 계산 속도 또한 감소하게 되는 단점이 있습니다.타입            이름      타입      가변              불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)a = &quot;5&quot;print(type(a))print(id(a))a = int(a)print(type(a))print(id(a))------------------&amp;lt;class &#39;str&#39;&amp;gt;139861785283696&amp;lt;class &#39;int&#39;&amp;gt;139861784516640참조a = 5변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 a라는 변수에 5라는 값을 담는 것이 아니라 a라는 이름이 Int 객체 5를 참조하는 것 입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다. 차이는 객체가 불변이냐 가변이냐의 차이일 뿐입니다.a라는 이름과 객체의 메모리 주소의 매핑 관계는 네임스페이스에 키-밸류 형태로 저장되는데 이 때의 네임스페이스는 메모리 상에서 코드 영역 또는 데이터 영역에 저장된다고 합니다. (스택이나 힙 영역은 아니라고 함, 참고)예를 들어, 왼쪽 그림에서 a가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 a는 새로운 값을 참조합니다.반면 오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사복사와 관련해서 진짜 복사(copy) 기능을 하는 것이 있고 흉내만 내는 것도 있습니다. 대표적으로 3가지 케이스가 있는데 하나씩 살펴보도록 하겠습니다.복사란 기존의 값과 같은 값을 가지는 변수를 하나 더 생성하며 각각의 변수는 독립적이어야 한다Aliasa = [1, 2, 3]b = a대입 연산자(=)를 이용한 경우를 alias(별칭)라고 합니다. 말 그대로 ‘[1, 2, 3]이라는 리스트 객체가 a라는 이름을 가지고 있었는데 b라는 이름을 하나 더 가지게 되었다.’ 정도로 이해할 수 있습니다. 이렇게 되면 a가 가르키는 [1, 2, 3]이 바뀌게 되면 b도 따라서 바뀌게 됩니다.a[2] = 100a -&amp;gt; [1, 2, 100]b -&amp;gt; [1, 2, 100](예시: 가수 ‘비’가 있습니다. ‘비’의 본명은 ‘정지훈’입니다. 만약 ‘비’가 머리를 잘랐다면 ‘정지훈’의 머리도 잘립니다.)Shallow Copy리스트의 슬라이싱 기능인 :를 이용하면 alias보다 더 복사같이 느껴집니다. 이를 얕은 복사(Shallow Copy)라고 합니다. 얕은 복사는 어떤 경우에는 정말 복사의 기능을 합니다.a = [1, 2, 3]c = a[:]a[2] = 100a -&amp;gt; [1, 2, 100]c -&amp;gt; [1, 2, 3]이렇게 봤을 때는 충분히 복사의 기능을 하고 있습니다. 하지만 변경하고자 하는 요소가 가변 객체이면 진짜 복사가 아니었다는 것이 드러나게 됩니다.a = [1, 2, [3, 4, 5]]c = a[:]a[2][2] = 100a -&amp;gt; [1, 2, [3, 4, 100]]c -&amp;gt; [1, 2, [3, 4, 100]]새로운 리스트 객체를 생성하긴 하지만 원래의 리스트 객체와 같은 ob_item(요소들의 포인터목록)을 가지고 생성되기 때문에, 요소가 가변 객체일 경우 따라서 변하게 됩니다.Deep Copy파이썬의 내장 모듈인 copy를 사용하면 어떤 상황에서도 복사를 제공합니다. 이를 깊은 복사(Deep Copy)라고 합니다. 깊은 복사는 아예 요소 자체를 새로 생성하기 때문에 ob_item도 다른 값을 가지는, 다시 말해서 완전히 같은 값을 새로운 메모리에 할당한 복사가 일어나게 됩니다.깊은 복사는 어떠한 상황에서도 복사를 보장하기 때문에 안정된 코드를 제공하지만, 메모리 낭비가 발생할 수 있습니다.import copya = [1, 2, 3]d = copy.deepcopy(a)참고  파이썬 알고리즘 인터뷰 책  YABOONG: 자바 메모리 관리 - 스택 &amp;amp; 힙  nina, memory-management-in-python-the-basics",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-12T21:01:35+09:00'>12 Mar 2021</time><a class='article__image' href='/python-data-type-intro'> <img src='/images/python_logo.png' alt='Python Basic Series [Part1]: 파이썬에서 데이터의 특성'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-intro'>Python Basic Series [Part1]: 파이썬에서 데이터의 특성</a> </h2><p class='article__excerpt'>이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DML(2): SELECT 중급 JOIN, SUBQUERY",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series2",
      "date"     : "Mar 12, 2021",
      "content"  : "Table of Contents  JOIN Statement  INNER JOIN  OUTER JOIN  JOIN 최적화  JOIN 동작 원리          중첩 루프 조인      해시 조인        UNION  Subquery          서브쿼리를 사용할 수 있는 위치        참고JOIN Statement여러 테이블을 합쳐서 하나의 테이블인 것처럼 보는 행위를 ‘조인(join)’이라고 합니다. 실무에서는 이 조인을 잘해야 제대로된 데이터 분석을 할 수 있습니다. 조인은 SQL을 얼마나 잘 쓰는지 판단하는 척도 중 하나일만큼 정말 중요한 개념입니다.INNER JOIN  JOIN의 조건(USING 또는 ON)을 만족하는 레코드가 두 테이블에 모두 있는 경우에만 조인된 결과를 레코드로 제공  MySQL에서는 INNER JOIN, JOIN, 콤마(,) 모두 INNER JOIN을 뜻함  조인 조건(USING, ON)없이 사용하는 경우를 Cartesian Product(곱집합) 이라고 함  ex. A: {1, 2, 3}, B: {x, y}일 때 FROM A, B를 하면 -&amp;gt; {[1, x], [1, y], [2, x], [2, y], [3, x], [3, y]}  INNER JOIN은 인덱스 유무에 따라 옵티마이저가 알아서 기준 테이블(Driving table)과 대상 테이블(Driven table)을 정함  드리븐 테이블이 성능 부하가 많은 편이라 인덱스가 있는 테이블을 드리븐 테이블로 사용해, 최대한 드리븐 테이블의 성능 부하를 낮춘다  핵심은 INNER JOIN은 MySQL의 성능 최적화 측면에서 제약 조건을 하나 줄여주는 셈 -&amp;gt; OUTER JOIN 보다 성능이 좋다-- 정석적인 작성법SELECT u.userid, u.name FROM usertbl AS u INNER JOIN buytbl AS b ON u.userid=b.userid WHERE u.userid=&quot;111&quot;-- 축약 작성법SELECT u.userid, u.name FROM usertbl u, buytbl b WHERE u.userid=b.userid AND u.userid=&quot;111&quot;/*내 생각에 위의 방법은 딱 교집합인 결과에서 WHERE 조건절로 필터링아래 방법은 곱집합으로 len(u) * len(b)만큼의 결과에서 WHERE 조건절 사용 -&amp;gt; 훨씬 느릴것 같다*/참고로 JOIN이 있는 쿼리문의 실행 순서는 다음과 같다.1. FROM2. ON3. JOIN4. WHERE5. GROUP BY6. HAVING7. SELECT8. ORDER BY9. LIMITOUTER JOIN  LEFT JOIN, RIGHT JOIN  LEFT는 첫 번째 테이블을 기준으로 두 번째 테이블을 조인, RIGHT는 두 번째 테이블이 기준  그래서 OUTER JOIN은 순서가 중요 -&amp;gt; 결과 자체의 측면과 성능적 측면 두 가지가 있음          우선 성능을 따지기 전에 결과 자체가 우리가 원하는 결과가 나와야 함 -&amp;gt; 모든 레코드가 나와야 하는 테이블을 기준 테이블      순서에 상관없이 결과가 같다고 판단되는 경우 성능을 따져야함 -&amp;gt; 인덱스가 있는 테이블을 드리븐 테이블로 쓰자(LEFT면 두 번째)        만약 OUTER JOIN, INNER JOIN 어떤 것을 써도 된다면 옵티마이저가 드라이빙 테이블을 선택할 수 있는 INNER JOIN이 나음SELECT STUDENT.NAME, PROFESSOR.NAME FROM STUDENT LEFT OUTER JOIN PROFESSORON STUDENT.PID = PROFESSOR.ID WHERE GRADE = 1JOIN 최적화인덱스 레인지 스캔은 인덱스를 탐색(Index Seek)하는 단계와 인덱스를 스캔(Index Scan)하는 과정으로 구분해 볼 수 있다. 일반적으로 인덱스를 이용해서 쿼리하는 작업에서는 가져오는 레코드의 건수가 소량(전체 데이터 크기의 20% 이내)이기 때문에 인덱스 스캔 작업은 부하가 작고, 특정 인덱스 키를 찾는 인덱스 탐색 작업이 부하가 높은 편이다.JOIN 작업에서 드라이빙 테이블을 읽을 때는 인덱스 탐색 작업을 단 한 번만 수행하고, 그 이후부터는 스캔만 실행하면 된다.하지만 드리븐 테이블에서는 인덱스 탐색 작업과 스캔 작업을 드라이빙 테이블에서 읽은 레코드 건수만큼 반복한다.드라이빙 테이블과 드리븐 테이블이 1:1 조인되더라도 드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지한다.그래서 옵티마이저는 항상 드라이빙 테이블이 아니라 드리븐 테이블을 최적으로 읽을 수 있게 실행 계획을 수립한다.SELECT *FROM employees e, dept_emp deWHERE e.emp_no=de.emp_no여기서 각 테이블의 emp_no 컬럼에 인덱스가 있을 때와 없을 때 조인 순서가 어떻게 달라지는 한 번 살펴보자.  두 컬럼 모두 인덱스가 있는 경우          어느 테이블을 드라이빙으로 선택하든 인덱스를 이용해 드리븐 테이블의 검색 작업을 빠르게 처리할 수 있다      보통의 경우 어느 쪽 테이블이 드라이빙 테이블이 되든 옵티마이저가 선택하는 방법이 최적일 때가 많다        employees 테이블에만 인덱스가 있는경우          이 때는 employees 테이블을 드리븐 테이블로 선택한다      드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지하기 때문에 드리븐 테이블에서 인덱스를 활용하는 것이 중요한다      INNER JOIN은 조인 대상 테이블 모두에 해당하는 레코드만 반환한다. 이같은 특성 때문에 OUTER JOIN으로만 조인을 실행하는 쿼리들도 자주 보인다. 하지만 대개의 경우 OUTER JOIN은 대상 테이블들의 데이터가 일관되지 않은 경우에만 필요하다.MySQL 옵티마이저는 OUTER JOIN시 조인 되는 테이블(FROM A LEFT JOIN B에서 B)을 드라이빙 테이블로 선택하지 못하기 때문에 무조건 앞에 등장하는 테이블을 드라이빙 테이블로 선택한다. 그 결과 인덱스 유무에 따라 조인 순서를 변경함으로써 얻게 되는 최적화의 이점을 얻지 못하기 때문에 쿼리 성능이 나빠질 수 있다. 그래서 꼭 필요한 경우가 아니라면 INNER JOIN을 사용하는 것이 쿼리의 성능에 도움이 된다.JOIN의 순서  INNER JOIN인 경우          어차피 A and B and C 이기 때문에 A JOIN B JOIN C이든 B JOIN A JOIN C이든 같다.        LEFT JOIN의 경우 결과도 성능도 달라진다.          일단 가장 먼저 등장하는 테이블이 드라이빙 테이블이 된다 -&amp;gt; 이 말은 뒤에 따라오는 테이블은 드리븐 테이블이 된다는 말이다 -&amp;gt; 드리븐 테이블은 인덱스가 없으면 성능이 떨어진다 -&amp;gt; 뒤에 조인되는 테이블의 인덱스 유무에 따라 쿼리 성능이 달라진다      결과 자체도 맨 앞에 등장하는 테이블의 모든 레코드가 기준이 되기 때문에 순서에 따라 달라진다        INNER JOIN과 OUTER JOIN이 결합되는 경우          가능하다면 INNER JOIN이 앞에 오도록 하는 것이 좋다      JOIN 동작 원리중첩 루프 조인  Nested Loop Join  중첩 for문과 비슷한 원리  조인의 시간복잡도가 O(n^2) 이어서 대용량 테이블에서는 사용하지 않음해시 조인  Hash Join  동등 조건에서만 사용 가능  시간복잡도가 (n)  해시 조인은 크게 빌드 단계와 프로브 단계로 나뉨          빌드 단계: 바이트 크기가 더 작은 테이블을 해시 테이블로 만들어 메모리에 올림 -&amp;gt; O(n)      (빌드 단계에서 사용하는 메모리의 크기는 join_buffer_size에 의해 조절)      프로브 단계: 다른 테이블의 레코드를 하나씩 돌며 해시 테이블에서 값을 가져옴 -&amp;gt; O(n)      UNION  같은 구조를 가지는 테이블을 합치는 것  UNION은 두 테이블이 같은 데이터를 가질 경우 한 개만 최종 테이블에 반영되도록 함 (중복 허용 X)  UNION ALL은 두 테이블이 같은 레코드를 가지더라도 합칠 경우 둘 다 최종 테이블에 반영(중복 허용)SELECT 필드이름 FROM 테이블이름UNIONSELECT 필드이름 FROM 테이블이름Subquery  서브쿼리(subquery)는 다른 쿼리 내부에 포함되어 있는 SELETE 문을 의미  서브쿼리는 괄호()로 감싸서 표현  메인쿼리 실행 중간에 서브쿼리 실행. 서브쿼리 종료 후, 메인쿼리도 실행 모두 마치고 종료  메인쿼리 실행 되면 먼저 FROM으로 메인 테이블 불러오기 때문에, 서브쿼리는 메인쿼리의 컬럼 사용 가능  서브쿼리는 가독성이 좋다는 장점이 있지만 JOIN 보다 성능이 느림 -&amp;gt; 둘다 가능한 경우 JOIN 사용  (최신 MySQL은 내부적으로 서브쿼리문을 실행할 때 조인문으로 변환)서브쿼리를 사용할 수 있는 위치SELECT FROMWHEREHAVINGINSERTUPDATE참고  인파, [MYSQL] 📚 서브쿼리 개념 &amp;amp; 문법 💯 정리  Navicat, Joins versus Subqueries: Which Is Faster?",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-12T21:01:35+09:00'>12 Mar 2021</time><a class='article__image' href='/mysql-series2'> <img src='/images/mysql_logo.png' alt='[MySQL] DML(2): SELECT 중급 JOIN, SUBQUERY'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series2'>[MySQL] DML(2): SELECT 중급 JOIN, SUBQUERY</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] DML(1): SELECT 기초",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-series1",
      "date"     : "Mar 11, 2021",
      "content"  : "Table of Contents  SELECT Statement          SELECT      FROM      WHERE      ORDER BY      LIMIT      GROUP BY      HAVING      SELECT문의 작성순서와 실행순서      SQL에서 제공하는 함수      NULL 데이터 다루는 방법        참고SELECT StatementMySQL에서 데이터를 조회하거나 분석할 때 필요한 SELECT문에 대해서 간단히 정리해 보겠습니다.SELECT  특정 컬럼이나 컬럼의 연산 결과를 지정SELECT *SELECT addressSELECT height / weightSELECT MAX(age)SELECT MAX(age) AS max_ageSELECT     (CASE         WHEN age IS NOT NULL THEN age         ELSE &quot;N/A&quot;     END) AS ageFROM  기준이 되는 테이블 지정FROM customersFROM orders-- 예시SELECT name FROM customers;WHERE  조회할 데이터를 필터링 하기 위해, 컬럼에 조건을 지정WHERE age = 20WHERE gender != &#39;m&#39;WHERE age &amp;gt;= 27WHERE age NOT BETWEEN 20 AND 30 -- 20~30WHERE age IN (20, 30) -- 20 or 30WHERE address LIKE &#39;서울%&#39;WHERE address LIKE &#39;%고양시%&#39;WHERE address LIKE BINARY &#39;%Kim%&#39; -- Kim 매칭, kim 매칭 xWHERE email LIKE &#39;__@%&#39; -- _는 임의의 문자 1개-- 예시SELECT * FROM customers WHERE age &amp;gt; 25;ORDER BY  정렬 기준을 지정ORDER BY height ASCORDER BY height DESC-- 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASC;LIMIT  보고자 하는 결과의 개수를 지정LIMIT 5 -- 5개LIMIT 10, 5 -- 10번째부터 5개-- 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASCLIMIT 3;GROUP BY  특정 컬럼의 값을 기준으로 그루핑  그루핑 하고나면 모든 함수연산 또한 그룹 단위로 실행GROUP BY genderGROUP BY countryGROUP BY country, genderGROUP BY SUBSTRING(address, 1, 2)-- 예시SELECT genderFROM customersGROUP BY gender;SELECT gender, MAX(age)FROM customersGROUP BY gender;GROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUP-- 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;;SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*)FROM memberGROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUPHAVING region IS NOT NULLORDER BY region ASC, gender DESC;HAVING  그루핑된 결과에 조건을 지정HAVING region = &#39;서울&#39;-- 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;🦊 WHERE과 HAVING의 차이점WHERE: 주어진 테이블의 전체 row에서 필터링을 하는 용도HAVING: GROUP BY 되고 난 후 row에서 필터링 하는 용도SELECT문의 작성순서와 실행순서작성 순서SELECT FROMWHEREGROUP BYHAVING ORDER BYLIMIT 실행 순서FROMWHERE GROUP BYHAVING SELECTORDER BYLIMIT JOIN 문이 있을 때 실행 순서FROMONJOINWHEREGROUP BYHAVINGSELECTORDER BYLIMITSQL에서 제공하는 함수-- 모든 데이터 타입COUNT(*)DISTINCT(gender)-- 문자열 데이터 타입SUBSTRING(address, 1, 2) -- address의 첫번째 문자에서 2개LENGTH(address)UPPER(address)LOWER(address)LPAD(address)RPAD(address)-- 숫자 데이터 타입-- 집계(aggregation) 함수MAX(height)MIN(weight)AVG(weight)-- 산술(mathematical) 함수ABS(balance)CEIL(height)FLOOR(height)ROUND(height)-- 날짜 및 시간 데이터 타입YEAR(birthday)MONTH(birthday)DAYOFMONTH(birthday)DATEDIFF(birthday, &#39;2002-01-01&#39;)-- 예시SELECT * FROM customers WHERE MONTH(birthday) IN (4, 5, 6);NULL 데이터 다루는 방법WHERE address IS NULLWHERE address IS NOT NULL-- COALESCE(a, b, c) 함수는 a, b, c 중 가장 먼저 NULL아닌 값 리턴COALESCE(height, &quot;키 정보 없음&quot;)COALESCE(height, weight * 2.5, &quot;키 정보 없음&quot;)-- IFNULL(a, b) 함수는 a가 NULL 아니면 a, NULL이면 b 리턴IFNULL(height, &quot;키 정보 없음&quot;)-- IF(condition, a, b) 함수는 condition이 True이면 a, False이면 b리턴IF(address IS NOT NULL, address, &quot;N/A&quot;)-- CASE 함수CASE    WHEN address IS NOT NULL THEN address    ELSE N/AEND-- 예시SELECT addressFROM customersWHERE address IS NOT NULL;SELECT COALESCE(height, &quot;키 정보 없음&quot;), COALESCE(gender, &quot;성별 정보 없음&quot;)FROM customers;SELECT IF(address IS NOT NULL, address, &quot;N/A&quot;)FROM customers;SELECT    CASE        WHEN address IS NOT NULL THEN address        ELSE N/A    ENDFROM customers;참고      MySQL 실습 제공 사이트        MySQL공식문서: Functions and Operators  ",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-11T21:01:35+09:00'>11 Mar 2021</time><a class='article__image' href='/mysql-series1'> <img src='/images/mysql_logo.png' alt='[MySQL] DML(1): SELECT 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series1'>[MySQL] DML(1): SELECT 기초</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "[MySQL] Intro",
      "category" : "data_engineering",
      "tags"     : "mysql",
      "url"      : "/mysql-intro",
      "date"     : "Mar 7, 2021",
      "content"  : "Table of Contents  DBMS          Database      DBMS      SQL      MySQL      DBMS의 종류      DBMS의 구조        SQL의 분류          DCL      DDL      DML      TCL      DBMS빅 데이터 시대에서 데이터 저장은 가장 중요한 부분 중 하나입니다. 힘들게 얻은 데이터를 저장하지 않는다면 큰 자원 낭비겠죠. 하지만 중요한 것은 단순히 저장에 그치는 것이 아니라, 어떤 식으로 저장해 그 후 데이터를 추가, 갱신, 삭제 할 때 문제(NULL, 중복 등)가 생기지 않도록 할 것인지에 대한 고민도 이루어져야 한다는 것 입니다. 이번 MySQL 시리즈에서 이런 문제들을 어떻게 해결할 것인지에 대해 공부해보도록 하겠습니다.Database데이터베이스는 데이터의 집합 또는 데이터 저장소라고 정의할 수 있습니다.DBMS데이터베이스를 보통 직접적으로 접근하지는 않습니다. 사용자들이 데이터베이스를 그냥 접근한다면 데이터의 일관성도 떨어질 것이고, 관리도 쉽지 않을 것 입니다. 이러한 이유로 데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS(DataBase Management System)이라고 합니다.DBMS는 데이터베이스를 구축하는 틀을 제공하고, 효율적으로 데이터를 검색하고 저장하는 기능을 제공합니다. 또한 응용 프로그램들이 데이터베이스에 접근할 수 있는 인터페이스를 제공하고, 장애에 대한 복구 기능, 사용자 권한에 따른 보안성 유지 기능 등을 제공합니다.SQLDBMS를 이용해 데이터베이스를 사용하게 됩니다. 그렇다면 저희는 DBMS와 소통하는 방법을 알아야 합니다. 여기서 DBMS와 소통하기 위한 언어를 SQL(Structured Query Language)라고 합니다. SQL을 이용하면 데이터베이스 조작에 필요한 모든 명령어를 DBMS에 전달함으로써 수행할 수 있습니다.MySQL처음 SQL이라는 언어는 IBM이라고 하는 회사에서 System/R이라는 DBMS와, 이것을 사용하기 위해 필요한 언어인 SEQUEL을 만들면서 처음 등장했습니다. 그런데 SEQUEL(Structured English Query Language)은 그 단어가 이미 다른 곳에서 사용되고 있다는 문제(상표권 문제) 때문에 그 이름이 SQL(Structured Query Language)로 변경되었습니다. 그러다가 1987년, 국제 표준화 기구(ISO)에서 SQL에 관한 국제 표준(ISO 9075:1987)이 제정되었습니다.하지만 우리가 실제로 사용하는 SQL은 이 국제 표준에 완벽히 부합하지는 않습니다. Oracle, Microsoft SQL Server, MySQL 등의 DBMS에서 지원되는 SQL이 표준을 완벽히 준수하지는 않는다는 뜻입니다. 그 이유는 다양하지만 일단 많은 DBMS 회사들이 성능 향상과 더 다양한 기능 제공을 위해서, 차라리 표준을 일부 벗어나는 것을 택하는 경우가 많기 때문입니다.MySQL은 가장 처음 MySQL AB라고 하는 스웨덴 회사에서 개발되었습니다. 현재는 인수 과정을 거쳐 Oracle의 소유입니다. 이로 인해 지금 Oracle에는 Oracle(회사명과 같은 DBMS)과 MySQL이라는 서비스를 함께 제공하고 있습니다.두 DBMS의 시장에서의 쓰임새를 보면 약간의 차이가 있습니다. 은행, 거래소 등과 같이 데이터 처리의 정확성, 운영의 안정성 등이 엄격하게 요구되는 분야에서는 오라클이 주로 사용되고 있고, 우리가 흔히 쓰는 앱, 웹 사이트 같은 서비스를 만들 때는 MySQL을 쓰는 경우가 많습니다.DBMS의 종류위와 같이 많은 회사에서 성능 향상과 목적에 맞게 SQL이라는 언어를 조금씩 변형, 개선하여 새로운 DBMS로 개발해왔습니다. 이러한 이유로 MySQL과 같이 ~SQL이라는 용어도 사실상은 그 언어를 지원하는 DBMS 자체를 의미하게 되었습니다. 그래서 약간 헷갈리지만 관계형 데이터를 위한 DBMS의 경우 RDBMS, 비 관계형 데이터를 위한 DBMS의 경우 NoSQL이라고 하게 되었습니다.RDBMS: MySQL, Oracle, MariaDB(MySQL 개발자들이 만든 오픈소스), PostgreSQL 등NoSQL: MongoDB, ElasticSearch, Cassandra 등DBMS의 구조  client(클라이언트 프로그램): 유저의 데이터베이스 관련 작업을 위해, SQL을 입력할 수 있는 화면 등을 제공하는 프로그램  server(서버 프로그램): client로부터 SQL 문 등을 전달받아 데이터베이스 관련 작업을 직접 처리하는 프로그램MySQL에서 서버 프로그램의 이름은 mysqld, 클라이언트 프로그램 이름은 mysql입니다. mysql은 보통 CLI 환경에서 사용하는 프로그램입니다. CLI 환경이 아니라 GUI 환경에서 mysql을 사용하려면 mysql을 GUI 환경에서 사용할 수 있도록 해주는 프로그램을 사용하면 됩니다. 대표적으로 Oracle이 공식적으로 제공하는 MySQL Workbench라는 프로그램이 있습니다.SQL의 분류DCL  Data Control Language  데이터베이스 접근 권한과 관련한 명령어  GRANT, REVOKE, DENYDDL  Data Definition Language  테이블과 같은 데이터 구조를 정의하는데 사용되는 명령어  CREATE, ALTER, RENAME, DROP, TRUNCATEDML  Data Manipulation Language  데이터 조회/삽입/수정/삭제와 관련한 명령어  SELECT, INSERT, UPDATE, DELETETCL  Transaction Control Language  데이터를 트랜잭션 단위로 처리하는데 필요한 명령어  COMMIT, ROLLBACK, SAVEPOINT",
      "article"  : "<div class='article animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-07T21:01:35+09:00'>07 Mar 2021</time><a class='article__image' href='/mysql-intro'> <img src='/images/mysql_logo.png' alt='[MySQL] Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-intro'>[MySQL] Intro</a> </h2><p class='article__excerpt'>데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS라고 한다</p></div></div></div>"
    } 
  
]

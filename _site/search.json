[
  
    {
      "title"    : "AWS Series [Part9]: AWS Analytics Service: Kinesis",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series9",
      "date"     : "Aug 11, 2022",
      "content"  : "Table of Contents  Kinesis  Kinesis Data Streams  Kinesis Data Firehose  Youtube  참고Kinesis  You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time  Apache Kafka와 같은 용도로 사용한다  Apache Kafka와 아키텍처까지 비슷한 서비스로는  Managed Streaming for Apache Kafka(MSK)가 있다  Kinesis가 MSK보다 조금 더 일찍 서비스로 제공되었기 때문에 아직까지는 MSK 보다 통합성이 좋지만 점차 MSK도 나아지는 중  Kinesis가 더 일찍 서비스되었기 때문에 실무에서는 MSK보다 Kinesis를 더 많이 사용아래 영상은 Kinesis와 MSK의 특징과 차이를 자세히 설명해준다.Kinesis Data Streams  Apache Kafka의 Broker와 비슷한 용도  아래 그림은 설정 화면을 캡처한 것인데 크게 설정할게 없는 것 같다.          중복없는 전송과 같은 설정은 script(python의 boto3 라이브러리)로 해결하거나 Source에서 해결해야 하는 것 같다        Kinesis Data Stream 서비스 자체에 Auto-scaling 설정 기능이 없다          Unlike some other AWS services, Kinesis does not provide a native auto-scaling solution like DynamoDB On-Demand or EC2 Auto Scaling. Therefore, there is a need for the right number of shards to be calculated for every stream based on the expected number of records and/or the size of the records        아래와 같은 방법이 하나의 솔루션이 된다 Kinesis Data Firehose  Apache Kafka의 Connector와 비슷한 용도  Source로 설정가능한 값은 Kinesis Data Stream 또는 Direct PUT 뿐이다  Target으로 가능한 것은 S3, Redshift와 같은 것들이 있다  Source로 Direct PUT을 사용하면 Apache Kafka + Kinesis Firehose 조합도 가능할 것 같다YoutubeKinesis Data Stream과 Kinesis Data Firehose의 Use case를 포함해 데이터 파이프라인에 관한 좋은 인사이트를 제공해준다.참고  AWS docs  whizlabs, AWS Kinesis Data Streams vs AWS Kinesis Data Firehose  Youtube AWS Korea: AWS에서 데이터 분석을 시작하기 위한 실시간, 배치 데이터 수집 방법 알아보기  Brandon Stanley, Amazon Kinesis Data Streams: Auto-scaling the number of shards",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-11T21:01:35+09:00'>11 Aug 2022</time><a class='article__image' href='/aws-series9'> <img src='/images/aws_logo.png' alt='AWS Series [Part9]: AWS Analytics Service: Kinesis'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series9'>AWS Series [Part9]: AWS Analytics Service: Kinesis</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part8]: AWS [Database, Analytics] Service: Redshift",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series8",
      "date"     : "Aug 11, 2022",
      "content"  : "Table of Contents  Introduction  Use Case  Examples          클러스터 생성      스키마 생성      쿼리 및 분석        참고IntroductionAmazon Redshift is a fully managed(setting up, operating, and scaling a data warehouse, provisioning capacity, monitoring and backing up the cluster), petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today.Redshift is an OLAP-style (Online Analytical Processing) column-oriented database. It is based on PostgreSQL version 8.0.2. This means regular SQL queries can be used with Redshift. But this is not what separates it from other services. The fast delivery to queries made on a large database with exabytes of data is what helps Redshift stand out.Fast querying is made possible by Massively Parallel Processing design or MPP. The technology was developed by ParAccel. With MPP, a large number of computer processors work in parallel to deliver the required computations. Sometimes processors situated across multiple servers can be used to deliver a process.Use CaseAmazon Redshift is used when the data to be analyzed is humongous. The data has to be at least of a petabyte-scale (1015 bytes) for Redshift to be a viable solution. The MPP technology used by Redshift can be leveraged only at that scale. Beyond the size of data, there are some specific use cases that warrant its use.(쿼리의 성능이 극대화됨 -&amp;gt; 대용량 데이터 or 실시간 분석에 적합 -&amp;gt; 그 외의 경우 요구사항 대비 지나친 성능으로 낭비가 될 수 있음)  more than petabyte-scale  processing real-time analytics  combining multiple data sources  business intelligence  log analysisExamples클러스터 생성  노드 유형, 개수는 작게 시작하는 것이 좋다 (Redshift는 비싸니까)  IAM 역할을 제대로 지정안하면 안됨 -&amp;gt; 나의 경우 Athena, Glue, S3의 FullAccess를 이용          처음에 RedshiftFullAccess도 추가해줬었는데 왜인지 모르겠지만 에러남      (왜 Redshift를 이용할 때 RedshiftFullAccess를 추가하면 에러가 날까)      스키마 생성  Glue의 Catalog가 있으면 Redshift를 사용할 때도 정말 편하다  Catalog 없으면 [Create Schema] -&amp;gt; [Create Table] -&amp;gt; [Load Data] 해줘야됨쿼리 및 분석  MongoDB에서 저장할 때 requirements라는 속성을 array형태로 저장했었다  array가 있으면 쿼리시 에러가 난다 -&amp;gt; unnesting을 진행했다참고  AWS docs, Redshift  AWS docs, Querying semistructured data  CLOUDZERO, AWS Redshift 101: What Is It and When Should You Use It?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-11T21:01:35+09:00'>11 Aug 2022</time><a class='article__image' href='/aws-series8'> <img src='/images/aws_logo.png' alt='AWS Series [Part8]: AWS [Database, Analytics] Service: Redshift'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series8'>AWS Series [Part8]: AWS [Database, Analytics] Service: Redshift</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part7]: AWS Database Service: DynamoDB",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series7",
      "date"     : "Aug 11, 2022",
      "content"  : "Table of Contents  What is DynamoDB  Features of DynamoDB  Components of DynamoDB  Storing Data in DynamoDB  Querying Data in DynamoDB  Configuration in DynamoDB  Use Case  Pricing          DynamoDB Provisioned Capacity      DynamoDB On-demand Pricing        참고What is DynamoDBAmazon DynamoDB is a cloud-native fully managed NoSQL primarily key-value database.DynamoDB’s NoSQL design is oriented towards simplicity and scalability, which appeal to developers and devops teams respectively. It can be used for a wide variety of semistructured data-driven applications prevalent in modern and emerging use cases beyond traditional databases, from the Internet of Things (IoT) to social apps or massive multiplayer games. With its broad programming language support, it is easy for developers to get started and to create very sophisticated applications using DynamoDB.While we cannot describe exactly what DynamoDB is, we can describe how you interact with it. When you set up DynamoDB on AWS, you do not provision specific servers or allocate set amounts of disk. Instead, you provision throughput — you define the database based on provisioned capacity — how many transactions and how many kilobytes of traffic you wish to support per second. Users specify a service level of read capacity units (RCUs) and write capacity units (WCUs).DynamoDB needed to “provide fast performance at any scale,” allowing developers to “start small with just the capacity they need and then increase the request capacity of a given table as their app grows in popularity.” Predictable performance was ensured by provisioning the database with guarantees of throughput, measured in “capacity units” of reads and writes. “Fast” was defined as single-digit milliseconds, based on data stored in Solid State Drives (SSDs).DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don’t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.You can scale up or scale down your tables’ throughput capacity without downtime or performance degradation. You can use the AWS Management Console to monitor resource utilization and performance metrics.DynamoDB allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant. For more information, see Expiring items by using DynamoDB Time to Live (TTL).Features of DynamoDB  NoSQL primarily key-value (and document using JSON) database service  Fully Managed Distributed Systems -&amp;gt; Stable Performance          Managed — provided ‘as-a-Service’ so users would not need to maintain the database      Scalable — automatically provision hardware on the backend, invisible to the user      Durable and highly available — multiple availability zones for failures/disaster recovery        Integrates well with other AWS services  Built-in support for ACID transactions  Encryption at rest  On-demand backup  Point-in-time recoveryComponents of DynamoDB  Attribute: single field that is attached to an item. Key-value pair  Item: unique set of attributes in a table. set of key-value pair  Table: group of items  Primary Key: The primary key uniquely identifies each item in the table          therefore, no two items can have the same key      primary key could be just partition key or partition key + sort key        Partiton Key: key for determining physical storage in which the item will be stored          input to an internal hash function. the output from the hash function determines the partition      필수 지정값. primary key로 partition key만 지정되면 partition key는 고유한 값을 가져야함        Sort Key: 동일한 파티션 키를 공유하는 모든 항목을 정렬하거나 검색하는데 이용 (선택 사항)          partition key + sort key -&amp;gt; Referred to as a composite primary key      All items with the same partition key value are stored together, in sorted order by sort key value.      In a table that has a partition key and a sort key, it’s possible for multiple items to have the same partition key value. However, those items must have different sort key values.        Secondary Indexes          You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn’t require that you use indexes, but they give your applications more flexibility when querying your data. After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table.      Global secondary index – An index with a partition key and sort key that can be different from those on the table.      Local secondary index – An index that has the same partition key as the table, but a different sort key.      In the example Music table shown previously, you can query data items by Artist (partition key) or by Artist and SongTitle (partition key and sort key). What if you also wanted to query the data by Genre and AlbumTitle? To do this, you could create an index on Genre and AlbumTitle, and then query the index in much the same way as you’d query the Music table.      Storing Data in DynamoDBA key-value store holds for each key a single value. Arguably, if the value can be an entire document, you can call this database a “document store”. In this sense, DynamoDB is a document store. The DynamoDB API lets you conveniently store a JSON document as the value, and also read or writes part of this document directly instead of reading or writing the entire document (although, you actually pay for reading and writing the entire document).보통은 Key-value store로 쓴다. Document store로 사용하고 싶은 경우 MongoDB의 AWS 버전인 DocumentDB를 쓰는 것을 추천한다.DynamoDB is a “wide column” style of NoSQL database. While the schema isn’t defined beyond the primary key at table construction time, the querying abilities are limited to primary keys or secondary indexes. Creating Global Secondary Indexes allows you to query against other attribute values. Local Secondary Indexes can be queried too, but they’re a bit of an odd duck. See here for a good comparison of the two secondary index types.If your needs do include querying inside the attributes, check out some of the “document-oriented” style of NoSQL databases, of which MongoDB is the one most people think of. If you’re already embedded in the AWS ecosystem and don’t want to break out of it, AWS offers DocumentDB as a MongoDB-compatible service managed by AWS.Wide-column and document-style data stores have different pro’s &amp;amp; cons. Generally-speaking, the wide-column approach is better for extreme scalability at consistent cost &amp;amp; speed, whereas the document-oriented approach gives more flexibility as your data access patterns evolve over time. Choose the one that suits your needs the best.Querying Data in DynamoDB  PartiQL (SQL like)  Primary Key 또는 Global Secondary Indexes(GSI)에 대해서만 쿼리 가능  Filter는 쿼리 이후 결과를 제한하는 용도일반 Attribute에 대해서는 쿼리가 안된다.아래와 같이 원하는 Attribute를 GSI로 만들고 나면 쿼리가 가능해진다.Configuration in DynamoDB위의 설정은 우선 기본 설정으로 테이블을 만든뒤 이후 설정값으르 수정할 수 도 있다.그 밖에 테이블 생성 이후 설정할 수 있는 설정값은 다음과 같다.Use CaseIn this tutorial, you will create a bookstore application that showcases a product catalog. Products typically contain unique identifiers and attributes such as descriptions, quantities, locations, and prices. The method for retrieving these types of attributes (specifically, the access pattern) is often a key-value lookup based on the product’s unique identifier. This means that an application can retrieve these other attributes when a product’s unique identifier is provided.While the product catalog can start off with a few products, it should have the ability to scale to billions if needed without having to be re-architected or requiring a different database. It should also maintain fast, predictable performance at any scale for these key-value lookups. With these requirements in mind, Amazon DynamoDB is a good fit as the durable system of record for the bookstore because it offers low latency performance and scales as your application grows. Another benefit is that you do not need to manage any servers or clusters.PricingDynamoDB can be extremely expensive to use. There are two pricing structures to choose from: provisioned capacity and on-demand capacity.DynamoDB Provisioned CapacityIn this Amazon DynamoDB Pricing Plan, you’re billed hourly per the use of operational capacity units (or read and write capacity units). You can control costs by specifying the maximum amount of resources needed by each database table being managed. The provisioned capacity provides autoscaling and dynamically adapts to an increase in traffic. However, it does not implement autoscaling for sudden changes in data traffic unless that’s enabled.You should use provisioned capacity when:  You have an idea of the maximum workload your application will have  Your application’s traffic is consistent and does not require scaling (unless you enable the autoscaling feature, which costs more)DynamoDB On-demand PricingThis plan is billed per request units (or read and write request units). You’re only charged for the requests you make, making this a truly serverless choice. This choice can become expensive when handling large production workloads, though. The on-demand capacity method is perfect for autoscaling if you’re not sure how much traffic to expect.Knowing which capacity best suits your requirements is the first step in optimizing your costs with DynamoDB. Here are some factors to consider before making your choice.You should use on-demand capacity when:  You’re not sure about the workload your application will have  You don’t know how consistent your application’s data traffic will be  You only want to pay for what you use참고  scylladb, Introduction to DynamoDB [추천]  Youtube, Be A Better Dev: AWS DynamoDB Tutorial For Beginners  AWS docs, Core components of Amazon DynamoDB  stackoverflow: How is it possible for DynamoDB to support both Key-Value and Document database properties at the same time  stackoverflow: AWS DynamoDB Query based on non-primary keys  cloudforecast: DynamoDB Pricing and Cost Optimization Guide",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-11T21:01:35+09:00'>11 Aug 2022</time><a class='article__image' href='/aws-series7'> <img src='/images/aws_logo.png' alt='AWS Series [Part7]: AWS Database Service: DynamoDB'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series7'>AWS Series [Part7]: AWS Database Service: DynamoDB</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part10]: AWS Analytics Service: Glue, Athena",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series10",
      "date"     : "Aug 11, 2022",
      "content"  : "Table of Contents  Introduction  AWS Glue  AWS Athena  S3 + Glue Catalog + Athena + Glue ETL: 환상의 조합  Glue Catalog  Glue ETL  참고Introduction데이터와 관련된 업무를 하다보면 실제로 데이터를 분석하고, 머신러닝과 같은 분야에 활용하는 시간은 약 30%, 나머지 시간은 수집, 적재, 변환과 같은 ETL 작업에 대부분의 시간을 할애하게 된다.첫 번째 문제(낮은 데이터 품질)를 해결하도록 도와주는 AWS 서비스에는 대표적으로 다음과 같은 것들이 있다.AWS GlueAWS Glue is a fully managed ETL service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there’s no infrastructure to set up or manage.No schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you wantAWS AthenaAmazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries.S3 + Glue Catalog + Athena + Glue ETL: 환상의 조합데이터 웨어하우스로 가기 전 단계에 활용하기 좋은 것 같다. 보통 S3와 같은 데이터 레이크에는 raw-data가 많기 때문에 이러한 데이터를 클렌징하고, 가공하는 과정이 수반되어야 하는데 이러한 작업들을 AWS Glue가 해준다.또한 S3의 데이터를 다른 곳으로 옮기기 전에 먼저 데이터를 분석하고 싶은 경우가 많다. 어떤 데이터가 있고, 스키마가 어떻고, 어떤 데이터를 옮기면 좋을지, 어떤 데이터가 가치가 있을지를 먼저 S3에서 충분히 탐색해야 한다. 이러한 기능을 하는 것이 바로 AWS Athena이다. Athena를 이용하면 쿼리를 통해 S3의 데이터를 탐색/분석할 수 있다. 근데 Athena는 반드시 Data Catalog에서 쿼리를 진행한다. (카탈로그(Catalog): 데이터에 대한 하나의 단일화된 뷰)따라서 S3를 다른 데이터 웨어하우스로 옮기기 전에 먼저 Glue를 통해 데이터를 카탈로그화 하고, 그 카탈로그를 Athena를 이용해 분석하고 다시 Glue의 ETL 작업을 통해 클렌징, 가공해 데이터 웨어하우스로 옮겨주는 것이 데이터 파이프라인의 좋은 예이다.참고로 크롤러를 통해 카탈로그화 시킬 수 있는 데이터 소스는 S3뿐만 아니라 DynamoDB, DocumentDB, DataLake 등이 있다.Glue Catalog  크롤러를 주기적으로 실행시킴으로써 스키마 변경을 감지하고 관리해줌  스키마의 버전을 관리하고 해당 스키마에 맞는 일관된 데이터 뷰 제공(대표적으로 Athena, EMR, Redshift에 제공)Glue ETL  ETL 작업을 그래프로 시각화하여 쉽게 파이프라인을 만들 수 있음  스케줄링 기능도 있음(이상하게 S3에 저장하는 부분에서 데이터가 사라짐.. 실제로 S3에 저장은 되지만 크기가 0Byte..)참고  AWS docs, What is AWS Glue?  Youtube, AWS Korea: AWS Glue를 통한 손쉬운 데이터 전처리 작업하기  Youtube, Johny Chivers: AWS Glue Tutorial for Beginners",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-11T21:01:35+09:00'>11 Aug 2022</time><a class='article__image' href='/aws-series10'> <img src='/images/aws_logo.png' alt='AWS Series [Part10]: AWS Analytics Service: Glue, Athena'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series10'>AWS Series [Part10]: AWS Analytics Service: Glue, Athena</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part27]: Redis",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series27",
      "date"     : "Aug 2, 2022",
      "content"  : "Table of ContentsRedis: an open source, in-memory key-value database that supports abstract data structures like lists, hashes, strings, and setsin this article i’m going to be covering everything you need to know about redis. starting with what redis is moving on to how to install it all of the commands you need to know and then finally ending up with a real world example of how you would implement redis yourself. this is a really important video to watch because redis can be included in every single production level application and it’s going to make your application more performant. so let’s get started now.redis is essentially a type of database and more specifically it is a nosql database. but it’s not very similar to any other nosql databases. it’s definitely not similar to mongodb and it’s obviously very different than sql databases like postgres and mysql and this is because redis doesn’t really have any idea of tables or documents and instead all of the data in redis isstored inside of key value pairs. so think about a json object you have the key name and you have the value kyle. this is essentially a key value pair and redis is just like one giant json object that has key value pairs and that’s all that you have inside of redis. so it’s not very good at storing a bunch of structured data like you have in sql .but it’s really good for storing you know individual key value pairs that you need to access or get data from another.important thing to note about redis is that unlike a normal database that runs on your disk and stores all your information on disk. redis actually runs inside of your working memory your ram on your computer and this means that redis is incredibly fast because it’s all working inside of ram. but it’s much more unstable because if all of a sudden your system crashes you’re going to lose everything that’s in redis unless you’re backing it up consistently, which is why redis is generally not used as like an actual persistent database store like you have with mongodb and postgres and instead it’s used more for caching where you take things that are really you know things that you access a lot or things that take a long time to compute and you store those values inside of redis that way when you need to access them in the future. it’s incredibly quick since redis is in the memory already loaded. it’s milliseconds to get data as opposed to hundreds of milliseconds or even seconds of time to get data from a traditional database.really the important thing to realize about redis is that it’s going to be built on top of a traditional database. almost always you’re going to have your mongodb or postgres database in the background and you’re going to have redis sitting in front of that database and any time that you have a really long or slow query to your database or you have data you access all the time but doesn’t change that much what you’re going to do is you’re going to store that data inside of redis as well as inside your database. and then when you go to get that information if it’s already in redis you can access that data in milliseconds as opposed to going all the way to the database. computing the data and then coming all the way back which is going to take you hundreds to even thousands of milliseconds depending on how complex your data is. so redis is going to take your app and make it hundreds to even thousands of times faster when it comes to querying these pieces of information.let’s actually talk about how we can install redis. installing redis on your computer is really simple if you have a mac or linux computer. if you use mac just use homebrew to do the install and if you’re on linux just use your package manager of choice to install it. it’s just called redis it’s that simple. but if you’re on windows it’s a bit more complex. because there is no way to install redis on windows. instead you need to go through the windows subsystem for linux which is pretty simple to install.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-02T21:01:35+09:00'>02 Aug 2022</time><a class='article__image' href='/data-engineering-series27'> <img src='/images/redis_logo.png' alt='Data Engineering Series [Part27]: Redis'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series27'>Data Engineering Series [Part27]: Redis</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part24] 인덱스",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series24",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  인덱스 확인  인덱스 통계 정보 확인  인덱스 생성          Create Table 내      Create Index 문      ALTER TABLE ADD INDEX 문        인덱스 삭제          보조인덱스 삭제      클러스터 인덱스 삭제        Execution Plan  Sargable Query  참고인덱스 확인SHOW INDEXFROM &amp;lt;테이블&amp;gt;  Table: 테이블의 이름을 표시함.  Non_unique: 인덱스가 중복된 값을 저장할 수 있으면 1, 저장할 수 없으면 0을 표시함.  Key_name: 인덱스의 이름을 표시하며, 인덱스가 해당 테이블의 기본 키라면 PRIMARY로 표시함.  Seq_in_index: 인덱스에서의 해당 필드의 순서를 표시함.  Column_name: 해당 필드의 이름을 표시함.  Collation: 인덱스에서 해당 필드가 정렬되는 방법을 표시함.  Cardinality: 인덱스에 저장된 유일한 값들의 수를 표시함.  Sub_part: 인덱스 접두어를 표시함.  Packed: 키가 압축되는(packed) 방법을 표시함.  Null: 해당 필드가 NULL을 저장할 수 있으면 YES를 표시하고, 저장할 수 없으면 ‘‘를 표시함.  Index_type: 인덱스에 사용되는 메소드(method)를 표시함.  Comment: 해당 필드를 설명하는 것이 아닌 인덱스에 관한 기타 정보를 표시함.  Index_comment: 인덱스에 관한 모든 기타 정보를 표시함.인덱스 통계 정보 확인SHOW TABLE STATUS LIKE &amp;lt;테이블&amp;gt;인덱스 생성Create Table 내CREATE TABLE books (  -- 같이 지정  id varchar(5) primary key, -- 기본키 지정 (클러스터 인덱스)  name varchar(20) unique, -- 인덱스 생성 (보조 인덱스) (중복 비허용)  writer varchar(20) NOT NULL,    INDEX idx_test (writer asc) -- 인덱스 생성 (보조 인덱스));Create Index 문CREATE INDEX 인덱스명 ON 테이블명 (컬럼명); -- 보조 인덱스 생성 (중복 허용) CREATE UNIQUE INDEX 인덱스명 ON 테이블명 (컬럼명); -- 보조 인덱스 생성 (중복 비허용) CREATE FULLTEXT INDEX 인덱스명 ON 테이블명 (컬럼명); -- 클러스터 인덱스 생성 CREATE UNIQUE INDEX 인덱스명 ON 테이블명 (컬럼명1, 컬러명2); -- 다중 컬럼 인덱스 생성 ANALYZE TABLE 테이블명; -- !! 생성한 인덱스 적용 !!ALTER TABLE ADD INDEX 문ALTER TABLE 테이블이름ADD INDEX 인덱스이름 (필드이름)-- 중복을 허용하는 인덱스.-- 보조 인덱스.-- 가장 느리지만 인덱스 안한 컬럼 조회하는 것보다 인덱스 붙인 컬럼 조회하는게 더 빠르다. 여러개 노멀키 를 지정할수 있다.  ALTER TABLE 테이블이름ADD UNIQUE INDEX 인덱스이름 (필드이름)-- 중복을 허용하지 않는 유일한 키. null 허용. -- 보조 인덱스.-- 고속으로 조회 가능  ALTER TABLE 테이블이름ADD PRIMARY KEY INDEX 인덱스이름 (필드이름)-- 중복되지 않은 유일한 키. null 비허용. -- 클러스터 인덱스-- where로 데이터를 조회할때 가장 고속으로 조회  ALTER TABLE 테이블이름ADD FULLTEXT INDEX 인덱스이름 (필드이름)-- 풀텍스트 인덱스-- 긴 문자열 데이터를 인덱스로 검색할 때 사용.인덱스 삭제보조인덱스 삭제ALTER TABLE 테이블이름DROP INDEX 인덱스이름클러스터 인덱스 삭제ALTER TABLE 테이블이름DROP PRIMARY KEY; -- 만일 외래키와 연결이 되어있을 경우 제약조건에 의해 삭제가 안될수 있음Execution PlanEXPLAIN SELECT ...  id: select id  select_type: SIMPLE, UNION, SUBQUERY, etc  type: data scan type (const, ALL, ref, range, index, fulltext, etc)  possible_keys: 인덱스로 사용가능한 키들  key: Selected key by Optimizer  key_len: index size  ref: const, columns  rows: access row count (statistical estimation)  filtered: left rows (statistical estimation)  Extra: Using where, Using Index, Using filesort, etcSargable Query  where, order by, group by 등에는 가능한 index가 걸린 컬럼 사용.  where 절에 함수, 연산, Like(시작 부분 %)문은 사거블하지 않다!  between, like, 대소비교(&amp;gt;, &amp;lt; 등)는 범위가 크면 사거블하지 않다.  or 연산자는 필터링의 반대 개념(로우수를 늘려가는)이므로 사거블이 아니다.  offset이 길어지면 사거블하지 않는다.  범위 보다는 in 절을 사용하는 게 좋고, in 보다는 exists가 더 좋다.  꼭 필요한 경우가 아니라면 서브 쿼리보다는 조인(Join)을 사용하자.참고  인파, [MYSQL] 📚 인덱스(index) 핵심 설계 &amp;amp; 사용 문법 💯 총정리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series24'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part24] 인덱스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series24'>MySQL Series [Part24] 인덱스</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part23] 유틸리티(View, CTE)",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series23",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  View          장점      단점      생성      수정      삭제      정보 확인        CTE(Common Table Expression)          RECURSIVE CTE      Table vs View      View vs CTE      CTE vs Subquery        참고View  뷰(view)는 데이터베이스에 존재하는 일종의 가상 테이블  실제로 데이터를 저장하고 있지는 않음  테이블은 실제 데이터를 저장, 뷰는 그저 SELECT문이 실행되고 난 후의 이미지 같은 느낌  때에 따라 서브쿼리가 이중 중첩, 삼중 중첩되는 경우  이 때 생기는 SELECT문의 복잡성을 줄이고자 뷰를 사용  특정 역할을 하는 SELECT문들을 뷰로 저장  코드 스니펫처럼 필요할 때마다 가져와서 사용 가능  뷰는 백엔드 개발자들의 자산장점  특정 사용자에게 테이블 전체가 아닌 필요한 필드만을 보여줄 수 있음  쿼리의 재사용성  이미 실행된 서브쿼리라는 점에서 더 빠르다고 할 수 있음단점  뷰는 수정할 수 없는 경우가 많음          SUM, AVG와 같은 집계 함수가 있는 경우, UNION ALL, DISTINCT, GROUP BY가 포함된 경우        삽입, 삭제, 갱신 작업에 제한 사항이 많음생성CREATE VIEW &amp;lt;뷰 이름&amp;gt; AS  SELECT 필드1, 필드2, ...  FROM 테이블  WHERE 조건수정ALTER VIEW &amp;lt;뷰 이름&amp;gt; AS  SELECT 필드1, 필드2, ...  FROM 테이블삭제DROP VIEW &amp;lt;뷰 이름&amp;gt;정보 확인SHOW TABLESSHOW CREATE VIEW &amp;lt;뷰 이름&amp;gt;DESC &amp;lt;뷰 이름&amp;gt;SELECT * FROM information_schema.views WHERE table_schema = &amp;lt;DB&amp;gt;CTE(Common Table Expression)  메모리에 임시 결과로 올려놓고 재사용  쿼리가 실행중인 동안만큼은 데이터가 메모리에 올라와 있음  순차적으로 쿼리 작성 가능RECURSIVE CTE  스스로 추가적인 Record를 생성할 수 있음  그래서 반드시 UNION 사용해야함Table vs View      Table: Table is a preliminary storage for storing data and information in RDBMS. A table is a collection of related data entries and it consists of columns and rows.        View: A view is a virtual table whose contents are defined by a query. Unless indexed, a view does not exist as a stored set of data values in a database. Advantages over table are  We can combine columns/rows from multiple table or another view and have a consolidated view.Views can be used as security mechanisms by letting users access data through the view, without granting the users permissions to directly access the underlying base tables of the viewIt acts as abstract layer to downstream systems, so any change in schema is not exposed and hence the downstream systems doesn’t get affected.View vs CTE  A view is an object in the database  Views can be indexed  A CTE only exists for the duration of a single query      CTE can’t be indexed    Ad-hoc queries: For queries that are referenced occasionally (or just once), it’s usually better to use a CTE. If you need the query again, you can just copy the CTE and modify it if necessary.  Frequently used queries: If you tend to reference the same query often, creating a corresponding view is a good idea. However, you’ll need create view permission in your database to create a view.  Access management: A view might be used to restrict particular users’ database access while still allowing them to get the information they need. You can give users access to specific views that query the data they’re allowed to see without exposing the whole database. In such a case, a view provides an additional access layer.CTE vs SubqueryCTE is just syntax so in theory it is just a subquery. you may not get any performance difference while using CTE and Subquery.I think the biggest benefit for using CTEs is readability. It makes it much easier to see what queries are being used as subqueries, and then it’s easy to join them into a query, much like a view.참고  stackoverflow, Difference between View and table in sql  stackoverflow, CTE vs View Performance in SQL Server  LearnSQL, What’s the Difference Between SQL CTEs and Views?  stackoverflow, Is there any performance difference btw using CTE, view and subquery?  인파, [MYSQL] 📚 WITH (임시 테이블 생성)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series23'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part23] 유틸리티(View, CTE)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series23'>MySQL Series [Part23] 유틸리티(View, CTE)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part22] TCL: COMMIT, ROLLBACK, SAVEPOINT",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series22",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  오토커밋 확인하기  트랜잭션 기능 활성화  트랜잭션 처리  상태 저장  트랜잭션은 Session 단위로 제어됨  DDL 작업에 대해서는 ROLLBACK 적용 안됨 (DDL은 AutoCommit)오토커밋 확인하기  AutoCommit은 DML 실행문이 자동으로 커밋되는 것을 의미  해제해야 트랜잭션 처리 가능SHOW VARIABLES LIKE &#39;%commit%&#39;트랜잭션 기능 활성화START TRANSACTIONSET AUTOCOMMIT = FALSE트랜잭션 처리-- DML 작업을 한 후UPDATE &amp;lt;테이블&amp;gt; SET &amp;lt;컬럼명&amp;gt; = &amp;lt;값&amp;gt; WHERE &amp;lt;조건&amp;gt;-- 커밋하고 싶은 경우COMMIT-- 롤백하고 싶은 경우ROLLBACK상태 저장-- DML 작업을 한 후UPDATE &amp;lt;테이블&amp;gt; SET &amp;lt;컬럼명&amp;gt; = &amp;lt;값&amp;gt; WHERE &amp;lt;조건&amp;gt;-- 이전 까지의 상태를 x로 저장SAVEPOINT x-- x 상태로 롤백ROLLBACK TO SAVEPOINT x",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series22'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part22] TCL: COMMIT, ROLLBACK, SAVEPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series22'>MySQL Series [Part22] TCL: COMMIT, ROLLBACK, SAVEPOINT</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part21] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series21",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  데이터베이스 생성  테이블 정보 확인  테이블 생성  테이블 설정 추가          테이블 이름 변경      컬럼 추가, 이름 변경      컬럼 삭제      컬럼 타입 변경      컬럼 속성 변경      테이블에 제약 사항 걸기      테이블의 제약 사항 삭제      컬럼 순서 앞으로 당기기      컬럼 순서 정하기      컬럼명 속성 동시에 바꾸기      외래키 설정      외래키 정책      외래키 삭제      외래키 파악        참고데이터베이스 생성--데이터베이스 생성CREATE DATABASE &amp;lt;DB이름&amp;gt;CREATE DATABASE IF NOT EXISTS &amp;lt;DB이름&amp;gt;--데이터베이스 지정USE &amp;lt;DB이름&amp;gt;--데이터베이스 삭제DROP DATABASE &amp;lt;DB이름&amp;gt;테이블 정보 확인DESCRIBE &amp;lt;테이블명&amp;gt;SHOW CREATE TABLE &amp;lt;테이블명&amp;gt;테이블 생성공식문서에서 [table_options]와 [partition_options] 부분만 제외하고 (create_definition)만 가지고 와봤다. 공식문서는 아래 참고란에 주석을 달아놓았다. 대략적인 공식문서를 해석하는 방법은 다음과 같다.  (): 반드시 필요한 설정  []: optional한 설정  {}: | 기호와 함께 사용되어 {A|B|C}인 경우 A 또는 B또는 C중 하나를 반드시 선택해야 한다는 의미공식문서 참고    CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    (create_definition,...)    [table_options]    [partition_options]CREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    [(create_definition,...)]    [table_options]    [partition_options]    [IGNORE | REPLACE]    [AS] query_expressionCREATE [TEMPORARY] TABLE [IF NOT EXISTS] tbl_name    { LIKE old_tbl_name | (LIKE old_tbl_name) }create_definition: {    col_name column_definition  | {INDEX | KEY} [index_name] [index_type] (key_part,...)      [index_option] ...  | {FULLTEXT | SPATIAL} [INDEX | KEY] [index_name] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] PRIMARY KEY      [index_type] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] UNIQUE [INDEX | KEY]      [index_name] [index_type] (key_part,...)      [index_option] ...  | [CONSTRAINT [symbol]] FOREIGN KEY      [index_name] (col_name,...)      reference_definition  | check_constraint_definition}column_definition: {    data_type [NOT NULL | NULL] [DEFAULT {literal | (expr)} ]      [VISIBLE | INVISIBLE]      [AUTO_INCREMENT] [UNIQUE [KEY]] [[PRIMARY] KEY]      [COMMENT &#39;string&#39;]      [COLLATE collation_name]      [COLUMN_FORMAT {FIXED | DYNAMIC | DEFAULT}]      [ENGINE_ATTRIBUTE [=] &#39;string&#39;]      [SECONDARY_ENGINE_ATTRIBUTE [=] &#39;string&#39;]      [STORAGE {DISK | MEMORY}]      [reference_definition]      [check_constraint_definition]  | data_type      [COLLATE collation_name]      [GENERATED ALWAYS] AS (expr)      [VIRTUAL | STORED] [NOT NULL | NULL]      [VISIBLE | INVISIBLE]      [UNIQUE [KEY]] [[PRIMARY] KEY]      [COMMENT &#39;string&#39;]      [reference_definition]      [check_constraint_definition]}data_type:    (see Chapter 11, Data Types)key_part: {col_name [(length)] | (expr)} [ASC | DESC]index_type:    USING {BTREE | HASH}index_option: {    KEY_BLOCK_SIZE [=] value  | index_type  | WITH PARSER parser_name  | COMMENT &#39;string&#39;  | {VISIBLE | INVISIBLE}  |ENGINE_ATTRIBUTE [=] &#39;string&#39;  |SECONDARY_ENGINE_ATTRIBUTE [=] &#39;string&#39;}check_constraint_definition:    [CONSTRAINT [symbol]] CHECK (expr) [[NOT] ENFORCED]reference_definition:    REFERENCES tbl_name (key_part,...)      [MATCH FULL | MATCH PARTIAL | MATCH SIMPLE]      [ON DELETE reference_option]      [ON UPDATE reference_option]reference_option:    RESTRICT | CASCADE | SET NULL | NO ACTION | SET DEFAULT      내가 생각했을 때 자주 사용할만한 문법들을 종합했을 때 다음과 같다.CREATE TABLE [IF NOT EXISTS] &amp;lt;테이블명&amp;gt; (    &amp;lt;컬럼명&amp;gt; &amp;lt;컬럼타입&amp;gt; [UNSIGNED] [NOT NULL] [DEFAULT &amp;lt;디폴트값&amp;gt;] [AUTO_INCREMENT] [COMMENT &amp;lt;코멘트&amp;gt;],    ...    [PRIMARY KEY &amp;lt;(프라이머리 키 컬럼명)&amp;gt;,]    [FOREIGN KEY &amp;lt;(외래 키 컬럼명)&amp;gt; REFERENCES &amp;lt;부모테이블&amp;gt; &amp;lt;(부모 컬럼명)&amp;gt; [ON DELETE &amp;lt;DELETE 정책&amp;gt;] [ON UPDATE &amp;lt;UPDATE 정책&amp;gt;],])하지만 보통 테이블 생성을 위해 처음부터 이렇게 스키마를 다 정해서 만들기는 힘들고, 이렇게 꼭 할 필요도 없다.왜냐하면 뒤에서 배울 ALTER라는 것이 새로운 컬럼을 추가하거나, 외래키를 설정하거나 하는 모든 추가적인 적용을 가능하게 하기 때문이다.또한 기존 테이블의 테이블 구조로 새로운 테이블을 만들 수도 있다.CREATE TABLE &amp;lt;테이블명&amp;gt; LIKE &amp;lt;기존 테이블명&amp;gt;기존 테이블 구조에 데이터까지 가져와 새로운 테이블로 만들 수도 있다.(인덱스는 복사 안됨. 그래서 프라이머리 키, 외래키와 같은 설정은 다시 직접 해줘야함)CREATE TABLE &amp;lt;테이블명&amp;gt; AS SELECT * FROM &amp;lt;기존 테이블명&amp;gt;테이블 설정 추가테이블 이름 변경ALTER TABLE &amp;lt;기존 테이블명&amp;gt; RENAME &amp;lt;새로운 테이블명&amp;gt;컬럼 추가, 이름 변경ALTER TABLE &amp;lt;테이블 이름&amp;gt;         ADD &amp;lt;추가할 컬럼&amp;gt; CHAR(10) NULL;  ALTER TABLE &amp;lt;테이블 이름&amp;gt;RENAME COLUMN &amp;lt;원래 컬럼명&amp;gt; TO &amp;lt;바꿀 컬럼명&amp;gt;;컬럼 삭제ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP COLUMN &amp;lt;삭제할 컬럼명&amp;gt;;컬럼 타입 변경ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT;컬럼 속성 변경-- NOT NULL 속성ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT NOT NULL;-- DEFAULT 속성ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT NOT NULL DEFAULT &amp;lt;주고 싶은 default값&amp;gt;;-- DATETIME, TIMESTAMP 타입에 줄 수 있는 특별한 속성-- DEFAULT CURRENT_TIMESTAMP: 값 입력 안되면 default로 현재 시간 입력ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; DATETIME DEFAULT CURRENT_TIMESTAMP;-- 처음 default로 현재 시간 넣어주고, 데이터 갱신될 때 마다 갱신된 시간 넣어줌  ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;-- UNIQUE 속성-- UNIQUE는 PRIMARY KEY와 다르게 NULL 허용ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;변경할 컬럼명&amp;gt; INT UNIQUE;테이블에 제약 사항 걸기   ALTER TABLE &amp;lt;테이블 이름&amp;gt;ADD CONSTRAINT &amp;lt;제약 사항 네이밍&amp;gt; CHECK &amp;lt;제약 사항(ex. age &amp;lt; 100)&amp;gt;;테이블의 제약 사항 삭제    ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP CONSTRAINT &amp;lt;제약 사항 이름&amp;gt;;컬럼 순서 앞으로 당기기ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;컬럼명&amp;gt; INT FIRST;컬럼 순서 정하기ALTER TABLE &amp;lt;테이블 이름&amp;gt;     MODIFY &amp;lt;뒤에 올 컬럼명] INT AFTER &amp;lt;앞에 있는 컬럼명&amp;gt;;컬럼명 속성 동시에 바꾸기 ALTER TALBE &amp;lt;테이블 이름&amp;gt;CHANGE &amp;lt;원래 컬럼명&amp;gt; &amp;lt;바꿀 컬럼명&amp;gt; VARCHAR(10) NOT NULL;외래키 설정외래키(Foreign Key)란 한 테이블의 컬럼 중에서 다른 테이블의 특정 컬럼을 식별할 수 있는 컬럼을 말합니다. 그리고 외래키에 의해 참조당하는 테이블을 부모 테이블(parent table), 참조당하는 테이블(referenced table)이라고 합니다. 외래키를 이용하면 테이블간의 참조 무결성을 지킬 수 있습니다. 참조 무결성이란 아래 그림과 같이 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미합니다.예를 들어, 강의 평가인 review 테이블에는 ‘컴퓨터 개론’에 관한 평가 데이터가 남아있지만, 강의 목록을 나타내는 course 테이블에는 ‘컴퓨터 개론’ 과목이 삭제된다면 이상한 상황이 벌어질 것입니다. 이 때 외래키를 통해 지정해 놓으면 이런 상황을 해결할 수 있습니다.   ALTER TABLE &amp;lt;테이블 이름&amp;gt;ADD CONSTRAINT &amp;lt;제약 사항 네이밍&amp;gt;   FOREIGN KEY (자식테이블의 컬럼)    REFERENCES 부모테이블 (부모테이블의 컬럼)     ON DELETE &amp;lt;DELETE정책&amp;gt;     ON UPDATE &amp;lt;UPDATE정책&amp;gt;;외래키 정책  RESTRICT: 자식 테이블에서 삭제/갱신해야만 부모 테이블에서도 삭제/갱신 가능  CASCADE: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터도 같이 삭제/갱신  SET NULL: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터의 컬럼에 NULL 지정외래키 삭제     ALTER TABLE &amp;lt;테이블 이름&amp;gt;DROP FOREIGN KEY &amp;lt;제약 사항이 걸린 테이블&amp;gt;;외래키 파악SELECT          i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME,          k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAME     FROM information_schema.TABLE_CONSTRAINTS iLEFT JOIN information_schema.KEY_COLUMN_USAGE k    USING (CONSTRAINT_NAME)    WHERE i.CONSTRAINT_TYPE = &#39;FOREIGN KEY&#39;;참고  MySQL 공식문서13.1.20 CREATE TABLE Statement",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series21'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part21] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series21'>MySQL Series [Part21] DDL: CREATE, ALTER, RENAME, DROP, TRUNCATE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part20] DCL(1): GRANT, REVOKE",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series20",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  유저 생성하기  권한 부여  권한 적용  권한 확인  권한 삭제  유저 삭제  현재 서버 사용중인 유저유저 생성하기-- 유저명, 호스트, 비밀번호는 따옴표로 감싸줘도 된다-- 호스트는 보통 localhost 또는 % 또는 IP주소를 사용한다. %는 모든 IP주소를 허용한다는 의미CREATE USER &amp;lt;유저명&amp;gt;@&amp;lt;호스트&amp;gt; IDENTIFIED BY &amp;lt;비밀번호&amp;gt;권한 부여-- 모든DB, 모든 테이블은 각각 *(asterisk)로 표현 가능하다-- *.*: 모든 DB의 모든 테이블에 대해 권한을 준다GRANT ALL PRIVILEGES ON &amp;lt;DB명&amp;gt;.&amp;lt;테이블명&amp;gt; TO &amp;lt;유저명&amp;gt;@&amp;lt;호스트&amp;gt;권한 적용-- 권한 적용하기FLUSH PRIVILEGES권한 확인SHOW GRANT FOR &amp;lt;유저명&amp;gt;&amp;gt;@&amp;lt;호스트&amp;gt;권한 삭제REVOKE ALL PRIVILEGES ON &amp;lt;DB&amp;gt;.&amp;lt;테이블&amp;gt; FROM &amp;lt;유저&amp;gt;@&amp;lt;호스트&amp;gt;유저 삭제DROP USER &amp;lt;유저&amp;gt;@&amp;lt;호스트&amp;gt;현재 서버 사용중인 유저SELECT CURRENT_USER()",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series20'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part20] DCL(1): GRANT, REVOKE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series20'>MySQL Series [Part20] DCL(1): GRANT, REVOKE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part19] DML(4): INSERT, UPDATE, DELETE",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series19",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  데이터 추가  데이터 갱신  데이터 삭제데이터 추가-- 데이터 추가INSERT INTO &amp;lt;테이블명&amp;gt; (col1, col2, col3, ...)     VALUES (val1, val2, val3, ...);-- 특정 col에만 데이터 넣을 수도 있다INSERT INTO &amp;lt;테이블명&amp;gt; (col1, col3)     VALUES (val1, val3);-- SET을 이용한 방법INSERT INTO &amp;lt;테이블명&amp;gt;         SET col1=val1, col2=val2;데이터 갱신-- 데이터 갱신UPDATE &amp;lt;테이블명&amp;gt;   SET col1 = &amp;lt;갱신 데이터&amp;gt; WHERE &amp;lt;조건&amp;gt;; -- 기존 값을 기준으로 갱신UPDATE &amp;lt;테이블명&amp;gt;   SET col1 = &amp;lt;col1 + 3&amp;gt; WHERE &amp;lt;조건&amp;gt;; 데이터 삭제-- 테이블을 사용했던 흔적이 남는다 -- (AUTO_INCREMENT된 프라이머리키가 15에서 모두 삭제돼도 다음 삽입되는 프라이머리 키가 1이 아니라 16이 됨)DELETE FROM &amp;lt;테이블명&amp;gt;      WHERE &amp;lt;조건&amp;gt;-- 테이블을 사용했던 흔적을 아예 없앤다TRUNCATE &amp;lt;테이블명&amp;gt;   WHERE &amp;lt;조건&amp;gt;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series19'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part19] DML(4): INSERT, UPDATE, DELETE'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series19'>MySQL Series [Part19] DML(4): INSERT, UPDATE, DELETE</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part18] DML(3): SELECT 중급 WINDOW",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series18",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  윈도우 함수  OVER절  윈도우 함수 예시          ROW_NUMBER()      RANK()      DENSE_RANK()      LEAD()      LAG()      윈도우 함수  특정 범위마다 함수를 적용하는 것을 윈도우 함수라고 함  MySQL에서 제공하는 윈도우 함수라고 따로 정의해둔 윈도우 함수 묶음이 있음  집계 함수도 OVER절을 이용해 범위를 정의하면 윈도우 함수로 사용할 수 있음(Most aggregate functions also can be used as window functions, MySQL 공식문서)  사용 방법: [윈도우 함수] + [OVER 절] or [집계 함수] + [OVER 절]  범위마다 함수를 적용한다는 점에서 GROUP BY와 비슷하게 느껴지지만, GROUP BY는 집계된 결과를 테이블로 보여주는 반면, 윈도우 함수는 집계된 결과를 기존 테이블에 하나의 열로 추가하여 결과를 볼 수 있음OVER절  윈도우 함수는 항상 OVER절과 함께 사용됨  2018년에 MySQL에 처음으로 윈도우 함수가 도입되었고, 윈도우 함수는 OVER절을 통해 접근할 수 있음  윈도우 함수는 set of rows에 특별한 함수 또는 계산을 위한 용도. 이러한 set of rows를 window라고 함  이러한 윈도우가 OVER절에 의해 정의됨  OVER clause which has three possible elements: partition definition, order definition, and frame definition.    {window_function(expression)] \| [aggregation_function(expression)} OVER (  [partition_defintion] [order_definition] [frame_definition])        PARTITION BY: 윈도우 범위 결정  ORDER BY: 정렬하여 계산윈도우 함수 예시ROW_NUMBER()  행 번호를 매길 수 있음RANK()  윈도우 내에서 순위를 매길 수 있음  공동 2등이 2명이면 다음은 4등 -&amp;gt; 1, 2, 2, 4DENSE_RANK()  윈도우 내에서 순위를 매길 수 있음  공동 2등이 2명 있더라도 다음은 3등 -&amp;gt; 1, 2, 2, 3LEAD()  현재 행에서 다음에 있는 행들과 관계를 맺을 수 있음LAG()  현재 행 앞에 있는 행들과 관계를 맺을 수 있음",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series18'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part18] DML(3): SELECT 중급 WINDOW'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series18'>MySQL Series [Part18] DML(3): SELECT 중급 WINDOW</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part17] DML(2): SELECT 중급 JOIN, SUBQUERY",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series17",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  INNER JOIN  LEFT JOIN, RIGHT JOIN  UNION  Subquery          서브쿼리를 사용할 수 있는 위치        참고INNER JOIN  두 테이블 모두 조건을 만족하는 레코드만 가지고 옴  MySQL에서는 INNER JOIN, JOIN, 콤마(,) 모두 INNER JOIN을 뜻함  ON에서 두 테이블을 연결시킬 조건없이 사용하는 경우를 Cartesian Product(곱집합) 이라고 함  ex. A: {1, 2, 3}, B: {x, y}일 때 FROM A, B를 하면 -&amp;gt; {[1, x], [1, y], [2, x], [2, y], [3, x], [3, y]}  INNER JOIN은 인덱스 유무에 따라 옵티마이저가 알아서 기준 테이블(Driving table)과 대상 테이블(Driven table)이 정해짐  드리븐 테이블이 성능 부하가 많은 편이라 인덱스가 있는 테이블을 드리븐 테이블로 사용해, 최대한 드리븐 테이블의 성능 부하를 낮춘다-- 정석적인 작성법SELECT u.userid, u.name FROM usertbl AS u INNER JOIN buytbl AS b ON u.userid=b.userid WHERE u.userid=&quot;111&quot;-- 축약 작성법SELECT u.userid, u.name FROM usertbl u, buytbl b WHERE u.userid=b.userid AND u.userid=&quot;111&quot;/*내 생각에 위의 방법은 딱 교집합인 결과에서 WHERE 조건절로 필터링아래 방법은 곱집합으로 len(u) * len(b)만큼의 결과에서 WHERE 조건절 사용 -&amp;gt; 훨씬 느릴것 같다*/참고로 JOIN이 있는 쿼리문의 실행 순서는 다음과 같다.  FROM  ON  JOIN  WHERE  GROUP BY  HAVING  SELECT  ORDER BY  LIMITLEFT JOIN, RIGHT JOIN  둘 다 OUTER JOIN  LEFT는 첫 번째 테이블을 기준으로 두 번째 테이블을 조인, RIGHT는 두 번째 테이블이 기준  그래서 OUTER JOIN은 순서가 중요 -&amp;gt; 결과 자체의 측면과 성능적 측면 두 가지가 있음          우선 성능을 따지기 전에 결과 자체가 우리가 원하는 결과가 나와야 함 -&amp;gt; 모든 레코드가 나와야 하는 테이블을 기준 테이블      순서에 상관없이 결과가 같다고 판단되는 경우 성능을 따져야함 -&amp;gt; 인덱스가 있는 테이블을 드리븐 테이블로 쓰자(LEFT면 두 번째)        만약 OUTER JOIN, INNER JOIN 어떤 것을 써도 된다면 옵티마이저가 드라이빙 테이블을 선택할 수 있는 INNER JOIN이 나음SELECT STUDENT.NAME, PROFESSOR.NAME FROM STUDENT LEFT OUTER JOIN PROFESSORON STUDENT.PID = PROFESSOR.ID WHERE GRADE = 1UNION  같은 구조를 가지는 테이블을 합치는 것  UNION은 두 테이블이 같은 데이터를 가질 경우 한 개만 최종 테이블에 반영되도록 함 (중복 허용 X)  UNION ALL은 두 테이블이 같은 레코드를 가지더라도 합칠 경우 둘 다 최종 테이블에 반영(중복 허용)SELECT 필드이름 FROM 테이블이름UNIONSELECT 필드이름 FROM 테이블이름Subquery  서브쿼리(subquery)는 다른 쿼리 내부에 포함되어 있는 SELETE 문을 의미  서브쿼리는 다음과 같이 괄호()로 감싸서 표현  메인쿼리 실행 중간에 서브쿼리 실행. 서브쿼리 종료 후, 메인쿼리도 실행 모두 마치고 종료  메인쿼리 실행 되면 먼저 FROM으로 메인 테이블 불러오기 때문에, 서브쿼리는 메인쿼리의 컬럼 사용 가능  서브쿼리는 가독성이 좋다는 장점이 있지만 JOIN 보다 성능이 느림 -&amp;gt; 둘다 가능한 경우 JOIN 사용  (최신 MySQL은 사용자가 서브쿼리문을 사용하면 자체적으로 조인문으로 변환하여 실행시키도록 업데이트 되었음)서브쿼리를 사용할 수 있는 위치SELECT FROMWHEREHAVINGINSERTUPDATE참고  인파, [MYSQL] 📚 서브쿼리 개념 &amp;amp; 문법 💯 정리  Navicat, Joins versus Subqueries: Which Is Faster?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series17'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part17] DML(2): SELECT 중급 JOIN, SUBQUERY'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series17'>MySQL Series [Part17] DML(2): SELECT 중급 JOIN, SUBQUERY</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part16] DML(1): SELECT 기초",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series16",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/mysql-series16'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part16] DML(1): SELECT 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series16'>MySQL Series [Part16] DML(1): SELECT 기초</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part26]: I 🤍 Logs(2) Data Integration",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series26",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  Introduction  Two Complications          Data is More Diverse      The Explosion of Specialized Data Systems        Log-Structured Data Flow  Experience at LinkedIn  Relationship to ETL and Data Warehouse  ETL and Scalability  Where Should We Put the Data Transformations?  Decoupling Systems  Scaling a Log  참고Introduction  Data Integration and Logs  Data Integration means making available all the data (that an organization has) to all the services and systems that need itThe more recognizable term ETL(populating a relational data warehouse) usually covers only a limited part of data integration.  Capturing all the relevant data and being able to put it together in an applicable processing environment  This data has to be modeled in a uniform way to make it easy to read and process  Process this data in various ways: MapReduce, real-time query systems, ans so onFocus on step-by-step  Reliable and complete data flow  Refining data modeling and consistency  Better visualization, reporting, algorithmic processing and predictionHow can we build reliable data flow throughout all the data systems?Two ComplicationsTwo things have made data integration an increasingly difficult proflem.Data is More DiverseTransactional data - things that are,  Event data - things that happenLog = Data structure what event data is loggedEvent data is generated from Web service, Financial organization, IoTThis type of event data shakes up traditional data integration approaches because it tends to be several orders of magnitude larger than transactional data.The Explosion of Specialized Data Systemsex. OLAP, Search service, Batch processing, Graph analysisLog-Structured Data FlowLog is the natural problem data structure for handling data flow between systems.Experience at LinkedInRelationship to ETL and Data WarehouseETL and ScalabilityWhere Should We Put the Data Transformations?Decoupling SystemsScaling a Log참고  책 I Heart Logs  Try Kill batch processing with unified log stream processing…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/data-engineering-series26'> <img src='/images/i_heart_log.png' alt='Data Engineering Series [Part26]: I 🤍 Logs(2) Data Integration'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series26'>Data Engineering Series [Part26]: I 🤍 Logs(2) Data Integration</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part25]: I 🤍 Logs(1) Introduction",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series25",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  What Is a Log?  Logs in Database  Logs in Distributed System          Log-Centric Design Pattern      What Is a Log?Yet other than perhaps occasionally tailing a log file, most engineers don’t think much about logs. To help remedy that, I’ll give an overview of how logs work in distributed systems, and then give some practical applications of these concepts to a variety of common uses: data integration, enterprise architecture, real-time data processing, and data system design.[2022-07-02 05:30:44] method=POST user=crazyboy0510 path=/movies/comment-create/ movie_id=16[2022-07-02 05:30:57] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7[2022-07-02 05:31:15] method=GET user=crazyboy0510 path=/movies/movie-play/16 movie_id=16[2022-07-02 05:31:18] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7[2022-07-02 05:31:19] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7Every programmer is familiar with this kind of log - a series of loosely structured requests, errors, or other messages in a sequence of rotating text files.The purpose of logs quickly becomes an input to queries in order to understand behavior across many machines, something that English text in files is not nearly as appropriate for as the kind of structured log I’ll be talking about.The log I’ll be discussing is a little more general and closer to what in the database or systems called a ‘commit log’. It is append-only sequence of records ordered by time.Each rectangle represents a record that was appended to the log. Records are stored in the order they were appended. The contents and format of the records aren’t important for the purposes of this discussion. To be concrete, we can just imagine each record to be a JSON blob.The log entry number can be thought of as the ‘timestamp’ of the entry. this is convenient property of being decoupled from any particular physical clock. This property is essential as we get to distributed systems.A log is just kind of table or file where the records are sorted by timetable: array of records  file: array of bytes  However it is important that we thing about the log as an abstract data structure, not a text file.Logs have a specific purpose: they record what happened and when. For distributed data systems, this is the heart of the problem.Logs in DatabaseThe usage in databases has to do with keeping in sync a variety of data structures and indexes in the presence of crashes. To make this atomic and durable, a database uses a log to write out information about the records it will be modifying before applying the changes to all the various data structures that it maintains.The log is the record of what happened, and each table or index is a projection of this history into some useful data structure or index.Over time, the usage of the log grew from an implementation detail of the ACID database properties to a method for replicating data between databases. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a remote replica database in sync. Oracle, MySQL, PostreSQL, and MongoDB include log shipping protocols to transmit portions of a log to replica databases that act as slaves. The slaves can then apply the changes recorded in the log to their own local data structures to stay in sync with the master.In fact, the use of logs is variations on the two uses in database internals:  The log is used as a publish/subscribe mechanism to transmit data to other replicas  The log is used as a consistency mechanism to order the updates that are applied to multiple replicasLogs in Distributed SystemThe same problems that databases solve with logs (like distributing data to replicas and agreeing on update order) are among the most fundamental problems for all distributed systems.The log-centric approach to distributed systems arises from a simple observation  If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state(Desterministic means that the processing isn’t timing dependent)The application to distributed computing is pretty obvious. You can reduce the problem of making multiple machines all do the same thing to the problem of implementaing a consistent log to feed input to theses processes. The purpose of the log here is to squeeze all the nondeterminism out of the input stream to ensure that each replica stays in sync.Discrete log entry numbers act as a clock for the state of the replicas - you can describe the state of each replica by a single number: the timestamp for the maximum log entry that it has processed. Two replicas at the same time will be in the same state.Log-Centric Design PatternThere are many variations on how this principle can be applied., depending on what is put in the log. For example, we can log the incoming requests to a service and have each replica process these independently. Or we can have one instance that processed requests and log the state changes that the service undergoes in response to a request.Database people generally differentiate between physical and logical logging. Physical or row-based logging means logging the contents of each row that is changed (로우별 실제 변경된 데이터를 저장하는 것). Logical or statement logging  means logging the SQL commands that lead to the row changes (insert, update, and delete statements).The distributed systems distinguished two broad approaches to processing and replication. The state machine model keep a log of the incoming requests and each replica processes each request in log order. primary backup elect one replica as the leader. This leader processes requests in the order they arrive and logs the changes to its state that occur as a result of processing the requests.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/data-engineering-series25'> <img src='/images/i_heart_log.png' alt='Data Engineering Series [Part25]: I 🤍 Logs(1) Introduction'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series25'>Data Engineering Series [Part25]: I 🤍 Logs(1) Introduction</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part22]: 행렬(Matrix) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/matrix",
      "date"     : "Aug 1, 2022",
      "content"  : "Table of Contents  MatrixMatrixLeetcode: Two-Pointers💟 ✅ ❎문제 리스트---------------------------------------------EASY 3문제- Richest Customer Wealth- Flipping an Image- Matrix Diagonal Sum- The K Weakest Rows in a Matrix- Toeplitz Matrix- Shift 2D Grid- Transpose Matrix---------------------------------------------MEDIUM 5문제- Sort the Matrix Diagonally- Remove All Ones With Row and Column Flips- Candy Crush- Max Area of Island- Rotate Image- Sparse Matrix Multiplication- Game of Life- Construct Quad Tree- Spiral Matrix II- Walls and Gates- Number of Islands- Rotting Oranges- Shortest Path in Binary Matrix- Word Search",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-08-01T21:01:35+09:00'>01 Aug 2022</time><a class='article__image' href='/matrix'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part22]: 행렬(Matrix) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/matrix'>Coding Test Series [Part22]: 행렬(Matrix) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part15] MySQL Optimizing SELECT Statements",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series15",
      "date"     : "Jul 29, 2022",
      "content"  : "Table of Contents  Optimization Overview  Things to Consider for Optimization          SELECT      WHERE      GROUP BY      JOIN      Subquery      Temporary Table      ORDER BY                  정렬 처리 방법                      참고Optimization OverviewDatabase performance depends on several factors at the database level, such as tables, queries, and configuration settings. These software constructs result in CPU and I/O operations at the hardware level, which you must minimize and make as efficient as possible.Things to Consider for Optimization      Are the tables structured properly? In particular, do the columns have the right data types, and does each table have the appropriate columns for the type of work? For example, applications that perform frequent updates often have many tables with few columns, while applications that analyze large amounts of data often have few tables with many columns.        Are the right indexes in place to make queries efficient?        Are you using the appropriate storage engine for each table, and taking advantage of the strengths and features of each storage engine you use? In particular, the choice of a transactional storage engine such as InnoDB or a nontransactional one such as MyISAM can be very important for performance and scalability.        Does each table use an appropriate row format? This choice also depends on the storage engine used for the table. In particular, compressed tables use less disk space and so require less disk I/O to read and write the data. Compression is available for all kinds of workloads with InnoDB tables, and for read-only MyISAM tables.        Does the application use an appropriate locking strategy? For example, by allowing shared access when possible so that database operations can run concurrently, and requesting exclusive access when appropriate so that critical operations get top priority. Again, the choice of storage engine is significant. The InnoDB storage engine handles most locking issues without involvement from you, allowing for better concurrency in the database and reducing the amount of experimentation and tuning for your code.        Are all memory areas used for caching sized correctly? That is, large enough to hold frequently accessed data, but not so large that they overload physical memory and cause paging. The main memory areas to configure are the InnoDB buffer pool and the MyISAM key cache.  SELECT  Avoid using *  Avoid using DISTINCT -&amp;gt; 중복 데이터 제거를 위해 테이블 풀 스캔 해야함WHERE  Use Indexes Where Appropriate  Avoid % Wildcard in a Predicate  Avoid using a function in the predicate of a query  BETWEEN, IN, &amp;lt;, &amp;gt;GROUP BY  GROUP BY 작업은 크게 인덱스를 사용하는 경우와 사용할 수 없는 경우(임시 테이블을 사용)  인덱스를 사용할 수 없는 경우, 전체 테이블을 스캔하여 각 그룹의 모든 행이 연속되는 새 임시 테이블을 만든 다음 이 임시 테이블을 사용하여 그룹을 검색하고 집계 함수를 적용하는 것          이렇게 인덱스를 사용할 수 없을 때 할 수 있는 최선의 방법은 WHERE절을 이용해 GROUP BY 하기 전에 데이터량을 줄이는 것        인덱스를 잘 설정한다면 임시 테이블을 생성하지 않고 빠르게 데이터를 가져올 수 있다          인덱스를 최대로 활용하기 위해서는 GROUP BY 컬럼과, 인덱스되어 있는 컬럼간의 순서가 중요함      SELECT절에 사용되는 집계함수의 경우 MIN(), MAX()는 인덱스의 성능을 최대로 활용할 수 있도록 함            참고로 MySQL 8.0부터는 GROUP BY를 한다고 해서 암묵적으로 정렬이 이루어지지 않음 -&amp;gt; 정렬 필요하면 명시적으로 ORDER BY 써야함    루스 인덱스 스캔을 사용할 수 있는 경우          루스 인덱스 스캔은 레코드를 건너뛰면서 필요한 부분만 가져오는 스캔 방식      EXPLAIN을 통헤 실행 계획을 확인해보면 Extra 컬럼에 ‘Using index for group-by’ 라고 표기됨      MIN(), MAX() 이외의 함수가 SELECT 절에 사용되면 루스 인덱스 스캔을 사용할 수 없음      인덱스가 (col1 col2, col3) 일 때 , GROUP BY col1, col2 과 같아야 함 (GROUP BY col2, col3은 안됨)      SELECT 절과 GROUP BY 절의 컬럼이 일치해야 함. SELECT col1, col2, MAX(col3) GROUP BY col1, col2 과 같아야 함        타이트 인덱스 스캔을 사용하는 경우          SELECT 절과 GROUP BY 절의 컬럼이 일치하지 않지만, 조건절을 이용해 범위 스캔이 가능한 경우                  SELECT c1, c2, c3 FROM t1 WHERE c2 = ‘a’ GROUP BY c1, c3;          SELECT c1, c2, c3 FROM t1 WHERE c1 = ‘a’ GROUP BY c2, c3;                    JOIN  INNER joins, order doesn’t matter  OUTER joins, the order matters  여러 조인을 포함하는 LOOP JOIN 에서는 드라이빙 테이블(Driving Table)이 행들을 최소한으로 리턴하도록 해야됨  JOIN 되는 컬럼의 한쪽에만 INDEX가 있는 경우는 INDEX가 지정된 TABLE이 DRIVING TABLE이 된다  JOIN 시 자주 사용하는 칼럼은 인덱스로 등록한다인덱스 레인지 스캔은 인덱스를 탐색(Index Seek)하는 단계와 인덱스를 스캔(Index Scan)하는 과정으로 구분해 볼 수 있다. 일반적으로 인덱스를 이용해서 쿼리하는 작업에서는 가져오는 레코드의 건수가 소량(전체 데이터 크기의 20% 이내)이기 때문에 인덱스 스캔 작업은 부하가 작고, 특정 인덱스 키를 찾는 인덱스 탐색 작업이 부하가 높은 편이다.JOIN 작업에서 드라이빙 테이블을 읽을 때는 인덱스 탐색 작업을 단 한 번만 수행하고, 그 이후부터는 스캔만 실행하면 된다.하지만 드리븐 테이블에서는 인덱스 탐색 작업과 스캔 작업을 드라이빙 테이블에서 읽은 레코드 건수만큼 반복한다.드라이빙 테이블과 드리븐 테이블이 1:1 조인되더라도 드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지한다.그래서 옵티마이저는 항상 드라이빙 테이블이 아니라 드리븐 테이블을 최적으로 읽을 수 있게 실행 계획을 수립한다.SELECT *FROM employees e, dept_emp deWHERE e.emp_no=de.emp_no여기서 각 테이블의 emp_no 컬럼에 인덱스가 있을 때와 없을 때 조인 순서가 어떻게 달라지는 한 번 살펴보자.  두 컬럼 모두 인덱스가 있는 경우          어느 테이블을 드라이빙으로 선택하든 인덱스를 이용해 드리븐 테이블의 검색 작업을 빠르게 처리할 수 있다      보통의 경우 어느 쪽 테이블이 드라이빙 테이블이 되든 옵티마이저가 선택하는 방법이 최적일 때가 많다        employees 테이블에만 인덱스가 있는경우          이 때는 employees 테이블을 드리븐 테이블로 선택한다      드리븐 테이블을 읽는 것이 훨씬 더 큰 부하를 차지하기 때문에 드리븐 테이블에서 인덱스를 활용하는 것이 중요한다      INNER JOIN은 조인 대상 테이블 모두에 해당하는 레코드만 반환한다. 이같은 특성 때문에 OUTER JOIN으로만 조인을 실행하는 쿼리들도 자주 보인다. 하지만 대개의 경우 OUTER JOIN은 대상 테이블들의 데이터가 일관되지 않은 경우에만 필요하다.MySQL 옵티마이저는 OUTER JOIN시 조인 되는 테이블(FROM A LEFT JOIN B에서 B)을 드라이빙 테이블로 선택하지 못하기 때문에 무조건 앞에 등장하는 테이블을 드라이빙 테이블로 선택한다. 그 결과 인덱스 유무에 따라 조인 순서를 변경함으로써 얻게 되는 최적화의 이점을 얻지 못하기 때문에 쿼리 성능이 나빠질 수 있다. 그래서 꼭 필요한 경우가 아니라면 INNER JOIN을 사용하는 것이 쿼리의 성능에 도움이 된다.JOIN의 순서  INNER JOIN인 경우          어차피 A and B and C 이기 때문에 A JOIN B JOIN C이든 B JOIN A JOIN C이든 같다.        LEFT JOIN의 경우 결과도 성능도 달라진다.          일단 가장 먼저 등장하는 테이블이 드라이빙 테이블이 된다 -&amp;gt; 이 말은 뒤에 따라오는 테이블은 드리븐 테이블이 된다는 말이다 -&amp;gt; 드리븐 테이블은 인덱스가 없으면 성능이 떨어진다 -&amp;gt; 뒤에 조인되는 테이블의 인덱스 유무에 따라 쿼리 성능이 달라진다      결과 자체도 맨 앞에 등장하는 테이블의 모든 레코드가 기준이 되기 때문에 순서에 따라 달라진다        INNER JOIN과 OUTER JOIN이 결합되는 경우          가능하다면 INNER JOIN이 앞에 오도록 하는 것이 좋다      Subquery  Avoid correlated sub queries as it searches row by row, impacting the speed of SQL query processing  JOIN으로 해결되면 서브쿼리 대신 JOIN을 사용하자  서브쿼리 안에 where절과 group by를 통해 불러오는 데이터양을 감소시킬 수 있습니다  서브쿼리는 인덱스 또는 제약 정보를 가지지 않기 때문에 최적화되지 못한다  윈도우 함수를 고려해보자Temporary Table  Use a temporary table to handle bulk data  Temporary table vs Using index accessORDER BY  대부분의 SELECT 쿼리에서 정렬은 필수적  정렬을 처리하는 방법은 인덱스를 이용하는 방법과 Filesort라는 별도의 처리를 이용하는 방법            방법      장점      단점              인덱스 이용      SELECT 문을 실행할 때 이미 인덱스가 정렬돼 있어 순서대로 읽기만 하면 되므로 매우 빠르다      INSERT, UPDATE, DELETE 작업시 부가적인 인덱스 추가/삭제 작업이 필요하므로 느리다              Filesort 이용      인덱스 이용과 반대로 INSERT, UPDATE, DELETE 작업이 빠르다      정렬 작업이 쿼리 실행 시 처리되어 쿼리의 응답 속도가 느려진다      Filesort를 사용해야 하는 경우  정렬 기준이 너무 많아서 모든 인덱스를 생성하는 것이 불가능한 경우  어떤 처리의 결과를 정렬해야 하는 경우  랜덤하게 결과 레코드를 가져와야 하는 경우소트 버퍼  MySQL은 정렬을 수행하기 위해 별도의 메모리 공간을 할당받아서 사용하는데 이 메모리 공간을 소트 버퍼라고 한다  정렬해야 할 레코드의 건수가 소트 버퍼의 크기보다 크다면 어떻게 해야 할까?          정렬해야 할 레코드를 여러 조각으로 나눠서 처리하게 됨. 이 과정에서 임시 저장을 위해 디스크를 사용      일부를 처리하고 디스크에 저장하기를 반복 수행함      정렬 알고리즘  정렬 대상 컬럼과 프라이머리 키만 가져와서 정렬하는 방식          정렬 대상 컬럼과 프라이머리 키 값만 소트 버퍼에 담아 정렬을 수행      그리고 다시 정렬 순서대로 프라이머리 키로 테이블을 읽어서 SELECT할 컬럼을 가져옴      가져오는 컬럼이 두 개 뿐이라 소트 버퍼에 많은 레코드를 한 번에 읽어올 수 있음      단점은 테이블을 두 번 읽어야 함        정렬 대상 컬럼과 SELECT문으로 요청한 컬럼을 모두 가져와서 정렬하는 방식          최신 버전의 MySQL에서 일반적으로 사용하는 방식      SELECT 문에서 요청한 컬럼의 개수가 많아지면 계속 분할해서 소트 버퍼에 읽어와야함      레코드의 크기나 건수가 작은 경우 성능이 좋음      정렬 처리 방법  인덱스를 사용한 정렬          인덱스를 이용해 정렬을 하기 위해서는 반드시 ORDER BY의 순서대로 생성된 인덱스가 있어야 함      인덱스를 이용해 정렬이 가능한 이유는 B-Tree 인덱스가 키 값으로 정렬되어 있기 때문        Filesort를 사용한 정렬          인덱스를 사용할 수 없는 경우, WHERE 조건에 일치하는 레코드를 검색해 정렬 버퍼에 저장하면서 정렬을 처리(FIlesort)함      참고  MySQL 공식문서: Optimizing SELECT Statements  MySQL Performance Tuning and Optimization Tips  ETL 성능 향상을 위한 몇 가지 팁들  전지적 송윤섭시점 TIL, GROUP BY 최적화  SQL 성능을 위한 25가지 규칙  패스트캠퍼스 SQL튜닝캠프 4일차 - 조인의 기본 원리와 활용  취미는 공부 특기는 기록, Nested Loop Join, Driving Table  stackoverflow, Does the join order matter in SQL?  코딩팩토리, [DB] 데이터베이스 NESTED LOOPS JOIN (중첩 루프 조인)에 대하여  고동의 데이터 분석, [SQL] “성능 관점”에서 보는 결합(Join)  고동의 데이터 분석, [SQL] 성능 관점에서의 서브쿼리(Subquery)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-29T21:01:35+09:00'>29 Jul 2022</time><a class='article__image' href='/mysql-series15'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part15] MySQL Optimizing SELECT Statements'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series15'>MySQL Series [Part15] MySQL Optimizing SELECT Statements</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part21]: 파이썬 문법",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/python-things",
      "date"     : "Jul 29, 2022",
      "content"  : "Table of Contents  In python, ‘and’ operation will not return a boolean valueIn python, ‘and’ operation will not return a boolean value  If it’s true, it will return the last true value, remember is the value, not True. Otherwise, it will return the first false value.&#39;ban&#39; and &#39;car&#39; -&amp;gt; &#39;car&#39;0 and &#39;car&#39; -&amp;gt; 0&#39;ban&#39; and False -&amp;gt; False&#39;ban&#39; or False -&amp;gt; &#39;ban&#39;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-29T21:01:35+09:00'>29 Jul 2022</time><a class='article__image' href='/python-things'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part21]: 파이썬 문법'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-things'>Coding Test Series [Part21]: 파이썬 문법</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part24]: 시스템 디자인(4) Database",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series24",
      "date"     : "Jul 26, 2022",
      "content"  : "Table of Contents  Factors  Caching Solution  File Storage Solution  Storage Solutions Offering Text Search Capability  Time Series Database  Data Warehousing Storage Solution  RDBMS vs NoSQLDatabases will not impact your functional requirements. Whichever database you use, you can still achieve your functional requirements somehow, but at the cost of huge performance degradation. So when we say requirement, we usually mean non-functional requirements.Factors  Structure of the data  Query pattern  Amount or scale that you need to handleThese are the factors we need to consider when selecting which database to use. Now let us look at various types of storage solutions and some use cases where they will be suitable.Caching SolutionIf you are calling your database very frequently or making a remote call to independent services with high latency, you might want to cache some data locally at your end. Some of the most commonly used caching solutions are Memcached, Hazelcast, and Redis. You could also use some other solutions; this is not an exhaustive list. In the following articles, we will usually use Redis as it is one of the most widely used and stable solutions.File Storage SolutionAssume you are working on something like Netflix and you need a data store for images, videos, etc. Now, in this case, a database is not very useful to us as we are storing files rather than information. Databases are meant to store information that can be queried, whereas files you do not need to query. You just deliver them as they are.This is when we use something called Blob (Binary Large Object) storage. Amazon S3 is an example of blob storage. Usually, blob storage is used in combination with a Content delivery network or a CDN. A CDN is a network of servers around the world that delivers content in different geographical locations with reduced latency. If the server you are getting content from is closer to your geographic location, the content will take less time (reduced latency) to be delivered from the server to you.Storage Solutions Offering Text Search CapabilityLet’s again take the Netflix example. Suppose you want to build a search functionality where the user can search by movie, genre, actor, actress, director, etc. Here you use a search engine like Solr or Elasticsearch which can support fuzzy search.To understand fuzzy search, let us take an example of an Uber user searching for airprot. If you notice this is a typo, what the user means to search is airport. But if, because of this typo, we don’t provide any search results, it will be a very poor user experience. So we search for terms similar to airport in the database. This is known as fuzzy search.Now a key point here is that these search engines are not databases. Databases provide a guarantee that once stored, our data will not be lost unless we delete it; search engines offer no such guarantee. This is why we should never use search engines like Elasticsearch as our primary data source. We can load the data to them from our primary database to reduce search latency and provide fuzzy and relevance-based text search.Time Series DatabaseSuppose we are trying to build a metric tracking system. We will need something called a time-series database. Time-series databases are, in a way, an extension of relational databases, but unlike a standard relational DB, time-series databases will never be randomly updated. It will be updated sequentially in an append-only format.Also, it will have more bulk reads for a certain time range as opposed to random reads. For example, how many people watched a video in the last 1 week, 10 days, 1 month, 1 year, and so on. Some examples of time series databases are OpenTSDB and InfluxDB.Data Warehousing Storage SolutionSometimes we need a large database to dump all of the data available to us, to perform analytics. Eg. a company like Uber will store all of their data so they can perform analytics to identify where Uber is not used very much, where are the hotspots, what are the peak hours, etc. These systems are not used for regular transactions but offline reporting. Hadoop is a very commonly used Data warehouse.RDBMS vs NoSQLIf you need ACID properties, then you need to use a relational DBMS. Some examples are MySQL, Oracle, and Postgres. But what if you don’t need ACID? Well, you can still use RDBMS, or you can use a Non-relational database.Let’s consider an example. Suppose you are trying to build a catalog for something like Amazon, where you want to store information about different products that have various attributes. These attributes will normally not be the same for different products. In such a case, our data cannot be represented as a table. This means we need to use a NoSQL database.Also, we don’t just need to store this data but also query on this data. Here comes the factor of query pattern. Which type of database we use here will be decided based on what type of data we store and what types of queries will be run on it. If we have vast data - not just volume but also a vast variety of attributes - and we need to run a vast variety of queries, we need to use something called a Document DB. Couchbase and MongoDB are some commonly used document databases. (Elasticsearch is special cases of document DB)But what if you don’t have a vast variety of attributes i.e. very limited variety of queries, but the size of the database increases very rapidly? For example, data collected by Uber for their drivers’ location pings. Now the number of Uber drivers will keep increasing day by day, and therefore so will the data collected every day. This results in an ever-increasing amount of data. In such cases, we use Columnar DBs like Cassandra or HBase.Using Cassandra is lighter to deploy, whereas HBase is built on top of Hadoop; we would need to first set up Hadoop and then setup HBase on top of it. This makes the setup of HBase a little lengthy, but performance-wise both are pretty much the same.Let us assume we have stored Uber’s ride-related data in a Cassandra with driver id as a partition key. Now when we want to fetch a ride for a particular driver on a particular date, Cassandra can find it easily enough. But if we want to find a customer’s ride on a particular date, Cassandra will have to fan out this query to all the partitions since customer id is not a partition key. So what is the point of using Cassandra if it is not going to scale well!Well, there is a simple enough fix. We can replicate the same data to another table or column family with a different partition key. Now when we receive the query for customer id and date, we can simply direct it to the table where the partition key is customer id. This is what we mean by a limited variety of queries but a huge scale. Cassandra (and HBase) can scale massively as long as the queries are of similar types.If the queries are more diverse, then we will have to replicate again and again for each partition key, which we can, but only to a certain limit. If we cannot control the types of queries, then something like MongoDB might be the way to go. But if we just need a huge scale for a few types of queries, the Cassandra is the perfect solution.Now let’s shake things up a bit!Let’s consider the Amazon example again. If for a product we have only one item in stock but multiple users are trying to buy it, it should only be sold to one user, which means we need ACID here. So we should choose a Relational DB like MySQL.But the orders-related data for Amazon will be ever-increasing and will have a variety of attributes. That means we should use a Columnar NoSQL database like Cassandra. So which one to go for? We decide to go with a combination of both. We can store the data of orders that are not yet delivered in a MySQL database, and once the order is completed, we can move it to Cassandra to be permanently stored.But again, our requirements might be a little more complex. Suppose you want to build a reporting system for how many people bought a particular item. Now, on Amazon, products are sold by various users of different brands and different variations. So the report can not target a single product, rather, it should target a subset of products, which can be in either Cassandra or MySQL.Such a requirement is an example of a situation where our best choice would be a document DB like Mongo DB. So we decide to keep a subset of this orders data in Mongo DB that tells us which users bought how much quantity of a certain product, at what time, on what date, etc. So suppose you want to check how many people bought sugar in the last month. You can get order ids from Mongo DB and use this order id to pick up the rest of the data from Cassandra or MySQL.That should be it for Storage Solutions in System Design!",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-26T21:01:35+09:00'>26 Jul 2022</time><a class='article__image' href='/data-engineering-series24'> <img src='/images/system_design_logo.png' alt='Data Engineering Series [Part24]: 시스템 디자인(4) Database'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series24'>Data Engineering Series [Part24]: 시스템 디자인(4) Database</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part23]: 시스템 디자인(4) Caching",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series23",
      "date"     : "Jul 26, 2022",
      "content"  : "Table of Contents  What is a Cache?  When to Cache?  How Does Caching Work?  When Not to Cache?  Cache Eviction Strategies          Time Based      Size Based        Metrics  Caching ProductsWhat is a Cache?A cache is high-speed data access and storage layer that helps us fetch data that we had previously retrieved or computed.If we need frequent or repeated access to certain information that we have already queried from a service or a database, then instead of repeatedly querying the service, it’s better to cache that information for subsequent use, for it to be readily available.For Example, for Twitter users, we can cache all the information that is needed to load the homepage of a user (like followers, tweets, etc.). Caching will help avoid repeatedly querying the same service, caused by the refreshing of the browser by the user or other similar use cases. In this case, caching helps to reduce latency and improve resource utilization of servers.When to Cache?A cache is primarily used in the following scenarios:      When the data involves a lot of computation, then to reduce latency and CPU utilization, it’s good to cache the pre-calculated information for fast retrieval later on.        To reduce frequent database or network (API) calls, it’s beneficial to cache the previously fetched data for fast retrieval. This helps reduce latency as well as bandwidth requirements.  How Does Caching Work?A cache is primarily used to store the most recently or frequently used data, in the hope that it will soon be fetched again. Caches are typically faster than databases and services when it comes to re-accessing this stored information. What makes them so fast is the fact that caches store data in SSDs and mostly RAMs which reduces the lookup time. However, this does not mean that we should cache everything.When Not to Cache?There are some scenarios where the negative aspects of caching outweigh its benefits:      High Consistency requirements: When we fetch the previously stored data from the cache, there is a possibility of stale data being displayed to the user. For example, for a social media app, then some stale data is probably fine. However, for a stock price display app, then the cache must be in sync with the primary data source.        Write heavy / Read Once: When write operations (updates to data) are more frequent than read operations (data retrieval). For example, caching the data of an analytics system would only increase the hardware maintenance cost.        Low repetition: When the action of retrieval of the same information is not frequently repeated by the user. For example, the cost calculated by the trip cost estimation module of a cab booking app between the exact two points need not be cached.  Cache Eviction StrategiesWe need to regularly expel data from the cache to limit the size of the cache and to maintain its speed while ensuring that the entries are up to date.Time BasedWe keep an entry in the cache for some amount of time. In this strategy, we set a TTL (Time To Live). We will evict the entry from the cache after a certain pre-determined time has elapsed. The time which an entry stays in the cache before being evicted is called TTL.Size BasedWe keep at most some number of entries in the cache.      FIFO (First In First Out): We harness the FIFO property of the queue data structure to evict old entries.        LFU (Least Frequently Used): When we must evict an entry, we will evict the least frequently used entry.        LRU (Least Recently Used): When we must evict an entry, we will evict the least recently used entry.        LFRU (Least Frequently and Recently Used): We evict the least valuable entry, the one that’s neither used frequently nor recently. This strategy gives the best results.  MetricsWe can use the following metrics to evaluate the performance of the cache:      Size: Increasing the size of the cache usually increases the response time and therefore reduces its performance.        Latency: The introduction of cache into the system must reduce latency from what it was earlier.        Cache Hit Rate: It is the ratio of results found in the cache to the requests made to the cache. If the cache hit rate is low i.e., requests to cache are not returning the desired data, then it essentially slows down the system.  Caching ProductsIt’s usually not recommended to write your own cache implementation, because there are a lot of really good cache implementations out there that you can use. Some of the most popular caching products that are available in the market are:  Ehcache  Hazelcast  Memcached  Redis",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-26T21:01:35+09:00'>26 Jul 2022</time><a class='article__image' href='/data-engineering-series23'> <img src='/images/system_design_logo.png' alt='Data Engineering Series [Part23]: 시스템 디자인(4) Caching'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series23'>Data Engineering Series [Part23]: 시스템 디자인(4) Caching</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part22]: 시스템 디자인(3) Consistent Hashing",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series22",
      "date"     : "Jul 26, 2022",
      "content"  : "Table of Contents  Why hashing?  Consistent Hashing  Where is it UsedWhy hashing?Hash is basically a function that takes a value and converts it into another value of a specific format. Hash functions that are commonly used are MD5, SHA1, SHA256, etc.Suppose you build a distributed cache, where the data is distributed over various nodes, sometimes spanning multiple data centers. When we want to store the data for a user, we need to decide which node will cache this data. And when we want to retrieve this cached value, we must query the same node. So let us use a simple hash function for this again.hash(user) = Sum(ASCII value of all characters in name) % 5Where 5 is the number of nodes. This way, the hashes generated for users will be as follows:Since hash(Alice) is 0, Alice’s data will be stored in node 0. Similarly, Bob’s data will be stored in node 1 and Eve’s in 4. Now when you need to look up the information for Alice, you will use the same hash function to determine which node to query. Since hash(Alice) equals 0, you will query node 0 and fetch the information.But there is a problem with this solution. This model is not scalable. If the traffic increases and you want to add a new node, the formula to calculate the hash will get updated tohash(user) = Sum(ASCII value of all characters in name) % 6And similarly hash(Alice) will get updated to 2. So now you will search for Alice’s information in node 2, but the information is actually stored in node 0 so you won’t find it.To fix this, you will need to rehash all the data every time a node is added or removed, and once rehashed, you need to move the data to their respective new nodes, which could be across different data centers. This is not a very good solution as it uses up a lot of CPU resources and bandwidth.This is where Consistent Hashing comes in.Consistent HashingConsistent Hashing tries to optimize the system in such a way that:  You don’t need to move around all the data while adding or removing nodes.  There will be minimal movement of data as if a node is removed, the data from that node will be supported by another node. Similarly, when a node is added, some data will be mapped to it as you don’t want it to sit idle.  You can have a nearly even distribution of data across all machines.The idea is removing the number of nodes in the system out of the equation while calculating the hash. Now hashes for data and nodes will all be independently calculated and adding or removing nodes won’t change these hashes.Now instead of assigning the data to these nodes in a sequential manner, we will plot these nodes, or their hashes, on the number line, and each node will be responsible for the range between its position and the position of the first node to its right.When the data comes in for storage, we will calculate its hash to determine its place on the number line. Based on which node’s range it falls in, we will map the data to that node.If you want to remove a node from this system, the range of the previous machine on the number line will be extended, and all the data from the removed node will be mapped to the previous node in the number line. This resolves the issue of data transfer as only the data from one machine will need to be mapped to another machine.But there is still a problem with this system. After removing a node, the ranges of certain machines might not be balanced.An ideal solution to this would be assigning data between various nodes.The idea is to assign the same machine to multiple hashes and map each of these hashes on the number line.This can be achieved by either using multiple hash functions or by assigning multiple identifiers to the same node, and then calculating the hashes for all of the instanced. Then, We can map them on the number line, or what we can now refer to as a consistent hashing ring, essentially representing the same node on the ring multiple times.Now the data handled by a removed node will be distributed across the ring. So when a node needs to be removed, all its identifiers will be removed and the data will be mapped to the nodes falling to the right of each identifier, as shown in the following diagram, making the distribution much more even.(노드가 삭제되도, 그 노드에 저장되어 있던 데이터가 여러 해시 함수에 의해 분리된 인스턴스에 흩어져서 저장되어 있었으므로, 다른 노드로 이동할 때도 각 인스턴스의 오른쪽 인스턴스로 저장되므로 여러 노드에 분산 저장되게 된다. -&amp;gt; 데이터가 훨씬 even해진다.)The reason why it gets even is because each identifier of machine is hashed separately, with the hash function, it is very likely that they would be arranged in a random fashion like Blue_4 - Red_1 - Blue_2 - Green_3 - Red_4 - Orange_1, which makes sure that when one machine is removed, it’s data us spread across multiple other machines.Similarly, when a new node is added, its identifiers will also be mapped across the ring, picking up data from various nodes, maintaining even distribution across the ring.Where is it UsedNow we just saw how we could use consistent hashing while building a caching system. There are a lot of systems out there that use consistent hashing for improving their performance. For example, Cassandra, a distributed NoSQL Columnar DB that deals with huge traffic uses consistent hashing to distribute its data. Amazon’s Dynamo DB is another such example. It is a managed distributed DB which uses consistent hashing for distributing its data. Similarly, Couchbase is another NoSQL DB (a document DB) which uses consistent hashing to distribute its data across various instances.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-26T21:01:35+09:00'>26 Jul 2022</time><a class='article__image' href='/data-engineering-series22'> <img src='/images/system_design_logo.png' alt='Data Engineering Series [Part22]: 시스템 디자인(3) Consistent Hashing'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series22'>Data Engineering Series [Part22]: 시스템 디자인(3) Consistent Hashing</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part21]: 시스템 디자인(2) Inter-Service Communication",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series21",
      "date"     : "Jul 26, 2022",
      "content"  : "Table of Contents  Inter-Service Communication  Modes of communication  Synchronous Approach  Asynchronous Approach          Best of Both Worlds      Message Queues        Protocols for communication          How clients and servers interact      HTTP      WebSocket      Inter-Service CommunicationIn this article, we will be looking at how services interact with each other. Why is this important? Well, when you have a huge system with a lot of microservices interacting with each other, their communication needs to be efficient to provide the best user experience and also to avoid any cascading effects across the system.Modes of communicationThere are primarily two modes of communication between services:      Synchronous: When a service waits for a downstream system to respond before responding back to the client with a success or failure response.        Asynchronous: This is a more of a fire and forget approach. A service will fire a call to the downstream system and won’t track it further.  Synchronous ApproachLet’s say you are building Amazon. You have a user U1 trying to place an order. U1 will reach out to the Order Service. Order Service will now talk to the Inventory Service to find out if a sufficient quantity of the product is available. If that is the case, Inventory Service will send a success response. Otherwise, it will respond with an error, and Order Service will respond to the user saying the order could not be placed.Now if the inventory response was a success, the Order Service will talk to the Payment Service to process the payment. Once the payment is successful, the Order Service will now talk to the Warehouse Service asking it to start packing and prepare for shipping the product to the user. Once Warehouse Service responds with a success, the Order Service will talk to a Notification Service to send an email to the user saying their order has been placed, with so and so payment details and sharing an ETA for the delivery of the product.Now, this is a happy scenario. What happens when one of the calls fails? Well, it depends on which call fails. If the call to Notification Service fails, does it make sense to cancel the order? No. We shouldn’t cancel an order just because the Notification Service failed. However, what if payment fails? Now we definitely need to cancel the order. But now we need to update the Inventory again to undo the change to the product quantity. What if the call to Inventory Service fails?So as you can see, there are some loopholes in a purely synchronous approach.  It has very high latency as the user does not get notified until all the calls have come back with a success or failure response.  The system is tightly coupled, and any failure will have cascading effects across the board.  The code becomes very complex since we need to handle all the cascading error scenarios.  Due to complexity, it requires extremely high maintenance.Asynchronous ApproachLet us see what happens in a purely asynchronous approach.U1 sends a call to the Order Service which makes asynchronous calls to all the downstream systems. In such a case, even if Inventory Service responds with an error code, or even if the payment fails, the order would get placed. Which is an even bigger mess! So how do we go about this?Well, as we can see, some parts of this process must be mandatory, and some can be done on a best-effort basis. If the Inventory Service or Payment Service responds with an error, we cannot place the order. But if the notification does not go through or the Warehouse Service is temporarily down, we don’t need to cancel our order. So we can follow a hybrid approach here; use a synchronous approach for the mandatory steps and an asynchronous approach for the rest.Best of Both WorldsThe Hybrid approach suggests that the mandatory tasks need to be performed in a synchronous manner and everything else can be done asynchronously.So Order Service will send out a synchronous call to Inventory Service, and wait for a response. In case of success, it will call the Payment Service. If the Payment Service gives a successful response, Order Service will make parallel asynchronous calls to the Warehouse Service and Notification Service and, at the same time, respond to the user saying the order has been placed. If the Payment Service call had failed, Order Service would send an asynchronous call to the Inventory Service reverting the quantity change.So this looks like a much better solution. There are still some misses here though. What if the asynchronous call to Warehouse Service failed? It would lose the details for that order. This is where we would use Message Queues.Message QueuesMessage Queues(Kafka, RabbitMQ, ActiveMQ 등) are highly fault-tolerant and persist messages for some time. How a message Queue works is, it has some Publishers adding messages to it, and some Subscribers listening to it and picking up the events meant for them at their own pace. Since these queues store messages for some time, if a subscriber is temporarily down, the messages will remain in the queue and will be picked up when the subscriber is running again.So now, when Order Service wants to make asynchronous calls to Warehouse and Notification services, it will instead put an event in the Message Queue. Warehouse Service and Notification Service, which will be listening to the queue, will pick up the events meant for them. If one of the systems is down, the messages will remain in the queue until the service is back up and ready to receive messages again. This way, none of the data gets lost.Protocols for communicationIn this article, we will look at the protocols we can use to interact with clients.How clients and servers interactIn a real-world scenario, rather than talking to a specific server, the client’s request will instead be sent to a data center, where it could be picked up by any of the servers. However, irrespective of which server receives the request, the response will be the same. Based on this flow, we can draw the following conclusions about this architecture:  It is client-driven. Only on the user’s button click will the client send the requests to the server, and the server will only respond to these requests.  It is a simple request-response model. For every request from the client, the server will respond with some information or a simple confirmation.  There are occasional requests from clients, only one request every few seconds based on the user’s actions i.e. from the client-side it is a low throughput system.  It is a stateless system i.e. irrespective of which server is responding, the response remains the same.HTTPThese requirements make this a perfect use case for HTTP(s) protocol. Although these days, most architectures on HTTP have moved to HTTPS, which is a more secure version of HTTP as it prevents man-in-the-middle attacks.Now, when we are using HTTP, REST is usually the best API standard to follow as it is very widely used and very user friendly.Let us look at an example for a REST request and response:Request:Method: GETURL: https://www.twitter.com/user/{id}Response:Status: 200 OKHeaders: &amp;lt;...&amp;gt;Body: {    “userId”: 1,    “Email”: “someone@example.com”}The client makes a request to twitter.com over HTTPS to get information about a user with an id. In response, the server sends a success status code along with the user’s user id and email. As you can see, REST API standard is pretty much self-documenting, which adds to its user friendliness.Now let us look at an example of a chat application.We know that HTTP is a client-driven protocol, so the server cannot initiate any contact with the client. It can only respond to the client upon receiving a request. So when U1 sends a message to U2 via chat server, U2 doesn’t receive the message until it asks the server to share any pending messages. This leads to a delay when receiving messages.A solution to this would be that U2 sends frequent requests to the chat server in the hopes of receiving a message. But this puts a huge load on the chat server as it will receive a huge number of requests from all its clients.The best approach would be if the server could send a notification to the user every time there is a message. For this, we use a protocol called WebSocket.(서버가 U2에게 보낼 메시지를 가지고 있음에도 불구하고, 능동적으로 U2에게 보내지 않는다. U2로부터 request를 받을때까지 기다린다.)(U2는 언제 자신이 받아야할 메시지가 서버에 도착했는지 모르므로, 계속 서버에 request를 보내야 한다.)(WebSocket을 사용하면 서버는 U2와 connection되어 있으면 request를 받지 않아도 알아서 U2에 메시지를 보낸다)(connection되어 있지 않으면, 가만히 있다가, connection되고 U2가 request보내면 메시지 보낸다)(WebSocket에도 단점은 있다. cost of maintaining a persistent connection with millions of users.)WebSocketA WebSocket connection is a persistent connection. It is also a bidirectional protocol, where communication can be initiated by the client or the server as long as there is an open connection. It is optimized for high-frequency communication.Let’s look at how our chat application would work in the case of WebSocket protocol.First, U1 and U2 will establish HTTP connections with the chat server, which are then upgraded to a WebSocket connection. When U1 sends a message for U2 via the chat server, it will store the message along with its status, RECEIVED, let’s say.The chat server, if it has an open connection with U2, will then send the message to U2 and update the status to SENT. If U2 was not online and there was no open connection between U2 and the server, the messages will be saved until U2 comes online and requests the server to send all pending messages. The server will send all messages with the status RECEIVED and update the status to SENT.As you can see, with this approach we have:  Reduced the latency, since the server can simply send the messages over an open connection  Saved on CPU and bandwidth, as the client doesn’t need to unnecessarily send requests to the server and the server is not under unnecessary load  Provided better user experienceEven with the benefits, there is a high cost to using WebSockets; that is the cost of maintaining a persistent connection with millions of users.So how do we decide whether to use HTTP or WebSocket? Do we always go for Websocket then? Well, not really, as WebSocket is much more expensive than HTTP. We can safely say, if the communication between client and server is at a lower throughput on the client-side, HTTP is the way to go. If the communication is always client-driven, WebSocket is not needed. Also, if you are on a tight budget, HTTP may be the better choice.On the other hand, if the communication from the client is at a higher throughput, WebSocket may be a better option. If the communication can be driven by both client and server, WebSocket is the way to go. Although here comes the tradeoff between cost and performance. We must decide if the optimization is really worth the huge cost of maintaining persistent connections with so many users.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-26T21:01:35+09:00'>26 Jul 2022</time><a class='article__image' href='/data-engineering-series21'> <img src='/images/system_design_logo.png' alt='Data Engineering Series [Part21]: 시스템 디자인(2) Inter-Service Communication'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series21'>Data Engineering Series [Part21]: 시스템 디자인(2) Inter-Service Communication</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part20]: 시스템 디자인(1) Intro + Architecture",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series20",
      "date"     : "Jul 26, 2022",
      "content"  : "Table of Contents  Intro          What will be covered?      High Level System Design      Low Level System Design (Code Quality)        Application Architecture          Monolithic Architecture      Microservice Architecture        ConclusionIntroWhat will be covered?  Introduction to System Design: Start with an introduction of the course, explanations on high level and low level design, interviewer’s expectations, and how to approach these kind in an interview.  System Design Basics: Build a strong foundation of System Design fundamentals that are useful in understanding the dynamics of highly scalable systems.  System Design Case Studies: Cover the 11 most frequently asked interview questions with a detailed walkthrough.  Problem Solving: Apply your understanding of the preceding chapters with the help of some practice problems and quizzes.High Level System Design      Overall architecture: How the system can be built such that it takes care of scalability, latency, and other performance requirements        Components and services: Any system is a collection of various services and other components interacting with each other. In this part of high level design, we need to decide how the system will be broken down, what will the microservices be, and their scope.        Interaction between the systems: How will the systems interact with each other? What protocols will you use? Will it be synchronous or asynchronous? You need to make these decisions based on the requirements.        Databases: What databases will you need? What kind of data do you need to store? SQL or NoSQL, what will be the database schema? Depending on how much detail the interviewer wants you to go into, you may have to make these decisions as well.  Low Level System Design (Code Quality)  API: If you’re trying to build a web server, define your APIs clearly. For example, if you decide to make REST APIs, ensure they follow REST standards. Similarly, if you are building a library, define the APIs in a very clean manner as publicly accessible functions such that the client can easily integrate them  Test Code: This means we should have a working code with some basic test cases that are passing for the logic we have written.  Modular: how easy it is to add new features without interfering with existing code.Application ArchitectureMonolithic ArchitectureBack when the internet was just starting to gain popularity, websites used to serve mostly static content. There wasn’t a lot of user interaction like we see now. The applications were much less complex, and so was their architecture. A single application used to take care of the entire user journey, everything from UI rendering to backend business logic to fetching the data from DBs or File Systems. This was the world of Web 1.0.Then came Web 2.0 with social networks, e-commerce, and online gaming and things became a lot more interactive. By this time everything was still maintained in a single huge codebase. If you consider an e-commerce system, back then everything from UI to business logic for payments, carts, orders, etc. was maintained in a single codebase. This is known as Monolithic Architecture.The problem with this approach was that the code was very complex, difficult to maintain, and hard to iterate and improve. On top of that, multiple people were working on the same codebase; it was a recipe for disaster.Disadvantage of Monolithic Architecture  One of the most common problems with monoliths is that you are bound to a single technology stack. Suppose you have a monolith built on Java with Spring framework. Now you need to add some Machine Learning logic, and you want to use Python for it. That is nearly impossible in monolithic architecture. You either need to figure out a way to do this with Java, or you need a standalone application that handles your machine learning logic, which defeats the purpose of a monolithic app.  Another problem would be that it is very easy to break things in such an architecture. That is because, when you have such a huge codebase, it is nearly impossible for everyone to know everything about how the entire system works. If you change some logic in the shared code, you might end up breaking someone else’s feature. Sure you can have test cases, but even those are not enough sometimes.  Another major issue is scalability. It is very tricky to scale a monolithic application. Let us look at the example of an e-commerce application. In case of a Black Friday sale, you might need to scale your payments and cart modules, but Warehouse and Notification modules can continue to work at the same pace. This cannot be done in a monolithic app. Since it is the same codebase, you will need to deploy the entire system again and again. This means the capacity meant for the Warehouse module might be sitting idle or that the Payment and Cart modules may choke the rest of the system.  Deployments are also a very tedious process here. Since the code is huge, it takes much longer to build, package, and deploy the code. That said, if you update the code for Warehousing, even the Payments module must be redeployed since everything is packaged together.Microservice ArchitectureThe idea is to break down the application into logical components such that these components become services of their own. Normally this is also how the teams would be structured, so each team would work on the services that handle their features. These services will now be communicating with each other via a set of API calls like REST APIs or Remote Procedure Calls.Benefits of Microservice Architecture  We are not bound to a single technology stack anymore. Different services can use different languages or databases as needed.  Each system does one thing and does it well, without worrying about breaking another set of features.  Engineers will be working on and maintaining a smaller codebase.  Iterating, deploying, and testing becomes a lot easier.  Unlike in monolith, we can now independently scale up Cart and Payments Services, and the rest of the services can continue working as they are. This ensures optimized use of resources and makes auto-scaling a lot easier.Disadvantage of Microservice Architecture  Latency          One of the key reasons is latency. Function calls are faster than API calls, so it makes sense that monoliths will have lower latency. Usually, this latency is not high enough to be noticed by the user, but if you are working on something that needs a response in a few microseconds then you should definitely think about using monolithic architecture.      With network calls comes the possibility of network failures and slightly increases the complexity of error handling and retries.        Backward Compatibility          If a service needs a new mandatory parameter and the other services have not made the change accordingly, certain flows will break.      In monolith it will be caught in the development phase since the compiler will throw an error. To solve this we need some good automated test cases.        Logging          If you need to trace logs for a user in monoliths, it is easy to do so as they are all in the same place. But in microservices, for each request from the user there will be multiple service calls.      In the below example, consider a single request from the user to the order service. The order service is talking to Inventory, Payment, Warehouse, and Notification Services, and thus there will be logs in each of these services. So tracing exactly what happened becomes very difficult as we need to check logs in each system.      A better approach would be to store all the logs in a central place where they can be queried. This is a cost since you either need to build a Log Aggregation System or buy a third-party system’s license.      ConclusionIn conclusion, if you are a small business working with a small team and have a limited set of features, monolith might be a better approach for you. However, if you are working on a huge product with hundreds of microservices, the maintenance cost of microservices will be worthwhile.Also, usually, when you think about HLD(High Level Design) interviews, chances are you will be developing an overall architecture for a complex system that might be difficult to design in a monolithic manner. So thinking in terms of microservices will be a good idea.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-26T21:01:35+09:00'>26 Jul 2022</time><a class='article__image' href='/data-engineering-series20'> <img src='/images/system_design_logo.png' alt='Data Engineering Series [Part20]: 시스템 디자인(1) Intro + Architecture'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series20'>Data Engineering Series [Part20]: 시스템 디자인(1) Intro + Architecture</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part19]: 로그 공부하기",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series19",
      "date"     : "Jul 25, 2022",
      "content"  : "Table of Contents  Log  Log in Distributed System  Kafka Log          Commit Log        RDBMS Log          PostgreSQL      MySQL / MariaDB      Write Ahead Log      Transaction Log(Redo log + Undo log)      Binary Log        참고Log      A log, in a computing context, is the automatically produced and time-stamped documentation of events relevant to a particular system. Virtually all software applications and systems produce log files.        Logs are everywhere in software development. Without them there’d be no relational databases, git version control, or most analytics platforms.        A log can be useful for keeping track of computer use, emergency recovery, and application improvement.  A few common types of logs:  Transaction Log (Redo Log, Undo Log): contains a history of database changes. The term is usually associated with RDBMS. When you insert, update or delete database rows, the changes are journaled in the Transaction Log. Real-world RDBMS systems are based on one of the following types of the journal.  Commit Log: includes only committed transactions in their commit order. Every committed transaction is appended to the log sequentially as a group of change events. Rolled-back transactions will not appear in the Commit Log.  Binary Log: binary log 에는 데이터베이스에 대한 모든 변경 사항(데이터 및 구조)과 각 명령문 실행 시간이 기록되어 있다. binary log 의 목적은 백업 작업을 지원할뿐만 아니라, 하나 이상의 마스터에서 하나 이상의 슬레이브 서버로 데이터가 전송되어 복제하도록 하기 위한 것이다.  Access Log: lists all requests for individual files that people have requested from a website  Audit Log: Audit logg is the record of activity within the software systems used across your organization. Audit logs record the occurrence of an event, the time at which it occurred, the responsible user or service, and the impacted entity  System Log: syslog are records of the operating system (OS) events that indicate how the system was running. Information, errors, and warnings related to the computer operating system are displayed in the syslog.Transaction Log, Redo Log, Commit Log, Binary Log들은 좀 혼용되서 사용하는 것 같다. DB마다 같은 용어인데 다르게 사용되기도 하고, 또 사람들도 결과적으로 혼용해서 사용하고 설명해서, 명확히 글로 적어내기가 힘들다. 그래서 그냥 자신이 사용하는 DB에 맞게 정리해야 할 것 같다. 참고로 MySQL은 Redo Log, Undo Log 그리고 Binary Log 이 3가지로 롤백(Rollback), 커밋(Commit), 복구(Recovery), 복제(Replication) 기능을 구현한다.Log in Distributed SystemThe log is a totally-ordered, append-only data structure. Whether it’s application logs, system logs, or access logs, logging is something every developer uses on a daily basis. Essentially, it’s a timestamp and an event, a when and a what, and typically appended to the end of a file. But when we generalize that pattern, we end up with something much more useful for a broad range of problems. It becomes more interesting when we look at the log not just as a system of record but a central piece in managing data and distributing it across the enterprise efficiently.There are a number of implementations of this idea: Apache Kafka, Amazon Kinesis, NATS Streaming, Tank, and Apache Pulsar to name a few. We can probably credit Kafka with popularizing the idea.There are three key priorities of these types of systems: performance, high availability, and scalability. If it’s not fast enough, the data becomes decreasingly useful. If it’s not highly available, it means we can’t reliably get our data in or out. And if it’s not scalable, it won’t be able to meet the needs of many enterprises.Kafka LogCommit LogUsually, Commit Log is associated with relational database systems. In fact, you can think about Commit Log as a part of your architecture where every transaction is written to some reliable log. The transactions are then read from the log and processed by consumers. This architecture can be implemented using the Event Streaming pattern. Kafka is designed for event streaming and acts very similar to a Commit Log. Kafka has a concept of partitions to parallelize the processing of events.RDBMS LogPostgreSQLPostgreSQL is based on the Write-Ahead Log. The log represents a binary stream of low-level physical instructions. PostgreSQL provides Logical Decoding to extract the changes in an eye-friendly format.MySQL / MariaDBMySQL has a concept of storage engines that represent different strategies for storing your database. There are many storage engines and you can write your own but the most interesting are InnoDB (B-tree index), MyRocks (LSM index), and MyISAM. This architecture is flexible but requires a separate WAL log used by a storage engine.Binary Log (also known as binlog) is the Commit Log used for replication and decoupled from storage engines. It has a simple defined format that can be easily parsed. Additionally, every storage engine maintains its own WAL log. The format of the log is specific to the storage engine.In MySQL, every transaction is written to the Commit Log and a WAL. Transactions are considered distributed as they require a two-phase commit (2PC) in the two log files.As mentioned above, rolled-back transactions will not appear in the Commit Log. In MySQL, this is true only for transactional storage engines like InnoDB and MyRocks. In non-transactional storage engines like MyISAM rolled-back transactions may appear in the log and make your data inconsistent.Write Ahead LogWrite-Ahead Logging (WAL) is a standard method for ensuring data integrity. A detailed description can be found in most (if not all) books about transaction processing. Briefly, WAL’s central concept is that changes to data files (where tables and indexes reside) must be written only after those changes have been logged, that is, after log records describing the changes have been flushed to permanent storage. If we follow this procedure, we do not need to flush data pages to disk on every transaction commit, because we know that in the event of a crash we will be able to recover the database using the log: any changes that have not been applied to the data pages can be redone from the log records. (This is roll-forward recovery, also known as REDO.)TipBecause WAL restores database file contents after a crash, journaled file systems are not necessary for reliable storage of the data files or WAL files. In fact, journaling overhead can reduce performance, especially if journaling causes file system data to be flushed to disk. Fortunately, data flushing during journaling can often be disabled with a file system mount option, e.g., data=writeback on a Linux ext3 file system. Journaled file systems do improve boot speed after a crash.Using WAL results in a significantly reduced number of disk writes, because only the log file needs to be flushed to disk to guarantee that a transaction is committed, rather than every data file changed by the transaction. The log file is written sequentially, and so the cost of syncing the log is much less than the cost of flushing the data pages. This is especially true for servers handling many small transactions touching different parts of the data store. Furthermore, when the server is processing many small concurrent transactions, one fsync of the log file may suffice to commit many transactions.WAL also makes it possible to support on-line backup and point-in-time recovery, as described in Section 26.3. By archiving the WAL data we can support reverting to any time instant covered by the available WAL data: we simply install a prior physical backup of the database, and replay the WAL log just as far as the desired time. What’s more, the physical backup doesn’t have to be an instantaneous snapshot of the database state — if it is made over some period of time, then replaying the WAL log for that period will fix any internal inconsistencies.Transaction Log(Redo log + Undo log)  transaction log in Oracle are knwon as the redo log.  The redo log is a disk-based data structure used during crash recovery to correct data written by incomplete transactions (MySQL 공식문서)  The transaction log serves the same functions as the REDO log in other databases- storing writes in a safe way and recovering in the case of a crash, although there are some details in implementation that differ in functionality from other RDMS  There are redo logs (ib_logfile0 and ib_logfile1)  The transaction log (usually in files iblog1 and iblog2) is vital to InnoDB for multiple reasons: ROLLBACK, crash recovery, and possibly more.Binary LogThe binary log contains “events” that describe database changes such as table creation operations or changes to table data. It also contains events for statements that potentially could have made changes (for example, a DELETE which matched no rows), unless row-based logging is used. The binary log also contains information about how long each statement took that updated data.The binary log has two important purposes:      For replication, the binary log on a replication source server provides a record of the data changes to be sent to replicas. The source sends the events contained in its binary log to its replicas, which execute those events to make the same data changes that were made on the source. See Section 16.2, “Replication Implementation”.        Certain data recovery operations require use of the binary log. After a backup has been restored, the events in the binary log that were recorded after the backup was made are re-executed. These events bring databases up to date from the point of the backup. See Section 7.5, “Point-in-Time (Incremental) Recovery”.  The binary log is generally resilient to unexpected halts because only complete transactions are logged or read back. The binary log is not used for statements such as SELECT or SHOW that do not modify data. To log all statements (for example, to identify a problem query), use the general query log.참고  Quora, What is a DB commit log?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-25T21:01:35+09:00'>25 Jul 2022</time><a class='article__image' href='/data-engineering-series19'> <img src='/images/log_logo.webp' alt='Data Engineering Series [Part19]: 로그 공부하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series19'>Data Engineering Series [Part19]: 로그 공부하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part17]: Distributed Systems(3) 네트워크와 운영체제",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series17",
      "date"     : "Jul 23, 2022",
      "content"  : "Table of Contents  운영체제 관점  네트워크: Inter Process Communication          Remote Procedure Call(RPC)      Message based Communication      Sockets        참고운영체제 관점네트워크: Inter Process CommunicationInter-process communication (IPC) is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. This allows running programs concurrently in an Operating System.Interprocess communication is at the heart of all distributed systems. It makes no sense to study distributed systems without carefully examining the ways that processes on different machines can exchange information. Communication in distributed systems is always based on low-level message passing as offered by the underlying network. Expressing communication through message passing is harder than using primitives based on shared memory, as available for nondistrib- uted platforms. Modem distributed systems often consist of thousands or even millions of processes scattered across a network with unreliable communication such as the Internet. Unless the primitive communication facilities of computer networks are replaced by something else, development of large-scale distributed applications is extremely difficult.In this chapter, we start by discussing the rules that communicating processes must adhere to, known as protocols, and concentrate on structuring those proto- cols in the form of layers. We then look at three widely-used models for commu- nication: Remote Procedure Call (RPC), Message-Oriented Middleware (MOM), and data streaming. We also discuss the general problem of sending data to multi- ple receivers, called multicasting.Our first model for communication in distributed systems is the remote proce- dure call (RPC). An RPC aims at hiding most of the intricacies of message pass- ing, and is ideal for client-server applications.In many distributed applications, communication does not follow the rather strict pattern of client-server interaction. In those cases, it turns out that thinking in terms of messages is more appropriate. However, the low-level communication facilities of computer networks are in many ways not suitable due to their lack of distribution transparency.An alternative is to use a high-level message-queuing model, in which communication proceeds much the same as in electronic maiI systems. Message-oriented middleware (MOM) is a subject important enough to warrant a section of its own.With the advent of multimedia distributed systems, it became apparent thatmany systems were lacking support for communication of continuous media, such as audio and video. What is needed is the notion of a stream that can support the continuous flow of messages, subject to various timing constraints. Streams are discussed in a separate section.Finally, since our understanding of setting up multicast facilities has im- proved, novel and elegant solutions for data dissemination have emerged. We pay separate attention to this subject in the last section of this chapter.Remote Procedure Call(RPC)Remote Procedure Call (RPC) is a communication technology that is used by one program to make a request to another program for utilizing its service on a network without even knowing the network’s details. A function call or a subroutine call are other terms for a procedure call.It is based on the client-server concept. The client is the program that makes the request, and the server is the program that gives the service. An RPC, like a local procedure call, is based on the synchronous operation that requires the requesting application to be stopped until the remote process returns its results. Multiple RPCs can be executed concurrently by utilizing lightweight processes or threads that share the same address space.Remote Procedure Call program as often as possible utilizes the Interface Definition Language (IDL), a determination language for describing a computer program component’s Application Programming Interface (API). In this circumstance, IDL acts as an interface between machines at either end of the connection, which may be running different operating systems and programming languages.Elements of RPC  Client: The client process initiates RPC. The client makes a standard call, which triggers a correlated procedure in the client stub.  Client Stub: Stubs are used by RPC to achieve semantic transparency. The client calls the client stub. Client stub does the following tasks:          The first task performed by client stub is when it receives a request from a client, it packs(marshalls) the parameters and required specifications of remote/target procedure in a message.      The second task performed by the client stub is upon receiving the result values after execution, it unpacks (unmarshalled) those results and sends them to the Client.        RPC Runtime: The RPC runtime is in charge of message transmission between client and server via the network. Retransmission, acknowledgment, routing, and encryption are all tasks performed by it. On the client-side, it receives the result values in a message from the server-side, and then it further sends it to the client stub whereas, on the server-side, RPC Runtime got the same message from the server stub when then it forwards to the client machine. It also accepts and forwards client machine call request messages to the server stub.  Server Stub: Server stub does the following tasks:          The first task performed by server stub is that it unpacks(unmarshalled) the call request message which is received from the local RPC Runtime and makes a regular call to invoke the required procedure in the server.      The second task performed by server stub is that when it receives the server’s procedure execution result, it packs it into a message and asks the local RPC Runtime to transmit it to the client stub where it is unpacked.        Server: After receiving a call request from the client machine, the server stub passes it to the server. The execution of the required procedure is made by the server and finally, it returns the result to the server stub so that it can be passed to the client machine using the local RPC Runtime.Working Procedure for RPC Model  The process arguments are placed in a precise location by the caller when the procedure needs to be called.  Control at that point passed to the body of the method, which is having a series of instructions.  The procedure body is run in a recently created execution environment that has duplicates of the calling instruction’s arguments.  At the end, after the completion of the operation, the calling point gets back the control, which returns a result.          The call to a procedure is possible only for those procedures that are not within the caller’s address space because both processes (caller and callee) have distinct address space and the access is restricted to the caller’s environment’s data and variables from the remote procedure.      The caller and callee processes in the RPC communicate to exchange information via the message-passing scheme.      The first task from the server-side is to extract the procedure’s parameters when a request message arrives, then the result, send a reply message, and finally wait for the next call message.      Only one process is enabled at a certain point in time.      The caller is not always required to be blocked.      The asynchronous mechanism could be employed in the RPC that permits the client to work even if the server has not responded yet.      In order to handle incoming requests, the server might create a thread that frees the server for handling consequent requests.      Advantages of Remote Procedure Calls  The technique of using procedure calls in RPC permits high-level languages to provide communication between clients and servers.  This method is like a local procedure call but with the difference that the called procedure is executed on another process and a different computer.  The thread-oriented model is also supported by RPC in addition to the process model.  The RPC mechanism is employed to conceal the core message passing method.  The amount of time and effort required to rewrite and develop the code is minimal.  The distributed and local environments can both benefit from remote procedure calls.  To increase performance, it omits several of the protocol layers.  Abstraction is provided via RPC.  To exemplify, the user is not known about the nature of message-passing in network communication.  RPC empowers the utilization of applications in a distributed environment.Disadvantages of Remote Procedure Calls  In Remote Procedure Calls parameters are only passed by values as pointer values are not allowed.  It involves a communication system with another machine and another process, so this mechanism is extremely prone to failure.  The RPC concept can be implemented in a variety of ways, hence there is no standard.  Due to the interaction-based nature, there is no flexibility for hardware architecture in RPC.  Due to a remote procedure call, the process’s cost has increased.Message based CommunicationIn the development of models and technologies, message abstraction is a necessary aspect that enables distributed computing. Distributed system is defined as a system in which components reside at networked communication and synchronise its functions only by movement of messages. In this, message recognizes any discrete data that is moved from one entity to another. It includes any kind of data representation having restriction of size and time, whereas it invokes a remote procedure or a sequence of object instance or a common message. This is the reason that “message-based communication model” can be beneficial to refer various model for inter-process communication, which is based on the data streaming abstraction.Various distributed programming model use this type of communication despite of the abstraction which is shown to developers for programming the co-ordination of shared components. Below are some major distributed programming models that uses “message-based communication model”  Message Passing          In this model, the concept of message as the major abstraction of model is introduced. The units which inter-change the data and information that is explicitly encode, in the form of message. According to then model, the schema and content of message changes or varies. Message Passing Interface and OpenMP are major example of this type of model.        Remote Procedure Call          This model explores the keys of procedure call beyond the restrictions of a single process, thus pointing the execution of program in remote processes. In this, primary client-server is implied. A remote process maintains a server component, thus enabling client processes to invoke the approaches and returns the output of the execution. Messages, created by the Remote Procedure Call (RPC) implementation, retrieve the information of the procedure itself and that procedure is to execute having necessary arguments and also returns the values. The use of messages regarding this referred as marshal-ling of the arguments and return values.      SocketsThis method is mostly used to communicate over a network between a client and a server. It allows for a standard connection which is computer and OS independent.참고  GeeksforGeeks: Interprocess Communication in Distributed Systems  조원호의 행복공간: [네트워크] IPC와 RPC의 차이점",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-23T21:01:35+09:00'>23 Jul 2022</time><a class='article__image' href='/data-engineering-series17'> <img src='/images/distributed_logo.png' alt='Data Engineering Series [Part17]: Distributed Systems(3) 네트워크와 운영체제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series17'>Data Engineering Series [Part17]: Distributed Systems(3) 네트워크와 운영체제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part16]: Distributed System(2) CAP Theorem",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series16",
      "date"     : "Jul 22, 2022",
      "content"  : "Table of Contents  CAP Theorem          AP – Availability + Partition Tolerance      CA – Consistency + Availability      CP – Consistency + Partition Tolerance        Importance of the CAP theorem  Conclusion  참고CAP TheoremThe CAP Theorem is one of the most fundamental theorems in the field of distributed systems. It outlines an inherent trade-off in the design of distributed systems.CAP theorem – or Brewer’s theorem – was introduced by the computer scientist Eric Brewer at Symposium on Principles of Distributed computing in 2000. The CAP stands for Consistency, Availability and Partition tolerance.The CAP Theorem (as put forth in a presentation by Eric Brewer in 2000) stated that distributed shared-data systems had three properties but systems could only choose to adhere to two of those properties:  Consistency: Consistency means that every successful read request receives the result of the most recent write request. Be aware that the definition of consistency for CAP means something different than to ACID (relational consistency).  Availability: The database is not allowed to be unavailable because it is busy with requests. Every request received by a non-failing node in the system must result in a response. Whether you want to read or write you will get some response back. If the server has not crashed, it is not allowed to ignore the client’s requests.  Partition tolerance: Databases which store big data will use a cluster of nodes that distribute the connections evenly over the whole cluster. If this system has partition tolerance, it will continue to operate despite a number of messages being delayed or even lost by the network between the cluster nodes.Distributed systems designed for fault tolerance are not much use if they cannot operate in a partitioned state (a state where one or more nodes are unreachable). Thus, partition-tolerance is always a requirement, so the two basic modes that most systems use are either Availability-Partition-tolerant (“AP”) or Consistency-Partition-tolerant (“CP”).In a distributed system, there is always the risk of a network partition. If this happens, the system needs to decide either to continue operating and compromise data consistency, or stop operating and compromise availability.According to the final statement of the CAP theorem, a distributed system can be either consistent or available in the presence of a network partition.Database systems designed to fulfill traditional ACID guarantees like relational database (management) systems (RDBMS) choose consistency over availability, whereas NoSQL databases are mostly systems designed referring to the BASE philosophy which prefer availability over consistency.AP 시스템은 네트워크 파티션이 생겼을 때 AP를 보장해준다. C는 파티션이 없을 때 해결된다.CA 시스템은 현실적으로 둘 다 완벽하게 지켜낼 수 없다. C를 무조건 보장하면서 A를 최대한 제공해주는 방식이다.AP – Availability + Partition ToleranceIf we have achieved Availability (our databases will always respond to our requests) as well as Partition Tolerance (all nodes of the database will work even if they cannot communicate), it will immediately mean that we cannot provide Consistency as all nodes will go out of sync as soon as we write new information to one of the nodes. The nodes will continue to accept the database transactions each separately, but they cannot transfer the transaction between each other keeping them in synchronization. We therefore cannot fully guarantee the system consistency. When the partition is resolved, the AP databases typically resync the nodes to repair all inconsistencies in the system.A well-known real world example of an AP system is the Domain Name System (DNS). This central network component is responsible for resolving domain names into IP addresses and focuses on the two properties of availability and failure tolerance. Thanks to the large number of servers, the system is available almost without exception. If a single DNS server fails,another one takes over. According to the CAP theorem, DNS is not consistent: If a DNS entry is changed, e.g. when a new domain has been registered or deleted, it can take a few days before this change is passed on to the entire system hierarchy and can be seen by all clients.CA – Consistency + AvailabilityGuarantee of full Consistency and Availability is practically impossible to achieve in a system which distributes data over several nodes. We can have databases over more than one node online and available, and we keep the data consistent between these nodes, but the nature of computer networks (LAN, WAN) is that the connection can get interrupted, meaning we cannot guarantee the Partition Tolerance and therefor not the reliability of having the whole database service online at all times.Database management systems based on the relational database models (RDBMS) are a good example of CA systems. These database systems are primarily characterized by a high level of consistency and strive for the highest possible availability. In case of doubt, however, availability can decrease in favor of consistency. Reliability by distributing data over partitions in order to make data reachable in any case – even if computer or network failure – meanwhile plays a subordinate role.CP – Consistency + Partition ToleranceIf the Consistency of data is given – which means that the data between two or more nodes always contain the up-to-date information – and Partition Tolerance is given as well – which means that we are avoiding any desynchronization of our data between all nodes, then we will lose Availability as soon as only one a partition occurs between any two nodes In most distributed systems, high availability is one of the most important properties, which is why CP systems tend to be a rarity in practice. These systems prove their worth particularly in the financial sector: banking applications that must reliably debit and transfer amounts of money on the account side are dependent on consistency and reliability by consistent redundancies to always be able to rule out incorrect postings – even in the event of disruptions in the data traffic. If consistency and reliability is not guaranteed, the system might be unavailable for the users.Importance of the CAP theoremThe CAP theorem is really important because it helped establish the basic limitations of all distributed systems.The CAP theorem forces designers of distributed systems to make explicit trade-offs between availability and consistency. Once the engineers become aware of these properties, they choose the appropriate system.ConclusionThe CAP Theorem is still an important topic to understand for data engineers and data scientists, but many modern databases enable us to switch between the possibilities within the CAP Theorem. For example, the Cosmos DB von Microsoft Azure offers many granular options to switch between the consistency, availability and partition tolerance . A common misunderstanding of the CAP theorem that it´s none-absoluteness: “All three properties are more continuous than binary. Availability is continuous from 0 to 100 percent, there are many levels of consistency, and even partitions have nuances. Exploring these nuances requires pushing the traditional way of dealing with partitions, which is the fundamental challenge. Because partitions are rare, CAP should allow perfect C and A most of the time, but when partitions are present or perceived, a strategy is in order.”참고  Understanding NoSQL Databases by the CAP Theorem",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-22T21:01:35+09:00'>22 Jul 2022</time><a class='article__image' href='/data-engineering-series16'> <img src='/images/distributed_logo.png' alt='Data Engineering Series [Part16]: Distributed System(2) CAP Theorem'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series16'>Data Engineering Series [Part16]: Distributed System(2) CAP Theorem</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part15]: Distributed System(1) Comprehensive Guide",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series15",
      "date"     : "Jul 21, 2022",
      "content"  : "Table of Contents  분산 시스템이란  장점          Performance      Scalability      Availability        High-level에서의 동작          Partitioning      Replication        어려운 점          Synchronization                  Clock Synchronization          Data Synchronization                    Network Asynchrony      Partial Failures        Low-level에서의 동작          Consensus Algorithm                  RAFT                    Failure Detector (Timeout)      De-duplication Algorithm        참고분산 시스템이란  A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another나는 분산 시스템을 다음과 같은 맥락으로 정리하려고 한다.장점Performance여기서 Performance는 단일 시스템에서의 Performance와 비교해 가격대비 더 낫다는 의미이다. Performance의 절대적인 수치 자체가 더 오를 이유는 없다. 오히려 네트워크 비용으로 감소할 가능성은 있다. 그럼에도 분산 시스템을 쓰는 이유는 가격적인 측면에서 그만큼 값싼 장비를 여러 대 사용하는 것이 낫고, 성능적인 측면 이외에도 분산 시스템이 주는 장점이 있기 때문이다.Scalability분산 시스템은 서비스 규모, 트래픽량, 작업량에 따라 시스템의 크기를 조절해 이를 핸들링할 능력이 있다. 물론 단일 시스템에서도 Vertical-scaling이 가능하다. 하지만 Horizontal-scaling이 가격적인 측면과 확장이 용이하다는 점에서 이점이 있다.Availability분산 시스템은 노드 일부에 장애가 발생하더라도 계속 같은 기능을 유지할 수 있다. 이는 24시간 내내 장애없는 서비스가 가능하다는 말이다. 물론 이를 위해 요구되는 조건들이 있는데 이 부분은 뒤에서 더 자세히 다룰 것이다.지금까지 분산 시스템의 장점에 대해서 얘기했다. 이러한 장점을 얻기위해 해야할 일이 있다. 우선 High-level에서 이에 대해 알아보겠다.High-level에서의 동작Partitioning파티셔닝은 분산 시스템의 장점 중에서도 Scalability, Performance를 얻기 위해 필요한 핵심이다. 파티셔닝은 처리(또는 저장)해야 할 데이터를 작게 나누어서 이를 처리(또는 저장)해야 할 노드에게 할당해주는 작업이다.하지만 오히려 하나의 작업을 위해 네트워크를 거쳐 여러 노드에 접근해야 한다는 점에서 단점이 되는 경우도 있다. 그래서 데이터를 처리할 때는 최대한 네트워크 비용을 줄이는 것이 관건이다.파티셔닝은 크게 range partitioning, hash partitioning, consistent hashing가 있다. Apache HBase는 range partitioning을 쓰고, Apache Cassandra는 consistent hasing을 쓴다.Replication복제(Replication)는 Availability를 위해 필요한 핵심이다. 복제는 같은 데이터를 여러 노드에 복수 저장함으로써, 노드 중 일부에 장애가 발생하더라도 계속 역할을 유지할 수 있게 한다.복제하는 것이 단순한 일은 아니다. 우선 복제 수 만큼 더 많은 저장용량이 필요하다. 또한 복사된 후에도 원본 데이터가 업데이트 될 때마다 동기화해야 한다.복제 방법 중 하나인 primary-backup replication에 대해 알아보자.We commonly refer to the remaining replicas as followers or secondaries. These can only handle read requests. Every time the leader receives an update, it executes it locally and also propagates the update to the other nodes. This ensures that all the replicas maintain a consistent view of the data.리더는 업데이트를 어떻게 팔로워들에게 전파할까There are two ways to propagate the updates: synchronously and asynchronously.Synchronous replicationIn synchronous replication, the node replies to the client to indicate the update is complete—only after receiving acknowledgments from the other replicas that they’ve also performed the update on their local storage. This guarantees that the client is able to view the update in a subsequent read after acknowledging it, no matter which replica the client reads from.Furthermore, synchronous replication provides increased durability. This is because the update is not lost even if the leader crashes right after it acknowledges the update.However, this technique can make writing requests slower. This is because the leader has to wait until it receives responses from all the replicas.Asynchronous replicationIn asynchronous replication, the node replies to the client as soon as it performs the update in its local storage, without waiting for responses from the other replicas.This technique increases performance significantly for write requests. This is because the client no longer pays the penalty of the network requests to the other replicas.However, this comes at the cost of reduced consistency and decreased durability. After a client receives a response for an update request, the client might read older (stale) values in a subsequent read. This is only possible if the operation happens in one of the replicas that have not yet performed the update. Moreover, if the leader node crashes right after it acknowledges an update, and the propagation requests to the other replicas are lost, any acknowledged update is eventually lost.Most widely used databases, such as PostgreSQL or MySQL, use a primary-backup replication technique that supports both asynchronous and synchronous replication.primary-backup replication에는 장점과 단점이 있습니다.장점  It is simple to understand and implement  Concurrent operations serialized in the leader node, remove the need for more complicated, distributed concurrency protocols. In general, this property also makes it easier to support transactional operations  It is scalable for read-heavy workloads, because the capacity for reading requests can be increased by adding more read replicas단점  It is not very scalable for write-heavy workloads, because a single node (the leader)’s capacity determines the capacity for writes  It imposes an obvious trade-off between performance, durability, and consistency  Scaling the read capacity by adding more follower nodes can create a bottleneck in the network bandwidth of the leader node, if there’s a large number of followers listening for updates  The process of failing over to a follower node when the leader node crashes, is not instant. This may create some downtime and also introduce the risk of errors또한 primary-backup replication은 항상 리더가 존재해야 한다. 따라서 리더가 잘 살아있는지 체크하고, 리더가 죽었다면 리더를 새로 선출하는 Leader election 문제도 고려해야 한다.어려운 점SynchronizationClock SynchronizationDistributed System is a collection of computers connected via the high speed communication network. In the distributed system, the hardware and software components communicate and coordinate their actions by message passing. Each node in distributed systems can share their resources with other nodes. So, there is need of proper allocation of resources to preserve the state of resources and help coordinate between the several processes. To resolve such conflicts, synchronization is used. Synchronization in distributed systems is achieved via clocks.The physical clocks are used to adjust the time of nodes.Each node in the system can share its local time with other nodes in the system. The time is set based on UTC (Universal Time Coordination). UTC is used as a reference time clock for the nodes in the system.The clock synchronization can be achieved by 2 ways: External and Internal Clock Synchronization.  External clock synchronization is the one in which an external reference clock is present. It is used as a reference and the nodes in the system can set and adjust their time accordingly.  Internal clock synchronization is the one in which each node shares its time with other nodes and all the nodes set and adjust their times accordingly.There are 2 types of clock synchronization algorithms: Centralized and Distributed.  Centralized is the one in which a time server is used as a reference. The single time server propagates its time to the nodes and all the nodes adjust the time accordingly. It is dependent on single time server so if that node fails, the whole system will lose synchronization. Examples of centralized are- Berkeley Algorithm, Passive Time Server, Active Time Server etc.  Distributed is the one in which there is no centralized time server present. Instead the nodes adjust their time by using their local time and then, taking the average of the differences of time with other nodes. Distributed algorithms overcome the issue of centralized algorithms like the scalability and single point failure. Examples of Distributed algorithms are – Global Averaging Algorithm, Localized Averaging Algorithm, NTP (Network time protocol) etc.Data Synchronization데이터 동기화 문제는 위에서 말했던 데이터 복제 과정에서, 그리고 업데이트시 복제된 데이터들간의 동기화를 말한다. 또한 트랜잭션과 같이 ACID 특성이 요구될 때, 데이터들이 Atomic하게 처리되는지와도 관련된다.Network AsynchronyNetwork asynchrony is a property of communication networks that cannot provide strong guarantees around delivering events, e.g., a maximum amount of time a message requires for delivery. This can create a lot of counter-intuitive behaviors that are not present in non-distributed systems. This contrasts to memory operations that provide much stricter guarantees. For instance, messages might take extremely long to deliver in a distributed system. They may even deliver out of order—or not at all.  분산 시스템을 이용한 인터넷 서비스에서 사용하는 어플리케이션의 아키텍처는 대부분 비공유 아키텍처(Shared-nothing)          각 노드는 CPU, RAM, 디스크를 독립적으로 사용(디스크도 독립된다는 점에서 Shared Disk Architecture와 다름)        비공유 시스템에서 장비들이 통신하는 유일한 수단은 네트워크  전송 측은 패킷이 전송 된건지, 네트워크 문제인지, 수신 측 서버 문제인지 알 수 없으며 심지어 전달이 실패한건지 아니면 지연된건지 조차 알 수 없음Partial FailuresPartial failures are the cases where only some components of a distributed system fail. This behavior can contrast with certain kinds of applications a single server deploys. These applications work under the assumption that either everything is working fine, or there has been a server crash. It introduces significant complexity when it requires atomicity across components in a distributed system. Thus, we must ensure that we either apply an operation to all the nodes of a system, or to none of them.Low-level에서의 동작Consensus Algorithm합의 알고리즘은 데이터 동기화 문제, 부분 장애 문제를 해결해준다.  whether a transaction has been committed or not  whether a message has been delivered or notRAFTRaft is a protocol for implementing distributed consensus.Raft decomposes consensus into three sub-problems:  Leader Election: A new leader needs to be elected in case of the failure of an existing one.  Log replication: The leader needs to keep the logs of all servers in sync with its own through replicationFailure Detector (Timeout)The asynchronous nature of the network in a distributed system can make it very hard for us to differentiate between a crashed node and a node that is just really slow to respond to requests.Timeouts is the main mechanism we can use to detect failures in distributed systems. Since an asynchronous network can infinitely delay messages, timeouts impose an artificial upper bound on these delays. As a result, we can assume that a node fails when it is slower than this bound.However, a timeout does not represent an actual limit. Thus, it creates the following trade-off.내가 정한 타임아웃 설정이 발생한 문제의 근본적인 원인을 해결하는 점은 아니다. 발생한 문제로 잃게되는 성능이 실제 이상으로 떨어질 수 밖에 없다. 타임아웃 값에 따라 Completeness와 Accruacy 사이에 트레이드 오프 발생Completeness corresponds to the percentage of crashed nodes a failure detector successfully identifies in a certain period.Accuracy corresponds to the number of mistakes a failure detector makes in a certain period.De-duplication AlgorithmIn the de-duplication approach, we give every message a unique identifier, and every retried message contains the same identifier as the original. In this way, the recipient can remember the set of identifiers it received and executed already. It will also avoid executing operations that are executed.It is important to note that in order to do this, we must have control on both sides of the system: sender and receiver. This is because the ID generation occurs on the sender side, but the de-duplication process occurs on the receiver side.리더는 데이터를 전달할 때마다 데이터에 고유한 식별자를 붙인다. 팔로워들은 식별자를 보고 자기가 가지고 있는 식별자보다 큰 값인 경우에만 받아서 저장한다. 만약 같은 식별자의 데이터를 받았다면 리더에게 ACK 메시지만 보내고, 저장은 하지 않는다.참고  Video Demo,  Raft: Understandable Distributed Consensus  Understanding the Raft consensus algorithm: an academic article summary  Building a Distributed Log from Scratch, Part 2: Data Replication",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-21T21:01:35+09:00'>21 Jul 2022</time><a class='article__image' href='/data-engineering-series15'> <img src='/images/distributed_logo.png' alt='Data Engineering Series [Part15]: Distributed System(1) Comprehensive Guide'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series15'>Data Engineering Series [Part15]: Distributed System(1) Comprehensive Guide</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part14]: Database Internals 트랜잭션 처리와 복구",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series14",
      "date"     : "Jul 20, 2022",
      "content"  : "Table of Contents  트랜잭션 처리와 복구          버퍼 관리                  캐싱          캐시 만료          페이지 고정          페이지 교체                    복구                  리두로그 언두로그                    트랜잭션 처리와 복구트랜잭션을 수행하기 위해서는 데이터를 디스크에 저장하고 유지하는 자료구조 외에도 여러 컴포넌트가 필요하다.트랜잭션 매니저는 트랜잭션의 세부 단계를 제어, 관리 및 스케줄링하는 컴포넌트다.잠금 매니저는 리소스에 대한 동시 접근을 제어하고 데이터 무결성을 보장한다. 잠금이 해제되거나 트랜잭션이 완료되면 잠금매니저는 대기 중인 트랜잭션에 이 사실을 알린다페이지 캐시는 디스크와 스토리지 엔진 사이에서 중개자 역할을 한다. 메인 메모리의 변경 사항을 저장하고 영구 저장소와 동기화되지 않은 페이지를 캐시한다. 모든 데이터베이스 상태에 대한 변경 사항은 우선 페이지 캐시에 저장된다로그 매니저는 영구 저장소와 동기화되지 않은 페이지 캐시의 내용이 손실되지 않도록 로그(작업 히스토리)를 저장한다.버퍼 관리대부분의 데이터베이스는 상대적으로 속도가 느린 디스크와 빠른 메인 메모리로 구성된 계층 구조이다. 따라서 영구 저장소 접근 횟수를 줄이기 위해 페이지를 메모리에 캐시하고 요청한 페이지가 캐시되어 있다면 디스크가 아닌 캐시에서 반환한다.다른 프로세스가 캐시된 페이지와 일치하는 디스크에 저장된 데이터를 변경하지 않았다면 메모리에 캐시된 페이지를 재사용할 수 있다. 이와 같은 방식을 페이지 캐시 또는 버퍼풀이라고 부른다. 이러한 방식은 요청된 페이지가 메모리에 없을 경우에만 물리적 저장소에 접근한다. 페이지 캐시는 디스크에서 읽은 페이지를 메모리에 캐시한다. 시스템 장애가 발생하거나 시스템이 비정상적으로 종료되면 캐시된 데이터는 사라진다.디스크에서 메모리로 복사하는 작업을 페이징이라고 한다. 그리고 디스크로 플러시되지 않은 변경된 페이지는 더티페이지라고 한다. 일반적으로 페이지 캐시의 메모리 영역은 전체 데이터셋보다 작기 때문에 결국 새로운 페이지를 추가하기 위해 기존 페이지를 만료시켜야 한다.페이지 캐시의 주요 기능은 다음과 같다.  페이지 내용을 메모리에 캐시한다  디스크에 저장된 페이지에 대한 변경 사항을 버퍼링하고 캐시된 페이지에 반영한다  캐시되지 않은 데이터가 요청될 경우 추가할 공간이 있으면 페이징하고 캐시된 버전을 반환한다  추가할 공간이 없다면 일부 페이지를 만료시키고 디스크로 플러시한다  캐시된 데이터가 요청된 경우에는 그냥 메모리에서 반환하면 된다캐싱버퍼에 대한 변경 사항은 디스크에 쓰기 전까지 메모리에 남겨둔다. 어떤 프로세스도 원본 파일을 수정할 수 없기 때문에 동기화는 메모리에서 디스크로 플러시하는 단방향 작업이다. 데이터베이스는 페이지 캐시를 사용해 메모리를 관리하고 디스크 접근을 제어한다.스토리지 엔진이 특정 페이지를 요청하면 우선 캐시된 버전이 있는지 확인하고 있을 경우 반환한다. 없다면 논리적 페이지 주소 또는 페이지 번호를 물리적 주소로 변환해 해당 페이지를 메모리로 복사하고 반환한다. 이 떄 해당 페이지가 저장된 버퍼는 참조 상태라고 한다.페이지가 변경된 경우에는 페이지에 더티 플래그를 설정한다. 더티 플르개는 해당 페이지가 디스크와 동기화되지 않았고, 지속성을 위해 디스크로 플러시돼야 한다는 것을 의미한다.캐시 만료캐시된 데이터가 많을수록 더 많은 읽기 요청을 디스크에 접근하지 않고 처리할 수 있다. 또한 같은 페이지에 대한 변경사항을 더 많이 같이 버퍼할 수 있다. 하지만 페이지 캐시의 크기는 한정적이기 때문에, 새로운 페이지를 저장하기 위해 오래된 페이지는 제거해야 한다. 페이지가 동기화됐고 고정 또는 참조 상태가 아니라면 바로 제거 될 수 있다. 더티 페이지는 제거되기 전에 먼저 플러시해야 한다. 참조 상태의 페이지는 사용이 끝나기 전까지는 제거될 수 없다.페이지를 제거할 때마다 디스크로 플러시한다면 성능을 저하시킬 수 있다. 따라서 별도의 백그라운드 프로세스가 제거될 가능성이 높은 더티 페이지를 주기적으로 디스크로 플러시한다. 또한 데이터베이스에 갑작스런 장애가 발생한 경우 플러시되지 않은 데이터는 사라질 수 있다. 이러한 데이터 손실을 방지하기 위해 체크포인트 프로세스가 플러시 시점을 제어한다. 체크포인트 프로세스는 선행 기록 로그(Write Ahead Log)와 페이지 캐시의 싱크가 맞도록 조정한다. (이 말은 선행 기록 로그가 디스크에 있는 데이터라는 말인가? 동기화는 메모리에서 디스크로 플러시하는 단방향 작업이라 했으니까 선행 기록 로그보다 페이지의 데이터가 더 최신? -&amp;gt; 그렇지. 캐시된 페이지에 제일 먼저 변화 생기고 -&amp;gt; 로그 버퍼에 리두 로그(WAL) 버퍼링 되고 버퍼된 리두 로그 디스크에 플러시 되고 -&amp;gt; 캐시된 페이지 플러시) 오직 플러시가 완료된 캐시된 페이지와 관련된 로그만 WAL에서 삭제될 수 있다. 이 과정이 완료될 때 끼지 더티 페이지는 제거될 수 없다.페이지 고정B-트리의 상위 레벨 노드는 대부분의 읽기 작업에서 접근한다. 이러한 트리의 일부를 캐시하면 상당한 도움이 될 수 있다. 따라서 우선 상위 레벨 노드를 메모리에 고정시키고 나머지 노드는 요청 시 페이징해도 된다.페이지 교체저장 공간이 부족한 캐시에 새로운 페이지를 추가하려면 일부 페이지를 만료시켜야 한다. 캐시된 페이지는 만료 정책(페이지 교체 알고리즘)에 따라 캐시에서 제거된다. 만료 정책은 다시 요청될 확률이 낮은 페이지를 만료시키고 해당 위치에 새로운 페이지를 페이징한다. 이렇게 페이지를 만료시키고 새로운 페이지로 교체하는 알고리즘은 대표적으로 FIFO, LRU 등이 있다.FIFO는 First In First Out의 약자로 가장 먼저 큐에 들어온 페이지 ID가 가리키는 페이지를 만료시킨다. 이 방식은 간단하지만 페이지 접근 순서를 전혀 고려하지 않기 때문에 실용적이지 않다.LRU는 FIFO를 확장한 방식이다. FIFO와 마찬가지로 요청 순서대로 큐에 추가되다가 재요청 되면 다시 큐의 끝에 추가한다.복구데이터베이스 개발자는 여러 장애 시나리오를 고려하고 약속된 데이터가 실제로 저장되게 해야 한다. 선행 기록 로그(WAL 또는 커밋 로그)는 장애 및 트랜잭션 복구를 위해 디스크에 저장하는 추가 전용(Append-only) 보조 자료 구조다. 페이지 캐시는 페이지에 대한 변경 사항을 메모리에 버퍼링한다. 캐시된 내용이 디스크로 플러시될 때 까지 관련 작업 이력의 유일한 디스크 복사본은 WAL이다. MySQL, PostreSQL과 같은 많은 데이터베이스가 추가 전용 WAL을 사용한다.WAL의 주요 기능은 다음과 같다.  캐시된 페이지가 디스크와 동기화 될 때 까지 작업 이력을 디스크에 저장한다. 데이터베이스의 상태를 변경하는 작업을 실제 페이지에 적용하기 전에 먼저 디스크에 로깅한다  장애 발생 시 로그를 기반으로 마지막 메모리 상태를 재구성한다  WAL은 데이터가 디스크에 저장되도록 보장하고, 장애 발생시 데이터 상태를 되돌리기 위해 필요한 로그를 저장한다WAL은 추가 전용 자료구조이며, 작성된 데이터는 불변하기 때문에 모든 쓰기 작업은 순차적이다.WAL은 여러 로그 레코드로 구성된다. 모든 레코드에는 단조 증가하는 고유 로그 시퀀스 번호(LSN, Log Sequence Number)가 있다. 로그 레코드의 크기는 디스크 블록의 크기보다 작을 수 있기 때문에, 로그 버퍼에 임시 저장하고 포스 작업시 디스크로 플러시한다. 포스 작업은 로그 버퍼가 가득 차면 수행되거나 트랜잭션 매니저 또는 페이지 캐시가 직접 요청할 수도 있다. 모든 로그 레코드는 LSN과 동일한 순서로 플러시돼야 한다.WAL는 작업 로그 레코드 외에도 트랜잭션 완료 여부를 나타내는 레코드를 저장한다. 트랜잭션의 커밋 레코드의 LSN까지 플러시되기 전까지는 해당 트랜잭션은 커밋된 것으로 간주 할 수 없다. 일부 시스템은 트랜잭션 롤백 또는 복구 중 장애가 발생해도 시스템이 계속해서 정상 작동할 수 있도록 보상 로그 레코드를 로그에 저장하고 언두 작업 시 사용한다.일반적으로 WAL은 체크포인트에 도달하면 이전 로그를 정리하는 인터페이스를 통해 기본 저장소와 동기화한다. 로깅은 데이터베이스의 정확성 측면에서 매우 중요한 작업이다. 로그 정리 작업과 데이터를 기본 스토리지에 저장하는 작업이 조금이라도 어긋나면 데이터는 손실될 수 있다.모든 데이터는 한 번에 디스크로 플러시하면 체크포인트 작업이 완료될 때 까지 다른 작업을 모두 중지해야 하기 떄문에 비효율적이다. 이 문제를 해결하기 위해 대부분의 데이터베이스 시스템은 퍼지(fuzzy) 체크포인트를 사용한다.마지막으로 성공한 체크포인트 작업에 대한 정보는 로그 헤더에 저장된 last_checkpoint 포인터에 저장한다. 퍼지 체크포인트는 begin_checkpoint라는 특별한 로그 레코드로 시작해 더티 페이지에 대한 정보와 트랜잭션 테이블의 내용을 저장한 end_checkpoint라는 로그 레코드로 끝난다. 이 로그 레코드에 명시된 모든 페이지가 플러시될 때 까지 해당 체크포인트는 미완료 상태다. 페이지는 비동기적으로 플러시되며, 이 작업이 끝나면 last_checkpoint 레코드를 begin_checkpoint의 LSN으로 업데이트 한다. 장애가 발생할 경우, 복구 프로세스는 해당 LSN에서부터 시작한다.리두로그 언두로그데이터베이스 시스템은 데이터의 지속성과 트랜잭션의 원자성을 보장하는 쓰기 시 복사(Copy-on-write) 방식의 섀도 페이징 기법을 사용한다. 새로운 데이터를 우선 내부 섀도 페이지에 쓴 다음, 이전 버전의 페이지를 가리키는 포인터를 섀도 페이지를 가리키도록 변경해 업데이트된 내용을 반영한다.모든 상태 변화는 이전 상태와 이후 상태의 조합으로 나타낼 수 있다. 또는 그에 대응되는 리두 작업과 언두 작업으로 나타낼 수 있다. 이전 상태에 리두 작업을 수행하면 이후 상태가 된다. 반대로 이후 상태에 언두 작업을 수행하면 이전 상태가 된다.장애 발생 후 데이터베이스 시스템을 재시작하면 복구는 다음 3단계로 진행된다      분석 단계: 페이지 캐시에 저장된 더티 페이지와 장애 발생 당시 수행 중이던 트랜잭션을 파악한다. 더티 페이지에 대한 정보를 기반으로 리두 단계의 시작 지점을 결정한다. 트랜잭션 목록은 언두 단계에서 미완료된 트랜잭션을 롤백하는 데 사용한다.        리두 단계: 장애가 발생하기 전까지의 작업을 재수행하고 데이터베이스를 이전 상태로 복원한다. 불완전한 트랜잭션, 커밋됐지만 디스크로 플러시되지 않은 트랜잭션을 롤백하기 위한 준비 단계다        언두 단계: 불완전한 트랜잭션을 롤백하고 데이터베이스를 마지막 일관된 상태로 복원한다. 모든 작업은 실제 수행 순어의 역순으로 롤백된다. 복구 중에도 장애가 발생할 수 있기 때문에 언두 작업도 로그에 기록해야 된다  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-20T21:01:35+09:00'>20 Jul 2022</time><a class='article__image' href='/data-engineering-series14'> <img src='/images/db_internals.jpg' alt='Data Engineering Series [Part14]: Database Internals 트랜잭션 처리와 복구'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series14'>Data Engineering Series [Part14]: Database Internals 트랜잭션 처리와 복구</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part13]: Database Internals 개요",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series13",
      "date"     : "Jul 19, 2022",
      "content"  : "Table of Contents  스토리지 엔진          소개 및 개요                  DBMS 구조          인메모리 DBMS, 디스크 기반 DBMS          로우형 DBMS, 컬럼형 DBMS                    데이터베이스에서 중요한 개념: B-트리 알고리즘, 캐시 정책, 상태 복구, 동시성 제어(실용적인 측면에서는, 데이터베이스별 기능, 얼마나 빠른지, 어떤 유형의 어플리케이션에 이용되는지, SQL, 쿼리 플랜)분산 데이터베이스에서 중요한 개념: 장애 감지, 리더 선출, 분산 트랜잭션, 합의 알고리즘스토리지 엔진데이터베이스 관리 시스템의 주 목적은 데이터를 안정적으로 저장하고 사용자에게 제공하는 것이다. 일반적으로 데이터베이스는 기본 데이터 스토어로 사용되며 애플리케이션의 여러 구성 요소가 공유한다.데이터베이스는 모듈식 시스템이다. 요청을 전달하는 전송 계층, 가장 효율적인 쿼리 실행 계획을 결정하는 쿼리 프로세서, 실제 작업을 수행하는 실행 엔진 그리고 스토리지 엔진으로 구성된다.스토리지 엔진은 DBMS에서 데이터를 메모리와 디스크에 저장하고 검색 및 관리하는 소프트웨어 컴포넌트로써 각 노드에 데이터를 영구 저장한다. 데이터베이스가 복잡한 쿼리를 수행할 수 있도록 스토리지 엔진은 데이터를 세밀하게 조작할 수 있는 간단한 API를 제공한다. 사용자는 이를 사용해 레코드를 CRUD할 수 있다. 따라서 데이터베이스는 스토리지 엔진 위에서 스키마와 쿼리 언어, 인덱싱, 트랜잭션 등의 유용한 기능을 제공하는 애플리케이션이라고 볼 수 있다.스토리지 엔진은 DBMS와 독립적으로 개발됐다. 데이터베이스 개발자는 플러그형 스토리지 엔진을 사용해 데이터베이스 시스템을 구현할 수 있다. 예를 들어 DBMS 중 MySQL은 InnoDB, MyISAM, RocksDB라는 스토리지 엔진을 가질 수 있고, 몽고DB는 WiredTiger, In-Memory 스토리지 엔진이 있다.소개 및 개요데이터베이스 관리 시스템의 용도는 다양하다. 일시적인 핫 데이터를 주로 저장하는 데 쓰이기도 하고, 장기 보관용 콜드 데이터 스토리지로 쓰이기도 한다. 복잡한 쿼리 분석을 지원하는 시스템도 있는가 하면, 키로만 값을 액세스할 수 있는 스토어도 있다.저장 매체에 따라 인메모리 DBMS와 디스크 기반 DBMS로 구분할 수도 있다. 레이아웃에 따라 로우형, 컬럼형, 와이드 컬럼형 DBMS로 구분할 수도 있다.또 사용 용도에 따라 온라인 트랜잭션 처리(OLTP), 온라인 분석 처리(OLAP) 데이터베이스로 구분할 수도 있다.저장 형태에 따라 로우형 레코드, 키-값 쌍, 다큐멘트형 데이터베이스가 있다.DBMS 구조클라이언트의 요청은 트랜스포트 서브시스템을 통해 전달된다. 요청은 쿼리 형태이고 주로 특정 쿼리 언어로 표현된다. 트랜스포트 서브시스템은 쿼리를 쿼리 프로세서에 전달한다. 쿼리 프로세서는 쿼리를 해석, 분석 및 검증한다. 분석된 쿼리는 쿼리 옵티마이저에 전달된다.쿼리는 일반적으로 실행 계획 형태로 표현한다. 실행 계획은 쿼리가 요구하는 결과를 도출하는 데 수행해야 하는 일련의 작업이다.스토리지 엔진은 다음과 같은 역할을 담당하는 컴포넌트로 구성된다.  트랜잭션 매니저: 트랜잭션을 스케줄링하고 데이터베이스 상태의 논리적 일관성을 보장한다  잠금 매니저: 트랜잭셔넹서 접근하는 데이터베이스 객체에 대한 잠금을 제어한다. 동시 수행 작업이 데이터 무결성을 침해하지 않도록 제어한다.  버퍼 매니저: 데이터 페이지를 메모리에 캐시한다  복구 매니저: 로그를 유지 관리하고 장애 발생 시 시스템을 복구한다스토리지 엔진은 버퍼링, 가변성, 순서화와 같은 요소에 따라 구분된다.  버퍼링          데이터를 디스크에 쓰기 전에 일부를 메모리에 저장할지 여부      물론 모든 디스크 기반도 어느 정도 버퍼를 사용한다(디스크의 가장 작은 전송 단위가 블록이므로 블록을 채워서 쓰기 위해 버퍼링 사용)        가변성          데이터를 갱신할 때 in-place vs (append-only, copy-on-write)        순서화          디스크 페이지에 레코드를 키 순서로 저장할지에 대한 여부      순서화 -&amp;gt; 빠른 읽기, 비순서화 -&amp;gt; 빠른 쓰기      인메모리 DBMS, 디스크 기반 DBMS  인메모리          메모리에 데이터 저장      디스크는 복구와 로그 저장 용도      단점: RAM의 휘발성(디스크로 보완, 배터리 장착된 RAM 사용), 가격        디스크 기반          디스크에 데이터 저장      메모리는 캐시 또는 임시 저장 용도      단점: 랜덤 디스크 접근 속도 « 랜덤 메모리 접근 속도      디스크 기반은 아무리 큰 페이지를 캐시해도 직렬화와 데이터 레이아웃을 유지하는 오버헤드 때문에 인메모리 보다 느리다      인메모리 데이터베이스의 지속성인메모리 DBMS는 데이터의 지속성을 보장하고 데잍어 손실을 방지하기 위해 데이터를 디스크에 백업한다. 모든 작업은 로그 파일에 작업 내용을 순차적으로 기록해야 완료된다. 시스템 시작과 복구 시 모든 로그를 재수행하지 않기 위해 디스크에 백업본을 유지한다. 그렇게 함으로써 복구시에는 백업본과 로그를 기반으로 데이터를 재구성한다. (백업본만 있으면 이전 로그는 삭제해도 된다)로우형 DBMS, 컬럼형 DBMS대부분의 데이터베이스는 열과 행으로 구성된 테이블에 레코드를 저장한다. 데이터를 디스크에 저장하는 방식에 따라 데이터베이스를 분류할 수 있다. 컬럼 저장 방식은 테이블을 수직 분할하고, 로우 저장 방식은 수평 분할한다.MySQL, PostreSQL 등 대부분의 전통적인 관계형 데이터베이스는 로우형 DBMS이다. MonetDB, C-Store는 컬럼형 오픈소스 데이터베이스이다.  로우형 데이터 레이아웃          이 방식은 여러 필드의 값을 고유 식별 키로 구분할 수 있는 레코드 형식에 적합하다      일반적으로 특정 사용자의 필드는 여러 개가 함꼐 요청되는 경우가 많다      로우형 DBMS는 한 개의 로우씩 접근하는 경우 적합하다      한 블록에 모든 컬럼의 값이 들어가게 된다      특정 사용자의 모든 정보를 읽어올 때는 효율적, 모든 사용자의 특정 필드를 읽어올 때는 비효율적이다        컬럼형 데이터 레이아웃          컬럼끼리 디스크에 연속해 저장하는 방식이다      컬럼형 DBMS는 데이터의 추세와 평균 등을 계산하는 집계 분석 작업에 적합하다      다시 레코드 형태로 재구성하기 위해 컬럼 사이의 관계를 정의하는 메타데이터가 필요하다 (각 값마다 키값을 중복저장해야 한다)      최근 몇 년 동안 대용량 데이터에 대한 복잡한 분석 쿼리 사용이 늘어나고 있다      결과적으로 파케이(Parquet), ORC와 같은 컬럼 기반 파일 포맷과 쿠두(Kude)와 같은 컬럼형 DBMS가 각광받고 있다      같은 컬럼의 여러 값을 한 번에 읽으면 캐시 활용도와 처리 효율성이 높아진다. 같은 자료형끼리 저장하게 되면 압축률도 증가한다. 컬럼형과 로우형 DBMS 중 어떤 것을 사용할지 선택하려면 액세스 패턴을 파악해야 한다. 데이터를 레코드 단위로 접근하고 일반 쿼리와 범위 스캔 요청이 많다면 로우형 DBMS, 여러 로우를 스캔하거나 일부 컬럼에 대한 집계 작업이 많다면 컬럼형 DBMS가 더 적합할 수 있다.HBase와 같은 와이드 컬럼 스토어는 같은 자료형을 가지는 컬럼을 단위로 로우 형식으로 저장한다. 이 방식은 키 단위 액세스 패턴에 적합하다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-19T21:01:35+09:00'>19 Jul 2022</time><a class='article__image' href='/data-engineering-series13'> <img src='/images/db_internals.jpg' alt='Data Engineering Series [Part13]: Database Internals 개요'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series13'>Data Engineering Series [Part13]: Database Internals 개요</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part11]: 네트워크 활용편(5) 도커 네트워크와 AWS 네트워크",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series11",
      "date"     : "Jul 17, 2022",
      "content"  : "Table of Contents  참고참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-17T21:01:35+09:00'>17 Jul 2022</time><a class='article__image' href='/network-series11'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part11]: 네트워크 활용편(5) 도커 네트워크와 AWS 네트워크'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series11'>Network Series [Part11]: 네트워크 활용편(5) 도커 네트워크와 AWS 네트워크</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series10",
      "date"     : "Jul 16, 2022",
      "content"  : "Table of Contents  로드 밸런싱의 정의  로드 밸런서의 특징  로드 밸런서가 제공하는 이점  로드 밸런싱 방법          Round Robin Algorithm      Least connections      Source IP hash      The least bandwidth method        로드 밸런싱의 이점  Software Load Balancers vs. Hardware Load Balancers  로드 밸런서 종류  참고로드 밸런싱의 정의  Load balancing is the process of distributing network traffic across multiple servers. This ensures no single server bears too much demand. By spreading the work evenly, load balancing improves application responsiveness. It also increases availability of applications and websites for users. Modern applications cannot run without load balancers. Over time, software load balancers have added additional capabilities including application security.로드 밸런서의 특징  the load balancer decides which servers can handle that traffic. This maintains a good UX  Load balancers manage the flow of information between the server and an endpoint device  Load balancers conduct continuous health checks on servers to ensure they can handle requests로드 밸런서가 제공하는 이점  Automatically detect server failures and redirect client traffic  Allow for server maintenance without any impact  Provide automated disaster recovery to backup sites  Add and remove application servers without disruption  Monitor and block malicious content로드 밸런싱 방법Round Robin AlgorithmRound robin (RR) algorithm is a circular distribution of requests to enterprise servers in a sequence. There are two types of Round Robin – Weighted Round Robin and Dynamic Round Robin. Used mainly for a cluster of different servers, in weighted round robin each server is assigned a weight depending on its composition. Based on the preassigned efficiency the load is distributed in a cyclical procedure. Dynamic round robin are used to forward requests to associated servers based on real time calculation of assigned server weights.Least connectionsBy taking into consideration the number of active and current connections to each application instance, ‘Least Connections’ load balancing algorithm distributes the load by choosing the server with the least number of active transactions (connections).Source IP hashIn a source IP Hash, load balancing a server is selected based on a unique hash key. The Hash key is generated by taking the source and destination of the request. Based on the generated hash key, servers are assigned to clients.The least bandwidth methodIn the least bandwidth method backend servers are selected based on the server’s bandwidth consumption i.e the server consuming least bandwidth is selected (measured in Mbps). Similar to least bandwidth method is the least packets method. Here the server transmitting least packets are selected by the load balancer.로드 밸런싱의 이점Load balancing can do more than just act as a network traffic cop. Software load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the software load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions.In the seven-layer Open System Interconnection (OSI) model, network firewalls are at levels one to three (L1-Physical Wiring, L2-Data Link and L3-Network). Meanwhile, load balancing happens between layers four to seven (L4-Transport, L5-Session, L6-Presentation and L7-Application).Load balancers have different capabilities, which include:L4 — directs traffic based on data from network and transport layer protocols, such as IP address and TCP port.L7 — adds content switching to load balancing. This allows routing decisions based on attributes like HTTP header, uniform resource identifier, SSL session ID and HTML form data.GSLB — Global Server Load Balancing extends L4 and L7 capabilities to servers in different geographic locations.More enterprises are seeking to deploy cloud-native applications in data centers and public clouds. This is leading to significant changes in the capability of load balancers. In turn, this creates both challenges and opportunities for infrastructure and operations leaders.Software Load Balancers vs. Hardware Load BalancersLoad balancers run as hardware appliances or are software-defined. Hardware appliances often run proprietary software optimized to run on custom processors. As traffic increases, the vendor simply adds more load balancing appliances to handle the volume. Software defined load balancers usually run on less-expensive, standard Intel x86 hardware. Installing the software in cloud environments like AWS EC2 eliminates the need for a physical appliance.로드 밸런서 종류      Network Server Load Balancers(NLB): Load balancers entered the market in the mid-1990s to support the surge of traffic on the internet. Load balancers had basic functionality designed to pool server resources to meet this demand. The load balancer managed connections based on the packet header. Specifically, they looked at the 5-tuple – source IP, destination IP, source port, destination port, and IP protocol. This is the entry of the network server load balancer or Layer 4 load balancer.        Application Load Balancers(ALB): As technology evolved, so did the load balancers. They became more advanced and started providing content awareness and content switching. These load balancers looked beyond the packet header and into the content payload. These load balancers look at the content such as the URL, HTTP header, and other things to make load balancing decisions. These are the application load balancers or Layer 7 load balancers.        Elastic Load Balancers(ELB): Elastic Load Balancing scales traffic to an application as demand changes over time. It uses system health checks to learn the status of application pool members (application servers) and routes traffic appropriately to available servers, manages fail-over to high availability targets, or automatically spins-up additional capacity.  참고  AVI Networks: What Is Load Balancing?  Progress Kemp: What is load balancing?  Jae Honey, L4 / L7 로드밸런서 차이 (Load balancer)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-16T21:01:35+09:00'>16 Jul 2022</time><a class='article__image' href='/network-series10'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series10'>Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series9",
      "date"     : "Jul 15, 2022",
      "content"  : "Table of Contents  네트워크 관련 리눅스 명령어          ifconfig      netstat      iptables      ping      wget      curl      ssh        참고네트워크 관련 리눅스 명령어ifconfig, ip, netstat, ss, iptables, ping, ssh, telnet, route, curl, wgetifconfig  네트워크 인터페이스의 활성/비활성 여부, ip 관련 정보를 확인할 수 있음ifconfig----------------------------------------------------------------------------en0: flags=8863&amp;lt;UP,BROADCAST,SMART,RUNNING,SIMPLEX,MULTICAST&amp;gt; mtu 1500    ...	inet &amp;lt;ip주소&amp;gt; netmask &amp;lt;서브넷 마스크&amp;gt; broadcast &amp;lt;브로드캐스트 주소&amp;gt;    ...	status: activenetstat      Print network connections, routing tables, interface statistics, masquerade connections, and multicast memberships    mac에서 netstat은 ubuntu, redhat 계열과는 가지고 있는 옵션이 다름          -a 기본 출력에 포함되지 않은 netstat의 출력에 서버 포트를 포함합니다.      -g 멀티 캐스트 연결과 관련된 정보를 표시합니다.      -I interface는 지정된 인터페이스에 대한 패킷 데이터를 제공합니다. 사용 가능한 모든 인터페이스는 -i 깃발이지만 en0 일반적으로 기본 발신 네트워크 인터페이스입니다. (소문자에 유의하십시오.)      -n 이름이있는 원격 주소의 레이블을 억제합니다. 이것은 제한된 정보만을 희생시키면서 netstat의 출력 속도를 크게 향상시킵니다.      -p 프로토콜은 특정 네트워킹 프로토콜과 관련된 트래픽을 나열합니다. 전체 프로토콜 목록은 / etc / protocols,하지만 가장 중요한 것은 udp 와 TCP.      -r 네트워크에서 패킷이 라우팅되는 방식을 보여주는 라우팅 테이블을 표시합니다.      -s 활성 여부에 관계없이 모든 프로토콜에 대한 네트워크 통계를 표시합니다.      -v 특히 각 열린 포트와 연관된 프로세스 ID (PID)를 표시하는 열을 추가하여 자세한 정보를 제공합니다.      자주 사용하는 예시        netstat -avp tcp# 포트 열고 LISTEN 하고 있는 네트워크 엔드포인트netstat -a | grep -i LISTEN                      ubuntu 계열          자주 사용하는 예시        netstat -nlp                            iptables  administration tool for IPv4 packet filtering(방화벽) and NATping  특정 서버가 켜져있는지/접근 가능한지 확인    ping &amp;lt;ip&amp;gt;ping -c &amp;lt;ping날릴 횟수&amp;gt; &amp;lt;ip&amp;gt;        포트 번호를 지정하고 싶은 경우에는 tcping 라는 커맨드 툴 설치해야함    brew install tcpingtcping &amp;lt;ip&amp;gt; &amp;lt;port&amp;gt;      wget  network downloader  기본적으로 다운이 디폴트    wget &amp;lt;URL&amp;gt;          curl  tool for transfering data from or to a server  기본적으로 터미널 화면에 출력이 디폴트    curl &amp;lt;URL&amp;gt;curl &amp;lt;URL&amp;gt; &amp;gt; &amp;lt;원하는 파일명&amp;gt;curl -o &amp;lt;원하는 파일명&amp;gt; &amp;lt;URL&amp;gt;curl -O &amp;lt;URL&amp;gt;/&amp;lt;원하는 파일명&amp;gt;            뭔가 curl은 파일의 다운로드 경로를 넣어줘도 HTML 코드가 출력된다 (파일을 다운로드하고자 할 때는 wget인건가 무조건?)  서버에 REST API 형태로 HTTP Request를 전송할 수도 있다    curl -X POST -H &amp;lt;헤더&amp;gt; &amp;lt;URL&amp;gt; -d &amp;lt;데이터&amp;gt;      ssh  remote login program    ssh -i &amp;lt;pem파일&amp;gt; &amp;lt;username&amp;gt;@&amp;lt;host&amp;gt;        어플리케이션 계층에서 제공참고  kibua20, 자주 사용하는 curl 명령어 옵션과 예제  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-15T21:01:35+09:00'>15 Jul 2022</time><a class='article__image' href='/network-series9'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series9'>Network Series [Part9]: 네트워크 활용편(3) Linux Network 관련 명령어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part8]: 네트워크 활용편(2) REST API",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series8",
      "date"     : "Jul 14, 2022",
      "content"  : "Table of Contents  참고REpresentational State TransferHTTP API와 REST API는 사실 거의 같은 의미로 사용됩니다.그런데 디테일하게 들어가면 차이가 있습니다.HTTP API는 HTTP를 사용해서 서로 정해둔 스펙으로 데이터를 주고 받으며 통신하는 것으로 이해하시면 됩니다.그래서 상당히 넓은 의미로 사용됩니다.반면에 REST API는 HTTP API에 여러가지 제약 조건이 추가됩니다.REST는 다음 4가지 제약조건을 만족해야 합니다.(https://ko.wikipedia.org/wiki/REST)      자원의 식별        메시지를 통한 리소스 조작        자기서술적 메서지        애플리케이션의 상태에 대한 엔진으로서 하이퍼미디어  여러가지가 있지만 대표적으로 구현하기 어려운 부분이 마지막에 있는 부분인데요. 이것은 HTML처럼 하이퍼링크가 추가되어서 다음에 어떤 API를 호출해야 하는지를 해당 링크를 통해서 받을 수 있어야 합니다.그리고 이런 부분을 완벽하게 지키면서 개발하는 것을 RESTful API라고 하는데요. 실무에서 이런 방법으로 개발하는 것은 현실적으로 어렵고, 또 추가 개발 비용대비 효과가 있는 것도 아닙니다.그런데 이미 많은 사람들이 해당 조건을 지키지 않아도 REST API라고 하기 때문에, HTTP API나 REST API를 거의 같은 의미로 사용하고 있습니다. 하지만 앞서 말씀드린 것 처럼 엄격하게 위의 내용들을 모두 지켜야 REST API라고 할 수 있습니다.(하지만 다들 HTTP API를 REST API라고 이미 하고 있기 때문에, 누군가 REST API라고 하면 그냥 아~ HTTP API를 이야기 하는구나 라고 생각하고 들으시면 됩니다. 물론 엄격하게는 다릅니다.)참고  이영한님의 HTTP API와 REST API의 차이에 관한 질문에 대한 답변  REST 논문을 정리한 자료  정상우, REST를 보다 RESTful 하게 API 만들기",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-14T21:01:35+09:00'>14 Jul 2022</time><a class='article__image' href='/network-series8'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part8]: 네트워크 활용편(2) REST API'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series8'>Network Series [Part8]: 네트워크 활용편(2) REST API</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series7",
      "date"     : "Jul 13, 2022",
      "content"  : "Table of Contents  웹 브라우저 렌더링 과정  캐시와 쿠키          캐시                  웹 브라우저 캐시와 CPU 캐시          웹 브라우저 캐시의 종류          프록시 캐시                    쿠키      세션        Do web browsers use different outgoing ports for different tabs?  참고웹 브라우저 렌더링 과정대기열: 브라우저는 주소창을 통해 입력한 요청을 대기열에 넣는다캐싱: 요청에 대한 응답값을 프록시 서버 캐시 또는 웹 브라우저 캐시에 저장하고 재요청시 캐싱된 값을 리턴브라우저 캐시: 쿠키, 로컬 스토리지, 세션 스토리지 등을 포함한 캐시공유 프록시 캐시: 서버 앞 단에 클라이언트와 가깝게 프록시 서버를 배치한다. 프록시 서버에 저장된 응답값이 프록시 캐시(Node.js 앞에 있는 nginx)DNS: URL의 도메인명을 IP주소로 변환한다. 이 과정 또한 바로 DNS로 요청을 보내지 않고, 호스트 파일과 같이 캐시된 데이터가 있는지 먼저 확인한다라우팅: DNS가 resolve한 IP주소와 ARP 과정을 통해 얻은 MAC 주소를 이용해 목적지 IP 주소로 라우팅 한다.초기연결: 라우팅할 때 처음부터 데이터를 요청하는 것이 아니라 먼저 TCP 3 way handshake를 이용해 연결을 한다다운로드: 이제 요청에 대한 응답값과, 그외에 필요한 HTML, CSS, 이미지 등을 서버로부터 받는다렌더링: 웹 브라우저가 서버로부터 받아온 데이터를 이용해 렌더링하여 사용자에게 제공한다캐시와 쿠키캐시  The goal of a caching is to store some resource “closer” to the access point than it was before  나중에 요청올 결과를 미리 저장해두었다가 빠르게 돌려주는 것웹 브라우저 캐시와 CPU 캐시  The CPU cache consists of special circuits within the CPU that are designed to cache memory, i.e. the cache is implemented in hardware as part of the physical CPU. This special cache hardware has absolutely nothing to do with the browser cache  웹 브라우저 캐시  We all have cache stored in our internet browser. It’s essentially a folder which contains small pieces of information taken from our browsing experience and the sites we visit. The aim is to make pages we revisit load faster.  CPU 캐시  The purpose of the cache memory component is to increase speed. It’s located physically closer to the processor than the RAM component making it 10 to 100 times faster than RAM. However, its function is very different. The cache memory stores instructions and data that are used repeatedly by programs, enhancing the overall performance.웹 브라우저 캐시의 종류  로컬 스토리지          키/값 쌍의 형태로 클라이언트 브라우저에서 데이터를 저장하는 방법      수동으로 삭제하지 않는 한 영구적으로 저장소에 남아 있습니다.        mac의 경우~/Library/Application Support/Google/Chrome/Default/Local Storage/                      세션 스토리지          로컬 스토리지와 유사하지만 탭을 닫을 때 삭제됨        쿠키프록시 캐시공유 프록시 캐시: 서버 앞 단에 클라이언트와 가깝게 프록시 서버를 배치한다. 프록시 서버에 저장된 응답값이 프록시 캐시(Node.js 앞에 있는 nginx, AWS의 CloudFront)쿠키쿠키는 웹 서버가 생성하여 웹 브라우저로 보내는 정보의 작은 파일입니다. 웹 브라우저는 미리 정해진 시간 동안 또는 웹 사이트에서 사용자의 세션 기간 동안 받은 쿠키를 저장합니다. 사용자가 웹 서버에 대해 향후 요청할 경우 관련 쿠키를 첨부합니다.쿠키는 웹 사이트가 사용자 경험을 개인화할 수 있도록 사용자에 대한 정보를 웹 사이트에 제공하는 데 도움이 됩니다. 예를 들어, 전자 상거래 웹사이트는 사용자들이 쇼핑 카트에 어떤 상품을 넣었는지 알기 위해 쿠키를 사용한다. 또한 일부 쿠키는 인증 쿠키와 같은 보안 목적으로 필요합니다(아래 참조).인터넷에서 사용되는 쿠키는 “HTTP 쿠키”라고도 불립니다. 대부분의 웹과 마찬가지로 쿠키는 HTTP 프로토콜을 사용하여 전송됩니다.cat ~/Library/Application Support/Google/Chrome/Default/Cookies세션Do web browsers use different outgoing ports for different tabs?Each connection to a website uses a different socket with default destination TCP port 80 for plain HTTP and 443 for HTTPS. For the socket to be unique, the combination of the source IP address, source TCP port, destination IP address and destination TCP port must be different.If you have multiple connections to the same website (assuming the website uses only 1 IP address) from the same computer, a different source TCP port must be used. This way, each connection is unique.참고  CLOUD FLARE: What are cookies?, Cookies definition",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-13T21:01:35+09:00'>13 Jul 2022</time><a class='article__image' href='/network-series7'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series7'>Network Series [Part7]: 네트워크 활용편(1) 웹 브라우저</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part20]: 투 포인터(Two-Pointers)) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/two-pointer",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  StackStackLeetcode: Two-Pointers💟 ✅ ❎문제 리스트---------------------------------------------EASY 3문제- Reverse String ✅- Count Binary Substrings 💟 ❎---------------------------------------------MEDIUM 5문제- Interval List Intersections 💟 ✅- Longest String Chain 💟 ❎- String Compression 💟 ❎- 3Sum With Multiplicity ❎- Next Permutation 💟 ❎",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/two-pointer'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part20]: 투 포인터(Two-Pointers)) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/two-pointer'>Coding Test Series [Part20]: 투 포인터(Two-Pointers)) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part19]: 스택(Stack) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/stack",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  StackStackLeetcode: Stack문제 리스트---------------------------------------------EASY 3문제- Increasing Order Search Tree- Remove All Adjacent Duplicates In String- Valid Parentheses---------------------------------------------MEDIUM 7문제- Minimum Remove to Make Valid Parentheses- Buildings With an Ocean View- Exclusive Time of Functions- Sum of Subarray Ranges- Decode String- Remove All Adjacent Duplicates in String II- Asteroid Collision- Simplify Path--------------------------------------------- HARD 4문제- Maximum Frequency Stack- Trapping Rain Water- Largest Rectangle in Histogram- Longest Valid Parentheses",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/stack'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part19]: 스택(Stack) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/stack'>Coding Test Series [Part19]: 스택(Stack) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part18]: 정렬 알고리즘(Sorting Algorithm) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/sorting",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  SortingSorting문제 리스트---------------------------------------------EASY 6문제- Maximum Units on a Truck- Squares of a Sorted Array- Intersection of Two Arrays- Majority Element- Contains Duplicate- Merge Sorted Array---------------------------------------------MEDIUM 16문제- Custom Sort String- Design A Leaderboard- K Closest Points to Origin- Group Anagrams- Minimum Moves to Equal Array Elements II- Rank Teams by Votes- Remove Covered Intervals- Reorder Data in Log Files- Minimum Time Difference- Sort Colors- Top K Frequent Words- 3Sum- Merge Intervals- Analyze User Website Visit Pattern- 4Sum- Largest Number--------------------------------------------- HARD 3문제- Employee Free Time- Maximum Profit in Job Scheduling- Russian Doll EnvelopesLeetcode: Sorting",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/sorting'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part18]: 정렬 알고리즘(Sorting Algorithm) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/sorting'>Coding Test Series [Part18]: 정렬 알고리즘(Sorting Algorithm) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part17]: 해시테이블(Hash Table) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/hash-table_2",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  HashTableHashTableLeetcode: Hash-Table💟 ✅ ❎문제 리스트---------------------------------------------EASY 10문제- Jewels and Stones ✅- Logger Rate Limiter ✅- High Five ✅- Unique Number of Occurrences 💟 ✅- Find Words That Can Be Formed by Characters 💟 ✅- Valid Anagram 💟 ✅- Missing Number ✅- Roman to Integer ✅- Find Winner on a Tic Tac Toe Game ✅---------------------------------------------MEDIUM 14문제- Dot Product of Two Sparse Vectors ✅- Minimum Number of Steps to Make Two Strings Anagram ✅- Subdomain Visit Count- Design Underground System- Custom Sort String- Group Anagrams- Find Duplicate File in System- Design File System- Shortest Word Distance II- Integer to Roman- Longest String Chain- Letter Combinations of a Phone Number- Reconstruct Original Digits from English- Subarray Sum Equals K--------------------------------------------- HARD 4문제- LFU Cache- Count Unique Characters of All Substrings of a Given String- Bus Routes- First Missing Positive",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/hash-table_2'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part17]: 해시테이블(Hash Table) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hash-table_2'>Coding Test Series [Part17]: 해시테이블(Hash Table) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part16]: 탐욕 알고리즘(Greedy Algorithm) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/greedy",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  Greedy AlgorithmGreedy AlgorithmLeetcode: Greedy Algorithm💟 ✅ ❎문제 리스트---------------------------------------------EASY- Maximum Units on a Truck 💟 ✅ - Valid Palindrome II 💟 ✅  - Can Place Flowers 💟 ❎---------------------------------------------MEDIUM- Minimum Add to Make Parentheses Valid- Minimum Number of Swaps to Make the String Balanced- Smallest String With A Given Numeric Value- Remove Colored Pieces if Both Neighbors are the Same Color- Two City Scheduling- Best Time to Buy and Sell Stock II- Minimum Deletions to Make Character Frequencies Unique- Container With Most Water- Boats to Save People- Meeting Rooms II- Gas Station- Largest Number--------------------------------------------- HARD- Minimize Deviation in Array- Candy",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/greedy'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part16]: 탐욕 알고리즘(Greedy Algorithm) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/greedy'>Coding Test Series [Part16]: 탐욕 알고리즘(Greedy Algorithm) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part15]: 이진 검색(Binary Search) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/binary-search",
      "date"     : "Jul 12, 2022",
      "content"  : "Table of Contents  Binary SearchBinary SearchLeetcode: Binary Search💟 ✅ ❎문제 리스트---------------------------------------------EASY 5문제- Intersection of Two Arrays ✅- Missing Number ✅- Kth Missing Positive Number 💟 ✅(O(n)) (이진 탐색으로 풀기 꽤 어려움.. 문제는 좋음)- Binary Search 💟 ✅- First Bad Version 💟 ✅---------------------------------------------MEDIUM 12문제- Capacity To Ship Packages Within D Days- Max Consecutive Ones III- Find the Duplicate Number- Koko Eating Bananas- Time Based Key-Value Store- Maximum Length of Repeated Subarray- Random Pick with Weight- Find Peak Element- Tweet Counts Per Frequency- Find First and Last Position of Element in Sorted Array- Search in Rotated Sorted Array- Maximum Earnings From Taxi--------------------------------------------- HARD 3문제- Split Array Largest Sum- Maximum Profit in Job Scheduling- Median of Two Sorted Arrays",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-12T21:01:35+09:00'>12 Jul 2022</time><a class='article__image' href='/binary-search'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part15]: 이진 검색(Binary Search) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/binary-search'>Coding Test Series [Part15]: 이진 검색(Binary Search) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part14]: 분할 정복(Divide and conquer) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/divide-and-conquer",
      "date"     : "Jul 11, 2022",
      "content"  : "Table of Contents  Divide and ConquerDivide and ConquerLeetcode: Divide and Conquer 문제 리스트--------------------------------------------- 6문제- Majority Element- K Closest Points to Origin- Top K Frequent Elements- Kth Largest Element in an Array- Maximum Subarray- Median of Two Sorted Arrays",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-11T21:01:35+09:00'>11 Jul 2022</time><a class='article__image' href='/divide-and-conquer'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part14]: 분할 정복(Divide and conquer) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/divide-and-conquer'>Coding Test Series [Part14]: 분할 정복(Divide and conquer) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part14] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series14",
      "date"     : "Jul 10, 2022",
      "content"  : "Table of Contents  JOIN과 SUBQUERY 둘 다 사용할 수 있다면 어떤 것을 쓰는게 좋을까  LEFT OUTER JOIN을 사용할 때 조건을 만족하는 행이 2개인 경우  FROM에서 콤마(,)가 사용된 경우  날짜 관련 함수  ROW_NUMBER()  윈도우 함수  OVER()  CTE(Common Table Expression)  LEAD, LAG 함수  3 NOT IN (null, 1, 2)  WHERE 조건절에 레코드(튜플) 사용할 수도 있음  JOIN 쿼리에서 ON절과 WHERE절에 표기하는 것의 차이  Subset  Comparing Sets  INNER JOIN은 ON이 없어도 된다?  ROW_NUMBER(), RANK(), DENSE_RANK()  서브쿼리에서는 메인쿼리의 컬럼을 사용할 수 있다  컬럼명이 SQL 문법에 포함되는 경우 쌍따옴표로 묶어주면 된다  UNION 말고 UNION ALL도 있다  참고JOIN과 SUBQUERY 둘 다 사용할 수 있다면 어떤 것을 쓰는게 좋을까  Sub-query solutionSELECT Name as Employee FROM Employee eWHERE Salary &amp;gt; (    Select Salary FROM Employee m WHERE m.Id = e.ManagerId)Advantages Of Subquery  Complex query can be broken down into a series of logical steps.  Subquery is easy to read, understand and maintain.  It allow to use the results of another query in the outer query.Disadvantages of Subquery  Execution is slower than JOIN.  We cannot modify a table and select from the same table within a subquery in the same SQL statement.  JOIN solutionSELECT     a.NAME AS EmployeeFROM Employee AS a JOIN Employee AS b     ON a.ManagerId = b.Id     AND a.Salary &amp;gt; b.SalaryAdvantage of a JOIN  Execution and retrieval time faster than subqueries.Disadvantages Of JOIN  Database server has to do more work when it comes to a lot of joins in a query =&amp;gt; more time consuming to retrieve data  Developer can be confused to choose the appropriate type among many types of joins.  Conclusion  Most developers prioritize speed optimizing while others prioritize logic. It ups to you in your specific case.LEFT OUTER JOIN을 사용할 때 조건을 만족하는 행이 2개인 경우문제 Consecutive Available Seats여기서 나는 LEFT OUTER JOIN을 2번 썼다. [현재 좌석, 이전 좌석, 이후 좌석] 이런식으로.정답은 맞았지만 이렇게 조인을 2번이나 써야하나 라는 생각에 다른 사람들의 풀이를 구경해봤다.풀이 중에select distinct a.seat_idfrom cinema ajoin cinema bon abs(a.seat_id - b.seat_id) = 1and a.free=true and b.free=trueorder by a.seat_id;을 봤다. 조인을 1번만 쓰고 있다. 근데 이 방법은 ON절에 사용된 조건이 driving table 행에 조인되는 derived table의 행이 2개가 조인되는 결과를 가져오게 될 것 같았다. 그래서 이러한 경우에는 결과 테이블이 어떻게 될지 궁금해 검색해봤다.haerong22, LEFT OUTER JOIN 의 함정위 블로그 내용을 보니이렇게 id 값이 중복되는 데이터 뻥튀기(?) 현상이 일어났다.이 때는  테이블간 제약 조건을 명확히 한다.  조인 조건을 추가  distinct 사용  group by 사용  top 1, limit 사용와 같은 방법을 이용해 해결할 수 있다. 위의 풀이에서는 distinct를 추가했다.FROM에서 콤마(,)가 사용된 경우FROM 절에서 여러 테이블을 함께 사용할 때 사람들마다 쿼리 작성법이 조금씩 달랐다. 표기법의 차이일 뿐 다음은 같은 역할을 한다.  INNER JOIN = JOIN = ,  LEFT OUTER JOIN = LEFT JOIN  RIGHT OUTER JOIN = RIGHT JOIN날짜 관련 함수  DATE_SUB()          INTERVAL        DATE_FORMAT()ROW_NUMBER()  Assigns a sequential integer to every row within its partition  We will show you how to add a sequential integer to each row or group of rows in the result set.  ROW_NUMBER() is a window function that returns a sequential number for each row, starting from 1 for the first row.윈도우 함수  특정 범위마다 함수를 적용하는 것을 윈도우 함수라고 함  MySQL에서 제공하는 윈도우 함수라고 따로 정의해둔 윈도우 함수 묶음이 있음  집계 함수도 OVER절을 이용해 범위를 정의하면 윈도우 함수로 사용할 수 있음(Most aggregate functions also can be used as window functions, MySQL 공식문서)  사용 방법: [윈도우 함수] + [OVER 절] or [집계 함수] + [OVER 절]  범위마다 함수를 적용한다는 점에서 GROUP BY와 비슷하게 느껴지지만, GROUP BY는 집계된 결과를 테이블로 보여주는 반면, 윈도우 함수는 집계된 결과를 기존 테이블에 하나의 열로 추가하여 결과를 볼 수 있음OVER()  If you want to learn window functions in MySQL, you need to understand the OVER clause  In 2018, MySQL introduced a new feature: window functions, which are accessed via the OVER clause. Window functions are a super powerful resource available in almost all SQL databases. They perform a specific calculation (e.g. sum, count, average, etc.) on a set of rows; this set of rows is called a “window” and is defined by the MySQL OVER clause.  OVER clause which has three possible elements: partition definition, order definition, and frame definition.    [window_function(expression)][aggregation_function(expression)] OVER (  [partition_defintion] [order_definition] [frame_definition])        PARTITION BY: 윈도우 범위 결정  ORDER BY: 정렬하여 계산CTE(Common Table Expression)  문제: All People Report to the Given Manager  In MySQL every query generates a temporary result or relation. In order to give a name to those temporary result set, CTE is used.  A CTE is defined using WITH clause  A recursive CTE is a subquery which refer to itself using its own name          The recursive CTEs are defined using WITH RECURSIVE clause      There should be a terminating condition to recursive CTE.      The recursive CTEs are used for series generation and traversal of hierarchical or tree-structured data      WITH RECURSIVE CTE AS (    SELECT employee_id    FROM Employees    WHERE manager_id = 1 AND employee_id != 1    UNION ALL    SELECT e.employee_id    FROM CTE c INNER JOIN Employees e ON c.employee_id = e.manager_id)SELECT employee_idFROM CTEORDER BY employee_idLEAD, LAG 함수  Non Aggregation Window Function 중 하나  lead -&amp;gt; 이끌다 -&amp;gt; 현재 행 다음  lag -&amp;gt; 질질 끌다 -&amp;gt; 현재 행 이전  LEAD(expr, N, default) OVER(PARTITION BY ~ ORDER BY ~)3 NOT IN (null, 1, 2)  Tree Node 문제  True로 여겨질 줄 알았으나 False였다WHERE 조건절에 레코드(튜플) 사용할 수도 있음SELECT student_id, MIN(course_id) AS course_id, gradeFROM EnrollmentsWHERE (student_id, grade) IN                            (SELECT student_id, MAX(grade)                            FROM Enrollments                            GROUP BY student_id)GROUP BY student_id, gradeORDER BY student_idJOIN 쿼리에서 ON절과 WHERE절에 표기하는 것의 차이SubsetComparing SetsINNER JOIN은 ON이 없어도 된다?  INNER JOIN, JOIN 또는 그냥 콤마(,)를 이용해 두 테이블을 조인할 때는 ON이 없어도 된다  (OUTER JOIN은 없으면 에러남)  ON없이 사용하는 경우를 Cartesian Product(곱집합) 이라고 함  A: {1, 2, 3}, B: {x, y} -&amp;gt; FROM A, B를 하면 -&amp;gt; {[1, x], [1, y], [2, x], [2, y], [3, x], [3, y]}  이 개념을 활용하면 LeetCode의 Shortest Distance in a Plane 문제를 풀 수 있다ROW_NUMBER(), RANK(), DENSE_RANK()            ROW_NUMBER()      공동 순위를 무시함 (ex: 1,2,3,4 …)              RANK()      공동 순위만큼 건너뜀 (ex: 1,2,2,4 …)              DENSE_RANK()      공동 순위를 뛰어넘지 않음 (ex: 1,2,2,3 …)      서브쿼리에서는 메인쿼리의 컬럼을 사용할 수 있다SELECT S1.Score, (    SELECT COUNT(DISTINCT Score) FROM Scores WHERE Score &amp;gt;= S1.Score) AS &quot;rank&quot;FROM Scores S1ORDER BY S1.Score DESC컬럼명이 SQL 문법에 포함되는 경우 쌍따옴표로 묶어주면 된다SELECT  score,  RANK() OVER (ORDER BY score) AS &quot;rank&quot;FROM ScoresUNION 말고 UNION ALL도 있다{[1,  ‘kim’]}과 {[1,  ‘kim’]}을 합칠 때  UNION -&amp;gt; {[1,  ‘kim’]}  UNION ALL -&amp;gt; {[1,  ‘kim’], [1,  ‘kim’]}참고  MySQL tutorial: MySQL ROW_NUMBER, This is How You Emulate It  SQL OVER 절  [MySQL] 윈도우함수(Window Function)  MySQL 공식문서: 12.21.1 Window Function Descriptions  LearnSQL: What Is the MySQL OVER Clause?  GeeksforGeeks: MySQL Recursive CTE (Common Table Expressions)  horang, [MySQL] 계층 쿼리 - WITH, WITH RECURSIVE 사용법",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-10T21:01:35+09:00'>10 Jul 2022</time><a class='article__image' href='/mysql-series14'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part14] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series14'>MySQL Series [Part14] SQL을 이용한 데이터 분석에서 겪었던 다양한 경험들</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part13] SQL 문제",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series13",
      "date"     : "Jul 10, 2022",
      "content"  : "Table of Contents  SQLSQLLeetcode: Database 💟 ✅ ❎문제 리스트--------------------------------------------- EASY- Rearrange Products Table 💟 ❎- Customer Who Visited but Did Not Make Any Transactions 💟 ✅ - Bank Account Summary II 💟 ✅- Group Sold Products By The Date 💟 ✅- Immediate Food Delivery I 💟 ✅- Reformat Department Table 💟 ✅- Product Sales Analysis I ✅- Product Sales Analysis II ✅- The Latest Login in 2020 ✅- Employees With Missing Information 💟 ✅- Swap Salary ✅- Game Play Analysis I ✅- Primary Department for Each Employee ✅- Find Customer Referee ✅- Big Countries ✅- Customer Placing the Largest Number of Orders ✅- Customer Order Frequency 💟 ✅- Combine Two Tables ✅- Duplicate Emails ✅- Consecutive Available Seats 💟 ✅- Employees Earning More Than Their Managers ✅- Reported Posts 💟 ✅- Customers Who Never Order ✅- Fix Product Name Format 💟 ✅- Delete Duplicate Emails ✅- User Activity for the Past 30 Days I 💟 ✅- Sales Analysis II 💟 ✅- The Number of Employees Which Report to Each Employee 💟 ✅- Classes More Than 5 Students ✅- Rising Temperature 💟 ✅- Friend Requests I: Overall Acceptance Rate 💟 ✅--------------------------------------------- MEDIUM- Find the Start and End Number of Continuous Ranges 💟 ❎ - Running Total for Different Genders 💟 ❎ - All People Report to the Given Manager ✅- Number of Calls Between Two Persons 💟 ✅- Game Play Analysis III 💟 ✅- Customers Who Bought Products A and B but Not C 💟 ✅- Biggest Window Between Visits 💟 ❎ - Evaluate Boolean Expression 💟 ✅- Tree Node ✅- Highest Grade For Each Student 💟 ✅- Number of Accounts That Did Not Stream ✅- Restaurant Growth 💟 ❎ - Exchange Seats 💟 ✅- Product Price at a Given Date 💟 ❎ - Customers Who Bought All Products ❎- Managers with at Least 5 Direct Reports ✅- Market Analysis I 💟 ✅- Find Interview Candidates 💟 ❎ - Immediate Food Delivery II 💟 ❎ - Shortest Distance in a Plane ✅- Rank Scores 💟 ✅- Movie Rating 💟 ✅- Countries You Can Safely Invest In 💟 ✅- League Statistics 💟 ✅- The Number of Passengers in Each Bus I- Department Highest Salary- Consecutive Numbers- Game Play Analysis IV- Active Users- Nth Highest Salary- Second Degree Follower- Second Highest Salary- Reported Posts II--------------------------------------------- HARD- Sales by Day of the Week- Median Employee Salary- Total Sales Amount by Year- Average Salary: Departments VS Company- Longest Winning Streak- Dynamic Pivoting of a Table- First and Last Call On the Same Day- Tournament Winners- The Number of Passengers in Each Bus II- Human Traffic of Stadium- Department Top Three Salaries- Find Median Given Frequency of Numbers- The Number of Seniors and Juniors to Join the Company- Trips and Users",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-10T21:01:35+09:00'>10 Jul 2022</time><a class='article__image' href='/mysql-series13'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part13] SQL 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series13'>MySQL Series [Part13] SQL 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part12]: Purpose-built Databases(OLTP, OLAP, Cache)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series12",
      "date"     : "Jul 9, 2022",
      "content"  : "Table of Contents  OLTP  OLAP  참고Purpose-built databases address different workloads, including online transactional processing (OLTP), online analytical processing (OLAP), and caching. The data models representing the entities your applications work with and their relationships are supported by relational, document, key-value or graph, or time-series databases.OLTP  사용자와의 상호 작용을 중시 -&amp;gt; 빠르고 안전한 저장, 빠른 읽기, ACID 지원 -&amp;gt; Redis, DynamoDB, MySQLOLAP  데이터 분석을 중시 -&amp;gt; 컬럼 기반, 빠른 분석 -&amp;gt; 컬럼지향 RDBMS가 베스트일듯 -&amp;gt; BigQuery, Redshift참고  ibm: OLAP vs. OLTP: What’s the Difference?  분석을 위해 등장한 데이터베이스, OLAP 따라잡기  OLAP이란?  What Is an OLTP Database?  Youtube: Tech Dummies Narendra L, How row oriented and column oriented db works?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-09T21:01:35+09:00'>09 Jul 2022</time><a class='article__image' href='/data-engineering-series12'> <img src='/images/olap_logo.jpeg' alt='Data Engineering Series [Part12]: Purpose-built Databases(OLTP, OLAP, Cache)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series12'>Data Engineering Series [Part12]: Purpose-built Databases(OLTP, OLAP, Cache)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part13]: 힙(Heap) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/heap_2",
      "date"     : "Jul 9, 2022",
      "content"  : "Table of Contents  HeapHeapLeetcode: Heap 문제 리스트--------------------------------------------- 10문제- Kth Largest Element in a Stream- K Closest Points to Origin- Top K Frequent Elements- Kth Largest Element in an Array- Car Pooling- Longest Happy String- Furthest Building You Can Reach- Find Median from Data Stream- Sliding Window Maximum- Minimum Number of Refueling Stops",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-09T21:01:35+09:00'>09 Jul 2022</time><a class='article__image' href='/heap_2'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part13]: 힙(Heap) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/heap_2'>Coding Test Series [Part13]: 힙(Heap) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Git Series [Part3]: Git 중급",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/git_series5",
      "date"     : "Jul 8, 2022",
      "content"  : "Table of Contents  git add -p  git commit –amend  stash  cherry-pick  rebase –onto  깃은 어렵다          원격이란 존재에 브랜치까지 더해지니 엄청 헷갈림      만약 로컬에서 커밋하지 않은 작업 내용이 있는 상황에서 원격을 pull 하라는 요청이 들어온다면?        참고git add -p  파일 한 개 내에서도 라인별로 커밋할지 안할지 결정하고 싶을 때  (기존 git add는 파일별로 커밋여부를 결정했다면 -p 옵션은 더 세분화해서 라인별로 가능하다는 말)git commit –amend  마지막 커밋에 코드를 수정하거나 메시지를 변경하고 싶을 때  stashcherry-pickrebase –onto  B라는 브랜치에서 분기된 C 브랜치를 A 브랜치에 rebase하고 싶은 경우  rebase –onto “브랜치 A” “브랜치 B” “브랜치 C”깃은 어렵다원격이란 존재에 브랜치까지 더해지니 엄청 헷갈림  나의 로컬에 있는 브랜치는 다른 사람들에게 안보임  나의 로컬에 있는 브랜치를 원격 브랜치와 연결해 푸시하면 다른 사람들에게 원격에 있는 브랜치는 보임만약 로컬에서 커밋하지 않은 작업 내용이 있는 상황에서 원격을 pull 하라는 요청이 들어온다면?  작업 내용을 로컬에 저장만 하고, 커밋은 안한 상황이다. 이 때 위에서 현재 원격 코드를 pull하라는 요청이 들어왔다  이 상태에서 pull을 하면 내 로컬의 최근 커밋 + 원격의 최근 커밋간의 병합이 일어날 것이고,  내가 저장만 한 작업 내용들은? 이럴때 stash를 쓰면 되겠지?  내 작업 내용들이 사라지진 않지만 pull 중에 에러가 난다고 한다참고  Happy Git and GitHub for the useR, Pull, but you have local work",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-08T21:01:35+09:00'>08 Jul 2022</time><a class='article__image' href='/git_series5'> <img src='/images/git_logo.png' alt='Git Series [Part3]: Git 중급'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git_series5'>Git Series [Part3]: Git 중급</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Git Series [Part3]: Github",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/git_series4",
      "date"     : "Jul 8, 2022",
      "content"  : "Table of Contents  Github  Personal Access Token  원격 저장소(Remote Repository) 생성  로컬과 원격 저장소 연동을 위한 세팅  원격 저장소를 이용해 협업하기          git pull      git push      Github  깃허브는 깃을 통해 생성한 파일의 여러 버전들을 원격 저장소에 저장하도록 해주는 웹 서비스입니다.  깃허브를 이용하면 버전관리 뿐만 아니라 전세계 개발자들과 함께 코드를 공유하며 협업할 수 있게 됩니다.  깃허브의 중요한 특징 중 하나는 누구든 자신의 커밋을 깃허브에 반영(push)하기 위해서는 먼저, 깃허브의 최신 상태를 자신의 로컬 컴퓨터에 먼저 반영(pull) 해야 한다. 이 과정에서 충돌이 발생할 가능성이 높으며 충돌이 발생한 곳을 잘 병합해야 한다.Personal Access Token원격 저장소(Remote Repository) 생성로컬과 원격 저장소 연동을 위한 세팅  로컬의 깃으로 관리되는 디렉토리를 원격 저장소와 연결    git remote add origin &quot;원격 저장소 주소&quot;              origin은 원격 저장소를 지칭하는 이름. 다른 것으로 해도 되지만 origin이 암묵적 규칙        현재 로컬 브랜치를 원격의 브랜치와 연결    git push -u origin &quot;원격 저장소 브랜치&quot;# 현재 로컬에서 나의 브랜치가 main인 경우# origin의 main 브랜치와 연결하겠다git push -u origin main              이렇게 한 번 연결하고 나면 다음부터는 그냥 git push하면 됨        현재 나의 로컬은 어느 리모트 저장소와 연결되어 있나    git remote      원격 저장소를 이용해 협업하기git pullgit push",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-08T21:01:35+09:00'>08 Jul 2022</time><a class='article__image' href='/git_series4'> <img src='/images/git_logo.png' alt='Git Series [Part3]: Github'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git_series4'>Git Series [Part3]: Github</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Git Series [Part3]: Git 브랜치",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/git_series3",
      "date"     : "Jul 7, 2022",
      "content"  : "Table of Contents  Branch  Branch와 관련한 기본 명령어          브랜치 생성      브랜치 목록      브랜치 이동      브랜치 이름 변경      브랜치 삭제        Branch 만들어보기  Branch 합치기          git merge      git rebase        Branch 충돌 해결하기          git merge 도중 충돌이 생긴 경우      git rebase 도중 충돌이 생긴 경우        HEAD          git checkout      Branch브랜치는 크게 다음과 같은 2가지의 경우에 사용된다  브랜치는 기존 브랜치에서 독립적인 작업을 하고 싶을 때 -&amp;gt; 신기능 개발  하나의 메인 브랜치를 두고 여러 모습으로 코드를 제공하고 싶을 때 -&amp;gt; 배포용, 테스트용, 개발용여기서는 신기능 개발과 같은 독립적인 작업을 하고 싶은 상황을 예로 들어 실습해 볼 것이다.(그래야 다시 브랜치를 합치는 상황도 예로 만들어 해볼 수 있기 때문에)먼저 브랜치와 관련된 기본적인 명령어들을 한 번 짚고 넘어가자.Branch와 관련한 기본 명령어브랜치 생성git branch &quot;생성할 브랜치명&quot;브랜치 목록git branch브랜치 이동git switch &quot;이동할 브랜치명&quot;브랜치 이름 변경git branch -m &quot;기존 브랜치명&quot; &quot;새 브랜치명&quot;브랜치 삭제git branch -d &quot;삭제할 브랜치명&quot;# 만약 다른 브랜치에는 없고, 오직 &quot;삭제할 브랜치&quot;만 가지는 내용의 커밋이 있으면 -d 옵션으로 삭제 안됨 -&amp;gt; Forcing 느낌의 -D 옵션 필요git branch -D &quot;삭제할 브랜치명&quot;Branch 만들어보기  기존의 main 브랜치  각 yaml 파일의 상단 멤버를 coach로 승격한 add-coach 브랜치  Zebra team을 새로 추가한 new-team 브랜치Branch 합치기git merge  두 브랜치를 하나로 합치는 커밋을 새로 생성  브랜치가 많아지면 히스토리가 복잡해짐    merge는 “기준 브랜치”에서 “대상 브랜치”를 merge 함    git switch &quot;기준 브랜치&quot;git merge &quot;대상 브랜치&quot;        만약 merge한 “대상 브랜치”가 마음에 안들면 git reset –hard “이전 커밋” 명령어로 돌아가도 됨  merge를 완료했으면 “대상 브랜치”는 삭제해도 됨git rebase  하나의 브랜치를 다른 브랜치 뒤에 이어 붙이는 방법  히스토리 내역이 깔끔해짐  협업할 때 이미 공유된 커밋들은 rebase 하지 않는 것이 좋음  rebase는 나 혼자 여러 개 브랜치를 만든 상황에서 하나로 정리할 때 사용하기 좋음  rebase는 merge와 반대로 “대상 브랜치”로 이동 후, “기준 브랜치”를 rebase 함    git switch &quot;대상 브랜치&quot;git rebase &quot;기준 브랜치&quot;        rebase 하고나면 “대상 브랜치”가 “기준 브랜치” 뒤에 잘 붙음. 근데 “기준 브랜치”가 아직 뒤에 머물러 있음. 앞으로 이동해야함    # 왜 merge인지 모르겠지만, 일단 merge를 해야 &quot;기준 브랜치&quot;가 rebase된 커밋까지 이동함# 이런 merge를 Fast-forward라고 함git switch &quot;기준 브랜치&quot;git merge &quot;대상 브랜치&quot;      Branch 충돌 해결하기  두 사람이 만든 커밋이 같은 파일 같은 줄에 서로 다른 내용을 입력했을 때  두 브랜치의 커밋이 같은 파일 같은 줄에 서로 다른 내용을 입력했을 때  해결 방법: 처음부터 방지. 그럴 수 없다면 merge를 한 사람이 직접 수정해야함git merge 도중 충돌이 생긴 경우git rebase 도중 충돌이 생긴 경우위의 예시로 main 브랜치가 변경되었다. 이 부분을 먼저 reset 명령어를 이용해 다시 돌려놓고 rebase 실습을 하겠다.이제 rebase로 브랜치를 합쳐보자.HEAD  현재 속한 브랜치의 최신 커밋git checkout  HEAD를 이동시킴  보통 특정 커밋에서 새로운 브랜치를 분기하고 싶은 경우 사용  또한 로컬내의 Remote branch로 이동할 때는 switch가 아닌 checkout 사용    git checkout origin/main        git checkout을 사용하는 몇 가지 방법    git checkout &quot;이동하고 싶은 커밋 ID&quot;  # 현재 HEAD에서 뒤로 이동하고 싶은만큼 ^를 붙인다git checkout HEAD^^^# ^ 대신 물결표시(~)를 붙여도 된다. 물결표시(~)는 뒤로 이동하고 싶은 칸 수를 숫자로 적을 수 있다git checkout HEAD~~~git checkout HEAD~3        위 그림을 보면 HEAD를 이동하는 즉시 새로운 브랜치가 생기는 것을 알 수 있음  이렇게 하는 이유는 git checkout의 주요 용도인 “새로운 브랜치 분기”의 특성을 최대한 살리기 위함  나는 그냥 과거의 코드 내용을 살피기 위한 목적으로 checkout한건데 이럴 때마다 브랜치 생긴다고? 그럼 너무 부담되는데          걱정할 필요 없음 -&amp;gt; 기존에 존재하던 브랜치중 아무데로 이동하는 즉시 임시 브랜치는 사라짐        새로운 브랜치를 분기해보자",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-07T21:01:35+09:00'>07 Jul 2022</time><a class='article__image' href='/git_series3'> <img src='/images/git_logo.png' alt='Git Series [Part3]: Git 브랜치'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git_series3'>Git Series [Part3]: Git 브랜치</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part12]: 동적 프로그래밍(Dynamic Programming) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/dp_2",
      "date"     : "Jul 7, 2022",
      "content"  : "Table of Contents  Dynamic Programming  좋은 풀이Dynamic ProgrammingLeetCode, Dynamic Programming💟 ✅ ❎문제 리스트--------------------------------------------- EASY- Fibonacci Number ✅- Pascal&#39;s Triangle 💟 ✅- Best Time to Buy and Sell Stock 💟 ❎ (이 문제가 DP? Greedy Algo아닌가?)- Climbing Stairs 💟 ✅- Is Subsequence ✅- Counting Bits ✅- Min Cost Climbing Stairs 💟 ✅--------------------------------------------- MEDIUM- All Possible Full Binary Trees 💟 ❎- Minimum Cost For Tickets 💟 ❎- Best Time to Buy and Sell Stock II 💟 ✅  - Unique Paths- Longest Palindromic Subsequence- Minimum ASCII Delete Sum for Two Strings- Minimum Path Sum 💟 ✅ - Flip String to Monotone Increasing- Maximum Sum of Two Non-Overlapping Subarrays- Longest String Chain- Champagne Tower- Longest Increasing Subsequence- House Robber 💟 ✅ - Maximum Subarray 💟- Word Break- Maximal Square- Maximum Length of Subarray With Positive Product- Coin Change- Jump Game- Jump Game II- Maximum Number of Points with Cost- Maximum Product Subarray- Sum of Subarray Minimums- Longest Palindromic Substring--------------------------------------------- HARD 5문제- Dungeon Game- Count All Valid Pickup and Delivery Options- Shortest Path Visiting All Nodes- Trapping Rain Water- Longest Increasing Path in a Matrix- Maximum Profit in Job Scheduling좋은 풀이  Best Time to Buy and Sell Stock II",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-07T21:01:35+09:00'>07 Jul 2022</time><a class='article__image' href='/dp_2'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part12]: 동적 프로그래밍(Dynamic Programming) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dp_2'>Coding Test Series [Part12]: 동적 프로그래밍(Dynamic Programming) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Git Series [Part2]: Git 되돌리기",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/git_series2",
      "date"     : "Jul 6, 2022",
      "content"  : "Table of Contents  시작하기 전에  git restore  git reset  git revert시작하기 전에시작하기 전에 한 가지 커밋을 추가하도록 하자.현재 yalco 디렉토리의 commit 이력은 다음과 같다.git restore  Working direcotry 또는 Staging Area에 기록된 변경 사항을 되돌리는 방법git reset  특정 커밋으로 돌아간다  soft, mixed, hard 옵션이 있음  git reset –soft “원하는 시점의 커밋ID”          돌아온 시점 이후의 변경사항들이 사라지지 않고 Staging Area에 올라와 있음      당연히 Working directory에도 반영되어 있음      완전히 다 삭제하고 싶으면 여기서 git reset –hard를 하거나 git restore를 쓰면 됨      가장 안전한 방법        git reset –mixed “원하는 시점의 커밋 ID”          돌아온 시점 이후의 변경사항들이 사라지지 않고 Working directory에 올라와 있음        git reset –hard “원하는 시점의 커밋 ID”          3가지 영역에서의 상태가 모두 돌아온 시점으로 동기화 되어 있음      가장 과감한 방법      git revert  특정 커밋에서 이루어진 작업을 취소하는 커밋을 새로 생성  만약 특정 커밋에서 이루어진 작업을 취소까지는 하는데 일단 Staging Area까지만 완성해놓고 커밋은 미루어 두고 싶다면          git revert –no-commit “되돌릴 커밋 ID”      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-06T21:01:35+09:00'>06 Jul 2022</time><a class='article__image' href='/git_series2'> <img src='/images/git_logo.png' alt='Git Series [Part2]: Git 되돌리기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git_series2'>Git Series [Part2]: Git 되돌리기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part11]: 그래프(Graph) - 문제",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/graph_5",
      "date"     : "Jul 6, 2022",
      "content"  : "Table of Contents  DFS, BFS  MST  Graph  BFS + GraphDFS, BFSLeetcode, DFS Leetcode, BFS💟 ✅ ❎문제 리스트--------------------------------------------- EASY- Range Sum of BST 💟 ✅ - Increasing Order Search Tree 💟 ❎- Flood Fill (+BFS) 💟 ✅(DFS)- Diameter of Binary Tree 💟 ❎ ✅ - Symmetric Tree (+BFS) 💟 ❎- Find if Path Exists in Graph (+BFS) 💟 ❎ ✅(DFS)- Path Sum (+BFS) 💟 ✅(DFS) - Closest Binary Search Tree Value 💟 ✅- Merge Two Binary Trees 💟 ❎- Island Perimeter ✅- Lowest Common Ancestor of a Binary Search Tree- Second Minimum Node In a Binary Tree 💟 ❎- Subtree of Another Tree- Cousins in Binary Tree--------------------------------------------- MEDIUM- Deepest Leaves Sum (+BFS) 💟 ✅(DFS)- All Paths From Source to Target (+BFS) 💟 ✅(DFS)- All Elements in Two Binary Search Trees 💟 ❎- Find Leaves of Binary Tree 💟 ❎- Distribute Coins in Binary Tree 💟 ❎- Max Area of Island (+BFS) 💟 ❎- Trim a Binary Search Tree 💟 ❎- Smallest Common Region (+BFS) 💟 ❎- All Nodes Distance K in Binary Tree (+BFS) 💟 ❎- Number of Distinct Islands (+BFS)- Evaluate Division (+BFS)- Lowest Common Ancestor of a Binary Tree (+BFS)- Accounts Merge (+BFS)- Number of Islands (+BFS)- Shortest Bridge (+BFS)- Course Schedule (+BFS)- Course Schedule II (+BFS)  source에서 출발한 모든 경로는 destination으로 끝나는가?Leetcode: All Paths from Source Lead to DestinationMST💟 ✅ ❎문제 리스트--------------------------------------------- MEDIUM- Min Cost to Connect All Points- Optimize Water Distribution in a VillageGraph💟 ✅ ❎문제 리스트--------------------------------------------- EASY- Find the Town Judge ❎--------------------------------------------- MEDIUM- Network Delay Time- Minimum Number of Vertices to Reach All Nodes- Parallel Courses- Maximal Network Rank- Find the City With the Smallest Number of Neighbors at a Threshold Distance- Find All Possible Recipes from Given Supplies- Find the CelebrityBFS + Graph💟 ✅ ❎- Number of Islands 💟 ✅ (BFS)- Max Area of Island 💟 ✅ (BFS)- Longest Increasing Path in a Matrix 💟 ❎- Making A Large Island- Swim in Rising Water- Shortest Bridge- Pacific Atlantic Water Flow- Path With Minimum Effort",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-06T21:01:35+09:00'>06 Jul 2022</time><a class='article__image' href='/graph_5'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part11]: 그래프(Graph) - 문제'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/graph_5'>Coding Test Series [Part11]: 그래프(Graph) - 문제</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Git Series [Part1]: Git 시작하기",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/git_series1",
      "date"     : "Jul 5, 2022",
      "content"  : "Table of Contents  Git 소개          Github는        Git 초기 설정하기  Git 시작하기          git init      git status      git add                  Git의 3가지 작업 영역                    git commit      git log                  Git Graph Extention을 이용하면 Git log를 시각화 할 수 있다                    .gitignore        git 명령어 정리          하나의 브랜치에서 사용하는 명령어      여러 브랜치 &amp;amp; 협업시 사용하는 명령어        다음 포스트에서는  참고Git 소개깃은 리눅스(Linux)의 아버지 리누스 토발즈(Linus Torvalds)라는 사람이 만들었습니다. 리누스 토발즈는 리눅스를 만든 이후에 리눅스의 버전을 관리하기 위해 BitKeeper라고 하는 툴을 이용하고 있었는데요. 그러다 리눅스 개발자 중 한 명이 BitKeeper의 내부 동작원리를 분석하려고 했다가 리눅스 커뮤니티와 BitKeeper 사이가 안좋아지게 되었습니다. 이 때문에 BitKeeper는 리눅스에게 서비스를 유료화 시켰고 리누스 토발즈는 결국 버전 관리 프로그램을 직접 만들어버렸습니다. 이렇게 만들어진 것이 바로 깃입니다.Github는  깃 허브는 깃을 통해 생성한 파일의 여러 버전들을 원격 저장소에 저장하도록 해주는 웹 서비스입니다.  깃 허브를 이용하면 버전관리 뿐만 아니라 전세계 개발자들과 함께 코드를 공유하며 협업할 수 있게 됩니다.Git 초기 설정하기# 사용자 정보 설정git config --global user.name &quot;(본인 이름)&quot;git config --global user.email &quot;(본인 이메일)&quot;# 설정값 확인하기git config --global user.namegit config --global user.email# defaultBranch명 main으로 설정하기git config --global init.defaultBranch mainGit 시작하기git init# Git으로 관리하고 싶은 폴더git init.git이라는 숨겨진 폴더가 생성되고 Git이 관리하는 모든 버전들이 이 폴더 안에 저장되게 된다.yalco라는 폴더를 Git으로 버전관리 하려고 한다. git init 명령어를 치고나면 Git repository가 .git 폴더에서 초기화 되었다고 말해준다..git 폴더의 역할aaa우선 간단하게 yaml 파일 2개를 만들어보았다.git statusGit 상태를 확인해 보자.git status보면 Untracked files에 방금 생성한 두 파일이 보인다. Untracked라는 말은 Git이 한 번도 관리해본 적 없는 파일이라는 말이다. 위의 두 yaml 파일을 이제 Git에게 맡겨보도록 하자.git add  git add는 Working directory(로컬 디렉토리)의 변경사항을 Staging Area(스냅샷을 찍기위한 무대)로 올림  나는 이러이러한 변경사항을 무대로 올리고(add), 스냅샷으로 찍어서 저장(commit)해둘거야  참고로 매번 git add 명령어의 대상이된 파일만 커밋되는 것은 아님 (tracked된 파일들은 git add 안하더라도 가장 최근에 커밋된 상태로 계속 커밋의 대상이됨. 이 내용은 아래의 카카오 프렌즈를 이용한 그림 참고) (이를 스냅샷 방식이라고 함 &amp;lt;-&amp;gt; 델타 방식과 대비)# 특정 파일만 맡기려는 경우git add &quot;파일명&quot;# 생성/삭제/변경된 모든 파일을 맡기려는 경우git add .new file 이라는 표시가 뜬다. Git 입장에서 이 두 파일은 새로 생성되었기 때문이다.이 상태에서 lions.yaml 파일의 manager를 Conan으로 변경해보자.그리고 다시 Git 상태를 보면Changes not staged for commit 라는 항목에 lions.yaml이 modified 되었다고 뜬다.변경되었지만 아직 Staging Area에 올라가지 않은 경우이다.Untracked files: 한 번도 Git이 관리한 적 없는 파일들Changes to be committed: git add한 파일들Changes not staged for commit: 수정하고 저장만 된 파일들Git의 3가지 작업 영역git add 되지 않은 파일들은 not staged 라고 한다. Git은 이렇게 어떤 파일이 최종적으로 하나의 버전으로 남기까지 3가지 영역을 오간다. 그림으로 보자.아래 그림은 명령어를 통해 파일들이 오고가는 영역을 나타낸 것이다. Github를 포함하면 remote repository라는 영역도 추가된다.다시 돌아가 보면 지금 우리의 파일 상태는 다음과 같았다.여기서 이제 커밋을 해보자.git commit커멋이 되었다고 한다. 커밋이 되면 Local Repository에 하나의 버전으로 남게된다.Local Repository는 git log 명령으로 확인할 수 있다.git log‘Create two files’라는 메시지로 잘 커밋이 되었다. 커밋된 버전은 f0c374e…이라는 식별자를 가지게 된다. 이 식별자를 이용해 나중에 버전을 되돌리거나 하는 등의 작업을 할 수 있다.Git Graph Extention을 이용하면 Git log를 시각화 할 수 있다위의 Uncommitted Changes는 무엇인지 보자.Conan으로 변경하고 커밋에는 포함하지 않았었음 -&amp;gt; 가장 최근 커밋된 모습(현재 Staging Area의 모습)과의 diff를 보여준다이 변경사항도 하나의 버전으로 만들어 보자..gitignore비밀번호와 같은 민감한 정보는 Git에 의해 관리되지 않는 것이 낫다.나중에 Github(Remote Repository)에 커밋을 저장(push)하면 다른 사람들에게 내 비밀번호가 공개될 수 있다.이러한 경우 .gitignore라는 파일을 이용해 관리하고 싶지 않은 파일들을 표기해주면 된다.git 명령어 정리하나의 브랜치에서 사용하는 명령어여러 브랜치 &amp;amp; 협업시 사용하는 명령어다음 포스트에서는지금까지는 계속 새로운 버전을 만들면서 앞으로 가는 방법만 배웠다.다음 포스트에서는 과거로 돌아가거나(reset), 과거의 특정 버전에서 했던 행동을 돌이키는 방법(revert)을 알아보자.참고  Sébastien Dubois, Git concepts for newcomers — Part 2: Git repository, working tree and staging area  Git committing unchanged files",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-05T21:01:35+09:00'>05 Jul 2022</time><a class='article__image' href='/git_series1'> <img src='/images/git_logo.png' alt='Git Series [Part1]: Git 시작하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/git_series1'>Git Series [Part1]: Git 시작하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part10]: 힙(Heap)",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/heap",
      "date"     : "Jul 4, 2022",
      "content"  : "Table of Contents  Heap          우선순위 큐      힙        Heap 관련 연산          힙에 삽입이 발생한 경우      힙에 삭제가 발생한 경우        Heap 구현          파이썬의 heapq 모듈      파이썬 리스트를 이용해 직접 구현        Heap 활용Heap  In many CS applications, we only need to access the largest or smallest element in the dataset. how do we efficiently access the largest or smallest element in the current dataset? The answer would be heap!우선순위 큐  힙과 우선순위 큐는 다른 의미  힙은 데이터 구조(low-level), 우선순위 큐는 추상 자료형(high-level)  힙은 우선순위 큐를 구현하는 한 가지 방법  힙 말고도 배열, 링크드 리스트로 우선순위 큐를 구현할 수 있음(물론 시간 복잡도 차이 있음)힙  힙은 완전 이진 트리의 특별한 케이스          완전 이진 트리: 이진 트리 + 리프 노드가 왼쪽부터 채워짐        각 노드들은 자식 노드보다 크다 (최대힙) (최소힙은 반대)Heap 관련 연산  삽입 연산: O(logN)  삭제 연산: O(logN)  최대값/최소값 얻기: O(1)  element가 힙에서 삽입/삭제되어도 계속 힙 성질이 유지되어야함          heapify 연산으로 힙 성질을 유지함      한 번 삽입/삭제 연산이 일어날 때마다 heapify 실행      heapify: 부모 노드와 자식 노드의 값을 계속 비교하며 스왑      힙에 삽입이 발생한 경우  삽입된 노드는 리프 노드에 추가 -&amp;gt; 추가된 리프 노드에서 부터 heapify힙에 삭제가 발생한 경우  삭제는 항상 루트 노드에서 발생 -&amp;gt; 루트 노드의 빈자리는 리프 노드가 채움 -&amp;gt; 루트 노드에서 부터 heapifyHeap 구현파이썬의 heapq 모듈  힙을 파이썬에서 사용할 때는 간단히 heapq 모듈을 이용해 사용 가능    import heapqheap_list = []# Insertheapq.heappush(heap_list, 5)assert heap_list == [5]heapq.heappush(heap_list, 1)assert heap_list == [1, 5]# Getheap_list[0]# Deleteheapq.heappop(heap_list)# List를 힙으로 만들 떄tmp_list = [5, 4, 6, 1, 0, 10]heapq.heapify(tmp_list)assert tmp_list == [0, 1, 6, 5, 4, 10]      파이썬 리스트를 이용해 직접 구현  힙을 직접 구현할 때는 리스트를 사용함          완전 이진 트리의 성질을 가지도록 하기 위해 부모 노드와 자식 노드의 관계를 정의해야함        root node: 1parent node: nleft child node: 2*nright child node: 2*n + 1                      heapify      class MinHeap:      def __init__(self, heapSize):          self.heapSize = heapSize          self.minheap = [0] * (heapSize + 1)          self.realSize = 0      def add(self, element):          self.realSize += 1          if self.realSize &amp;gt; self.heapSize:              print(&quot;Added too many elements!&quot;)              self.realSize -= 1              return          self.minheap[self.realSize] = element          index = self.realSize          parent = index // 2          while (self.minheap[index] &amp;lt; self.minheap[parent] and index &amp;gt; 1):              self.minheap[parent], self.minheap[index] = self.minheap[index], self.minheap[parent]              index = parent              parent = index // 2              def peek(self):          return self.minheap[1]              def pop(self):          if self.realSize &amp;lt; 1:              print(&quot;Don&#39;t have any element!&quot;)              return sys.maxsize          else:              removeElement = self.minheap[1]              self.minheap[1] = self.minheap[self.realSize]              self.realSize -= 1              index = 1              while (index &amp;lt;= self.realSize // 2):                  left = index * 2                  right = (index * 2) + 1                  if (self.minheap[index] &amp;gt; self.minheap[left] or self.minheap[index] &amp;gt; self.minheap[right]):                      if self.minheap[left] &amp;lt; self.minheap[right]:                          self.minheap[left], self.minheap[index] = self.minheap[index], self.minheap[left]                          index = left                      else:                          self.minheap[right], self.minheap[index] = self.minheap[index], self.minheap[right]                          index = right                  else:                      break              return removeElement              # return the number of elements in the Heap      def size(self):          return self.realSize              def __str__(self):          return str(self.minheap[1 : self.realSize + 1])              if __name__ == &quot;__main__&quot;:          # Test cases          minHeap = MinHeap(5)          minHeap.add(3)          minHeap.add(1)          minHeap.add(2)          # [1,3,2]          print(minHeap)          # 1          print(minHeap.peek())          # 1          print(minHeap.pop())          # 2          print(minHeap.pop())          # 3          print(minHeap.pop())          minHeap.add(4)          minHeap.add(5)          # [4,5]          print(minHeap)      Heap 활용      힙 정렬        Top-K 문제        K-th 요소  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-04T21:01:35+09:00'>04 Jul 2022</time><a class='article__image' href='/heap'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part10]: 힙(Heap)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/heap'>Coding Test Series [Part10]: 힙(Heap)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part9]: 동적 프로그래밍(Dynamic Programming)",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/dp",
      "date"     : "Jul 4, 2022",
      "content"  : "Table of Contents  Dynamic Programming          언제 적용 가능한가      적용 가능한 문제의 대표적인 특징      문제 예시      DP를 쓰는 이유        DP 알고리즘 구현          Bottom-Up      Top-Down        DP 문제를 풀기위해 필요한 구성요소 (Top-Down 기준)  DP, Greedy Algorithm, Divide and ConquerDynamic Programming  Explore all possible solution efficiently, systematically  모든 가능한 경우를 다 확인해야 하는 경우에 효율적이고 정형화된 코드를 작성하도록 하는 알고리즘언제 적용 가능한가  하나의 문제를 여러 개의 작은 문제로 나눌 수 있는 경우          작은 문제: overlapping subproblems -&amp;gt; reused multiple times        작은 문제의 해를 통해 원래 문제의 해를 만들 수 있는 경우F(n-1) + F(n-2)   =   F(n)  subproblems  original problem즉, 중복되는 부분 문제의 최적해를 통해 원래 문제의 최적 해를 얻을 수 있는 경우하지만 주어진 문제가 이런 조건에 맞는지 알기 어렵다. 반대로, DP로 풀리는 문제가 가지는 대표적인 특징을 알아보자.적용 가능한 문제의 대표적인 특징  최대값, 최소값을 묻는 경우 (최소비용, 최대이윤, 가장 긴 거리)  ~할 수 있는 방법의 수를 묻는 경우 (A에서 B로 갈 수 있는 방법의 수)  위의 2가지 특징은 Greedy 알고리즘 문제에서도 나타나는 특징  Greedy 알고리즘과 구분되는 DP 문제만의 특징: 미래의 결정이 이전의 결정에 의존          Greedy 알고리즒은 매 시점에서 최대의 이익을 얻는 방법      문제 예시House Robber: 도둑이 집을 털 때, 훔칠 수 있는 돈의 최대 금액문제가 여기서 끝이라면 Greedy 알고리즘을 써도 된다.하지만 다음과 같은 제약사항이 있다고 해보자: adjacent하게 집을 털 수는 없다각 집들이 가지고 있는 돈 = [2, 7, 9, 3, 1]adjacent하게 집을 털 수 없기 때문에, 2를 선택할 경우: 2 -&amp;gt; 9 -&amp;gt; 17을 선택할 경우: 7 -&amp;gt; 3과거에 2를 선택했는지, 7을 선택했는지에 따라 다음 내가 선택할 수 있는 결정이 달라진다 -&amp;gt; DP로 접근해야함DP를 쓰는 이유  DP는 복잡한 문제를 다루기 쉬운 하위 문제로 나눔  중복되는 하위 문제의 불필요한 재계산을 피함 (하위 문제의 결과를 재사용) -&amp;gt; 시간 복잡도 낮춤DP 알고리즘 구현Bottom-Up  Tabulation 이라고도 함  Iteration + Array 을 사용  Start at the base case (ex. F(0) = 0, F(1) = 1)  일반적으로 Top-Down 보다 빠름  부분 문제들 간의 논리적 순서를 따라야함Top-Down  Memoization 이라고도 함  Recursion + Hash Table 을 사용  Continue recursively until we reach the base case  불필요한 계산을 피하기 위해 결과를 Memoization  Bottom-Up보다 코드가 간결  부분 문제들 간의 순서는 상관없음DP 문제를 풀기위해 필요한 구성요소 (Top-Down 기준)  매 주어진 상태에 대해 정답을 리턴하는 함수 또는 자료구조: dp[n]  점화식: dp[i] = dp[i-1] dp[i-2]  멈춤의 조건이 되는 베이스케이스: dp[0] = 0  캐싱: Caching results from function calls and then refering to those results in the future  Bottom-Up은 베이스 케이스에서 시작해서 최종해에 도달할 때 까지 반복DP, Greedy Algorithm, Divide and Conquer  부분 문제의 최적해를 통해 원래 문제의 최적해를 찾는 방식의 알고리즘  Greedy Algorithm과 Divide and Conquer는 부분 문제가 not overlapping  Divide and Conquer는 not overlapping하기 때문에 병렬적으로 시행 가능",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-04T21:01:35+09:00'>04 Jul 2022</time><a class='article__image' href='/dp'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part9]: 동적 프로그래밍(Dynamic Programming)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dp'>Coding Test Series [Part9]: 동적 프로그래밍(Dynamic Programming)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part9]: 그래프(Graph) - Dijkstra, Kruskal",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/graph_4",
      "date"     : "Jul 3, 2022",
      "content"  : "Table of Contents  Dijkstra  KruskalDijkstraKruskal  최소 신장 트리를 찾는 알고리즘  알고리즘 실행 순서          모든 엣지(edge)를 가중치를 기준으로 오름차순 정렬한다      정렬된 엣지를 하나씩 가져와 연결하고, cyclic한지 체크. acyclic한 경우만 선택 -&amp;gt; cyclic한지는 Union Find 이용      N-1개의 엣지를 선택할때 까지 2를 반복      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-03T21:01:35+09:00'>03 Jul 2022</time><a class='article__image' href='/graph_4'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part9]: 그래프(Graph) - Dijkstra, Kruskal'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/graph_4'>Coding Test Series [Part9]: 그래프(Graph) - Dijkstra, Kruskal</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part8]: 그래프(Graph) - Union-Find",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/graph_3",
      "date"     : "Jul 3, 2022",
      "content"  : "Table of Contents  Union-Find          Key points of Union-Find      Use Case of Union-Find        Implementation of Union-Find          Quick Find      Quick Union      Union by Rank      Path Compression      Union-Find  The primary use of Union-Find is to address the connectivity between the components of a network.Key points of Union-Find  find: function finds the root node of a given vertex.  union: function unions two vertices and makes their root nodes the same.Use Case of Union-Find  Determine Connection  Detect CycleImplementation of Union-FindQuick Find  self.root의 인덱스는 자기 자신, 값은 루트 노드를 가리킴  self.root의 값 자체가 인덱스 노드의 루트 노드이기 때문에 find는 O(1)  x와 y를 union하기 위해 x와 같은 루트 노드를 가지는 모든 노드의 루트 노드를 y의 루트 노드로 변경 -&amp;gt; 항상 O(N)  find는 O(1). union은 O(N)# Quick Findclass UnionFind:    def __init__(self, size):        self.root = [i for i in range(size)]        def find(self, x):        return self.root[x]        def union(self, x, y):        if self.root[x] != self.root[y]:            root_x = self.find(x)            root_y = self.find(y)            for i in range(len(self.root)):                if self.root[i] == root_x:                    self.root[i] = root_y        def connected(self, x, y):        if self.find(x) == self.find(y):            return True        return Falseuf = UnionFind(9)edges = [(0, 1), (0, 2), (1, 3), (4, 8), (5, 6), (5, 7)]for edge in edges:    uf.union(*edge)print(uf.connected(0, 3))print(uf.connected(1, 5))print(uf.connected(7, 8))Quick Union  self.parent의 인덱스는 자기 자신, 값은 부모 노드를 가리킴  self.parent의 배열에서 계속 부모 노드를 타고 올라가면서 루트 노드가 나올 때 까지 반복 -&amp;gt; find는 O(N)  x의 루트 노드의 부모 노드를 y의 루트 노드로 변경 -&amp;gt; union도 O(N)  find와 union 모두 O(N)이기 때문에 Quick Union이 시간 측면에서 성능이 낮아보이지만 union은 최악의 경우만 O(N)이지 Quick Find처럼 항상 O(N)은 아니다. 두 메소드 모두 정확히는 루트 노드를 찾는 과정만 필요로 하므로 정확히는 O(H)이다.  union, find -&amp;gt; O(H). (H는 해당 노드에서 루트 노드까지의 높이)# Quick Unionclass UnionFind:    def __init__(self, size):        self.parent = [i for i in range(size)]        def find(self, x):        while x != self.parent[x]:            x = self.parent[x]        return x        def union(self, x, y):        if self.parent[x] != self.parent[y]:            root_x = self.find(x)            root_y = self.find(y)            self.parent[root_x] = root_y        def connected(self, x, y):        if self.find(x) == self.find(y):            return True        return FalseUnion by Rank  rank: 각 노드의 높이  union하고자 하는 두 트리의 루트 노드의 rank를 비교  rank가 높은 트리의 루트 노드가 union된 결과의 루트 노드가 되도록 하자 -&amp;gt; 최종 결과 트리의 높이가 유지될 수 있음  두 트리의 높이가 같으면 임의로 하나를 기준 루트 노드로 잡는다 -&amp;gt; 하지만 rank가 1증가 할 수 밖에 없음  Quick-Union의 union 메서드를 개선한 방법  Quick Union에서 O(H)가 O(N)이 아닌 O(logN)으로 되도록 함  union된 결과가 일자로 길어지는 상황을 피할 수 있음# Union by rankclass UnionFind:    def __init__(self, size):        self.parent = [i for i in range(size)]        self.rank = [1] * size        def find(self, x):        while x != self.parent[x]:            x = self.parent[x]        return x        def union(self, x, y):        if self.parent[x] != self.parent[y]:            root_x = self.find(x)            root_y = self.find(y)            if self.rank[root_x] &amp;gt; self.rank[root_y]:                self.parent[root_y] = root_x            elif self.rank[root_x] &amp;lt; self.rank[root_y]:                self.parent[root_x] = root_y            else:                self.parent[root_x] = root_y                self.rank[root_y] += 1                    def connected(self, x, y):        if self.find(x) == self.find(y):            return True        return FalsePath Compression  기존 Quick Union에서 find는 루트 노드를 찾기 위해 parent를 계속 traverse함  If we search the root node of the same element again, we repeat the same operations. Is there any way to optimize this process?  After finding the root node, we can update the parent node of all traversed elements to their root node. When we search for the root node of the same element again, we only need to traverse two elements to find its root node, which is highly efficient. So, how could we efficiently update the parent nodes of all traversed elements to the root node? The answer is to use “recursion”. This optimization is called “path compression”, which optimizes the find function.  대상 노드의 루트 노드를 찾는 과정에서 거쳐가는 모든 노드의 루트 노드를 찾아 저장한다# Path compressionclass UnionFind:    def __init__(self, size):        self.parent = [i for i in range(size)]        self.rank = [1] * size        # Path Compression    def find(self, x):        if x == self.parent[x]:            return x        self.parent[x] = self.find(self.parent[x])        return self.parent[x]        def union(self, x, y):        if self.parent[x] != self.parent[y]:            root_x = self.find(x)            root_y = self.find(y)            if self.rank[root_x] &amp;gt; self.rank[root_y]:                self.parent[root_y] = root_x            elif self.rank[root_x] &amp;lt; self.rank[root_y]:                self.parent[root_x] = root_y            else:                self.parent[root_x] = root_y                self.rank[root_y] += 1                    def connected(self, x, y):        if self.find(x) == self.find(y):            return True        return False",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-03T21:01:35+09:00'>03 Jul 2022</time><a class='article__image' href='/graph_3'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part8]: 그래프(Graph) - Union-Find'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/graph_3'>Coding Test Series [Part8]: 그래프(Graph) - Union-Find</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part7]: 그래프(Graph) - DFS, BFS",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/graph_2",
      "date"     : "Jul 3, 2022",
      "content"  : "Table of Contents  DFS          Traversing all Vertices      Traversing all paths between two vertices        BFS          Traversing all Vertices      Shortest Path Between Two Vertices        BacktrackingDFSGiven a graph, how can we find all of its vertices, and how can we find all paths between two vertices?The depth-first search algorithm is ideal in solving these kinds of problems because it can explore all paths from the start vertex to all other verticesIn Graph theory, the depth-first search algorithm (abbreviated as DFS) is mainly used to:  Traverse all vertices in a “graph”  Traverse all paths between any two vertices in a “graph”.Traversing all VerticesTraversing all paths between two verticesBFSthe “breadth-first search” algorithm can traverse all vertices of a “graph” and traverse all paths between two vertices. However, the most advantageous use case of “breadth-first search” is to efficiently find the shortest path between two vertices in a “graph” where all edges have equal and positive weights.Although the “depth-first search” algorithm can find the shortest path between two vertices in a “graph” with equal and positive weights, it must traverse all paths between two vertices before finding the shortest one. The “breadth-first search” algorithm, in most cases, can find the shortest path without traversing all paths. This is because when using “breadth-first search”, as soon as a path between the source vertex and target vertex is found, it is guaranteed to be the shortest path between the two nodes.In Graph theory, the primary use cases of the “breadth-first search” (“BFS”) algorithm are:  Traversing all vertices in the “graph”  Finding the shortest path between two vertices in a graph where all edges have equal and positive weights.Traversing all VerticesShortest Path Between Two VerticesBacktracking",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-03T21:01:35+09:00'>03 Jul 2022</time><a class='article__image' href='/graph_2'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part7]: 그래프(Graph) - DFS, BFS'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/graph_2'>Coding Test Series [Part7]: 그래프(Graph) - DFS, BFS</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part6]: 그래프(Graph) - Intro to Graph",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/graph_1",
      "date"     : "Jul 3, 2022",
      "content"  : "Table of Contents  Graph          그래프의 정의      트리와 그래프의 차이                  트리          그래프          신장 트리(Spanning Tree)                      Graph와 관련한 문제 유형          그래프 탐색(모든 정점 방문)      그래프 최단 거리(두 정점간 가능한 모든 경로 찾기)      모든 정점에서 다른 모든 정점까지의 최단 경로      그래프 분석      최소 신장 트리        참고Graph그래프의 정의  노드(Node)와 노드를 연결하는 간선(Edge)으로 구성된 자료구조  노드간의 관계를 표현하는데 사용되는 자료구조트리와 그래프의 차이트리  트리는 그래프의 특별한 케이스  그래프의 정의 + 몇 가지 조건          그래프의 정의: 노드(Node)와 노드를 연결하는 간선(Edge)으로 구성된 자료구조      몇 가지 조건                  노드 사이를 연결하는 간선은 모두 1개씩만 존재한다 (노드의 수가 V개 이면, 간선의 수는 V-1개)          루트 노드가 있다 (부모-자식이라는 계층적 관계가 있다, 부모 노드에서 자식 노드라는 방향이 있다)          비순환                      위의 내용을 종합해 트리를 DAG(Directed Acyclic Graph)이라고도 한다  트리의 종류: 이진트리, 이진탐색트리, 힙트리, B-트리 등  트리 예시: 회사 조직도그래프  트리에는 없는 그래프만의 특징          순환/비순환으로 구분된다      방향/무방향으로 구분된다      간선에 가중치가 있는 경우도 있다      노드를 잇는 간선이 2개 이상인 경우도 있다        그래프 예시: SNS, 지하철 노선, 지도  순환/비순환 + 방향/무방향 + 가중치/비가중치 그래프신장 트리(Spanning Tree)  신장 트리는 그냥 그래프에서 유래된 ‘트리’  그래프가 주어졌을 떄 V-1개의 간선만 남기고 다른 간선들은 제거한다  남은 V-1개의 간선으로 이루어진 그래프가 비순환하다면 트리가 된다 -&amp;gt; 이렇게 만들어진 트리를 ‘신장 트리’라고 한다  1개의 그래프에 1개 이상의 신장 트리가 만들어진다  가중치 그래프인 경우 남은 간선들의 가중치 합을 최소로 하는 신장 트리를 ‘최소 신장 트리’라고 한다  신장 트리의 예시: 통신 네트워크 구축Graph와 관련한 문제 유형그래프 탐색(모든 정점 방문)  그래프에서 사용하는 탐색 알고리즘          DFS(Depth First Search)                  재귀함수 또는 반복문으로 구현          연관 키워드: Preorder, Top-Down                    BFS(Breadth First Search)                  반복문으로 구현                      트리에서 사용하는 탐색 알고리즘          Preorder      Inorder      Postorder      3가지 모두 재귀함수, 반복문으로 구현      재귀함수로 구현할 때 Top-Down과 Bottom-Up approach 있음      그래프 최단 거리(두 정점간 가능한 모든 경로 찾기)  최단 경로에서 사용하는 알고리즘          그래프 탐색 알고리즘와 마찬가지로 DFS, BFS      시간 복잡도 낮추기 위해 백트래킹(Backtracking) 추가 가능                  (특별한 것 아니고 그냥 가지치기)          (중간에 조건을 만족하지 않게 되면 탐색 중단)                    가중치 있는 그래프의 최단 경로 알고리즘                  Dijkstra 알고리즘          음의 가중치 가지는 경우: Bellman-Ford 알고리즘                    모든 정점에서 다른 모든 정점까지의 최단 경로  모든 정점들간의 경로중 최단 경로 찾는 알고리즘          Floyd-Warshall 알고리즘 (DP와 연관)      그래프 분석  Disjoint Set (or Union-FInd)          Union-Find라고도 불리는 이유는 Disjoint Set 자료구조가 가지는 가장 중요한 메소드가 union과 find      용도                  두 노드가 서로 연결되어 있는가          Detect Cycle: 그래프가 순환 그래프인가                          현재 선택된 엣지로 연결된 두 노드가, 하나의 집합이라면(서로 연결되어 있다면)              이 엣지를 선택하게 되면 그래프는 순환 그래프가 된다                                          구현 방법                  Quick Union          Quick Find                    최소 신장 트리  최소 신장 트리에서 사용하는 알고리즘          Kruskal 알고리즘 (Union-Find와 관련)      참고  HyeonGyu, 그래프와 트리의 정의 및 차이점  GeeksforGeeks, Disjoint Set",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-03T21:01:35+09:00'>03 Jul 2022</time><a class='article__image' href='/graph_1'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part6]: 그래프(Graph) - Intro to Graph'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/graph_1'>Coding Test Series [Part6]: 그래프(Graph) - Intro to Graph</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part5]: 이진 트리(Binary Tree) - Binary Search Tree",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/binary_tree_2",
      "date"     : "Jul 2, 2022",
      "content"  : "Table of Contents  Binary Tree  Binary Search  Binary Search TreeBinary TreeBinary SearchBinary Search Tree",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-02T21:01:35+09:00'>02 Jul 2022</time><a class='article__image' href='/binary_tree_2'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part5]: 이진 트리(Binary Tree) - Binary Search Tree'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/binary_tree_2'>Coding Test Series [Part5]: 이진 트리(Binary Tree) - Binary Search Tree</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part5]: Flink 설치",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series6",
      "date"     : "Jul 1, 2022",
      "content"  : "Table of Contents  Flink 설치  pyflink 설치Flink 설치실행환경: M1 맥북, 도커-컴포즈처음에는 도커 허브에서 Flink가 공식적으로 제공해주는 이미지를 이용했다.근데 자꾸 LD_PRELOAD cannot be preloaded에러가 터진다.unset LD_PRELOAD 해보면 된다고 하길래docker-compose.yml에서 command: sh -c unset LD_PRELOAD도 해보고, Dockerfile에서 ENV LD_PRELOAD= 도 해봤는데 안됨… 도커만 있으면 뭐든 편할 줄 알았는데 흑..어쨋든 결국 플링크를 직접 설치하기로 함. 설치는 간단함. 근데 문제는..플링크는 간단하게 설치할 수 있는데, pyflink를 설치하기 위한 pip install apache-flink에서 계속 에러가 터짐구글링 해본 결과로는 m1이 문제인 것 같다.(무슨 이유인지 Flink 1.12 버전만 고집하긴 했다. 어떤 글에서 m1을 지원하는 버전은 아직 1.12 정도 뿐이라는 걸 읽은 것 같아서.. 1.12를 위해 파이썬 버전도 3.8은 안된다길래 3.7, 3.6 다 해보았지만 같은 에러)(Flink 1.14 버전도 해보자..해봤는데 안됨.. 같은 에러)어쨋든 pyflink를 고려하지 않고, Flink를 설치하는 법이라도 기록해두자  python 설치나는 python3.7-buster 라는 이미지를 컨테이너로 띄웠다docker run -it python3.7-buster /bin/bash  java 설치java는 8, 11 원하는걸 쓰면 된다길래 8로 설치했다.처음에는 wget 명령어로 깔아봤는데 로컬로 받았을 때와 디렉터리 구조가 달라서 뭔가 찝찝해서 로컬로 다운 받았다. (실제로 apache-flink 설치할 때 java안에 include라는 폴더가 없다고 에러가 터졌었음)https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html 접속본인 환경에 맞는 파일 다운, 나의 경우 jdk-8u311-linux-aarch64.tar.gz 설치함로컬에 다운받은 파일 도커 컨테이너로 복사docker cp [로컬파일경로] [컨테이너ID]:[원하는 컨테이너경로]압축 풀고 환경변수 설정tar -xzvf jdk-8u311-linux-aarch64.tar.gzexport JAVA_HOME=/jdk1.8.0_311export PATH=$JAVA_HOME/bin:$PATH. /etc/profile  플링크 설치https://flink.apache.org/downloads.html 에서 원하는 버전 로컬에 다운마찬가지로 플링크 파일 도커 컨테이너로 복사docker cp [로컬파일경로] [컨테이너ID]:[원하는 컨테이너경로]컨테이너에 접속해서 해당 파일 압축 풀기tar -xzf flink-[플링크 버전]-bin-scala_[스칼라 버전].tgz끝!  실행클러스터 실행: ./bin/start-cluster.sh파이썬 예제 실행: ./bin/flink run --python ./examples/python/table/batch/word_count.pylocalhost:8081에 접속해서 UI로 확인 가능pyflink 설치pyflink가 설치가 안됨. 일단 Flink 1.14로 버전 올려서 다시 도전… 안됨….ㅋㅋㅋ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-01T21:01:35+09:00'>01 Jul 2022</time><a class='article__image' href='/flink-series6'> <img src='/images/flink_logo.png' alt='Flink Series [Part5]: Flink 설치'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series6'>Flink Series [Part5]: Flink 설치</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part5]: PyFlink",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series5",
      "date"     : "Jul 1, 2022",
      "content"  : "Table of Contents  PyFlink  DataStream API          DataStream API 맛보기      Execution Mode (Batch/Streaming)      When can/should I use BATCH execution mode?      Configuring BATCH execution mode        Table API  참고PyFlinkpip install apache-flink  PyFlink는 실시간 데이터 처리 파이프라인 작업을 수행하는데 필요한 고수준 API를 제공  크게 두 가지의 Table API, DataStream API를 제공      제공받은 API를 이용해 실시간 데이터 처리를 위한 스크립트를 파이썬 언어로 작성할 수 있음    Table API는 SQL과 유사한 형태의 강력한 관계형 쿼리를 작성하는데 필요한 기능을 제공DataStream API  DataStream API는 시간, 상태와 같은 스트림 처리의 핵심이 되는 개념들을 다루는데 필요한 기능을 제공  Filtering, Update state, Defining window, Aggregating과 같은 스트림 데이터 변환 기능을 제공DataStream API 맛보기# DataStream API 관련 패키지 임포트하기from pyflink.common import WatermarkStrategy, Encoder, Typesfrom pyflink.datastream import StreamExecutionEnvironment, RuntimeExecutionModefrom pyflink.datastream.connectors import (FileSource, StreamFormat, FileSink, OutputFileConfig, RollingPolicy)  create a StreamExecutionEnvironment# 스트리밍 프로그램이 실행되는 실행환경# 작업의 특성을 설정# 소스 생성# 작업의 실행 트리거env = StreamExecutionEnvironment.get_execution_environment()env.set_runtime_mode(RuntimeExecutionMode.BATCH)env.set_parallelism(1)  create Source DataSream# env를 이용해서 소스 생성# 소스는 외부시스템에서 데이터 가져옴ds = env.from_source(    source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),                                               input_path)                     .process_static_file_set().build(),    watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),    source_name=&quot;file_source&quot;)  define the execution logicds = ds.map(lambda a: Row(a % 4, 1), output_type=Types.ROW([Types.LONG(), Types.LONG()])) \        .key_by(lambda a: a[0]) \        .map(MyMapFunction(), output_type=Types.TUPLE([Types.LONG(), Types.LONG()]))  create sink and emit result to sink# 데이터 변환# 싱크에 데이터 쓰기ds.sink_to(    sink=FileSink.for_row_format(        base_path=output_path,        encoder=Encoder.simple_string_encoder())    .with_output_file_config(        OutputFileConfig.builder()        .with_part_prefix(&quot;prefix&quot;)        .with_part_suffix(&quot;.ext&quot;)        .build())    .with_rolling_policy(RollingPolicy.default_rolling_policy())    .build())def split(line):    yield from line.split()ds = ds.flat_map(split) \       .map(lambda i: (i, 1), output_type=Types.TUPLE([Types.STRING(), Types.INT()])) \       .key_by(lambda i: i[0]) \       .reduce(lambda i, j: (i[0], i[1] + j[1]))# 스트림 작업 실행# 플링크는 lazy operation -&amp;gt; MapReduce에서 Map과 같은 연산이 일어나는 시점에 클러스터로 작업 전달env.execute()전체 코드 참고Execution Mode (Batch/Streaming)The DataStream API supports different runtime execution modes from which you can choose depending on the requirements of your use case and the characteristics of your job.There is the “classic” execution behavior of the DataStream API, which we call STREAMING execution mode. This should be used for unbounded jobs that require continuous incremental processing and are expected to stay online indefinitely.Additionally, there is a batch-style execution mode that we call BATCH execution mode. This executes jobs in a way that is more reminiscent of batch processing frameworks such as MapReduce. This should be used for bounded jobs for which you have a known fixed input and which do not run continuously.Apache Flink’s unified approach to stream and batch processing means that a DataStream application executed over bounded input will produce the same final results regardless of the configured execution mode. It is important to note what final means here: a job executing in STREAMING mode might produce incremental updates (think upserts in a database) while a BATCH job would only produce one final result at the end. The final result will be the same if interpreted correctly but the way to get there can be different.By enabling BATCH execution, we allow Flink to apply additional optimizations that we can only do when we know that our input is bounded. For example, different join/aggregation strategies can be used, in addition to a different shuffle implementation that allows more efficient task scheduling and failure recovery behavior. We will go into some of the details of the execution behavior below.When can/should I use BATCH execution mode?The BATCH execution mode can only be used for Jobs/Flink Programs that are bounded. Boundedness is a property of a data source that tells us whether all the input coming from that source is known before execution or whether new data will show up, potentially indefinitely. A job, in turn, is bounded if all its sources are bounded, and unbounded otherwise.STREAMING execution mode, on the other hand, can be used for both bounded and unbounded jobs.As a rule of thumb, you should be using BATCH execution mode when your program is bounded because this will be more efficient. You have to use STREAMING execution mode when your program is unbounded because only this mode is general enough to be able to deal with continuous data streams.Another case where you might run a bounded job using STREAMING mode is when writing tests for code that will eventually run with unbounded sources. For testing it can be more natural to use a bounded source in those cases.Configuring BATCH execution mode- STREAMING: The classic DataStream execution mode (default)- BATCH: Batch-style execution on the DataStream API- AUTOMATIC: Let the system decide based on the boundedness of the sourcesStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setRuntimeMode(RuntimeExecutionMode.BATCH);  StreamingIn STREAMING execution mode, all tasks need to be online/running all the time. This allows Flink to immediately process new records through the whole pipeline, which we need for continuous and low-latency stream processing. This also means that the TaskManagers that are allotted to a job need to have enough resources to run all the tasks at the same time.Network shuffles are pipelined, meaning that records are immediately sent to downstream tasks, with some buffering on the network layer. Again, this is required because when processing a continuous stream of data there are no natural points (in time) where data could be materialized between tasks (or pipelines of tasks). This contrasts with BATCH execution mode where intermediate results can be materialized, as explained below.  BatchIn BATCH execution mode, the tasks of a job can be separated into stages that can be executed one after another. We can do this because the input is bounded and Flink can therefore fully process one stage of the pipeline before moving on to the next. In the above example the job would have three stages that correspond to the three tasks that are separated by the shuffle barriers.Instead of sending records immediately to downstream tasks, as explained above for STREAMING mode, processing in stages requires Flink to materialize intermediate results of tasks to some non-ephemeral storage which allows downstream tasks to read them after upstream tasks have already gone off line. This will increase the latency of processing but comes with other interesting properties. For one, this allows Flink to backtrack to the latest available results when a failure happens instead of restarting the whole job. Another side effect is that BATCH jobs can execute on fewer resources (in terms of available slots at TaskManagers) because the system can execute tasks sequentially one after the other.TaskManagers will keep intermediate results at least as long as downstream tasks have not consumed them. (Technically, they will be kept until the consuming pipelined regions have produced their output.) After that, they will be kept for as long as space allows in order to allow the aforementioned backtracking to earlier results in case of a failure.Table API  배치, 스트림 처리를 위한 필요한 기능을 제공  EDA, ETL과 같은 애플리케이션을 쉽게 정의하기 위해 일반적으로 사용됨  데이터가 유한(Bounded), 무한(Unbounded)한 경우에 관계없이 같은 의미from pyflink.common import Rowfrom pyflink.table import (EnvironmentSettings, TableEnvironment,                                  TableDescriptor, Schema,                           DataTypes, FormatDescriptor)from pyflink.table.expressions import lit, colfrom pyflink.table.udf import udtf# 테이블 환경 생성# 플링크 런타임과 상호작용하기 위한 엔트리 포인트# 실행을 위한 세팅(재시작 전략, 병렬성 등)t_env = TableEnvironment.create(EnvironmentSettings.in_streaming_mode())t_env.get_config().set(&quot;parallelism.default&quot;, &quot;1&quot;)# 테이블 생성t_env.create_temporary_table(    &#39;source&#39;,    TableDescriptor.for_connector(&#39;filesystem&#39;)        .schema(Schema.new_builder()                .column(&#39;word&#39;, DataTypes.STRING())                .build())        .option(&#39;path&#39;, input_path)        .format(&#39;csv&#39;)        .build())tab = t_env.from_path(&#39;source&#39;)t_env.create_temporary_table(    &#39;sink&#39;,    TableDescriptor.for_connector(&#39;filesystem&#39;)        .schema(Schema.new_builder()                .column(&#39;word&#39;, DataTypes.STRING())                .column(&#39;count&#39;, DataTypes.BIGINT())                .build())        .option(&#39;path&#39;, output_path)        .format(FormatDescriptor.for_format(&#39;canal-json&#39;)                .build())        .build())테이블을 정의하기 위해 TableEnvironment.execute_sql() 메서드를 사용할 수도 있다my_source_ddl = &quot;&quot;&quot;    create table source (        word STRING    ) with (        &#39;connector&#39; = &#39;filesystem&#39;,        &#39;format&#39; = &#39;csv&#39;,        &#39;path&#39; = &#39;{}&#39;    )&quot;&quot;&quot;.format(input_path)my_sink_ddl = &quot;&quot;&quot;    create table sink (        word STRING,        `count` BIGINT    ) with (        &#39;connector&#39; = &#39;filesystem&#39;,        &#39;format&#39; = &#39;canal-json&#39;,        &#39;path&#39; = &#39;{}&#39;    )&quot;&quot;&quot;.format(output_path)t_env.execute_sql(my_source_ddl)t_env.execute_sql(my_sink_ddl)source라는 테이블과 sink라는 이름의 테이블을 t_env에 등록한다소스를 만들고, 변환하고, 싱크에 데이터를 쓰는 모든 작업은 lazy하게 실행된다. execute와 같은 메서드가 호출되었을 떄만 모든 작업이 전달된다.@udtf(result_types=[DataTypes.STRING()])def split(line: Row):    for s in line[0].split():        yield Row(s)# compute word counttab.flat_map(split).alias(&#39;word&#39;) \   .group_by(col(&#39;word&#39;)) \   .select(col(&#39;word&#39;), lit(1).count) \   .execute_insert(&#39;sink&#39;) \   .wait()전체 코드 참고참고  Apache Flink: pyflink 공식문서  Apache Flink” pyflink Docs 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-01T21:01:35+09:00'>01 Jul 2022</time><a class='article__image' href='/flink-series5'> <img src='/images/flink_logo.png' alt='Flink Series [Part5]: PyFlink'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series5'>Flink Series [Part5]: PyFlink</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part4]: 이진 트리(Binary Tree) - Traverse a Binary Tree",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/binary_tree_1",
      "date"     : "Jul 1, 2022",
      "content"  : "Table of Contents  Iteration          Preorder      Inorder      Postorder        Recursion          Preorder      Inorder      Postorder      IterationPreorder첫 번째 방법현재 꺼낸 노드를 정답에 추가자식 노드 여부에 따라 스택(to_visit)에 추가def preorder(root):    visited = []    to_visit = [root]    while to_visit:        now_node = to_visit.pop()        visited.append(now_node.val)        if now_node.right:            to_visit.append(now_node.right)        if now_node.left:            to_visit.append(now_node.left)    return visited두 번째 방법매번 본인 노드를 방문할 차례가 아니라면 오른쪽 자식 노드, 본인, 왼쪽 자식 노드를 스택에 추가본인 노드를 추가할 때는 (본인 노드가 다시 스택에서 꺼내질때는 방문할 차례가 되었기 때문에) True로 표시def postorder(root):    res, stack = [], [(root, False)]    while stack:        node, visited = stack.pop()        if node:            if visited:                res.append(node.val)            else:                stack.append((node.right, False))                stack.append((node.left, False))                stack.append((node, True))    return resInorder첫 번째 방법매번 본인 노드를 방문할 차례가 아니라면 오른쪽 자식 노드, 본인, 왼쪽 자식 노드를 스택에 추가본인 노드를 추가할 때는 (본인 노드가 다시 스택에서 꺼내질때는 왼쪽 자식 노드를 방문을 마친 후기 때문에) True로 표시def inorder(root):    res, stack = [], [(root, False)]    while stack:        node, visited = stack.pop()        if node:            if visited:                res.append(node.val)            else:                stack.append((node.right, False))                stack.append((node, True))                stack.append((node.left, False))    return res두 번째 방법  왼쪽 자식 노드가 있을 때마다 최대한 깊숙히 들어간 다음 왼쪽 자식 노드가 없으면 자기 자신의 노드를 정답에 추가(방문 표시)하고,오른쪽 자식 노드가 있으면 오른쪽 자식 노드로 접근. 더이상 방문할 노드가 없으면 반복문 종료  def inorder(root):    res, stack = [], []        # this following &quot;while True&quot; block keeps running until &quot;return&quot;    while True:        # goes all the way to left end&#39;s None, append every step onto &quot;stack&quot;        while root:            stack.append(root)            root = root.left        # if stack has nothing left, then return result        if not stack:            return res                # take the last step out, append its value to result        node = stack.pop()        res.append(node.val)        # moves to right before going all the way to left end&#39;s None again        root = node.right    Postorder첫 번째 방법매번 본인 노드를 방문할 차례가 아니라면 오른쪽 자식 노드, 본인, 왼쪽 자식 노드를 스택에 추가본인 노드를 추가할 때는 (본인 노드가 다시 스택에서 꺼내질때는 왼쪽/오른쪽 자식 노드를 방문을 마친 후기 때문에) True로 표시def postorder(root):    res, stack = [], [(root, False)]    while stack:        node, visited = stack.pop()        if node:            if visited:                res.append(node.val)            else:                stack.append((node, True))                stack.append((node.right, False))                stack.append((node.left, False))    return resRecursion트리 순회를 Recursion으로 구현할 때는 크게 Top-Down approach와 Bottom-Up approach가 있습니다.여기서 Top-Down, Bottom-Up은 DP에서도 사용하는 용어인데 값의 흐름을 얘기할 뿐, 같은 의미는 아닙니다.(여기서 값은 숫자, 문자열, 리스트 등이 될 수 있습니다.)Top-Down은 부모 노드를 처리하는 함수가 자신이 가지고 있는 값을 자식 노드를 처리하는 함수의 인자로 전달하는 접근법입니다.def f(node, val):    if val == 0:        return    else:        f(node.left, val+1)        f(node.right, val-1)Bottom-Up은 return문으로 계속 자식 노드를 처리하는 함수를 호출해 자식 노드를 처리하는 함수가 리턴하는 값을 계속 누적하는 접근법입니다.def f(node):  return f(node.left) + f(node.right) + [node.val]Preorder# Bottom-Updef preorder(root):  return [root.val] + preorder(root.left) + preorder(root.right) if root else []# Top-Downdef preorder(root):    res = []    helper(root, res)    return res    def helper(root, res):    if root:        res.append(root.val)        helper(root.left, res)        helper(root.right, res)Inorder# Bottom-Updef inorder(root):  return  inorder(root.left) + [root.val] + inorder(root.right) if root else []# Top-Downdef inorder(root):    res = []    helper(root, res)    return res    def helper(root, res):    if root:        helper(root.left, res)        res.append(root.val)        helper(root.right, res)Postorder# Bottom-Updef postorder(root):  return  postorder(root.left) + postorder(root.right) + [root.val] if root else []# Top-Downdef postorder(root):    res = []    helper(root, res)    return res    def helper(root, res):    if root:        helper(root.left, res)        helper(root.right, res)        res.append(root.val)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-07-01T21:01:35+09:00'>01 Jul 2022</time><a class='article__image' href='/binary_tree_1'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part4]: 이진 트리(Binary Tree) - Traverse a Binary Tree'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/binary_tree_1'>Coding Test Series [Part4]: 이진 트리(Binary Tree) - Traverse a Binary Tree</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part12]: 주키퍼 없이 카프카 사용하기",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series12",
      "date"     : "Jun 23, 2022",
      "content"  : "Table of Contents  참고참고  KRaft: Apache Kafka Without ZooKeeper  The Log of All Logs: Raft-based Consensus Inside Kafka      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-23T21:01:35+09:00'>23 Jun 2022</time><a class='article__image' href='/kafka-series12'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part12]: 주키퍼 없이 카프카 사용하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series12'>Kafka Series [Part12]: 주키퍼 없이 카프카 사용하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part6]: AWS Security Service: IAM",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series6",
      "date"     : "Jun 21, 2022",
      "content"  : "Table of Contents  AWS Security  Security의 종류  IAM  IAM 동작 원리  IAM 구성요소          User      Group      Role      Policy                  Policy JSON Fromat                      참고AWS Security  AWS Security services assist you in safeguarding sensitive data and information while also meeting compliance and confidentiality standards  AWS enables you to automate tedious security processes so you can concentrate on growing and innovating your company. Furthermore, you only pay for the services you utilizeSecurity의 종류  AWS IAM, AWS GuardDuty, AWS Macie, AWS Config, AWS CloudTrailIAM  Identity and Access Management  IAM safeguards accesses to AWS services and resources and to create and manage AWS users and groups and use permissions to grant or deny access to AWS servicesIAM 동작 원리  Principal          A User or a role can be a principal      An action on AWS resource can be performed by a principal      (AWS 서비스에 접근하려는 주체)        Authentication          It is a process of confirming identity of the principal who is trying to access an AWS product      To authenticate from console, you must provide your credentials or required keys      (어떤 주체에게 AWS 서비스에 접근 가능하도록 권한을 주려면 필요한 키를 주체에게 주어야 한다)        Request          When a principal attempts to access the AWS Console, API or CLI, he sends a request to AWS      (주체가 이제 접근을 한다)        Authorizatioin          Here IAM uses information from the request context to check for matching policies      IAM determine whether to allow or deny the request      (IAM은 주체가 가지고 있는 키를 이용해 접근 가능한 주체인지 확인한다)        Actions          After authorization of the request, AWS approves the action      Using this, you can either edit, or delete or even create a resource      (주체의 자격을 검증하고 나면 주체는 본인이 가진 권한만큼 리소스에 접근(생성, 수정, 삭제 등)할 수 있다)        Resources          Resource perform a set of requested actions      IAM 구성요소User  AWS users are AWS entities that represent the person or application that interacts with AWS  IAM users have a name and credentials which are formed when you gave them  The root users of an AWS account is not the same as an IAM user with administrator accessGroup  A set of IAM users is referred to as an IAM user group  User groups allow you to specify permissions for many users, making it easier to manage those user’s permissions  In a resource-based policy, a user goup can’t be desinated as a Principal. A user group is a technique to apply policies to ao group of people all at onceRole  An IAM role is an identity with permission policies that govern what the identity can and cannot do  There are no long-term credentials connected with a role. Instead, when you take on a role, you’re given temporary security credentials for the duration of your role sesion (Role은 Session -&amp;gt; 유효 기간 있음)Policy  Policies are created and linked to IAM identities(user, group, role) or AWS resource  A policy is an object that defines the rights of an identity or resource  Most policies are saved as JSON documentsPolicy JSON Fromat      AWS 공식문서 참고    Identity based Policy          example        {  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Action&quot;: [        &quot;s3.*&quot;,        &quot;s3-object-lambda:*&quot;      ],      &quot;Resource&quot;: &quot;*&quot;,      &quot;Condition&quot;: {}    }  ]}                      Resource based Policy          example        {  &quot;Version&quot;: &quot;2012-10-17&quot;,  &quot;Statement&quot;: [    {       &quot;Sid&quot;: &quot;Statement1&quot;,      &quot;Effect&quot;: &quot;Allow&quot;,      &quot;Principal&quot;: {        &quot;AWS&quot;: &quot;Identity, ex. arn:aws:iam::1111111:root&quot;      },      &quot;Action&quot;: [        &quot;s3.*&quot;,        &quot;s3-object-lambda:*&quot;      ],      &quot;Resource&quot;: &quot;*&quot;,      &quot;Condition&quot;: {}    }  ]}                      Version          정책의 처리에 사용할 언어 구문 규칙을 지정      example        &quot;Version&quot;: &quot;2012-10-17&quot; // 정책 언어의 현재 버전&quot;Version&quot;: &quot;2008-10-17&quot; // 이전 정책 언어 버전                      Sid          선택사항      정책 문에 입력되는 식별자를 제공      Sid 값은 JSON 정책 내 고유성이 보장되어야함      ASCII 대문자(A~Z), 소문자(a~z) 및 숫자(0~9)를 지원합니다.      example        &quot;Sid&quot;: &quot;1&quot;&quot;Sid&quot;: &quot;Statement1&quot;                      Effect          필수값      example        &quot;Effect&quot;: &quot;Allow&quot;&quot;Effect&quot;: &quot;Deny&quot;                      Action          필수값      허용 또는 거부할 작업 지정      서비스 네임스페이스를 작업 접두사(iam, ec2, sqs, sns, s3 등)로 사용      작업 이름을 사용하여 값을 지정                  (이름은 서비스에서 지원되는 작업과 일치해야함)          (작업 목록은 각 서비스별 Doc의 Actions에 있음)                    example        &quot;Action&quot;: &quot;ec2:StartInstances&quot;&quot;Action&quot;: &quot;s3:GetObject&quot;&quot;Action&quot;: [ &quot;sqs:SendMessage&quot;, &quot;sqs:ReceiveMessage&quot;, &quot;iam:ChangePassword&quot;, &quot;s3:GetObject&quot; ]&quot;Action&quot;: &quot;s3:*&quot;&quot;Action&quot;: &quot;iam:*AccessKey*&quot;                      Resource          어떤 서비스에 접근할 것인지 지정      example        &quot;Resource&quot;: &quot;arn:aws:s3:::DOC-EXAMPLE-BUCKET/*/test/*&quot;&quot;Resource&quot;: [                &quot;arn:aws:dynamodb:us-east-2:account-ID-without-hyphens:table/books_table&quot;,                &quot;arn:aws:dynamodb:us-east-2:account-ID-without-hyphens:table/magazines_table&quot;            ]                      Condition          정책의 효력이 발생하는 시점에 대한 조건을 지정      example        &quot;Condition&quot; : { &quot;StringEquals&quot; : { &quot;aws:username&quot; : &quot;johndoe&quot; }}&quot;Condition&quot;: {&quot;StringLike&quot;: {&quot;s3:prefix&quot;: [&quot;janedoe/*&quot;]}}                    참고  Youtube Simplilearn: AWS S3 Tutorial",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-21T21:01:35+09:00'>21 Jun 2022</time><a class='article__image' href='/aws-series6'> <img src='/images/aws_logo.png' alt='AWS Series [Part6]: AWS Security Service: IAM'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series6'>AWS Series [Part6]: AWS Security Service: IAM</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part4]: 메모리",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series4",
      "date"     : "Jun 15, 2022",
      "content"  : "Table of Contents  메모리 관리 개요          메모리 주소      메모리 오버레이      스왑      메모리 분할        가상 메모리 기술에 기반한 메모리 관리          가상 주소      페이징 기법      세그먼테이션 기법      세그먼테이션-페이징 혼용 기법      페이지 교체 알고리즘                  FIFO          LRU          LFU          NUR                      참고메모리: CPU의 작업 공간, 프로세스가 실행될 때 올라가는 공간메모리 관리 개요메모리 관리의 복잡성메모리는 1Byte 크기로 나뉘어 각 영역을 메모리 주소로 구분한다. 메모리 주소는 보통 0번지부터 시작한다.CPU는 메모리에 있는 내용을 가져오거나 작업 결과를 메모리에 저장하기 위해 메모리 주소 레지스터(MAR)를 사용한다.(MAR에 필요한 메모리 주소를 넣으면 데이터를 메모리에서 가져오거나 메모리에 데이터를 옮길 수 있다)시분할 시스템에서는 모든 응용 프로그램이 메모리에 올라와 실행되기 때문에 메모리 관리가 복잡하다.여러 작업을 동시에 처리(시분할 시스템)할 때 메모리 관리는 메모리 관리 시스템(MMS)이 담당한다.메모리 관리의 이중성메모리 관리의 이중성이란 프로세스 입장에서는 메모리를 독차지하려고 하고, 메모리 관리자 입장에서는 되도록 관리를 효율적으로 하고 싶어하는 것을 말한다.프로세스가 작업하는 도중에 할당된 공간이 부족하면 메모리 관리자는 새로운 공간을 확보하기 위해 옆의 프로세스를 밀어내거나 더 큰 공간으로 해당 프로세스를 옮겨준다. 작업을 마치고 난 후 빈 공간이 생기면 다음 작업을 위해 빈 공간을 어떻게 처리할지도 결정한다. 빈 공간이 여러 개 생기면 합쳐서 하나의 큰 공간을 만드는데, 이렇게 하기 위해 현재 작업 중인 공간을 옆으로 밀고 작은 공간을 합쳐나간다. 이처럼 매번 작업 공간을 키웠다 줄였다 하는 것은 매우 복잡한 일이다.메모리 관리자의 역할메모리 관리는 메모리 관리자가 담당한다.메모리 관리자는 정확히 말해 메모리 관리 유닛(MMU)이라는 하드웨어이다.메모리 관리자의 작업은 가져오기, 배치, 재배치이다  가져오기(fetch)          사용자가 요청시 메모리 관리자가 작업을 수행      프로세스와 데이터를 메모리로 가져오는 작업      어떤 경우는 데이터를 일부만 가져오기도 하고, 미리 가져오기(prefetch)도 한다        배치(placement)          가져온 프로세스와 데이터를 메모리의 어떤 부분에 올려놓을지 결정하는 작업      배치 작업 전에 메모리를 어떤 크기로 자를 것인지가 매우 중요      페이징: 메모리를 같은 크기로 자르는 것      세그먼테이션: 프로세스의 크기에 맞게 자르는 것        재배치(replacement)          새로운 프로세스를 가져올 때, 메모리가 꽉 찬 경우 메모리에 있는 오래된 프로세스를 스왑 아웃      페이지 교체 알고리즘      메모리 주소메모리에 접근할 때는 주소를 이용한다.메모리 주소는 절대 주소와 상대 주소로 나뉜다.메모리 영역의 구분메모리는 크게 운영체제가 올라가는 영역과 사용자의 응용 프로그램이 올라가는 영역으로 나뉜다. 운영체제는 중요한 역할을 하기 때문에 사용자가 운영체제를 침범하지 못하도록 분리하는 것이 중요하다. 운영체제가 0~359번지를 사용하면 사용자는 360번지 부터 사용해야 한다. 사용자 영역이 운영체제 영역으로 침범하는 것을 막으려면 하드웨어의 도움이 필요한데, 이는 CPU 내에 있는 경계 레지스터가 담당한다. 경계 레지스터는 운영체제 영역과 사용자 영역 경계 지점의 주소를 가진 레지스터이다.절대 주소와 상대 주소  절대 주소          실제 물리 주소      메모리 관리자 입장에서 바라본 주소      MAR이 사용하는 주소        상대 주소(논리주소)          메모리 중 사용자 영역의 시작 위치를 0번지로 고정하고 사용하는 지정 방식      컴파일 방식을 사용하는 프로그램의 경우 컴파일 시점에 변수가 메모리 어느 위치에 올라갈지 미리 알 수 없다      그래서 우선 0번지로 고정하고 실제 물리 주소에 맞게 매핑한다      사용자 프로세스 입장에서 바라본 주소      32bit CPU와 64bit CPU의 차이CPU의 비트는 한 번에 다룰 수 있는 데이터의 최대 크기CPU 내부 부품은 모두 이 비트를 기준으로 제작된다.32bit CPU 내의 레지스터 크기는 전부 32bit산술 논리 연산장치도 32bit를 처리할 수 있도록 설계된다.데이터를 전송하는 각종 버스의 크기(대역폭)도 32bit -&amp;gt; 한 번에 옮겨지는 데이터의 크기는 32bitCPU의 비트는 메모리 주소 공간의 크기와도 연관이 있다. 32bit CPU의 경우 메모리 주소를 지정하는 레지스터(MAR)의 크기가 32bit이므로 표현할 수 있는 메모리 주소의 범위가 0 ~ 2의 32승 - 1이다. 메모리 공간 하나의 크기가 1Byte이므로 총 크기는 4GB이다. 즉 32bit CPU 컴퓨터는 최대 4GB의 메모리를 가질 수 있다. (64bit CPU는 16,777,216TB)메모리 오버레이과거에는 메모리가 값비싼 저장장치라 큰 메모리를 사용할 수 없었다. 이 때문에 큰 프로그램을 어떻게 실행시킬지에 대한 고민이 있었다. 예를 들어 1MB 메모리의 컴퓨터에 10MB 크기의 프로그램을 실행하려면 어떻게 해야 할까?방법은 간단하다. 프로그램의 크기가 물리 메모리에 남은 용량보다 더 클 때 전체 프로그램을 메모리에 가져오는 대신 적당한 크기로 잘라서 가져오는 기법을 메모리 오버레이(momory overlay)라고 한다. 예를 들어 한글 문서 편집기에는 기본 문서 편집 기능 외에 맞춤법 검사, 그림판 등의 기능이 있다. 이러한 기능은 각각 모듈 형태로 분리되어 있다가 프로그램이 실행되면 필요한 모듈만 메모리에 올라와 실행된다. 이렇게 하면 프로그램 전체를 메모리에 올려놓고 실행하는 것보다 속도가 느리지만 용량이 큰 프로그램도 실행 할 수 있다.메모리 오버레이에서 어떤 모듈을 가져오고 내보낼지는 CPU 레지스터 중 하나인 프로그램 카운터(PC)가 결정한다. 프로그램 카운터는 앞으로 실행할 명령어의 위치를 가리키는 레지스터로, 해당 모듈이 메모리에 없으면 메모리 관리자에게 요청해 메모리로 가져오게 한다.스왑메모리 오버레이가 언제나 가능한 건 아니다. 남은 모듈을 임시 보관할 수 있는 저장공간이 있어야 한다. 예를 들어 1MB 메모리에 10MB 프로그램을 실행하려면 최소한 9MB 만큼 크기의 임시 저장공간이 필요한데 이러한 공간을 스왑(swap) 영역이라고 한다.이처럼 물리 메모리의 저장 공간이 부족해서 남은 프로세스를 저장하는 공간을 스왑 영역이라고 하며 스왑 영역에서 메모리로 데이터를 가져오는 작업을 스왑인, 스왑 영역으로 데이터를 내보내는 작업을 스왑아웃 이라고 한다.스왑 영역은 메모리 관리자가 관리한다. 원래 하드디스크 같은 저장장치는 저장장치 관리자가 관리하지만, 스왑 영역은 메모리에 있다가 스왑아웃된 데이터가 머무는 곳이기 때문에 메모리 관리자가 관리한다.하드디스크의 일부를 스왑 영역으로 사용함으로써 사용자는 실제 메모리의 크기와 스왑 영역의 크기를 합쳐서 전체 메모리로 인식하고 사용할 수 있다.메모리 분할한 번에 여러 프로세스를 실행하는 경우 메모리 관리가 더욱 복잡해진다. 가장 큰 문제는 프로세스들의 크기가 달라 메모리를 어떻게 나누어 사용하는가이다.  가변 분할 방식          프로세스의 크기에 따라 메모리를 나누는 것      하나의 프로세스끼리 연속된 공간에 배치해야 한다      (하나의 프로세스가 메모리의 가용 크기 넘어서면? 오버레이 일어나나? 그럼에도 연속된 공간 배치 지켜지나?)      연속 메모리 할당이라고도 한다      메모리 관리가 복잡하다      (군데군데 남은 공간보다 큰 프로세스가 들어오면 빈공간을 하나로 합치기 위해 기존 프로세스의 자리 이동이 발생)      (빈공간이 발생하는 것을 외부 단편화라고 하고,  합치는 것을 조각 모음(defragmentation)이라고 한다)      (가변 분할 배치는 선처리, 조각 모음은 후처리인 것이다)      (이런 조각 모음은 하드디스크에 발생, 저장-삭제 많이 일어나면 군데군데 빈 공간 발생 -&amp;gt; 성능 저하 -&amp;gt; 조각 모음 필요)      장점: 프로세스를 한 덩어리로 관리 가능      단점: 빈 공간의 관리가 어려움        고정 분할 방식          프로세스의 크기와 상관없이 메모리를 같은 크기로 나누는 것      같은 프로세스들이 분산되어 배치된다      비연속 메모리 할당이라고도 한다      관리가 수월하다      고정 크기보다 작은 프로세스가 들어오면 군데군데 빈 공간 발생(고정 크기 내 다른 프로세스와 공간 공유x) -&amp;gt; 메모리 낭비      (고정 분할에서 빈 공간이 생기는 것을 내부 단편화라고 함)      내부 단편화는 조각 모음도 안되고, 남은 공간을 다른 프로세스에 배정해 주지도 못한다      장점: 메모리 관리가 편리      단점: 프로세스가 분할되어 처리됨, 메모리 낭비 발생      현대 운영체제에서 메모리 관리는 기본적으로 고정 분할 방식을 사용하면서 일부분 가변 분할 방식을 혼합한다.가상 메모리 기술에 기반한 메모리 관리가상 메모리 기술: 물리 메모리의 크기와 상관없이 프로세스를 실행할 수 있도록 지원하는 기술(원래는 프로그램 크기에 대한 고민이 필요했지만 이제는 그런 걱정 없이 프로그램 개발이 가능해졌다)현대 메모리 관리의 가장 큰 특징은 물리 메모리의 크기와 프로세스가 올라갈 메모리의 위치를 신경쓰지 않고 프로그래밍하도록 지원한다는 것이다. 이러한 메모리 시스템을 가상 메모리라고 부른다.가상 메모리는 물리 메모리의 크기와 스왑 영역을 합한 크기이다.가상 메모리 시스템에서는 물리 메모리의 내용 중 일부를 하드디스크의 일부 공간, 즉 스왑 영역으로 옮긴다. 스왑 영역은 하드디스크에 존재하지만 메모리 관리자가 관리하는 영역이다. 즉 물리 메모리가 꽉 찼을 떄, 일부 프로세스를 스왑 영역으로 보내고, 몇 개의 프로세스가 작업을 마치면 스왑 영역에 있는 프로세스를 메모리에 가져온다.가상 메모리 시스템에서 가변 분할 방식을 세그멘테이션(segmentation), 고정 분할 방식을 페이징(paging)이라고 한다.메모리를 관리할 때는 매핑 테이블을 작성하여 관리한다. 가상 메모리 시스템에서 가상 주소는 실제로 물리 주소나 스왑 영역 중 한 곳에 위치하며, 메모리 관리자는 가상 주소와 물리 주소를 일대일 매핑 테이블로 관리한다. 메모리 매핑 테이블을 이용하면 가상 주소가 물리 메모리의 어느 위치에 있는지 알 수 있다.페이징도 위의 세그멘테이션과 똑같은 방식으로 적용된다. 페이징 기법에서 사용하는 매핑 테이블은 페이지 매핑 테이블, 세그먼테이션 기법에서 사용하는 매핑 테이블은 세그먼테이션 매핑 테이블이라고 부른다.가상 주소위에서 용량이 큰 프로그램도 메모리에 걱정없이 올릴 수 있도록 하기 위해 메모리 오버레이를 한다고 했습니다. 모듈화된 프로그램 중 필요한 부분만 물리 메모리에 올리고, 나머지는 하드디스크의 스왑 영역에 보관해 둡니다.이런 상황에서 사용자가 어떤 부분이 메모리에 올라와 있는지, 스왑 영역에 있는지 고민해야 한다면 사용하기 굉장히 불편할 것이다. 이런 문제를 해결해주는 것이 가상 주소입니다.프로세스는 가상 주소를 할당 받는다. 사용자 눈에는 마치 가상 메모리라는 메모리가 있고 그 위에 프로세스가 올라온 것 같은 경험을 한다. 그러나 실제로 가상 메모리라는 것은 없고, 논리적인 개념일 뿐이다. 사용자가 어떤 프로세스에 접근을 하면 메모리 관리자는 매핑 테이블을 통해 물리 주소로 매핑해준다. 물리 주소를 보면 실제로 요청된 프로세스가 물리 메모리에 있을 수도 있고, 스왑 영역에 있을 수도 있다. 스왑 영역에 있다면 페이지 교체 알고리즘을 통해 스왑 영역에 있는 프로세스를 스왑인하여 물리 메모리에 올리고 그 주소를 매핑 테이블에 업데이트 한 후, 사용자에게 돌려준다.페이징 기법사용자 입장에서 프로세스는 가상 메모리(가상 주소 공간)에 올라간다. 위에서 프로세스가 메모리에 올라갈 때 고정 분할 방식과 가변 분할 방식이 있다고 했다. 가상 메모리는 같은 개념을 페이징, 세그멘테이션이라고 한다.가상 메모리에 프로세스를 올릴 때 메모리를 할당하는 방식페이징: 프로세스 크기에 상관없이 일정한 크기로 메모리를 할당세그멘테이션: 프로세스 크기에 비례해 메모리를 할당페이지와 프레임의 크기는 같다가상 주소를 물리 주소로 변경하는 방법페이지 테이블 관리페이지 테이블 매핑 방식세그먼테이션 기법가상 주소를 물리 주소로 변경하는 방법세그먼테이션-페이징 혼용 기법가상 주소를 물리 주소로 변경하는 방법페이지 교체 알고리즘FIFOLRULFUNUR참고      쉽게 배우는 운영체제 책 참고        Microsoft: 가상 주소 공간  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-15T21:01:35+09:00'>15 Jun 2022</time><a class='article__image' href='/os-series4'> <img src='/images/os_16.png' alt='OS Series [Part4]: 메모리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series4'>OS Series [Part4]: 메모리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part3]: 프로세스 관리",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series3",
      "date"     : "Jun 14, 2022",
      "content"  : "Table of Contents  프로세스          프로세스의 네 가지 상태      프로세스의 구조      프로세스의 생성과 복사      프로세스의 전환      프로세스의 계층 구조        스레드          스레드의 개념      멀티스레드        프로세스 간 통신          동기화      프로세스 간 통신의 종류                  전역 변수를 이용한 통신          파일을 이용한 통신          소켓을 이용한 통신                    임계구역                  임계구역 해결 조건          임계구역 해결 방법                      교착 상태          자원 할당 그래프      교착 상태 필요조건      교착 상태 해결방법                  교착 상태 검출과 회복                      프로세스와 스레드 비교 그리고 비동기 프로그래밍          멀티 태스킹      멀티 프로세싱      멀티 스레딩과 비동기 프로그래밍은 다르다      콜백 함수      Thread Safe      ‘1개의 코어에 2개의 스레드’할 때 그 스레드랑 같은 의미인가?      멀티 코어를 가진 CPU 1개 vs 싱글 코어를 가진 CPU 여러개        참고프로세스프로세스 관리는 운영체제의 핵심적인 역할프로세스가 생성되고 작업을 마칠 때까지의 상태 변화프로세스와 스레드 비교스레드 사용의 이점운영체제에서 프로세스는 하나의 작업 단위프로그램을 실행하면 프로세스가 된다프로그램은 하드디스크 같은 저장장치에 저장프로그램을 실행하면 해당 코드가 메모리에 올라와서 작업이 진행된다프로그램은 저장장치에 저장되어 있는 정적인 상태프로세스는 실행을 위해 메모리에 올라온 동적인 상태프로그램 - 클래스 - 레시피프로세스 - 객체  - 요리CPU가 시간을 쪼개어 여러 프로세스에 적당히 배분함으로써 동시에 실행하는 것처럼 느껴진다프로세스의 네 가지 상태시분할 방식에서의 예상치 못한 상황 처리새우튀김을 해야하는데 새우가 아직 손질되지 않았다고 가정해보자. 메인 셰프가 보조 요리사에게 새우 손질을 지시한다  그동안 메인 셰프가 아무것도 하지 않으면 비효율적이다.  메인 셰프는 새우튀김을 &#39;대기 목록&#39;(대기 상태)으로 옮기고 주문 목록(준비 상태)에 있는 다른 주문서를 가져와서 요리를 해야한다.  새우 손질이 끝났다고 가정해보자. 그렇다고 바로 대기 목록에서 새우튀김을 가져와서 바로 요리하지는 않는다.  우선 현재 작업 중이던 요리를 마친 뒤 새우튀김을 만드는 것이 좋다. 따라서 새우튀김 요리 주문서를 대기 목록에서 주문 목록 뒤에 이동시킨 뒤 순서를 기다린다.  손님이 급한 볼일을 보느라 음식을 천천히 달라고 요청하는 경우도 있다. 손님의 지연 요청은 언제 끝날지 알 수 없기 때문에 무작정 대기 목록에 올려둘 수 없다. (손님이 지연 끝에 취소를 할 수도 있다)  이와 같이 언제 다시 시작될지 모르는 주문서는 &#39;보류 목록&#39;으로 옮기고, 손님이 음식을 달라고 하면 보류 목록에 있는 주문서를 &#39;주문 목록&#39;으로 보낸다.  보류 목록은 언제 다시 시작될지 모르거나, 중간에 그만둘지 모르는 작업들을 모아두는 곳이다현대의 운영체제는 시분할 방식을 기본으로 사용한다. 프로세스가 여러 상태를 오가며 실행된다. 그렇다면 운영체제가 어떻게 여러 프로세스를 함께 처리하는지 살펴보자.프로그램에서 프로세스로의 전환프로세스는 컴퓨터 시스템의 작업 단위로 태스크라고도 불린다.시분할 방식 시스템에서 프로그램이 프로세스로 전환될 때 어떤 일이 일어나는지 살펴보자우선 운영체제는 프로그램을 메모리의 적당한 위치로 가져온다. 그와 동시에 주문서에 해당하는 작업 지시서(프로세스 제어블록)를 만든다. 프로세스 제어블록에는 프로세스를 처리하는 데 필요한 다양한 정보가 들어 있다. 프로그램이 프로세스로 전환되려면 운영체제로부터 프로세스 제어블록을 받아야 한다. (프로세스 제어블록은 운영체제가 해당 프로세스를 위해 관리하는 데이터 구조이기 때문에 운영체제 영역에 만들어진다) 프로세스가 종료되면 메모리에서 삭제되고, 프로세스 제어블록도 폐기된다.프로세스 제어블록에 포함된 대표적인 정보- 프로세스 구분자: 프로세스를 구분하는 구분자(PID)- 메모리 관련 정보: 프로세스가 올라가 있는 메모리의 위치 정보- 각종 중간값: 프로세스가 사용했던 각종 중간값 (시분할 시스템이기 때문에 중간값을 저장해야 한다)             (ex. 다음 작업할 코드의 위치가 담긴 프로그램 카운터, 각종 중간값을 보관 중인 레지스터)프로세스의 상태앞서 레스토랑의 예에서 주문서가 다양한 목록을 옮겨 다니는 것과 같이, 운영체제에서도 여러 가지 이유로 프로세스 상태가 변화된다. 시분할 시스템에서 프로세스는 CPU 리소스를 넘겨주고 다시 받고 하는 일이 빈번하게 일어난다. 프로세스는 상황에 따라 다섯 가지 상태를 오간다.  생성 상태          프로세스가 메모리에 올라와 실행 준비를 완료한 상태      프로세스를 관리하는데 필요한 프로세스 제어블록이 생성됨        준비 상태          생성된 프로세스가 CPU를 얻을 때까지 기다리는 상태      디스패치: CPU 스케줄러가 준비 상태에 있는 여러 프로세스 중 다음에 실행할 프로세스를 선정하는 일        실행 상태          준비 상태에 있던 프로세스가 CPU를 얻어 실제 작업을 수행하는 상태      프로세스 제어블록을 CPU에 전달      주어진 시간내 작업이 끝나지 않은 경우 다시 준비 상태로 돌아감(클록이 인터럽트를 사용해 CPU에 알림)      작업이 끝날때까지 준비 상태와 실행 상태를 왔다 갔다함        완료 상태          실행 상태의 프로세스가 시간내 작업을 마치면 완료 상태로 진입      프로세스 제어블록이 사라진 상태        대기 상태          프로세스가 입출력을 요청하면 입출력 관리자가 입출력을 완료하기 전까지 프로세스를 대기 상태로 옮겨둔다      입출력 관리자가 입출력을 완료하면 프로세스를 대기 상태에서 준비 상태로 옮긴다      컨텍스트 스위칭컨텍스트 스위칭(context switching)은 CPU를 차지하던 프로세스가 나가고 새로운 프로세스를 받아들이는 작업을 말한다. 이 때 두 프로세스 제어블록의 내용이 변경된다. 실행 상태에서 나가는 프로세스 제어블록에는 CPU에 있던 지금까지의 작업 내용을 저장하고, 반대로 실행 상태로 들어오는 프로세스 제어블록의 내용으로 CPU를 다시 세팅한다. 이와 같이 두 프로세스의 프로세스 제어블록을 교환하는 작업을 컨텍스트 스위칭이라 한다.컨텍스트 스위칭은 인터럽트가 걸렸을 때 발생한다.  타임아웃 인터럽트: 프로세스가 할당받은 시간을 초과한 경우  Out-Of-Memory 인터럽트: 프로세스가 할당받은 메모리를 초과한 경우프로세스의 구조  코드 영역          프로그램의 코드가 기술된 곳으로 텍스트 영역이라고도 함      자기 자신을 수정하는 프로그램은 존재하지 않기 때문에, 코드는 읽기 전용으로 처리        데이터 영역          코드가 실행되면서 사용되는 변수나 파일 등의 각종 데이터를 모아놓는 곳      변수는 기본적으로 읽기와 쓰기가 가능        스택 영역          프로세스를 실행하기 위해 부수적으로 필요한 데이터를 모아놓은 곳      예를들어, 함수가 호출되면 함수를 수행하고 마치면 다시 되돌아올 위치를 이 영역에 저장      스택 영역은 운영체제가 사용자의 프로세스를 작동하기 위해 유지하는 영역이므로 사용자에게는 보이지 않음        힙 영역          동적으로 할당되는 데이터들을 위해 존재하는 공간이다. ex) malloc      위의 특징 외에도 코드 영역과 데이터 영역은 선언할 때 그 크기가 결정되는 정적 영역이지만,스택 영역과 힙 영역은 프로세스가 실행되는 동안 크기가 늘어났다 줄어들기도 하는 동적 영역이다원칙적으로 서로 다른 프로세스간의 메모리 공간 접근은 허용되지 않는다.만약 프로세스간 서로 다른 자원에 접근하려면 프로세스간의 통신을 해야한다.프로세스의 생성과 복사프로세스는 프로그램을 실행할 때 새로 생성된다. 사용자가 프로그램을 실행하면 운영체제는 프로그램을 메모리에 가져와 코드 영역에 넣고 프로세스 제어블록을 생성한다. 그리고 메모리에 데이터 영역과 스택 영역을 확보한 후 프로세스를 실행한다.프로세스를 새로 생성하는 방법뿐만 아니라 실행 중인 프로세스로부터 새로운 프로세스를 복사하는 방법도 있다.fork()커널이 제공하는 fork() 시스템 호출은 실행 중인 프로세스로부터 새로운 프로세스를 복사하는 함수이다. 이 함수는 워드 프로그램, 브라우저 등에서 프로그램을 하나 더 실행할 때 전형적으로 호출된다. 이렇게 프로세스를 복사하게 되면 기존의 프로세스는 부모 프로세스가 되고, 새로 생긴 프로세스는 자식 프로세스가 된다.프로세스의 전환exec()exec() 시스템 호출을 사용하면 복사된 프로세스를 새로운 프로세스로 전환할 수 있다. 이 함수를 사용하는 목적은 프로세스의 구조체를 재활용하기 위함이다.새로운 프로세스를 만들려면 제어블록을 만들고 메모리의 자리를 확보하는 과정이 필요하다. 또한 프로세스를 종료하고 사용한 메모리를 청소(garbage collection)하기 위해 상위 프로세스와 부모-자식 관계를 만들어야 한다.exec()를 사용하면 이미 만들어진 제어블록, 메모리 영역, 부모-자식 관계를 그대로 사용할 수 있어 편리하다. 새로운 코드 영역만 가져오면 되기 때문에 운영체제의 작업이 수월하다.exec() 함수의 동작과정은 간단하다. 호출을 하면 코드 영역에 있는 기존의 내용을 지우고 새로운 코드로 바꾼다. 또한 데이터 영역이 새로운 변수로 채워지고 스택 영역이 리셋된다. 제어블록의 내용 중 프로세스 구분자, 부모 프로세스 구분자, 메모리 관련 사항 등은 변하지 않지만 프로그램 카운터 레지스터 값을 비롯한 각종 레지스터와 사용한 파일 정보가 모두 리셋된다.프로세스의 계층 구조위에서 배운 프로세스의 복사와 전환은 프로세스의 계층 구조를 이해하는데 핵심 열쇠가 된다.fork()와 exec()를 사용하면 프로세스들을 자동적으로 부모-자식 프로세스의 계층 구조로 만들어준다. 프로세스를 계층 구조로 만들면 프로세스 간의 책임 관계가 분명해져서 시스템을 관리하기가 수월하다. 프로세스가 작업을 마쳐서 그 프로세스가 사용하던 자원을 회수(garbage collection)할 때 특히 편리하다. 만약 모든 프로세스가 독립적으로 만들어지면 프로세스가 종료될 때마다 운영체제가 직접 자원을 회수해야 하기 때문에 작업이 복잡해질 것이다. 그러나 모든 프로세스를 부모-자식 관계로 만들면 자식 프로세스가 작업을 마쳤을 때 사용하던 자원을 부모 프로세스가 회수하면 된다.고아 프로세스부모 프로세스는 자원을 회수하기 위해 자식 프로세스가 끝날 때까지 기다려야 된다. 그런데 부모 프로세스가 먼저 종료되거나 자식 프로세스가 비정상적으로 종료되어 부모 프로세스에 연락이 안되는 경우도 있다. 이렇게 부모 프로세스가 먼저 종료되고 비정상적으로 남아있는 자식 프로세스를 고아 프로세스 또는 좀비 프로세스라고 한다. 좀비 프로세스는 자식 프로세스를 종료했음에도 부모가 뒤처리를 하지 않을 때 발생한다. 이런 좀비 프로세스가 많아지면 자원이 낭비되게 된다. 따라서 운영체제는 반환되지 못한 자원을 주기적으로 회수해야 한다.스레드오늘날의 운영체제는 프로세스의 낭비 요소를 제거하고, 프로세스 작업의 유연성을 얻기 위해 멀티스레드를 사용한다.스레드의 개념프로세스는 요리 작업 전체, 스레드는 요리를 완성하기 위해 수행하는 각각의 조리에 해당CPU 스케줄러가 CPU에 전달하는 일 하나가 스레드. 그러므로 CPU가 처리하는 작업의 단위는 프로세스로부터 전달받은 스레드운영체제 입장에서의 작업 단위는 프로세스이고, CPU 입장에서의 작업 단위는 스레드프로세스의 코드에 정의된 절차에 따라 CPU에 작업 요청을 하는 실행 단위프로세스와 스레드의 차이프로세스끼리는 약하게 연결되어 있는 반면, 스레드끼리는 강하게 연결되어 있음프로세스는 스테이크, 스프와 같이 서로 큰 영향을 미치지 않음스레드는 스테이크 요리 내에서 고기 굽기, 채소 굽기, 소스 뿌리기와 같이 서로 강하게 연결되어 있음멀티 태스크운영체제가 CPU에 작업을 줄 때 시간을 잘게 나누어 배분하는 기법시분할 시스템에서 운영체제가 CPU에 전달하는 작업은 프로세스가 아니라 스레드이다.서로 독립적인 여러 개의 프로세스로 구성프로세스 간 데이터를 주고 받기 위해 IPC(Inter Process Communication)을 이용한다.멀티 프로세싱멀티프로세싱은 CPU를 여러 개 사용하여 여러 개의 스레드를 동시에 처리하는 작업 환경을 말한다.멀티프로세싱은 하나의 컴퓨터에 여러 개의 CPU 혹은 하나의 CPU내 여러 개의 코어에 스레드를 배정하여 동시에 작동하는 것.멀티 스레드멀티스레드는 프로세스 내 작업을 여러 개의 스레드로 분할함으로써 작업의 부담을 줄이는 프로세스 운영 기법멀티스레드는 변수나 파일 등을 공유하고 전역 변수나 함수 호출 등의 방법으로 스레드간 통신을 한다.CPU 멀티 스레드CPU 멀티스레드는 한 번에 하나씩 처리해야 하는 스레드를 파이프라인 기법을 이용하여 동시에 여러 스레드를 처리하도록 만든 병렬 처리 기법하드웨어적인 방법으로 하나의 CPU에서 여러 스레드를 동시에 처리하는 병렬 처리 기법(멀티스레드는 운영체제가 소프트웨어적으로 프로세스를 작은 단위의 스레드로 분할하여 운영하는 기법)멀티스레드CPU와 프로그래밍 기술이 발전하면서 여러 개의 코어를 가진 CPU가 생겨나 멀티스레드를 지원하기 시작멀티스레드의 구조스레드가 어떻게 생겨나고 어떻게 구성되는지 프로세스의 입장에서 살펴보자. C언어와 같은 절차지향 프로그래밍 언어는 순차적으로 실행되기 때문에 프로세스로 여러 개의 작업을 동시에 처리하기가 불편했다. 여러 개의 작업을 동시에 처리하기 위해 fork() 시스템 호출을 사용해 프로세스를 생성하는 방법을 이용했다. 그러나 fork()는 코드 영역과 데이터 영역의 일부가 메모리에 중복되어 존재하는 낭비 요소가 생길 수 밖에 없었다.예를 들어, 워드프로세서 같은 문서 편집기를 이용해 숙제, 편지쓰기, 자료 정리 작업을 동시에 진행한다고 해보자. 이 때 각각의 워드프로세서를 여러 개 실행할 경우, 내부적으로는 fork() 시스템 호출이 발생한다. 이렇게 되면 워드프로세서의 코드 일부, 프로세스 제어 블록, 공유 변수가 메모리의 여러 곳에 중복되어 메모리가 낭비된다.스레드는 이러한 멀티태스킹의 낭비 요소를 제거하기 위해 사용한다. 비슷한 일을 하는 2개의 프로세스를 만드는 대신, 코드, 데이터 등을 공유하면서 여러 개의 일을 하나의 프로세스 내에서 하는 것이다.프로세스는 크게 정적인 영역과 동적인 영역으로 구분된다. 동적인 영역의 대표적인 예는 레지스터 값, 스택, 힙 등이다.오늘날에는 여러 작업을 하기 위해 fork() 시스템 호출 대신 하나의 프로세스에 여러 개의 스레드를 만들어 사용한다.멀티스레드는 코드, 파일 등의 자원을 공유함으로써 자원의 낭비를 막고 효율성을 향상한다.멀티스레드의 장점  응답성 향상: 한 스레드가 입출력으로 인해 작업이 진행되지 않더라도 다른 스레드가 작업을 계속하여 사용자의 작업 요구에 빨리 응답할 수 있다  자원 공유: 한 프로세스 내에서 독립적인 스레드를 생성하면 프로세스가 가진 자원을 모든 스레드가 공유하게 되어 작업을 원활하게 진행할 수 있다  효율성 향상: 불필요한 자원의 중복을 막음으로써 시스템의 효율이 향상된다  다중 CPU 지원: 2개 이상의 CPU를 가진 컴퓨터에서 멀티스레드를 사용하면 다중 CPU가 멀티스레드를 동시에 처리하여 CPU 사용량이 증가하고 프로세스의 처리 시간이 단축된다멀티스레드의 단점모든 스레드가 자원을 공유하기 때문에 한 스레드에 문제가 생기면 전체 프로세스에 영향을 미친다.프로세스 간 통신프로세스는 서로 독립적이기 때문에 데이터를 주고 받기 위해서는 프로세스 간 통신을 해야 한다.프로세스끼리 통신을 할 때에는 누가 먼저 작업할지, 작업이 언제 끝날지 등을 서로 알려주어야 하는데 이를 동기화라고 한다.그리고 같은 데이터를 여러 프로세스가 사용할 때는 서로 침범하면 안되는 임계구역이 존재한다.프로세스 간 통신(IPC, Inter Process Communication)에는 같은 컴퓨터내에 있는 프로세스 간 통신도 있고, 네트워크로 연결된 다른 컴퓨터에 있는 프로세스 간 통신도 있다.  프로세스 내부 데이터 통신: 하나의 프로세스 내에 2개 이상의 스레드가 존재하는 경우의 통신. 프로세스 내부의 스레드는 전역 변수나 파일을 이용해 데이터를 주고 받는다  프로세스 간 데이터 통신: 같은 컴퓨터에 있는 여러 프로세스끼리 통신하는 경우로, 공용 파일 또는 운영체제가 제공하는 파이프를 사용하여 통신한다  네트워크를 이용한 데이터 통신: 여러 컴퓨터가 네트워크로 연결되어 있을 때도 통신이 가능한데, 이 경우 프로세스는 소켓을 이용하여 데이터를 주고받는다. 이처럼 소켓을 이용하는 프로세스 간 통신을 네트워킹이라고 한다. 다른 컴퓨터에 있는 함수를 호출하여 통신하는 원격 프로시저 호출(RPC)도 여기에 해당한다.(같은 프로세스끼리도 루프백(127.0.0.1) 주소를 사용하면 소켓을 이용하여 통신할 수도 있지만, 소켓을 사용하려면 많은 전처리를 해야하기 때문에 같은 컴퓨터에 있는 프로세스 간 통신에는 소켓을 거의 사용하지 않는다)동기화프로세스 간 통신은 겉으로 보기에는 단순하다. 데이터를 주거나(send), 받는것(receive)을 의미한다. 예를 들어 전역 변수를 이용해 통신하는 경우를 생각해보자. 데이터를 보내는 프로세스는 데이터를 전역 변수에 저장한다. 데이터를 받는 프로세스는 전역 변수에 저장된 데이터를 읽어간다.하지만 내부적으로는 이것보다는 복잡하다. 예를 들어 통신하려는 상대 프로세스를 어떻게 찾을지, 데이터의 크기는 얼마로 할지, 데이터 도착 여부를 어떻게 확인할지 등의 문제를 해결해야 한다.프로세스 간 통신 방법을 분류할 때는 크게 두 가지가 기준이 된다. 바로 통신 방향과 동기화의 유무이다.  단방향: 모스 신호처럼 한쪽 방향으로만 데이터를 전송할 수 있는 구조. (전역 변수, 파이프)      양방향: 데이터를 동시에 양쪽 방향으로 전송할 수 있는 구조 (소켓 통신)    동기화: 상대쪽에서 데이터를 전송하면 운영체제가 데이터가 도착했음을 알려준다. (파이프, 소켓)  비동기화: 상대쪽에서 데이터를 전송했는지 반복문을 무한 실행하며 기다려야 한다(바쁜 대기). (전역 변수, 파일)프로세스 간 통신의 종류전역 변수를 이용한 통신전역 변수를 이용한 통신은 공동으로 관리하는 메모리를 사용하여 데이터를 주고 받는 것이다.전역 변수를 이용한 통신은 변수를 두 개를 사용하면 양방향 통신을 가능하게 한다.하지만 동기화 문제가 해결되지 않는다.파일을 이용한 통신파일을 이용한 통신은 부모-자식 프로세스 간 통신에 많이 사용운영체제가 별다른 동기화를 제공하지 않기 때문에, 프로세스가 알아서 동기화를 해야 한다.동기화를 위해 주로 부모 프로세스가 wait() 함수를 이용하여 자식 프로세스의 작업이 끝날 때까지 기다렸다가 작업을 시작한다.소켓을 이용한 통신서로 다른 컴퓨터에 있는 프로세스 간 통신은 원격 프로시저 호출(RPC)이나 소켓을 이용한다.원격 프로시저 호출은 다른 컴퓨터에 있는 함수를 호출하는 것을 뜻한다.객체지향 언어에서 다른 컴퓨터에 있는 객체의 메소드를 불러와 사용하는 것이 원격 프로시저 호출의 대표적인 예이다.원격 프로시저 호출은 일반적으로 소켓을 이용하여 구현한다.프로세스는 소켓에 쓰기 연산을 통해 데이터를 전송하고 읽기 연산을 통해 데이터를 받는다.소켓은 동기화를 지원한다.소켓은 하나만 사용해도 양방향 통신이 가능하다.네트워크 프로그래밍을 흔히 소켓 프로그래밍이라 부르는 이유는 네트워킹의 기본이 소켓이기 때문이다.여러 컴퓨터에 있는 프로세스에 데이터를 전달하는 방법 중 가장 대중화된 것은 소켓을 이용한 네트워킹이다. 소켓을 이용한 네트워킹에서도 open(), read()/write(), close() 구조를 사용한다.위의 그림은 클라이언트와 서버가 어떤 절차를 거쳐서 통신하는지를 보여준다. 클라이언트와 서버는 둘 다 소켓을 사용한다. 소켓은 양방향 통신을 지원하고 동기화도 지원한다.클라이언트는 소켓을 생성한 후 connect()를 사용하여 서버와의 접속을 시도한다. 서버와 접속되면 read() 혹은 write() 작업을 하며, 작업이 끝나면 사용한 소켓 디스크립터를 닫고 종료한다.서버 쪽 통신 절차는 좀 더 복잡하다. 서버는 소켓을 생성한 후 bind()를 삿용하여 생성한 소켓을 특정 포트에 등록한다. 포트는 한 컴퓨터 내에 존재하는 여러 프로세스를 구분하기 위한 목적이다. 포트를 사용해 어떤 프로세스와 통신할지 구분할 수 있다.하나의 포트 번호에 소켓이 하나만 생성되는 것은 아니다. 네이버 홈페이지를 운영하는 서버의 포트 번호는 80번인데, 여기에 소켓을 하나만 생성할 수 있다면단 한사람에게만 서비스를 할 수 있을 것이다. 서버는 동시에 여러 클라이언트에 서비스를 하기 위해 하나의 포트 번호에 여러 개의 소켓을 생성한다. 따라서 bind()는 특정 포트에 새로운 소켓을 등록하겠다는 의미이다.bind()로 소켓이 정상적으로 등록되면 listen()을 실행하여 클라이언트를 받을 준비를 한다. accept()는 클라이언트의 connect(), 즉 연결 요청을 기다리다가 여러 명의 클라이언트가 동시에 connect()를 하는 경우 그중 하나를 골라 작업을 시작하게 해준다. 따라서 클라이언트가 accept()되면 소켓 디스크립터가 생성되고 작업이 시작된다. read()혹은 write() 작업을 마치면 생성된 소켓 디스크립터를 닫고 다음 클라이언트를 기다린다.서버에서의 소켓 생성은 listen()으로 클라이언트의 접속을 확인한 후, accept()에서 이루어진다.서버의 경우 계속 클라이언트를 받아 작업해야 하기 때문에 무한 루프를 돌며 작업을 반복한다.임계구역임계구역은 공유 자원 접근 순서에 따라 실행 결과가 달라지는 프로그램의 영역을 말한다.임계구역 해결 조건  상호 배제: 한 프로세스가 임계구역에 들어가면 다른 프로세스는 들어갈 수 없다  한정 대기: 어떤 프로세스도 무한 대기하지 않아야 한다  진행의 융통성: 한 프로세스가 다른 프로세스의 진행을 방해해서는 안된다임계구역 해결 방법  세마포어: 임계구역에 진입하기 전에 스위치를 사용 중으로 놓고 임계구역으로 들어가는 방법교착 상태교착 상태는 2개 이상의 프로세스가 다른 프로세스의 작업이 끝나기만 기다리며 작업을 더 이상 진행하지 못하는 상태를 말한다.컴퓨터 시스템에서 교착 상태는 시스템 자원, 공유 변수(또는 파일), 응용 프로그램(데이터베이스) 등을 사용할 때 발생할 수 있다.자원 할당 그래프프로세스가 어떤 자원을 사용 중이고 어떤 자원을 기다리고 있는지를 방향성이 있는 그래프로 표현한 것이다.자원 할당 그래프를 사용하면 자원의 할당과 대기 상태를 한눈에 파악할 수 있다.교착 상태 필요조건  상호 배제: 한 프로세스가 사용하는 자원은 다른 프로세스와 공유할 수 없는 배타적인 자원이어야 한다  비선점: 한 프로세스가 사용 중인 자원은 다른 프로세스가 빼앗을 수 없는 비선점 자원이어야 한다  점유와 대기: 프로세스가 어떤 자원을 할당받은 상태에서 다른 자원을 기다리는 상태이어야 한다  원형 대기: 점유와 대기를 하는 프로세스 간에 관계가 원을 이루어야 한다위 조건이 하나라도 만족되지 않으면 교착 상태는 일어나지 않는다.교착 상태 해결방법교착 상태 검출과 회복교착 상태 검출교착 상태 검출은 운영체제가 프로세스의 작업을 관찰하면서 교착 상태 발생 여부를 계속 주시하는 방법. 만약 교착 상태가 발견되면 이를 해결하기 위해 교착 상태 회복 단계를 밟는다.교착 상태를 검출하는 방법에는 크게 타임아웃 방법과 자원 할당 그래프를 이용하는 방법이 있다.타임아웃일정 시간 동안 작업이 진행되지 않은 프로세스를 교착 상태가 발생한 것으로 간주하여 처리하는 방법이다.  장점: 쉽게 구현  단점: 엉뚱한 프로세스가 종료될 수 있다. 분산 데이터베이스에는 적용 불가(교착 상태 때무인지, 네트워크 때문인지 알 수 없음)위와 같은 문제에도 불구하고 타임아웃은 대부분의 데이터베이스와 운영체제에서 많이 선호한다. 그 이유는 자원 할당 그래프를 이용한 방법은 구현하기가 힘들기 때문이다.자원 할당 그래프를 이용  장점: 교착 상태를 정확하게 파악 할 수 있다  단점: 그래프를 유지, 갱신하고 사이클을 검사하는 추가 작업으로 인한 오버헤드가 발생교착 상태 회복교착 상태를 유발한 프로세스를 강제 종료한다  교착 상태를 일으킨 모든 프로세스를 동시에 종료          이 방법은 작업을 시작하면 다시 교착 상태를 일으킬 가능성이 높다      그래서 다시 실행할 때는 어떤 프로세스를 먼저 실행할지 순서를 정해 순차적으로 실행해야 한다        교착 상태를 일으킨 프로세스 중 하나를 골라 순서대로 종료          순서대로 종료하면서 나머지 프로세스의 상태를 파악하는 방법      우선순위가 낮은 프로세스를 먼저 종료한다      우선순위가 같은 경우 작업시간이 짧은 프로세스를 먼저 종료한다      위의 두 조건이 같은 경우 자원을 많이 사용하는 프로세스를 먼저 종료한다      프로세스와 스레드 비교 그리고 비동기 프로그래밍  프로세스          운영체제로부터 자원을 할당받는 작업의 단위      프로그램이 인스턴스화 되어 메모리에 올라온 상태        스레드          프로세스가 할당받은 자원을 이용하는 실행의 단위      스레드는 프로세스 내에서 각각 Stack만 따로 할당받고 Code, Data, Heap 영역은 공유      멀티 태스킹멀티 프로세싱멀티 스레딩과 비동기 프로그래밍은 다르다콜백 함수  함수 안에 인자로 전달되는 함수  fun1(fun2) -&amp;gt; fun1 함수를 실행하는 도중에 fun2 함수가 실행되고 fun2 함수가 종료된 후 fun1 함수가 마저 실행됨  fun2와 같은 함수를 콜백 함수라고 함Thread Safe‘1개의 코어에 2개의 스레드’할 때 그 스레드랑 같은 의미인가?Threads don’t exist in the CPU. They are a feature of the operating system, used in programs for concurrent programming. Meaning, having different threads of a program running at the same time. While you could just have a process rather than a thread, it’s less efficient, and harder to communicate between the pieces.As for cores, that is a CPU thing. Starting in the 1970s, the microprocessor could run one program at a time. Into the 80s, and 90s, they still could run only one program at a time. The operating system simply switched programs as needed, giving each program a time slice of the CPU time. For example, simplistically, 10 programs could each be run 1/10s during a second ( that’s not really how it’s done, but good enough for this discussion.)CPUs eventually hit a performance wall. The ability to make their clock speed faster became harder to do. To compensate, they started effectively putting more than one CPU on the silicon die. So that multiple programs could run truly simultaneously. The more cores, the more programs that can run at the same time. Of course there are way more programs running on a typical Windows or Linux computer than your CPU has cores. So there still needs to be some sharing of the CPU, which is handled by the operating system.Note that many programs could be seen as consisting of a single thread. And some programs have more than one thread. Multiple threaded programs may also be happy to have more CPU cores, as they may run faster, depending on the program. For example, there are applications, for example video editing, where the programs will work better with more cores. I mention this, because this may be one reason people associate threads and cores.스레드가 CPU에 물리적으로 존재하는게 아니라 논리적인 존재라면 왜 최대 2개 밖에 안되는걸까?Because running more than 2 threads on a single core would increase core complexity without improving performance.Even having 2 threads running simultaneous on a single CPU core is not universal. CPU cores were originally designed to run 1 thread at a time. A multitasking OS on a single core machine gives the illusion that it runs multiple threads at the same time, but it does so by running a single thread for a limited amount of time before switching to the next thread.However, for a while now, CPU cores have had more than one execution unit. For instance, the original Intel Pentium CPU (which is a single core CPU) had 2 integer execution units and 1 floating point unit. This means that it can, in principle, perform multiple instructions at the same time. However, this is only possible when running a single thread, if consecutive instructions are independent of each other, which means that the input of the second instruction does not depend on the output of the first one.Modern CPUs, such as the Intel i3/i5/i7 series have as many as 8 execution units per core, and they can start execution of 4 new instructions each clock cycle. Even with some instructions taking more than 1 clock cycle, this means that most of the time, some of the execution units have nothing to do.Hyperthreading is a technology, originally introduced by Intel in its Pentium 4 processors, to fill up these unused execution units with instructions from a different thread. So a CPU core with hyperthreading can run 2 threads at the same time.Now we finally come to your question: why does hyperthreading limit itself to 2 threads? why not more? The answer is that a single hyperthreading core running 2 threads is nowhere near as fast as two separate cores running these same threads. Since each thread has to make do with the execution units not used by the other one, neither thread runs at maximum speed. If you were to allow more than 2 threads on a single core, this would increase core complexity considerably without much tangible benefit.멀티 코어를 가진 CPU 1개 vs 싱글 코어를 가진 CPU 여러개참고  쉽게 배우는 운영체제 책 참고  위키백과 스레드  우당탕탕 개발, 멀티프로세싱, 멀티프로그래밍,멀티태스킹,멀티스레딩  wooody92’s blog, 멀티 프로세스(Multi Process)와 멀티 스레드(Multi Thread)  인파, 프로세스 vs 쓰레드 차이 정리  Noah, Process, Thread 차이가 뭐예요?  비동기와 멀티스레딩  Kishore’s Blog, Process vs Thread vs Core  Quora, Why does a core only have two threads in a processor?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-14T21:01:35+09:00'>14 Jun 2022</time><a class='article__image' href='/os-series3'> <img src='/images/os_3.png' alt='OS Series [Part3]: 프로세스 관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series3'>OS Series [Part3]: 프로세스 관리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Spring Series [Part4]: JPA",
      "category" : "",
      "tags"     : "Spring",
      "url"      : "/spring_series4",
      "date"     : "Jun 7, 2022",
      "content"  : "Table of Contents  JPA  스프링 데이터 JPA  QuerydslJPA스프링 데이터 JPAQuerydsl",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-07T21:01:35+09:00'>07 Jun 2022</time><a class='article__image' href='/spring_series4'> <img src='/images/spring_logo.png' alt='Spring Series [Part4]: JPA'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spring_series4'>Spring Series [Part4]: JPA</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Spring Series [Part3]: JDBC",
      "category" : "",
      "tags"     : "Spring",
      "url"      : "/spring_series3",
      "date"     : "Jun 7, 2022",
      "content"  : "Table of Contents  JDBC  JDBC Template  MyBatisJDBCJDBC TemplateMyBatis",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-07T21:01:35+09:00'>07 Jun 2022</time><a class='article__image' href='/spring_series3'> <img src='/images/spring_logo.png' alt='Spring Series [Part3]: JDBC'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spring_series3'>Spring Series [Part3]: JDBC</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Spring Series [Part2]: 스프링 MVC",
      "category" : "",
      "tags"     : "Spring",
      "url"      : "/spring_series2",
      "date"     : "Jun 7, 2022",
      "content"  : "Table of Contents  웹 애플리케이션 서버  서블릿  스프링 MVC웹 애플리케이션 서버서블릿스프링 MVC",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-07T21:01:35+09:00'>07 Jun 2022</time><a class='article__image' href='/spring_series2'> <img src='/images/spring_logo.png' alt='Spring Series [Part2]: 스프링 MVC'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spring_series2'>Spring Series [Part2]: 스프링 MVC</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Spring Series [Part1]: 스프링 핵심 원리",
      "category" : "",
      "tags"     : "Spring",
      "url"      : "/spring_series1",
      "date"     : "Jun 6, 2022",
      "content"  : "Table of Contents  스프링          스프링 프레임워크      스프링 부트      스프링의 핵심 컨셉      객체 지향                  다형성          제어의 역전, 의존 관계 주입                      스프링 컨테이너와 빈  싱글톤 컨테이너  컴포넌트 스캔  의존관계 자동 주입  빈 스코프스프링스프링 프레임워크  핵심 기술: 스프링 DI 컨테이너, AOP, 이벤트 등  웹 기술: 스프링 MVC  데이터 접근 기술: 트랜잭션, JDBC  기술 통합: 캐시, 이메일, 원격접근, 스케줄링  테스트: 스프링 기반 테스트 지원스프링 부트  스프링을 편리하게 사용할 수 있도록 지원  Tomcat 같은 웹 서버 내장  스프링과 외부 라이브러리 자동 구성스프링의 핵심 컨셉  웹 애플리케이션을 만들어주는 프레임워크  데이터베이스에 접근을 편리하게 함  객체 지향 설계를 지원하는 프레임워크객체 지향다형성  객체 지향 코드를 작성할 때 얻을 수 있는 가장 큰 특징은 다형성  다형성은 역할(인터페이스)과 구현(클래스)으로 나누어 코드를 설계  코드 설계시 역할만 알아도 됨  빠른 설계와 유연한 확장 가능제어의 역전, 의존 관계 주입  스프링은 다형성을 위해 제어의 역전(IoC), 의존관계 주입(DI) 방식으로 개발하도록 지원  스프링 컨테이너(ApplicationContext)가 위 방식을 가능하게 함  서비스를 공연에 비유하면 스프링 컨테이너는 공연 기획자에 해당스프링 컨테이너와 빈싱글톤 컨테이너컴포넌트 스캔의존관계 자동 주입빈 스코프",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-06-06T21:01:35+09:00'>06 Jun 2022</time><a class='article__image' href='/spring_series1'> <img src='/images/spring_logo.png' alt='Spring Series [Part1]: 스프링 핵심 원리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spring_series1'>Spring Series [Part1]: 스프링 핵심 원리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part11]: 정규표현식(4) 정규표현식을 활용한 메서드(feat.파이썬)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series11",
      "date"     : "May 20, 2022",
      "content"  : "Table of Contents  re 라이브러리 메서드          match()      search()      findall()      finditer()      fullmatch()        matchObj 객체의 메서드          group()      start()      end()      span()        참고re 라이브러리 메서드match()search()findall()finditer()fullmatch()matchObj 객체의 메서드group()start()end()span()참고  greeksharifa, 파이썬 정규표현식(re) 사용법  regexone: 정규표현식 문제  regexr: 정규표현식 테스트  regexper: 정규표현식 시각화  프로그래머스: 정규표현식 문제  백준: 정규표현식 문제",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-20T21:01:35+09:00'>20 May 2022</time><a class='article__image' href='/data-engineering-series11'> <img src='/images/regex_logo.png' alt='Data Engineering Series [Part11]: 정규표현식(4) 정규표현식을 활용한 메서드(feat.파이썬)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series11'>Data Engineering Series [Part11]: 정규표현식(4) 정규표현식을 활용한 메서드(feat.파이썬)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part10]: 정규표현식(3) 그루핑",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series10",
      "date"     : "May 19, 2022",
      "content"  : "Table of Contents  ()          문자 그루핑      이중 그루핑      문자 캡처링      그루핑만 원하는 경우: (?:)      재참조: \숫자      활용        참고()정규표현식에서 ()는 크게 다음과 같은 기능을 가집니다.그룹핑캡처링재참조문자 그루핑yahoo+ 정규표현식은 앞의 문자 o가 1회 이상 반복되는 yahoo, yahooooo와 같은 문자와 매칭됩니다.yahoo라는 문자가 반복되는 부분과 매칭하고 싶을때 그루핑을 사용할 수 있습니다.이중 그루핑문자 캡처링정규표현식을 사용할 때 매칭되는 문자열의 특정 부분만 추출하고 싶은 경우가 있습니다. 이런 경우에도 () 문자를 사용해서 캡처링할 수 있습니다.그루핑은 정규표현식으로 패턴을 만들기 위한 목적이고, 캡처링은 매칭된 문자열 중 특정 부분을 사용하기 위한 목적입니다.캡처링만 원하는 경우에는 그냥 ()만 사용하고 뒤에 별다른 정량자를 사용하지 않으면 됩니다. 반면 그루핑만 원하는 경우에는 해당 괄호가 캡처링되지 않도록 하기 위해 ‘이 괄호는 비캡처 그루핑이다’라는 표기로 (?:)과 같이 표기합니다.그루핑만 원하는 경우: (?:)원래 그루핑을 하게되면 캡처링도 자동적으로 따라오게 됩니다. 하지만 단순 문자열 반복의 목적으로 그루핑만 원하고, 캡처링되지 않기를 원하는 경우도 있습니다. 이 때는 해당 ()안에 ?:를 넣어서 정규표현식을 작성하면 됩니다.(?:&amp;lt;regex&amp;gt;) : 그루핑용으로만 사용하고 캡처링되지 않도록 처리합니다.재참조: \숫자캡처링한 부분과 똑같은 문자열을 다시 참조하고 싶은 경우 \숫자 (숫자는 캡처링 숫자와 일치)를 사용할 수 있습니다.자주 사용되는 예시는 ‘토마토’, ‘기러기’, ‘zabz’와 같이 똑같은 문자로 시작해서 끝나는 단어를 매칭하고 싶을 때 입니다.활용()의 캡처링은 정규표현식을 활용한 메서드에서 자주 활용되기 때문에 다음 포스트에서 자세히 다루도록 하겠습니다.참고  greeksharifa, 파이썬 정규표현식(re) 사용법  regexone: 정규표현식 문제  regexr: 정규표현식 테스트  regexper: 정규표현식 시각화  프로그래머스: 정규표현식 문제  백준: 정규표현식 문제",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-19T21:01:35+09:00'>19 May 2022</time><a class='article__image' href='/data-engineering-series10'> <img src='/images/regex_logo.png' alt='Data Engineering Series [Part10]: 정규표현식(3) 그루핑'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series10'>Data Engineering Series [Part10]: 정규표현식(3) 그루핑</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part10]: 자바의 객체지향 프로그래밍(2)",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series10",
      "date"     : "May 18, 2022",
      "content"  : "Table of Contents  Intro          상속      포함 관계      오버라이딩(overriding)      super      super()        제어자(modifier)          static      final      abstract      public, protected, private        다형성  추상 클래스  인터페이스  참고Intro상속상속이란 기존의 클래스를 재사용하여 새로운 클래스를 작성하는 것입니다. 상속을 이용하면 코드의 재사용성을 높이고 코드의 중복을 제거할 수 있습니다. 자바에서 상속은 간단히 extends키워드를 사용해 표현할 수 있습니다.class Child extends Parent {  ...}상속을 해주는 클래스를 부모 클래스라고 하고, 상속을 받는 클래스를 자식 클래스라고 합니다.자식 클래스는 부모 클래스의 변수와 메서드를 상속 받으므로 부모 클래스의 변동은 자식 클래스에게도 영향을 미치지만, 자식 클래스의 변동은 부모 클래스에 아무런 영향을 주지 않습니다.또한 항상 자식 클래스가 부모 클래스보다 같거나 많은 멤버를 갖게 됩니다. 즉, 상속에 상속을 거듭할수록 상속받는 클래스의 멤버 개수는 점점 늘어나게 됩니다. 그래서 상속을 받는다는 것은 부모 클래스를 확장(extend)한다는 의미로 해석할 수도 있습니다.class Person {    String name;    int age;    char gender;}class Worker extends Person {    int salary;}class Test {    public static void main(String[] args) {        Worker w = new Worker();        w.age = 25;        w.name = &quot;Peter&quot;;        w.gender = &#39;f&#39;;        w.salary = 200;        Person p = new Person();        p.age = 20;        p.name = &quot;Mike&quot;;        p.gender = &#39;f&#39;;        p.salary = 100; // cannot find variable salary    }}포함 관계상속을 이용한 방법 말고도 클래스 간의 관계를 맺어주고 클래스를 재사용하는 방법이 있는데, 그것은 바로 포함(composite)관계를 이용하는 방법입니다.다음 코드는 사람을 나타내는 Person클래스와, 사람의 자산을 나타내는 Property클래스의 포함관계를 나타낸 것입니다.class Person {    String name;    int age;    char gender;    Property pty = new Property();}이렇게 코드 재사용성을 높이기 위해 클래스 간 관계를 나타내는 방법에는 크게 상속과 포함관계가 있는데, 그러면 언제 상속을 사용하고 언제 포함관계를 사용할까요?- A(노동자)는 B(사람)이다 -&amp;gt; A는 B의 자식 클래스 -&amp;gt; 상속- A(자산)은 B(사람)에 속한다 -&amp;gt; 포함 관계  전체 코드는 아래와 같습니다.class Person {    String name;    int age;    char gender;    Property pty = new Property();}class Property {    int balance;    boolean house;    boolean car;}class Test {    public static void main(String[] args) {        Person p = new Person();        p.age = 20;        p.name = &quot;Mike&quot;;        p.gender = &#39;f&#39;;        p.pty.balance = 5000;        p.pty.car = false;        p.pty.house = true;    }}오버라이딩(overriding)부모 클래스로부터 상속받은 메서드의 내용을 변경하는 것을 오버라이딩이라고 합니다. 상속받은 메서드를 그대로 사용하기도 하지만, 보통 자식 클래스에서 자신에 맞게 변경하는 경우가 많습니다.여기서 말하는 내용 변경은 구현부에 해당하는 얘기입니다. 그렇기 때문에 메서드의 선언부(메서드 이름, 매개변수, 반환타입)는 부모 클래스와 완전히 일치해야 합니다.다만 접근 제어자(access modifier)와 예외(exception)는 제한된 조건에서 다르게 변경할 수 있습니다.접근 제어자는 부모 클래스의 메서드와 같거나 더 넓은 범위로 변경 가능부모 클래스의 메서드보다 많은 수의 예외를 선언할 수 없음오버라이딩 예시는 다음과 같습니다. Person클래스의 introduceMyself() 메서드를 Worker 클래스에서 오버라이딩 하였습니다.class Person {    String name;    int age;    char gender;    void introduceMyself() {        System.out.println(&quot;Hi I&#39;m &quot; + name + &quot; I&#39;m just person&quot;);    }}class Worker extends Person {    String position;    int salary;    @Override    void introduceMyself() {        System.out.println(&quot;Hi I&#39;m &quot; + name + &quot; I&#39;m working as &quot; + position + &quot; My salary is &quot; + salary);    }}class Test {    public static void main(String[] args) {        Person p = new Person();        p.age = 20;        p.name = &quot;Mike&quot;;        p.gender = &#39;f&#39;;        p.introduceMyself();        Worker w = new Worker();        w.age = 25;        w.name = &quot;Peter&quot;;        w.gender = &#39;m&#39;;        w.position = &quot;Manager&quot;;        w.salary = 100;        w.introduceMyself();    }}supersuper는 자식 클래스에서 부모 클래스 인스턴스를 지칭하는 방법입니다. 클래스 안에서 자기 자신의 인스턴스를 this로 나타낸 것과 유사합니다. 자식 클래스에서 super를 사용하면 자식 클래스의 변수, 메서드와 이름이 같은 부모 클래스의 변수, 메서드를 구별할 수 있습니다.위의 코드에서 자식클래스의 introduceMyself() 메서드를 다음과 같이 수정해보겠습니다.class Worker extends Person {    String position;    int salary;    @Override    void introduceMyself() {        super.introduceMyself(); // Hi I&#39;m Peter I&#39;m just person        System.out.println(&quot;I&#39;m working as &quot; + position + &quot; My salary is &quot; + salary); // I&#39;m working as Manager My salary is 100    }}위와 같이 super를 이용해 부모클래스의 introduceMyself() 메서드를 호출할 수 있습니다.참고로 위의 @Override는 오버라이딩 애너테이션(annotation)으로 오버라이딩을 위해 반드시 표기해야 하는 것은 아닙니다. 하지만 제가 자바 컴파일러에게 introduceMyself() 메서드를 오버라이딩 한 것이라고 명시적으로 알려줌으로써 부모 클래스에 introduceMyself()가 있는지 확인하는 등 제대로 오버라이딩을 했는지 컴파일 단계에서 확인해줍니다.super()super()는 부모클래스의 생성자입니다. 생성자는 변수를 초기화하는 메서드라고 했습니다.그렇기 때문에 super()는 부모클래스를 상속 받은 자식클래스에서 부모클래스의 멤버 변수를 초기화 할 때 사용합니다.아래는 super()를 사용하지 않고 부모클래스의 변수를 초기화한 경우입니다. Worker클래스에서 부모클래스의 변수인 name, age, gender를 초기화하는 코드를 this.name = name과 같은 방법으로 정의하였습니다.class Person {    String name;    int age;    char gender;    Person(String name, int age, char gender) {        this.name = name;        this.age = age;        this.gender = gender;    }}class Worker extends Person {    String position;    int salary;    Worker(String name, int age, char gender, String position, int salary) {        this.name = name;        this.age = age;        this.gender = gender;        this.position = position;        this.salary = salary;    }}class Test {    public static void main(String[] args) {        Person p = new Person(&quot;Mike&quot;, 20, &#39;f&#39;);        Worker w = new Worker(&quot;Peter&quot;, 20, &#39;m&#39;, &quot;Manager&quot;, 100);    }}이렇게 직접 this.name = name으로 정의하는 것도 틀린 코드는 아니지만, super()를 이용하면 코드의 중복을 제거하는 조금 더 객체지향적인 코드를 작성할 수 있기 때문에 super()를 이용해서 다시 작성하면 다음과 같습니다.class Worker extends Person {    String position;    int salary;    Worker(String name, int age, char gender, String position, int salary) {        super(name, age, gender);        this.position = position;        this.salary = salary;    }}제어자(modifier)제어자는 클래스, 변수 또는 메서드의 선언부에 함께 사용되어 부가적인 의미를 부여합니다.접근 제어자  public, protected, (default), private  네 가지중 한 개만 사용 가능  보통 선언부에서 가장 먼저 표기그 외  static, final, abstract 등  여러 개 조합하여 사용 가능static  사용될 수 있는 곳: 변수, 메서드      인스턴스 생성하지 않고 사용 가능    변수에 사용할 경우          모든 인스턴스에 공통적으로 사용되는 클래스 변수가 된다      인스턴스 생성하지 않고 사용 가능한 변수가 된다      클래스가 메모리에 로드될 때 생성된다        class Person {    static int personNumber;    String name;    int age;    char gender;}class Test {    public static void main(String[] args) {        System.out.println(Person.personNumber); // 인스턴스 없이 personNumber 사용    }}        메서드에 사용할 경우          인스턴스 생성하지 않고 호출 가능한 클래스 메서드가 된다      클래스 메서드에서는 인스턴스 멤버를 직접 사용할 수 없다      클래스 메서드는 오버라이딩 할 수 없다 (자바관련 면접 질문)      Overriding depends on having an instance of a class. The point of polymorphism is that you can subclass a class and the objects implementing those subclasses will have different behaviors for the same methods defined in the superclass (and overridden in the subclasses). A static method is not associated with any instance of a class so the concept is not applicable.        class Person {    static int personNumber;    String name;    int age;    char gender;    static void countPersonNumber() {        System.out.println(personNumber); // 변수는 클래스 변수 personNumber만 사용 가능    }}class Test {    public static void main(String[] args) {        Person.countPersonNumber(); // 인스턴스 없이 countPersonNumber() 메서드 호출    }}      final  사용할 수 있는 곳: 클래스, 메서드, 변수  클래스에 사용할 경우          자신을 확장하는 자식클래스 정의 못하게 함 (자식 안낳는다)        final class Person {      }class Worker extends Person { // Cannot inherit from final &#39;Person&#39;      }        메서드에 사용할 경우          자식클래스가 오버라이딩 할 수 없게 함        class Person {    final void countPersonNumber() {        System.out.println(&quot;사람 숫자 세는 중&quot;);    }}class Worker extends Person {    @Override    void countPersonNumber() { // Cannot Override; Overriden method is &#39;final&#39;        System.out.println(&quot;일꾼 숫자 세는 중&quot;);    }}                      변수에 사용할 경우          변경할 수 없는 상수가 됨      abstract추상 클래스 또는 추상 메서드를 정의할 때 사용합니다.  사용할 수 있는 곳: 클래스, 메서드  클래스에 사용할 경우          클래스 내에 추상 메서드가 선언되어 있음을 의미      추상 클래스는 아직 완성되지 않은 메서드가 있음을 의미하므로 인스턴스 생성 불가        abstract class Person {    abstract void countPersonNumber();}class Test {    public static void main(String[] args) {        Person p = new Person(); // &#39;Person&#39; is abstract, cannot be instantiated    }}                      메서드에 사용할 경우          아직 구현부가 작성되지 않은 추상 메서드임을 알림      자식 클래스에서 추상 메서드를 오버라이딩 하도록 강제        abstract class Person {    abstract void countPersonNumber();}class Worker extends Person { // Worker does not override abstract method countPersonNumber() in Person    void work() {        System.out.println(&quot;I&#39;m working&quot;);    }}class Test {    public static void main(String[] args) {        Worker w = new Worker();        w.work();    }}                public, protected, private            접근 제어자가 사용될 수 있는 곳: 클래스, 변수, 메서드, 생성자public: 접근 제한이 전혀 없음protected: 같은 패키지 내에서는 접근 제한 없음, 다른 패키지인 경우 자식 클래스 한정(default): 같은 패키지 내에서는 접근 제한 없음private: 같은 클래스 내에서만 접근 가능이러한 접근 제어자를 사용하는 경우는 보통 다른 클래스나 패키지에서의 접근을 제한하기 위한 용도로 사용합니다. 이렇게 접근을 제한하는 것을 객체지향에서 캡슐화라고 합니다.이런 경우 보통 변수는 private이나 protected로 접근 범위를 제한하고, 읽기 메서드(getter), 쓰기 메서드(setter)는 public으로 제공함으로써 변수를 다룰 수 있도록 합니다.다형성다형성이란 ‘여러 가지 형태를 가질 수 있는 능력’을 의미합니다. 다형성은 객체지향 코드의 가장 큰 장점이라고 할 수 있습니다. 자바에서 다형성은 하나의 공통적인 역할을 하는 클래스를 인터페이스로 정의하고, 그 인터페이스를 구현하는 다양한 클래스를 만드는 방식을 의미합니다.이렇게 다향성을 가지도록 코드를 작성하게 되면 다음과 같은 이점을 얻을 수 있습니다.  인터페이스만 알아도 다른 쪽에서 무리없이 개발할 수 있다    class Car {};class HyundaiCar extends Car {};class TeslaCar extends Car {};class User {    String name;    int age;    Car c; // 무슨 차인지 모르더라도 Car로 두고 개발을 지속할 수 있다};            위의 이유로 코드의 확장성이 좋아진다    class Car {  String brand;  int year;  public void start() {      System.out.println(&quot;차가 출발합니다&quot;);  }  public void stop() {      System.out.println(&quot;차를 멈춥니다&quot;);  }  public void normalMode() {      System.out.println(&quot;일반 모드&quot;);  }}class HyundaiCar extends Car {    public void hyundaiMode() {        System.out.println(&quot;현대차 모드&quot;);    }}class TeslaCar extends Car {    public void teslaMode() {        System.out.println(&quot;테슬라 모드&quot;);    }}        여러 구현체를 인터페이스의 공통된 타입을 이용해 매개변수로 받을 수 있다    class Car {};class HyundaiCar extends Car {};class TeslaCar extends Car {};class Test {    public static void main(String[] agrs) {        HyundaiCar h = new HyundaiCar();        TeslaCar t = new TeslaCar();    }    public static void carSpec(Car c) { // 매개변수의 타입을 Car로 하면 HyundaiCar, TeslaCar 모두 인자로 받을 수 있다        System.out.println(c.brand + &quot; &quot; + c.year)    }}      추상 클래스추상 클래스는 클래스에 완성되지 않은 메서드가 있음을 명시적으로 알리고 이를 자식클래스에서 반드시 오버라이딩하도록 하는 부모클래스의 일종입니다. 추상 클래스는 완성되지 않은 부분을 가지고 있는 클래스이기 때문에 인스턴스를 생성할 수 없습니다. 인스턴스 생성을 자신을 상속받는 자식클래스에게 위임합니다. (부모클래스의 불완전함을 자식클래스가 이어받아 개선하여 최종적으로 인스턴스를 생성한다)추상 클래스를 만드는 방법은 클래스 선언부와 완성되지 않은 메서드의 선언부에 각각 abstract 키워드를 붙입니다. 메서드에는 구현부가 없으므로 {}를 생략합니다.abstract class Car {    String brand;    int year;    abstract void carMode();}class HyundaiCar extends Car {    @Override // @Override 어노테이션은 필수 아님, but 컴파일 오류 체크해주므로 권장    void carMode() {        System.out.println(&quot;현대차 모드&quot;)    }}인터페이스인터페이스는 일종의 추상클래스입니다. 추상클래스에는 메서드 중 일부가 구현되어 있지 않았던 반면, 인터페이스는 가지고 있는 모든 메서드에 구현부가 없습니다. 인터페이스는 오직 추상메서드와 상수만을 멤버로 가질 수 있습니다.인터페이스 예시는 다음과 같습니다.interface Car {    // 모든 멤버변수는 public static final이어야 함. 생략 가능    public static final int numWheel = 1;        // 모든 메서드는 public abstract. 생략 가능    public abstract void start();}추상 클래스는 자식클래스가 상속받는다고 표현했습니다. 인터페이스는 구현(implements)한다고 표현합니다.class Hyundai implements Car {    public void start() {        System.out.println(&quot;차가 달립니다&quot;)    }}인터페이스는 위에서 배웠던 다형성을 위한 목적으로 주로 사용됩니다.참고  남궁성, 자바의 정석 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-18T21:01:35+09:00'>18 May 2022</time><a class='article__image' href='/java-series10'> <img src='/images/java_logo.png' alt='Java Series [Part10]: 자바의 객체지향 프로그래밍(2)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series10'>Java Series [Part10]: 자바의 객체지향 프로그래밍(2)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part9]: 정규표현식(2) 문자 반복",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series9",
      "date"     : "May 18, 2022",
      "content"  : "Table of Contents  문자 반복          OR : |      0회 이상 반복 : *      1회 이상 반복 : +      n회 이상 m회 이하 반복 : {n, m}      0회 또는 1회 반복 : ?      탐욕정량자, 나태정량자        참고문자 반복OR : |0회 이상 반복 : ** : 바로 앞에 있는 문자가 0회 이상 반복되는 부분과 매칭됩니다one*은 on, one, oneee와 같은 문자에 매칭됩니다.1회 이상 반복 : ++ : 바로 앞에 있는 문자가 1회 이상 반복되는 부분과 매칭됩니다one+는 e가 최소 한 번 이상 등장하는 부분과 매칭되기 때문에 one, oneee와 같은 문자에 매칭됩니다.n회 이상 m회 이하 반복 : {n, m}{n, m} : 바로 앞에 있는 문자가 n회 이상 m회 이하 반복되는 부분과 매칭됩니다.0회 또는 1회 반복 : ?? : 바로 앞에 있는 문자가 0회 이상 1회 이하 반복되는 부분과 매칭됩니다.탐욕정량자, 나태정량자기본적으로 모든 정량자(*, +, {n, m}, ?)는 탐욕정량자입니다.탐욕정량자 : 일치되는 부분을 찾을 때 최대한 많이 일치되도록 합니다나태정량자 : 정량자 뒤에 ?를 붙여주면 됩니다참고  greeksharifa, 파이썬 정규표현식(re) 사용법  regexone: 정규표현식 문제  regexr: 정규표현식 테스트  regexper: 정규표현식 시각화  프로그래머스: 정규표현식 문제  백준: 정규표현식 문제",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-18T21:01:35+09:00'>18 May 2022</time><a class='article__image' href='/data-engineering-series9'> <img src='/images/regex_logo.png' alt='Data Engineering Series [Part9]: 정규표현식(2) 문자 반복'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series9'>Data Engineering Series [Part9]: 정규표현식(2) 문자 반복</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part9]: 자바의 객체지향 프로그래밍(1)",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series9",
      "date"     : "May 17, 2022",
      "content"  : "Table of Contents  Intro          객체지향 언어      클래스와 객체      객체 생성      클래스 정의      변수      메서드      메서드의 매개변수      static      메서드 오버로딩      인스턴스 초기화 메서드: 생성자      Intro객체지향 언어객체지향 언어는 절차지향 언어와 자주 비교되는 언어중 하나입니다. 절차지향 언어는 코드의 순서에 의미를 가지고 있고, 객체지향 언어는 코드의 관계에 중요한 의미를 가지고 있습니다.객체지향 언어의 대표적인 장점은 코드의 재사용성이 높다는 것입니다. 코드의 재사용성이 높아짐으로써 불필요한 중복 코드가 제거되며, 개발과 유지보수에 들어가는 시간이 줄고, 확장성이 높아집니다.위의 장점을 얻기 위해 코드를 작성하기 전에 어떤식으로 객체를 정의하고 관계를 지을지 계획하는 것이 굉장히 중요합니다. 하지만 너무 많은 시간을 계획에 몰두하는 것은 오히려 개발의 지연을 발생시키므로, 일단 프로그램을 기능적으로 완성한 후 어떻게 객체지향적으로 코드를 개선할 수 있을지 고민하며 점차 개선해 나가는 것이 좋습니다.클래스와 객체  클래스: 실체를 만들기 전에 작성하는 설계도. 변수(속성)와 메서드(기능)를 바탕으로 정의  객체: 클래스에 정의된 내용을 바탕으로 메모리에 생성된 실체객체 생성클래스로부터 객체를 생성하는 방법은 다음과 같습니다.// &amp;lt;참조변수의 타입&amp;gt; &amp;lt;참조변수&amp;gt; = new &amp;lt;클래스명&amp;gt;();Person p = new Person();어떤 클래스의 경우, 연산자 new를 생략하기도 하고, 위의 코드를 두 줄로 나눠 적기도 하고, 매개변수를 전달하기도 하는 등 객체 생성의 몇가지 변형된 방법 또한 존재합니다.객체 생성과 변수에 할당되는 과정은 다음과 같습니다.ex)1. Person p;Person 클래스 타입의 참조변수 p를 선언하면, 메모리에 참조변수 p를 위한 공간이 마련(아직 객체가 생성되지는 않았으므로 비어있음)2. p = new Person();   연산자 new에 의해 Person클래스의 객체가 메모리의 빈 공간에 생성(멤버변수는 각 자료형에 해당하는 기본값으로 초기화)대입연산자(=)에 의해 객체의 주소갑이 참조변수 p에 저장클래스 정의객체를 생성하려면 생성하려는 객체의 타입인 클래스를 먼저 정의해야 합니다. 클래스는 크게 변수와 메서드로 구성되어 있습니다. 선언하는 위치와 제어자의 종류에 따라 변수와 메서드의 성질이 변하게 되는데 이에 대해서는 뒤에서 정리하도록 하며, 여기서는 가장 간단하게만 짚고 넘어가겠습니다.class Person {    // 변수    String name;    int age;    // 메서드    void introduceMyself() {        System.out.println(&quot;My name is &quot; + this.name)    }}변수위에서 변수를 정의하는 코드를 간단하게 살펴봤습니다. 변수는 클래스의 어느 위치에서 선언하냐에 따라 종류가 달라지게 됩니다. 종류에는 클래스 변수, 인스턴스 변수, 지역 변수가 있습니다.      인스턴스 변수인스턴스 변수는 인스턴스가 생성될 때 같이 생성되는 변수로, 인스턴스마다 별도의 저장공간을 가지기 때문에 서로 다른 값을 가질 수 있습니다.        클래스 변수클래스가 메모리에 올라가는 시점에 생성되는 변수로, 모든 인스턴스가 공통된 저장공간을 공유하게 됩니다. 클래스 변수는 앞에 static을 붙여주기만 하면 되고 &amp;lt;클래스명&amp;gt;.&amp;lt;클래스 변수명&amp;gt;으로 사용합니다.  메서드메서드는 특정 작업을 수행하는 코드를 하나로 묶은 것입니다. 예를 들어 두 수의 덧셈, 게임 캐릭터의 공격과 같은 것들이 메서드에 해당합니다.메서드는 크게 선언부와 구현부로 이루어져 있습니다.class Person {        // 선언부    int add(int a, int b)        // 구현부        {         int result = a + b;        return result;        }}      메서드 선언부선언할 수 있는 매개변수의 개수는 거의 제한이 없지만, 입력해야 할 값의 개수가 많은 경우에는 배열이나 참조변수를 사용하면 됩니다.    메서드 선언부 앞에 메서드 수행 후 반환할 결과값의 타입을 적습니다. 반환값이 없는 경우에는 void라고 적습니다.        메서드 구현부메서드 내에 선언된 변수들은 그 메서드 내에서만 사용할 수 있는 지역변수 입니다.    메서드에서 반환할 결과값을 return문 뒤에 작성합니다. 결과값은 선언부에서 정의한 타입과 일치해야 합니다.  메서드의 매개변수자바에서는 메서드를 호출할 때 인자 값을 메서드의 매개변수에 복사해서 넘겨줍니다. 넘겨준 인자의 타입이 기본형인지 참조형인지에 따라 복사되는 값이 다릅니다.      기본형 매개변수매개변수가 기본형인 경우에는 기본형 값을 복사해 오기 때문에, 값을 읽을 수만 있고 수정할 수는 없습니다.    class Person {    public static void main(String[] args) {        int a = 10;                  System.out.println(introduceMyself(a)); // 1000        System.out.println(a); // 10 (변경안됨)    }    // 기본형(정수) 매개변수 a    static int introduceMyself( int a ) {        a = 1000;        return a;    }}            참조형 매개변수매개변수의 타입이 참조형인 경우에는 값이 아니라, 값을 가지고 있는 인스턴스의 주소를 복사해 옵니다. 주소가 복사되기 때문에 값을 읽어오는 것 뿐 아니라 변경할 수도 있게 됩니다.    class Person {  public static void main(String[] args) {      Data d = new Data();      d.x = 10;      System.out.println(introduceMyself(d)); // 1000      // introduceMyself 메서드 안에서 변경된 값이 반영됨      System.out.println(d.x); // 1000  }  // 참조형(Data 타입) 매개변수 d  static int introduceMyself( Data d ) {      d.x = 1000;      return d.x;  }}class Data { int x; }      static위의 예제에서 중간중간 등장했던 static 키워드에 대해 짚고 넘어가도록 하겠습니다. static은 인스턴스 단위가 아닌 클래스 단위로 변수와 메서드를 정의하도록 해줍니다.static 키워드를 붙였을 때의 특징은 다음과 같습니다.  인스턴스를 생성하지 않아도 사용할 수 있다  모든 인스턴스가 공유해서 사용한다  static이 붙은 변수를 클래스 변수, 메서드를 클래스 메서드라고 한다  클래스 메서드는 인스턴스 변수를 사용할 수 없다static은 다음과 같은 경우에 사용합니다.  멤버변수 중 모든 인스턴스에서 공통된 값을 가져야 하는 경우 또는 가져도 되는 경우(메모리 효율성)  메서드 중 인스턴스 변수나 인스턴스 메서드를 사용하지 않는 경우표기법은 다음과 같습니다.변수에 붙는 경우: static &amp;lt;변수 타입&amp;gt; &amp;lt;변수명&amp;gt;ex) static int a = 5;메서드에 붙는 경우: &amp;lt;접근 제어자&amp;gt; static &amp;lt;리턴 타입&amp;gt; &amp;lt;변수명&amp;gt;ex) public static void main() {}-&amp;gt; 타입 앞에 붙는다메서드 오버로딩한 클래스 내에 같은 이름의 메서드를 여러 개 정의하는 것을 메서드 오버로딩 또는 간단히 오버로딩이라고 합니다. 오버로딩이 성립하기 위해서는 다음과 같은 조건을 만족해야 합니다.  메서드 이름이 같아야 한다  매개변수의 개수 또는 타입이 달라야 한다  반환 타입은 관계없다오버로딩을 사용하는 경우는 같은 기능을 하지만 매개변수의 개수나 타입이 달라질 때입니다.인스턴스 초기화 메서드: 생성자생성자는 인스턴스가 생성될 때 가장 먼저 호출되는 인스턴스 초기화 메서드입니다. 인스턴스 초기화는 인스턴스 변수들을 초기화하는 것을 뜻합니다.생성자 역시 메서드처럼 클래스내에 선언되며, 구조도 메서드와 유사하지만 리턴값이 없다는 점이 특징입니다. 그렇다고 void를 사용하는 것은 아니고 그냥 아무것도 적지 않으면 됩니다.생성자를 작성하는 규칙은 다음과 같습니다.  생성자의 이름은 클래스의 이름과 같아야 한다  생성자는 리턴 값이 없다생성자의 특징은 다음과 같습니다.  생성자도 오버로딩이 가능하기 때문에 하나의 클래스에 여러개의 생성자가 존재할 수 있다  기본(default) 생성자가 있어서 정의하지 않아도 사용할 수 있다  새로운 생성자를 추가한 경우, 기본 생성자도 다시 정의해줘야 한다생성자는 다음과 같은 경우에 정의합니다.  인스턴스 변수의 초기화가 필요한 경우  필요하지 않으면 컴파일러가 제공핳는 기본 생성자를 사용하면 된다class Person {    // 인스턴스 변수    String name;    int age;    // 매개 변수가 없으면 기본 생성자    Person() {}     // 매개 변수가 있는 생성자    Person(String name, int age) {        this.name = name;        this.age = age;    }    public static void main(String[] args) {        Person p1 = new Person();        Person p2 = new Person(&quot;Peter&quot;, 25);        System.out.println(&quot;p1의 이름: &quot; + p1.name + &quot;, p1의 나이: &quot; + p1.age); //p1의 이름: null, p1의 나이: 0        System.out.println(&quot;p2의 이름: &quot; + p2.name + &quot;, p2의 나이: &quot; + p2.age); //p2의 이름: Peter, p2의 나이: 25    }}생성자에서 this()를 사용해 다른 생성자를 호출할 수도 있습니다. 필수적인 것은 아니고 코드가 조금 더 간결해지기 때문에 사용합니다.위의 예시에서 기본 생성자가 사용될 경우 이름이 String 객체의 기본값인 null로 초기화되어 있는 것을 확인할 수 있습니다. 이렇게 매개변수가 없을 때 name의 초기값을 no-name으로 하고 싶다고 가정해보겠습니다.Person() {  name = &quot;no-name&quot;;  age = 0;}이런 식으로 작성할 수도 있지만 this()를 사용해 조금 더 간결하게 나타낼 수 있습니다.Person() {  // Person 클래스의 Person(String name, int age) 생성자를 this()로 호출  this(&quot;no-name&quot;, 0)}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-17T21:01:35+09:00'>17 May 2022</time><a class='article__image' href='/java-series9'> <img src='/images/java_logo.png' alt='Java Series [Part9]: 자바의 객체지향 프로그래밍(1)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series9'>Java Series [Part9]: 자바의 객체지향 프로그래밍(1)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part8]: 정규표현식(1) 문자 한 개와 매치",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series8",
      "date"     : "May 17, 2022",
      "content"  : "Table of Contents  정규표현식의 기초          대괄호: []      마침표: .      단어 문자: \w, 비 단어 문자: \W      숫자 문자: \d, 비 숫자 문자: \D      공백 문자: \s, 비 공백 문자: \S      단어 경계: \b, 비 단어 경계: \B      문자열 시작: ^      문자열 끝: $        참고정규표현식의 기초정규표현식은 문자열 안에서 특정 패턴을 매칭하도록 도와줍니다. 정규표현식을 이용하면 단순한 패턴부터, 복잡한 패턴까지 수식화할 수 있습니다. 정규표현식을 배우면 문자열 내에서 html 태그 제거, 주민등록번호 뒷자리 가리기, 특수문자 제거, 전화번호 표기 통일 등 다양한 방법으로 문자열 데이터의 품질을 향상시킬 수 있습니다.정규표현식 안에서 메타문자를 제외한 모든 문자 하나는 일반 문자열 하나와 매칭됩니다.  (메타문자는 리터럴 문자와 다르게 어떤 기능을 가진 문자를 말합니다)대괄호: []~중에 문자 1개[] 사이에 원하는 문자를 여러 개 넣으면, 문자열이 넣은 문자 중 하나와 일치하면 매칭이 이루어집니다. 여기서 중요한 것은 무조건 딱 한 문자와 일치된다는 것입니다.위에서 정규표현식이 [ab]일 때, a에도 매칭되고, b에도 매칭되는데 중요한 것은 ab가 아니라 a 또는 b 문자 한 개라는 것입니다.그리고 대괄호 안에서는 메타문자 역할을 하는 것은 오직 \, ^, -, ] 4개뿐입니다.\ : 대괄호 안에서 메타문자 역할을 하는 \, ^, -, ]가 리터럴 문자가 되도록합니다^ : NOT 기능을 합니다- : 범위 기능을 합니다 (알파벳 소문자: a-z, 알파펫: a-zA-Z)마침표: .아무 문자 1개단어 문자: \w, 비 단어 문자: \W\w : 단어 문자 1개와 일치됩니다. (단어 문자는 영문 대소문자, 숫자 0-9, 언더바 ‘_’ 를 포함)\W : \w와 반대로 매칭됩니다. (특수문자, 공백문자)숫자 문자: \d, 비 숫자 문자: \D\d : 숫자 문자 1개와 일치됩니다.\D : 비 숫자 문자 1개와 일치됩니다.공백 문자: \s, 비 공백 문자: \S\s : 스페이스, 탭, 개행 문자 1개와 일치됩니다\S : 공백 문자가 아닌 문자 1개와 일치됩니다.단어 경계: \b, 비 단어 경계: \B\b : 문자열과 비문자열 사이 경계, 문자열 시작 또는 끝과 일치됩니다\B : 문자열과 문자열 사이 경계, 비문자열과 비문자열 사이 경계와 일치됩니다문자열 시작: ^문자열 끝: $참고  greeksharifa, 파이썬 정규표현식(re) 사용법  regexone: 정규표현식 문제  regexr: 정규표현식 테스트  regexper: 정규표현식 시각화  프로그래머스: 정규표현식 문제  백준: 정규표현식 문제",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-17T21:01:35+09:00'>17 May 2022</time><a class='article__image' href='/data-engineering-series8'> <img src='/images/regex_logo.png' alt='Data Engineering Series [Part8]: 정규표현식(1) 문자 한 개와 매치'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series8'>Data Engineering Series [Part8]: 정규표현식(1) 문자 한 개와 매치</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part7]: CDC(Change Data Capture)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series7",
      "date"     : "May 17, 2022",
      "content"  : "Table of Contents  CDC          CDC를 구현하는 방법                  컬럼 검사          테이블 델타          로그 기반                      Debezium  참고관계형 데이터베이스에 캡처된 비즈니스 트랜잭션은 비즈니스 운영 상태를 이해하는 데 매우 중요합니다. 데이터의 가치는 시간이 지남에 따라 빠르게 떨어지기 때문에 조직에서는 데이터가 생성될 때 이를 분석할 수 있는 방법이 필요합니다. 운영 데이터베이스의 중단을 방지하기 위해 기업은 일반적으로 분석을 위해 데이터를 데이터 웨어하우스에 복제합니다.기존에는 배치 기반 방식을 사용하여 하루에 한 번 또는 여러 번 데이터를 이동했습니다. 그러나 배치 이동은 지연 시간을 유발하고 조직에 운영 가치를 감소시킵니다.CDC(Change Data Capture)는 관계형 데이터베이스에서 데이터 웨어하우스, 데이터 레이크 또는 기타 데이터베이스로 데이터를 거의 실시간으로 이동하기 위한 이상적인 솔루션으로 부상했습니다. 이 게시물에서는 거의 실시간에 가까운 비즈니스 인텔리전스 및 클라우드 마이그레이션에 Change Data Capture가 이상적인 이유와 4가지 다른 Change Data Capture 방법에 대해 알아보겠습니다.CDCCDC는 데이터베이스에서 데이터에 대한 변경 사항을 식별하고 추적하는 소프트웨어 프로세스입니다. CDC는 새로운 데이터베이스 이벤트가 발생할 때 지속적으로 데이터를 이동하고 처리하여 실시간 또는 거의 실시간에 가까운 데이터 이동을 제공합니다.시간에 민감한 의사 결정이 내려지는 고속 데이터 환경에서 Change Data Capture는 지연 시간이 짧고 안정적이며 확장 가능한 데이터 복제를 달성하는 데 매우 적합합니다.2025년까지 80% 이상의 기업이 멀티 클라우드 전략을 구현할 계획을 세우고 있는 가운데, 여러 환경에서 데이터를 복제해야 하는 상황에서 비즈니스에 적합한 CDC 방법을 선택하는 것이 그 어느 때보다 중요합니다.CDC를 구현하는 방법컬럼 검사By using existing “LAST_UPDATED” or “DATE_MODIFIED” columns, or by adding them if not available in the application, you can create your own change data capture solution at the application level. This approach retrieves only the rows that have been changed since the data was last extracted.The CDC logic for the technique would be:Step 1: Get the maximum value of both the target (blue) table’s ‘Created_Time’ and ‘Updated_Time’ columnsStep 2: Select all the rows from the data source with ‘Created_Time’ greater than (&amp;gt;) the target table’s maximum ‘Created_Time’ , which are all the newly created rows since the last CDC process was executed.Step 3: Select all rows from the source table that have an ‘Updated_Time’ greater than (&amp;gt;) the target table’s maximum ‘Updated_Time’ but less than (&amp;lt;) its maximum ‘Created_Time’. The reason for the exclusion of rows less than the maximum target create date is that they were included in step 2.Step 4: Insert new rows from step 2 or modify existing rows from step 3 in the target.테이블 델타SQL문을 사용해서 소스 테이블과 타겟 테이블간의 테이블 델타(table delta 또는 tablediff)를 타겟 테이블에 적용(apply 또는 propagation)(Ways to compare and find differences for SQL Server tables and data 참고)  장점          변경된 데이터를 정확히 볼 수 있다        단점          데이터, 이전 스냅샷, 현재 스냅샷을 모두 저장하고 있어야 해서 용량 측면에서 비효율적      데이터가 커질수록 변경된 부분 캡처하는데 CPU 자원 사용량이 선형적으로 증가      캡처하는데 지연이 발생하기 때문에 실시간 데이터 환경에서 활용도 떨어짐      로그 기반데이터베이스에는 충돌 시 데이터베이스를 복구할 수 있는 모든 데이터베이스 이벤트를 저장하는 트랜잭션 로그(redo 로그라고도 함)가 있습니다. 로그 기반 변경 데이터 캡처를 사용하면 삽입, 업데이트 및 삭제를 포함한 새로운 데이터베이스 트랜잭션이 원본 데이터베이스의 기본 트랜잭션 로그에서 읽힙니다.(Log-Based Change Data Capture: the Best Method for CDC 참고)애플리케이션 수준을 변경하지 않고 운영 테이블을 검색하지 않아도 변경 사항이 캡처됩니다. 이러한 변경 사항은 워크로드를 추가하고 소스 시스템의 성능을 저하시킵니다.  장점          프로덕션 데이터베이스 시스템에 미치는 영향 최소화      각 트랜잭션에 대해 추가 쿼리 필요 없음      여러 시스템에 걸쳐 ACID 신뢰성을 유지할 수 있습니다.      프로덕션 데이터베이스 시스템의 스키마를 변경할 필요가 없거나 테이블을 추가할 필요가 없음        어려운 점          데이터베이스의 내부 로깅 형식을 구문 분석하는 것은 복잡합니다. 대부분의 데이터베이스는 형식을 문서화하지 않으며 새로운 릴리스에서 데이터베이스의 변경 사항을 알리지 않습니다. 이 경우 새 데이터베이스 릴리스마다 데이터베이스 로그 구문 분석 논리를 변경해야 할 수 있습니다.      원본 데이터베이스 변경 이벤트 메타데이터를 관리하는 시스템이 필요합니다.      검색 가능한 트랜잭션 로그를 생성하는 데 필요한 추가 로그 레벨로 인해 한계 성능 오버헤드가 추가될 수 있음      Every database supporting transactions first writes any changes (inserts, updates, and deletes) to a database log before writing it to the database. This is done to assure the integrity of transactions from unexpected occurrences like power failure etc., while the transaction is still inflight.The database logs cycle between active log (redo log) and archive logs based on file size or time interval events. Depending on the target database’s data latency requirements, the CDC tool can access the active log or archive logs for the source database.Since the CDC tool only reads from the source database logs, it does not add to the source database load. Moreover, the changelog is typically many orders of magnitude smaller than the source table. Such a flow of such a changelog over the network does not cause any network load issues.Since database logs only contain information about the most recent inserts, updates, and deletes, changelog calculation becomes a trivial undertaking. However, since there are many database vendors, each with its own (sometimes proprietary) log format, such tasks are best left to commercial CDC tool vendors.DebeziumAnother option for implementing a CDC solution is by using the native database logs that both MySQL and Postgres can produce when configured to do so. These database logs record every operation that is executed against the database which can then be used to replicate these changes in a target system.The advantage of using database logs is that firstly, you don’t need to write any code or add any extra logic to your tables as you do with update timestamps. Second, it also supports deletion of records, something that isn’t possible with update timestamps.In MySQL you do this by turning on the binlog and in Postgres, you configure the Write Ahead Log (WAL) for replication. Once the database is configured to write these logs you can choose a CDC system to help capture the changes. Two popular options are Debezium and Amazon Database Migration Service (DMS). Both of these systems utilise the binlog for MySQL and WAL for Postgres.Debezium works natively with Kafka. It picks up the relevant changes, converts them into a JSON object that contains a payload describing what has changed and the schema of the table and puts it on a Kafka topic. This payload contains all the context required to apply these changes to our target system, we just need to write a consumer or use a Kafka Connect sink to write the data. As Debezium uses Kafka, we get all the benefits of Kafka such as fault tolerance and scalability.AWS DMS works in a similar way to Debezium. It supports many different source and target systems and integrates natively with all of the popular AWS data services including Kinesis and Redshift.The main benefit of using DMS over Debezium is that it’s effectively a “serverless” offering. With Debezium, if you want the flexibility and fault tolerance of Kafka, you have the overhead of deploying a Kafka cluster. DMS as its name states is a service. You configure the source and target endpoints and AWS takes care of handling the infrastructure to deal with monitoring the database logs and copying the data to the target.However, this serverless approach does have its drawbacks, mainly in its feature set.When weighing up which pattern to follow it’s important to assess your specific use case. Using update timestamps works when you only want to capture inserts and updates, if you already have a Kafka cluster you can get up and running with this very quickly, especially if most tables already include some kind of update timestamp.If you’d rather go with the database log approach, maybe because you want exact replication then you should look to use a service like Debezium or AWS DMS. I would suggest first checking which system supports the source and target systems you require. If you have some more advanced use cases such as masking sensitive data or re-routing data to different queues based on its content then Debezium is probably the best choice. If you’re just looking for simple replication with little overhead then DMS will work for you if it supports your source and target system.If you have real-time analytics needs, you may consider using a target database like Rockset as an analytics serving layer. Rockset integrates with MySQL and Postgres, using AWS DMS, to ingest CDC streams and index the data for sub-second analytics at scale. Rockset can also read CDC streams from NoSQL databases, such as MongoDB and Amazon DynamoDB.The right answer depends on your specific use case and there are many more options than have been discussed here, these are just some of the more popular ways to implement a modern CDC system.참고  striim: Change Data Capture (CDC): What it is and How it Works  Farhan Siddiqui, Change Data Capture(CDC) for Data Lake Data Ingestion  How to Implement CDC for MySQL and Postgres  /blog What are the Different Methods of Change Data Capture (CDC)?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-17T21:01:35+09:00'>17 May 2022</time><a class='article__image' href='/data-engineering-series7'> <img src='/images/cdc_logo.png' alt='Data Engineering Series [Part7]: CDC(Change Data Capture)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series7'>Data Engineering Series [Part7]: CDC(Change Data Capture)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part8]: 자바의 애너테이션",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series8",
      "date"     : "May 16, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-16T21:01:35+09:00'>16 May 2022</time><a class='article__image' href='/java-series8'> <img src='/images/java_logo.png' alt='Java Series [Part8]: 자바의 애너테이션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series8'>Java Series [Part8]: 자바의 애너테이션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part7]: 자바의 제네릭",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series7",
      "date"     : "May 16, 2022",
      "content"  : "Table of Contents  제네릭(Generic)  제네릭 클래스  제네릭 메서드제네릭(Generic)제네릭은 메서드나 클래스 안에서 사용되는 객체의 타입을 지정하고 컴파일 시에 체크하도록 하는 기능을 합니다.이렇게 하면 String 타입을 받는 메서드, Integer 타입을 받는 메서드를 각각 따로 정의할 필요가 없고, 제네릭 메서드 하나로 정의할 수 있습니다.또 타입을 지정함으로써 컴파일 시점에 타입 체크 기능도 해줄 수 있습니다. (오류 중에 가장 좋은 오류는 컴파일 오류)제네릭의 장점- 타입을 메서드나 클래스의 외부에서 동적으로 지정할 수 있다- 지정한 타입만 사용한다는 보장이 있기 때문에 별다른 형변환이 필요 없어진다제네릭 사용 예시를 보도록 하겠습니다.import java.util.ArrayList;class Test {    public static void main(String[] args) {        ArrayList&amp;lt;String&amp;gt; list1 = new ArrayList&amp;lt;String&amp;gt;();        // 또는 ArrayList&amp;lt;String&amp;gt; list1 = new ArrayList&amp;lt;&amp;gt;();                list1.add(&quot;1&quot;);        list1.add(1); // 컴파일 오류    }}import java.util.HashMap;class Test {    public static void main(String[] args) {        HashMap&amp;lt;Integer, String&amp;gt; map1 = new HashMap&amp;lt;&amp;gt;();        map1.put(1, &quot;apple&quot;);        map1.put(&quot;1&quot;, &quot;value2&quot;); // 컴파일 오류    }}class Car {    String brand;    int year;    void start() {        System.out.println(&quot;차가 출발합니다&quot;);    }    void stop() {        System.out.println(&quot;차가 멈춥니다&quot;);    }}class Driver&amp;lt;C&amp;gt; {    C car = new C(); // type parameter &#39;C&#39; cannot be instantiated directly    void driveStart() {        car.start();    }    void driveStop() {        car.stop();    }}class Test {    public static void main(String[] args) {        Driver&amp;lt;Car&amp;gt; d = new Driver&amp;lt;&amp;gt;();        d.driveStart();        d.driveStop();    }}제네릭 클래스제네릭 메서드",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-16T21:01:35+09:00'>16 May 2022</time><a class='article__image' href='/java-series7'> <img src='/images/java_logo.png' alt='Java Series [Part7]: 자바의 제네릭'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series7'>Java Series [Part7]: 자바의 제네릭</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part6]: 자바의 컬렉션 프레임웍",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series6",
      "date"     : "May 15, 2022",
      "content"  : "Table of Contents  컬렉션 프레임웍  컬렉션 프레임웍의 핵심 인터페이스          List      Set      Map        몇 가지 중요한 구현클래스          ArrayList      HashMap      컬렉션 프레임웍JAVA API문서에서는 컬렉션 프레임웍을 ‘데이터 군(group)을 다루고 표현하기 위한 단일화된 구조’라고 정의하고 있습니다. 컬렉션 프레임웍은 컬렉션, 다수의 데이터를 다루는데 필요한 다양하고 풍부한 클래스들을 제공합니다.컬렉션 프레임웍의 핵심 인터페이스컬렉션 프레임웍에서는 컬렉션(데이터 그룹)을 크게 List, Set, Map이라는 3가지 인터페이스를 정의함으로써 컬렉션을 다루는데 필요한 기능을 제공하고 있습니다. 그리고 List와 Set 인터페이스의 공통된 부분을 뽑아서 새로운 인터페이스인 Collection을 추가로 정의하였습니다.List  순서가 있는 데이터의 집합으로 데이터 중복을 허용  구현클래스: ArrayList, LinkedList, Stack 등Set  순서를 유지하지 않는 데이터의 집합, 데이터의 중복을 허용하지 않음  구현클래스: HashSet, TreeSet 등Map  키(Key)와 값(Value)으로 이루어진 쌍을 데이터로 갖는 집합  구현클래스: HashMap, TreeMap, Hashtable 등몇 가지 중요한 구현클래스ArrayListArrayList list1 = new ArrayList();list1.add(new Integer(5));list1.add(new Integer(3));HashMapHashMap map = new HashMap();map.put(&quot;key1&quot;, &quot;value1&quot;);map.put(&quot;key2&quot;, &quot;value2&quot;);",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-15T21:01:35+09:00'>15 May 2022</time><a class='article__image' href='/java-series6'> <img src='/images/java_logo.png' alt='Java Series [Part6]: 자바의 컬렉션 프레임웍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series6'>Java Series [Part6]: 자바의 컬렉션 프레임웍</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Recruitment Data Pipeline",
      "category" : "",
      "tags"     : "",
      "url"      : "/project-clickstream-analysis",
      "date"     : "May 15, 2022",
      "content"  : "프로젝트 목표  다양한 곳에서 웹서비스 형태로 제공되는 채용관련 정보를 데이터로 모아 검색/시각화 형태로 서비스를 제공하고자 한다  이 프로젝트는 크게 두 단계로 계획하고 있다          1단계: 개발자 모드 - 코드 실행으로 검색/시각화 구현      2단계: 데모 모드 - 간단한 프론트엔드를 통해 검색/시각화 서비스 제공      프로젝트를 시작하게 된 이유프로젝트 일지노션 링크프로젝트 코드Photo by Sam Bark on Unsplash",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-15T00:00:00+09:00'>15 May 2022</time><a class='article__image' href='/project-clickstream-analysis'> <img src='/images/recruitment_1.jpeg' alt='Recruitment Data Pipeline'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/project-clickstream-analysis'>Recruitment Data Pipeline</a> </h2><p class='article__excerpt'>크롤링을 통해 채용 관련 데이터를 수집하여 검색/시각화 서비스 제공</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part12] MySQL 서버 설치, 설정, 사용자 및 권한 관리",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series12",
      "date"     : "May 13, 2022",
      "content"  : "Table of Contents  참고참고  Real MySQL 8.0 (1권) 책  How to Import CSV File into MySQL Table in 4 Different Ways  Ubuntu 18.04에 MySQL 설치하기 (feat. docker)  Docker : run ufw, iptables command in docker container  [MySQL] sql_mode로 알아보는 시스템 변수 permanent, runtime설정  리눅스 MySQL 시작, 정지, 재시작, 상태확인  MySQL refuses to accept remote connections  [문제해결] mysql 접속에러  [우분투] MySql 원격 접속 허용하기 – PhpMyAdmin 사용    ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-13T21:01:35+09:00'>13 May 2022</time><a class='article__image' href='/mysql-series12'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part12] MySQL 서버 설치, 설정, 사용자 및 권한 관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series12'>MySQL Series [Part12] MySQL 서버 설치, 설정, 사용자 및 권한 관리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part11]: 카프카를 공부하면서 궁금한 점들",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series11",
      "date"     : "May 11, 2022",
      "content"  : "Table of Contents  Connect          프로듀서/컨슈머의 설정값과 커넥터의 관계        참고Connect프로듀서/컨슈머의 설정값과 커넥터의 관계프로듀서와 컨슈머를 커넥터로 연결하고 나면 딱히 건드릴게 없다. 근데 생각해보면 프로듀서와 컨슈머 각각 설정할 configuration들이 굉장히 많다. 그러면 그런것들은 각각의 커넥터를 REST API로 등록할 때 바꿀 수 있는 것인가? devidea: [Kafka] Connector-level producer/consumer configuration 글을 보면 그런 것 같은데 아직 Confuent 쪽에서는 커넥터에 관해 이런 Configuration을 커스텀하도록 fully 지원하지는 않는 것 같기도 하다. 아마 예를 들어 MongoDB, MySQL 등 각각의 프로듀서/컨슈머별로 지원해야 하는 특성들을 자기들 생각에는 잘 설정해놓았기 때문에 사용자들이 직접 건드릴 필요가 없다고 생각해서 그런 건가?참고  JDBC Sink Connector Configuration Properties  [Kafka] Sink Connector 생성  JDBC Connector (Source and Sink) for Confluent Platform  [Kafka] Kafka Connect 개념/예제  [Kafka] Kafka Connect - JDBC Connector 예제  How to save a DataFrame to MySQL in PySpark  devidea: [Kafka] Connector-level producer/consumer configuration",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-05-11T21:01:35+09:00'>11 May 2022</time><a class='article__image' href='/kafka-series11'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part11]: 카프카를 공부하면서 궁금한 점들'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series11'>Kafka Series [Part11]: 카프카를 공부하면서 궁금한 점들</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series6",
      "date"     : "Apr 21, 2022",
      "content"  : "Table of Contents  데이터 파이프라인 설계데이터 파이프라인 설계https://hevodata.com/learn/connect-kafka-to-s3/https://swalloow.github.io/kafka-connect/https://data-engineer-tech.tistory.com/34https://www.confluent.io/ko-kr/blog/apache-kafka-to-amazon-s3-exactly-once/https://stackoverflow.com/questions/46731746/move-data-from-postgres-mysql-to-s3-using-airflowhttps://reddit.fun/161619/import-data-from-bucket-mysql-database-using-connectivity",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-21T21:01:35+09:00'>21 Apr 2022</time><a class='article__image' href='/data-engineering-series6'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series6'>Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series6",
      "date"     : "Apr 17, 2022",
      "content"  : "Table of Contents  도커 컴포즈로 컨테이너 띄우기  spark-client 컨테이너에서 pyspark 셸을 실행  pyspark 셸에서 몽고DB와 연결  참고도커 컴포즈로 컨테이너 띄우기version: &#39;3.2&#39;services:    spark-client:      image: kimziont/spark:1.0      hostname: spark-client      depends_on:        - spark-master      command:         - bash        - -c        - |          apt -y install python-is-python3          sleep infinity    spark-master:      image: kimziont/spark-master:1.0      hostname: spark-master      ports:        - &quot;4041:8080&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-master.sh &amp;amp;          sleep infinity    spark-worker1:      image: kimziont/spark-worker:1.0      hostname: worker1      depends_on:        - spark-master      ports:        - &quot;4042:8081&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;          sleep infinity    spark-worker2:      image: kimziont/spark-worker:1.0      hostname: worker2      depends_on:        - spark-master      ports:        - &quot;4043:8081&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;          sleep infinity        mongodb:      image: mongo:latest      hostname: mongodb      ports:        - &quot;27017:27017&quot;      environment:        MONGO_INITDB_ROOT_USERNAME: root        MONGO_INITDB_ROOT_PASSWORD: root      tty: true  마스터의 UI는 디폴트로 8080포트로 보여준다, 워커는 8081포트이다  워커들은 마스터의 7077포트로 연결될 수 있다docker compose upspark-client 컨테이너에서 pyspark 셸을 실행./bin/pyspark --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 --jars /spark/mysql-connector-java-8.0.29.jarpyspark 셸에서 몽고DB와 연결from pyspark.sql import SparkSessionspark = SparkSession.builder.master(&#39;spark://spark-master:7077&#39;).config(&#39;spark.mongodb.input.uri&#39;, &#39;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&#39;).getOrCreate()df = spark.read.format(&quot;mongo&quot;).option(&quot;uri&quot;, &quot;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&quot;).load()df.show()spark = SparkSession.builder.master(&#39;spark://spark-master:7077&#39;).config(&#39;spark.mongodb.input.uri&#39;, &#39;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&#39;).config(&quot;spark.jars&quot;, &quot;/spark/mysql-connector-java-8.0.29.jar&quot;).master(&quot;spark-master:7077&quot;).getOrCreate()참고  MongoDB 공식문서: Connection String URI Format  MongoDB 공식문서: Connection String URI Format: authSource  Error connecting from pyspark to mongodb with password",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-17T21:01:35+09:00'>17 Apr 2022</time><a class='article__image' href='/spark-series6'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series6'>Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part11] 옵티마이저를 이용한 실행 최적화",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series11",
      "date"     : "Apr 15, 2022",
      "content"  : "Table of Contents  옵티마이저          ORDER BY 처리                  정렬 처리 방법                    GROUP BY 처리      DISTINCT 처리        실행 계획          실행 계획 분석        참고옵티마이저옵티마이저: 쿼리를 최적으로 실행하기 위해 각 테이블의 데이터가 어떤 분포로 저장돼 있는지 통계 정보를 참조하여 최적의 실행 계획을 수립쿼리 실행 절차  MySQL 엔진의 SQL파서에서 SQL을 트리 형태로 파싱한다  MySQL 엔진의 옵티마이저에서 파싱 정보를 확인하면서 통계 정보를 활용해 어떤 인덱스를 이용해 테이블을 읽을지 선택한다  2 단계가 완료되면 실행 계획이 만들어진다  스토리지 엔진에 실행 계획대로 레코드를 읽어오도록 요청한다  MySQL 엔진의 SQL 실행기가 스토리지 엔진으로부터 받아온 레코드를 조인하거나 정렬하는 작업을 수행한다ORDER BY 처리  대부분의 SELECT 쿼리에서 정렬은 필수적  정렬을 처리하는 방법은 인덱스를 이용하는 방법과 Filesort라는 별도의 처리를 이용하는 방법            방법      장점      단점              인덱스 이용      SELECT 문을 실행할 때 이미 인덱스가 정렬돼 있어 순서대로 읽기만 하면 되므로 매우 빠르다      INSERT, UPDATE, DELETE 작업시 부가적인 인덱스 추가/삭제 작업이 필요하므로 느리다              Filesort 이용      인덱스 이용과 반대로 INSERT, UPDATE, DELETE 작업이 빠르다      정렬 작업이 쿼리 실행 시 처리되어 쿼리의 응답 속도가 느려진다      Filesort를 사용해야 하는 경우  정렬 기준이 너무 많아서 모든 인덱스를 생성하는 것이 불가능한 경우  어떤 처리의 결과를 정렬해야 하는 경우  랜덤하게 결과 레코드를 가져와야 하는 경우소트 버퍼  MySQL은 정렬을 수행하기 위해 별도의 메모리 공간을 할당받아서 사용하는데 이 메모리 공간을 소트 버퍼라고 한다  정렬해야 할 레코드의 건수가 소트 버퍼의 크기보다 크다면 어떻게 해야 할까?          정렬해야 할 레코드를 여러 조각으로 나눠서 처리하게 됨. 이 과정에서 임시 저장을 위해 디스크를 사용      일부를 처리하고 디스크에 저장하기를 반복 수행함      정렬 알고리즘  정렬 대상 컬럼과 프라이머리 키만 가져와서 정렬하는 방식          정렬 대상 컬럼과 프라이머리 키 값만 소트 버퍼에 담아 정렬을 수행      그리고 다시 정렬 순서대로 프라이머리 키로 테이블을 읽어서 SELECT할 컬럼을 가져옴      가져오는 컬럼이 두 개 뿐이라 소트 버퍼에 많은 레코드를 한 번에 읽어올 수 있음      단점은 테이블을 두 번 읽어야 함        정렬 대상 컬럼과 SELECT문으로 요청한 컬럼을 모두 가져와서 정렬하는 방식          최신 버전의 MySQL에서 일반적으로 사용하는 방식      SELECT 문에서 요청한 컬럼의 개수가 많아지면 계속 분할해서 소트 버퍼에 읽어와야함      레코드의 크기나 건수가 작은 경우 성능이 좋음      정렬 처리 방법  인덱스를 사용한 정렬          인덱스를 이용해 정렬을 하기 위해서는 반드시 ORDER BY의 순서대로 생성된 인덱스가 있어야 함      인덱스를 이용해 정렬이 가능한 이유는 B-Tree 인덱스가 키 값으로 정렬되어 있기 때문        Filesort를 사용한 정렬          인덱스를 사용할 수 없는 경우, WHERE 조건에 일치하는 레코드를 검색해 정렬 버퍼에 저장하면서 정렬을 처리(FIlesort)함      GROUP BY 처리  GROUP BY에 사용된 HAVING절은 인덱스를 사용해서 처리될 수 없으므로 굳이 튜닝하려고 할 필요 없다  GROUP BY 처리는 인덱스를 사용할 수 있는 경우와 그렇지 못한 경우가 있음  인덱스가 있으면 인덱스를 차례대로 읽으면서 그루핑 작업을 수행DISTINCT 처리  DISTINCT는 SELECT하는 레코드를 유니크하게 SELECT 하는 것이지, 특정 컬럼만 유니크하게 조회하는 것이 아님  SELECT DISTINCT(first_name), last_name FROM employees;          DISTINCT는 함수가 아니라서 위처럼 괄호를 해놓아도 그냥 무시함      그래서 결론적으로 first_name과 last_name의 조합이 유니크한 레코드를 가져오게 됨        집합 함수(COUNT(), MIN(), MAX()) 같은 집합 함수 내에서 DISTINCT 키워드가 사용된 경우는 함수의 인자로 전달된 컬럼값이 유니크한 것들을 가져온다실행 계획실행 계획을 이해하고 실행 계획의 불합리한 부분을 찾아내고, 더 최적화된 방법으로 실행 계획을 수립하도록 유도하는 법을 배우자옵티마이저가 사용자의 개입 없이 항상 좋은 실행 계획을 만들어낼 수 있는 것은 아니다사용자가 보완할 수 있도록 EXPLAIN 명령으로 옵티마이저가 수립한 실행 계획을 확인할 수 있게 해준다.실행 계획 출력EXPLAINSELECT문쿼리의 실행 계획과 단계별 소요된 시간 정보 출력EXPLAIN ANALYZESELECT문실행 계획에 가장 큰 영향을 미치는 것은 통계 정보통계 정보  테이블의 전체 레코드 수  인덱스된 컬럼이 가지는 유니크한 값의 개수  각 컬럼의 데이터 분포도(히스토그램)실행 계획 분석EXPLAINSELECT *FROM employees AS eINNER JOIN salaries AS sON s.emp_no=e.emp_noWHERE first_name=&#39;ABC&#39;;             id      select_type      table      partitions      type      possible_keys      key      key_len      ref      rows      filtered      Extra              1      SIMPLE      e      NULL      ref      PRIMARY,ix_firstname      ix_firstname      58      const      1      100.00      NULL              1      SIMPLE      s      NULL      ref      PRIMARY      PRIMARY      4      employees.e.emp_no      10      100.00      NULL      표의 각 라인은 쿼리 문장에서 사용된 테이블의 개수만큼 출력된다. 실행 순서는 위에서 아래로 순서대로 표시된다  id 컬럼          SELECT 쿼리별로 부여되는 식별자 값      SELECT 문장은 하나인데, 여러 개의 테이블이 조인되는 경우에는 id값이 증가하지 않고 같은 id 값이 부여된다      테이블 접근 순서와 무관        select_type 컬럼          각 단위 SELECT 쿼리가 어떤 타입의 쿼리인지 표시되는 컬럼      표시될 수 있는 값은 SIMPLE, PRIMARY, UNION, DEPENDENT UNION, SUBQUERY 등      참고  Real MySQL 8.0 (1권) 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-15T21:01:35+09:00'>15 Apr 2022</time><a class='article__image' href='/mysql-series11'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part11] 옵티마이저를 이용한 실행 최적화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series11'>MySQL Series [Part11] 옵티마이저를 이용한 실행 최적화</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part10] MySQL의 로그",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series10",
      "date"     : "Apr 14, 2022",
      "content"  : "Table of Contents  Log의 중요성  MySQL의 로그  MySQL 모니터링을 위한 로그          General Query Log      Slow Query Log      Error Log      Binary Log        트랜잭션 처리를 위한 로그          Undo Log      Redo Log      Binary Log        Redo log와 Binary log  Update문 실행 예제  참고Log의 중요성MySQL은 오픈 소스 관계형 데이터베이스이며 효율성과 보안을 향상시키기 위해 MySQL 데이터베이스 로그를 사용하는 방법을 배워야 한다. 장기적으로 MySQL 인스턴스의 성능을 진단하고 모니터링하는 방법을 이해하는 것이 중요하다.MySQL을 서비스 단계에서 사용하다 보면 슬로우 쿼리(slow query), 데드락(deadlock), 커넥션 실패(aborted connection)과 같은 문제를 마주하게 된다. 로깅은 이러한 문제를 진단하는데 필수적이다. 또한 로그는 보안 문제를 진단하는데도 사용된다.MySQL의 로그MySQL에는 6가지의 로그 타입이 있다.  Redo log(WAL)  Undo log  Binary log  Error log  Slow query log  General log여기서 Redo log와 Undo log 그리고 Binary log는 트랜잭션 처리와 밀접한 관련이 있는 로그들이다. 다른 로그들은 MySQL 서버를 모니터링 하는데 도움이 되는 로그들이다.MySQL 모니터링을 위한 로그위에서 MySQL의 로그를 이용해 슬로우 쿼리, 데드락, 연결 실패, 보안과 관련한 문제들을 진단하는데 도움을 준다고 했다. MySQL의 로그를 모니터링함으로써 이러한 문제를 해결하고 성능을 향상시켜 보자.모니터링에 사용되는 로그들은 다음과 같다.- General Query Log- Slow Query Log- Error Log- Binary Log# 설정값 확인하기mysql&amp;gt; show variables;# Enable Logging on MySQLmysql&amp;gt;SET GLOBAL general_log = ‘ON’;# Log가 저장되는 파일 경로mysql&amp;gt;SET GLOBAL general_log_file = ‘path_on_your_system’;# Enable Logging on MySQLmysql&amp;gt;SET GLOBAL slow_query_log = ‘ON’;# Log가 저장되는 파일 경로mysql&amp;gt;SET GLOBAL slow_query_log_file = ‘path_on_your_system’;General Query LogGeneral query log는 서버가 요청받은 명령어들을 기록한다. 예를 들어 클라이언트의 연결/해제 부터해서 클라이언트가 요청한 SQL문도 기록한다. 이 로그를 통해 서버에 에러가 의심될 때 어떤 요청이 들어왔는지 알 수 있다.General query log에 기록되는 순서는 요청이 들어온 순으로 기록된다. 처리가 완료되지 않았더라도 일단 General query log에 기록된다. 이런 점 때문에 MySQL 서버의 장애를 디버깅하기에 좋은 로그가 된다.처리되지 않은 요청도 모두 로그에 기록되기 때문에, 로그 파일의 사이즈가 비교적 빨리 커진다. 그래서 만약 데이터에 변화를 준 요청만 기록된 로그를 보고 싶다면 Binary log를 사용하는 것이 낫다.Slow Query Log서비스의 규모가 커지게 되면, 빨랐던 쿼리들도 느려지는 일이 발생한다. 이렇게 느린 쿼리 문제를 디버깅할 때에 Slow query log를 사용하는 것이 좋은 출발점이 된다.Slow query log는 내가 설정한 기준(threshold) 시간을 넘어가는 쿼리 실행문을 저장한다. 디폴트는 10초이다.# threshold 값 설정하기SET GLOBAL long_query_time = 5.0;아래와 같은 쿼리를 실행해서 내가 설정한 대로 제대로 동작하는지 확인해 볼 수도 있다.SELECT SLEEP(7);대개 인덱스를 사용하지 못하는 쿼리들이 문제의 대상이 된다. 그래서 만약 log_queries_not_using_indexes 시스템 변수를 ON으로 하면 실행 시간에 상관없이 무조건 Slow query log에 인덱스를 사용하지 않는 실행문을 남길 수 있다.SHOW variables LIKE &#39;slow%&#39;;# 슬로우 로그 조회하기SELECT * FROM mysql.slow_log;# 또는SELECT start_time, user_host, query_time, lock_time, rows_sent, rows_examined, db, CONVERT(sql_text USING utf8 ) sql_textFROM mysql.slow_log;Error LogMySQL은 Error log를 사용하여 서버 시작 및 종료 시 그리고 서버가 실행되는 동안 발생하는 진단 메시지, 경고 및 메모를 기록합니다. Error log에는 MySQL 시작 및 종료 시간도 기록됩니다.Error log는 항상 활성화 되어 있습니다. 만약 Error log 파일 경로를 명시하지 않으면 콘솔에라도 출력하게 됩니다.보통 에러로그에 많이 저장되는 에러  Permission errors  Configuration errors  Out of memory errors  Errors with initiation or shutdown of plugins and InnoDBBinary LogBinary log는 데이터 또는 스키마에 변경을 일으킨 실행문만 기록한다. 예를 들어 INSERT, DELETE, UPDATE와 같은 실행문은 기록하고, SELECT, SHOW와 같은 실행문은 로그에 기록하지 않는다.  Binary 로그는 각 실행문이 실행되는데 얼마나 걸렸는지 기록되어 있다.Redo log는 트랜잭션 처리를 위한 InnoDB 엔진만의 특별한 존재이다. 그렇다면 다른 스토리지 엔진을 쓰는 경우는 어떻게 할까?그래서 스토리지 엔진에 상관없이 MySQL Server레벨에서 가지는 로그가 있는데 그게 바로 Binary log이다.Binary log는 General query log와 다르게, 실행문이 만들어낸 변화가 커밋되었을 떄 Binary log에 기록된다.Binary log는 말그대로 바이너리 포맷으로 저장된다, 그래서 Binary log를 읽으려면 mysqlbinlog 유틸리티를 사용해야 한다.아래는 Binary log binlog.0000001을 읽을 수 있는 방법을 소개한다.mysql&amp;gt; mysqlbinlog binlog.0000001MySQL은 Binary log가 로깅될 때 크게 3가지 포맷을 제공한다.  Statement-based logging: In this format, MySQL records the SQL statements that produce data changes. Statement-based logging is useful when many rows are affected by an event because it is more efficient to log a few statements than many rows.  Row-based logging: In this format, changes to individual rows are recorded instead of the SQL statements. This is useful for queries that require a lot of execution time on the source but result in just a few rows being modified.  Mixed logging: This is the recommended logging format. It uses statement-based logging by default but switches to row-based logging when required.런타임이나 복제가 진행 중일 때는 Binary log 포맷을 바꾸지 않는 것이 좋다. 아래는 바꾸는 코드다.SET GLOBAL binlog_format = &#39;STATEMENT&#39;;SET GLOBAL binlog_format = &#39;ROW&#39;;SET GLOBAL binlog_format = &#39;MIXED&#39;;트랜잭션 처리를 위한 로그데이터베이스는 보통 트랜잭션 수행을 위해 로그를 사용한다. 보통 이러한 역할을 수행하는 로그를 트랜잭션 로그라고 하며 MySQL의 경우 Redo Log, Undo Log, Binary Log가 이에 해당한다. Undo Log는 트랜잭션의 Atomicity, Redo Log는 Durability를 제공해준다.MySQL은 디스크의 I/O으로 인한 성능 저하를 줄이기 위해 캐싱 메커니즘을 사용한다.데이터베이스를 수정하는 쿼리가 들어오면, InnoDB는 먼저 메모리에 데이터가 있는지 확인하고, 없으면 디스크에서 불러와 메모리에 올리고 데이터를 수정한다. 이렇게 메모리에서만 계속 읽고 쓰게되면 장애로 서버가 종료될 때 데이터가 날아가게 된다. 이 문제를 해결하기 위해 MySQL에서는 Redo Log를 사용한다. 더 자세한 내용은 밑에서 살펴볼 것이다. 또한 데이터 수정 쿼리가 들어올 때마다 메모리의 데이터를 바로 수정하지는 않고, 이 전 값을 Undo Log에 보관해두고 수정을 가함으로써, 트랜잭션이 실패할 경우 데이터를 롤백(Roll back)할 준비를 한다. 이 내용도 밑에서 더 자세히 살펴보자.(참고로 디스크에 있는 파일중 데이터를 저장하는 파일을 데이터 파일, 로그를 저장하를 파일을 로그 파일이라고 함)(메모리에 있는 버퍼중 데이터 페이지를 캐시해놓는 위치를 데이터 버퍼, 로그를 캐시해놓는 위치를 로그 버퍼라고 함)(InnoDB엔진이 데이터를 읽고 쓸 때는 해당하는 데이터와 인덱스를 메모리 버퍼 풀에 올려둔다.)Memory Buffer pool: occupies the largest block of memory. The cache used to store various data includes index pages, data pages, undo pages, insert buffers, adaptive hash indexes, lock information stored in innodb, data dictionary information, etc. The working method always reads the database file into the buffer pool by page (16k per page), and then retains the cached data in the buffer pool according to the least recently used (lru) algorithm. If the database file needs to be modified, always modify the page in the buffer pool first (dirty page after the modification occurs), and then flush the dirty page of the buffer pool to the file at a certain frequency.Undo LogUndo log is to achieve atomicity of transactions. Undo Log is also used to implement multi-version concurrency control (referred to as: MVCC).  Undo log는 트랜잭션의 Atomicity 특성을 지키기 위한 로그  Undo log는 MVCC(multi-version concurrency control)를 위해서도 사용Undo log는 Undo log file이라는 파일이 따로 없고, Table space라는 공간의 일부인 Undo segment라는 곳에 저장된다.Delete/Update 작업의 내부 메커니즘Undo log의 원리는 굉장히 간단하다. 트랜잭션의 원자적 특성을 지키기 위해, 어떤 작업을 하려는 데이터는 그 전에 먼저 데이터를 Undo log에 백업해둔다. 그리고 난 후 Delete/Update한다. 만약 중간에 장애가 발생하거나, ROLLBACK이 일어나면 Undo log를 이용해 트랜잭션이 발생하기 전 데이터로 돌아갈 수 있다.예를 들어 A=1과 B=2인 데이터가 있을 떄 각각 +2를 하는 작업을 수행한다고 해보자.- Record A=1 to undo log. - Modify A=3. - Record B=2 to undo log. - Modify B=4.---------------------------------- - Write undo log to disk. - Write data to disk. 여기서 중요한 점은,  데이터를 변경하기 전에 먼저 Undo log에 기록해둔다.  데이터를 커밋하기 전에 먼저 Undo log를 Log buffer에서 Undo segment로 flush한다.Redo LogRedo log는 실행된 SQL문을 저장하기 위한 파일이다. 만약 MySQL 서버에 장애가 발생했다면 Redo log에 저장된 SQL문을 재실행 함으로써 원하는 데이터 상태를 다시 얻을 수 있다.The redo log exists as a separate file on the disk. There will be two files by default, named ib_logfile0 and ib_logfile1.  innodb_log_file_size: the size of the redo log  innodb_log_file_in_group specifies: the number of the redo log, and the default is 2  innodb_log_group_home_dir: the path where the redo log is locatedinnodb_additional_mem_pool_size = 100Minnodb_buffer_pool_size = 128Minnodb_data_home_dir =/home/mysql/local/mysql/varinnodb_data_file_path = ibdata1:1G: autoextendinnodb_file_io_threads = 4innodb_thread_concurrency = 16innodb_flush_log_at_trx_commit = 1innodb_log_buffer_size = 8Minnodb_log_file_size = 128Minnodb_log_file_in_group = 2innodb_log_group_home_dir =/home/mysql/local/mysql/varInnoDB는 Undo log의 작업(operation)을 Redo log의 로그로 기록한다. 그러면 Undo log를 자주 flush 하지 않아도 된다. Redo log에 기록되었기 때문에. Redo log만 자주 flush해주면 된다.데이터 변경 작업이 일어날 때 하는 Undo log의 행동(Undo log를 기록한다, 데이터를 변경한다)들을 하나의 실행문으로 생각하고 Redo log에 저장해둔다.Redo log의 I/O 퍼포먼스데이터 작업은 최대한 메모리 위에 있는 페이지에서 하고, 디스크로 flush하는 작업은 DB의 리소스가 여유로울 때 하는 것이 좋다. 그래서 메모리 버퍼 풀을 이용하는 것이다. 근데 그러면 장애 발생으로 서버가 종료될 때 문제가 된다.그래서 복구가 가능하도록 하기 위해 등장한 것이 Redo log이고, 중간중간 Redo log만 Log buffer에서 Log file로 flush함으로써 문제를 해결할 수 있다.여기서 드는 생각은 “성능 저하의 주요 요인이 디스크로 flush하는 작업이었는데, Redo log를 디스크로 flush하면 결국 달라지는 게 없지 않는가?”라는 점이다.하지만 디스크 작업에도 비교적 빠른 작업이 있고, 느린 작업이 있다. 빠른 작업은 디스크의 내용을 순차적으로 읽는 Sequential Access, 느린 작업은 디스크의 블럭을 자주 옮겨 다녀야 하는 Random Access이다.다시 Redo log로 돌아와보면, Redo log는 디스크의 Log file에 단순히 SQL statement를 append하는 형식이다. 그래서 이 작업은 Sequential Access에 해당한다. 하지만 데이터를 디스크로 flush하는 작업은 디스크 내의 데이터 위치를 찾아야 하기 때문에 Random Access가 발생한다. 이것이 바로 Redo log를 사용하는 것이 더 빠른 이유다.트랜잭션 처리시 일어나는 Undo log와 Redo log의 작업1A. Transaction start. 2B. Record A=1 to undo log. 3C. Modify A=3. 4D. Record 2, 3 to redo log. 5E. Record B=2 to undo log. 6F. Modify B=4. 7G. Record 5, 6 to redo log. 8H. Write redo log to disk. 9I. Transaction commit한 가지 알아야 할 점은 Redo log가 있다고 하더라도, 트랜잭션 한 번마다 Redo log가 flush되는게 아니라면 데이터 손실은 피할 수 없다. 그렇기 때문에 은행과 같이 데이터 손실이 하나라도 발생해서는 안되는 곳에서는 트랜잭션 하나를 완료하기 전에 무조건 디스크로 Redo log를 flush하고 커밋을 하게 된다. (트랜잭션 한 개 마다 Redo log flush)the role of redo &amp;amp; undo log  Data persistence          The buffer pool maintains a linked list in the order of dirty page modification, called flush_list. Flush data to persistent storage according to the order of pages in flush_list. The pages are arranged in the order of the earliest modification. Under normal circumstances, when is the dirty page flushed to the disk?                  When the redo space is full, part of the dirty page will be flushed to the disk, and then part of the redo log will be released.          When you need to allocate a page in the Buffer pool, but it is full, you must flush dirty pages to disk. Generally, this situation can be controlled by the startup parameter innodb_max_dirty_pages_pct. When the dirty page in the buffer pool reaches this ratio, the dirty page is flushed to the disk.          When the system is detected to be idle, it will flush.                      Data Recovery          Over time, Redo Log will become very large. If you start to recover from the first record every time, the recovery process will be very slow and cannot be tolerated. In order to reduce the recovery time, the Checkpoint mechanism is introduced. Suppose that at a certain point in time, all dirty pages have been flushed to disk. All Redo Logs before this time point do not need to be redone. The system records the end of the redo log at this point in time as the checkpoint. When restoring, just start from this checkpoint position. The log before the checkpoint point is no longer needed and can be deleted.      Binary Log  Redo log는 트랜잭션 처리를 위한 InnoDB 엔진만의 특별한 존재이다. 그렇다면 다른 스토리지 엔진을 쓰는 경우는 어떻게 할까?  그래서 스토리지 엔진에 상관없이 MySQL Server레벨에서 가지는 로그가 있는데 그게 바로 Binry log이다.  Binary log는 스키마, 테이블에 관한 모든 변경 항을 기록한다. (SELECT와 SHOW 쿼리는 X)  Binary log는 실행문의 실행 시간도 기록해둔다.Redo log와 Binary log  The content is different: redo log is a physical log and the content is based on the Page on disk, bin-log content is binary and depending on the binlog_format parameter, may be based on SQL statements, on the data itself, or a mixture of the two.  Different levels: redo log works with InnoDB and the engine, bin-log is located at the MySQL Server level and is available to all engines.  Different forms of disk storage: redo log writes cyclically, bin-log accumulates, so it can be used for data recovery or primary-secondary synchronization  The timing of writing is different: bin-log are written when a transaction usually commits or when N transactions commit once, redo log are written at a variety of times, either every time a transaction commits, by another threaded transaction, or every second when the disk is flushed. (Note: uncommitted transactions in redo log may also be flushed to disk)  Different roles: redo log is used for crash recovery to ensure that MySQL downtime does not affect persistence; bin-log is used for point-in-time recovery to ensure that the server can recover data based on the point in time, in addition bin-log is also used for primary-secondary replication.Two-phase CommitBecause redo-log is in the InnoDB tier and bin-log is in the Server tier, this introduces a new problem.If the redo log is written successfully and the bin-log crashes before it is written to disk, the transaction has not yet been committed, so the new data written to the redo-log is invalid.Restarting the database for data recovery restores the data in the redo-log to disk, which creates invalid data.In this case, as you wisely know, a two-phase commit is introduced.  In the first stage, the redo-log is written and in the prepared state.  After the Server layer saves the bin-log data and drops it to disk, the transaction commits the redo-log at the same time, so that the redo-log becomes committed, which ensures the consistency of the redo-log data and the bin-log data.Update문 실행 예제With the previous knowledge, you can now explore how the update statement is executed in MySQL.Suppose we now execute the SQL : update table_test set a = a+1 where id = 2;  First, the client connects via the connector and determines the permissions.  After verification, the SQL goes through the parser for lexical and syntax analysis (AST) and if it is an Update statement, MySQL will clear all the query cache for the query table table_test. (As you can see, it is not recommended to turn on the query cache)  The optimizer optimizes the validated SQL, plans to match the id index, and generates an execution plan.  The executor gets the final SQL and calls the interface of the corresponding storage engine to start executing the update SQL.  The InnoDB engine opens a transaction, the execution engine first queries from memory whether there is data with id=2, if it matches then the corresponding data with field+1, and then saves it to memory. If it does not query the data with id=2 then it will go to the disk, the query will read the data into memory in pages, then update it and save it to memory.  The InnoDB engine will then save the data rows to redo-log, which is pre-committed, notifying the Server’s executor that it is ready to commit the transaction.  The executor will generate the corresponding bin-log and write it to disk.  The transaction is committed and the redo-log is then committed.  This is where the execution of a transaction is complete.참고  해커의 개발일기, 데이터베이스의 무결성을 보장해주는 Write-Ahead-Log  alibabacloud, What are the Differences and Functions of the Redo Log, Undo Log, and Binlog in MySQL?  Coralogix, 5 Essential MySQL Database Logs To Keep an Eye On  scaling.dev, Transaction Log. Commit Log. WAL.  PostgreSQL 공식문서, Write-Ahead Logging (WAL)  Dwen, MySQL’s RedoLog and BinLog  developPAPER, You must understand the three MySQL logs – binlog, redo log and undo log  MySQL 공식문서, 14.6.6 Redo Log  Katastros, InnoDB transaction log (redo log and undo log) detailed  heesuk-ahn, [데이터베이스] binary log 란?  The Binary Log  Log-structured storage  ghyeong, 슬로우 쿼리(Slow Query) 조회 쿼리(Oracle, MS-SQL, Mysql, postgreSQL)  MySQL 공식문서, The Slow Query Log",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-14T21:01:35+09:00'>14 Apr 2022</time><a class='article__image' href='/mysql-series10'> <img src='/images/mysql_log_logo.png' alt='MySQL Series [Part10] MySQL의 로그'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series10'>MySQL Series [Part10] MySQL의 로그</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part9] 인덱스",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series9",
      "date"     : "Apr 13, 2022",
      "content"  : "Table of Contents  디스크 읽기 방식          HDD와 SSD      랜덤 I/O와 순차 I/O        인덱스란?  B-Tree 인덱스          MyISAM 스토리지 엔진의 B-Tree 인덱스 구조 및 특성      InnoDB 스토리지 엔진의 B-Tree 인덱스 구조 및 특성      데이터의 Insert, Update, Delete시 인덱스에서의 동작                  인덱스 키 추가          인덱스 키 삭제          인덱스 키 검색                    B-Tree 인덱스 사용에 영향을 미치는 요소                  인덱스 키 값의 크기          B-Tree 깊이          선택도(유니크한 값의 수)          읽어야 하는 레코드의 건수                    데이터의 Select시 인덱스에서의 동작                  인덱스 레인지 스캔          인덱스 풀 스캔          테이블 풀 스캔          루스 인덱스 스캔          인덱스 스킵 스캔                      클러스터링 인덱스          클러스터링 인덱스의 장점과 단점      프라이머리 키 사용시 주의 사항        참고인덱스는 데이터베이스 쿼리의 성능과 관련해서 빼놓을 수 없는 중요한 부분입니다. 인덱스에 대한 지식은 개발자나 관리자 모두에게 중요한 부분이며, 쿼리 튜닝의 기본이 됩니다.디스크 읽기 방식보통 컴퓨터에서 가장 큰 성능 저하는 디스크 I/O에서 발생합니다. 따라서 데이터베이스의 성능 튜닝은 어떻게 디스크 I/O을 줄이느냐가 관건일 때가 상당히 많습니다.HDD와 SSD데이터베이스 서버에서 순차 I/O 작업보다는 랜덤 I/O이 차지하는 비중이 훨씬 큽니다. 그리고 이러한 랜덤 I/O의 속도를 훨씬 높여준 장치가 바로 SSD입니다. 이러한 이유로 DBMS용 스토리지에 SSD는 최적의 장치라고 할 수 있습니다.랜덤 I/O와 순차 I/O랜덤 I/O은 데이터의 개수만큼 데이터의 위치를 찾아야 하고, 순차 I/O은 한 번만 데이터의 위치를 찾으면 되기 때문에 랜덤 I/O으로 인한 작업 부하가 훨씬 더 크게 발생합니다. HDD는 매번 데이터의 위치를 찾기 위해 디스크 헤드를 움직여야 하기 때문에 랜덤 I/O의 작업 부하는 훨씬 더 커지게 됩니다. SSD는 디스크 원판이 아닌 플래시 메모리를 사용하기 때문에 차이가 없을 것 같지만 마찬가지로 랜덤 I/O에서 성능이 저하됩니다.그래서 일반적으로 쿼리 튜닝의 목적은 랜덤 I/O의 회수를 줄이는 것입니다. 여기서 랜덤 I/O을 줄인다는 것은 쿼리를 처리하는 데 꼭 필요한 데이터만 읽도록 쿼리를 개선하는 것을 의미합니다. 그리고 랜덤 I/O을 줄이기 위해 저희는 인덱스라는 것을 활용할 것입니다.DBMS에서는 랜덤 I/O이 자주 발생하고, 쿼리의 성능을 높이기 위해서는 랜덤 I/O을 줄여야 한다.  이렇게 랜덤 I/O을 줄이기 위해 DBMS에서는 인덱스를 사용한다.인덱스란?보통 인덱스를 설명할 때 책 맨 끝에 있는 색인을 예시로 많이 사용합니다. 예를 들어 책에서 ‘무궁화’라는 단어를 찾고 싶다면 저희는 책 페이지에서 내용을 하나씩 찾아보기 보다는 색인에서 ‘무궁화’라는 단어가 포함된 페이지의 쪽수를 찾게 될 겁니다.DBMS도 데이터베이스 테이블의 모든 데이터를 검색해서 원하는 레코드(Row)를 가져오려면 시간이 오래 걸릴 것입니다. 그래서 컬럼의 값과 그 값을 가지는 레코드가 저장된 주소를 매핑한 인덱스를 만들어 두는 것입니다.책의 색인: 단어 - 책 페이지 매핑DBMS의 인덱스: 컬럼 값 - 값을 가지는 레코드(Row)의 주소 매핑그리고 인덱스의 중요한 특성 중 하나는 키(컬럼 값)가 정렬되어 있다는 것입니다. 예를 들어 테이블의 사람 이름을 나타내는 name이라는 컬럼으로 인덱스를 만들었다고 했을 때, name 값을 정렬하여 각각의 주소를 매핑해 인덱스를 만들게 됩니다.아래는 인덱스 페이지의 예시로 다음과 같이 name이 알파벳 순으로 정렬되어 있습니다.            name      레코드 주소              Alice      14345342              Bob      61345549              Carl      24641345              Doson      41127651      이렇게 인덱스 페이지를 정렬했을 때 장단점이 있습니다.  장점: 정렬되어 있기 때문에 인덱스(컬럼 값)를 빠르게 찾고 결과적으로 데이터를 빠르게 읽어온다  단점: 컬럼 값에 INSERT, UPDATE, DELETE가 발생할 때마다 인덱스 파일을 정렬하기 때문에 저장 속도가 느리다결론적으로 DBMS에서 인덱스는 데이터의 저장(INSERT, UPDATE, DELETE) 성능을 희생하고 데이터의 읽기 속도를 높여주게 됩니다. 그래서 인덱스 파일을 하나 더 추가할지 말지는 데이터의 저장 속도를 얼마만큼 희생하여, 그 결과로 읽기 성능을 얼마나 더 빠르게 만들지에 따라 결정되게 됩니다.위에서 인덱스 페이지가 정렬되어 있어 인덱스 값을 빨리 찾을 수 있다고 했습니다. 정렬되어 있다는 것의 이점은 탐색 알고리즘에서 선형 탐색이 아닌 이진 탐색과 비슷하지만 이보다 더 빠른 탐색 알고리즘을 사용할 수 있다는 것입니다.인덱스 파일은 이진 탐색을 지원하는 이진 트리 자료 구조와 비슷하지만 더 빠른 탐색을 가능하게 하는 Balanced Tree(B-Tree)라는 자료구조로 구현되어 있습니다.이외에도 대표적으로 Hash Table 자료구조를 이용한 방법도 있으며 최근에는 Fractal-Tree, Merge-Tree와 같은 알고리즘을 사용하는 DBMS도 개발되고 있습니다.B-Tree 인덱스  Balanced Tree  가장 일반적으로 사용되는 인덱스 형태  컬럼 값을 변형하지 않고 원래의 값을 이용해 인덱싱  B-Tree를 응용한 많은 자료구조가 등장Hash Table 인덱스  컬럼 값을 해시한 결과를 인덱스로 사용  매우 빠른 검색 지원  해시값을 인덱스로 사용해 컬럼 값의 일부만 검색하거나 범위를 검색할 때는 사용 불가  주로 메모리 기반의 데이터베이스에서 많이 사용B-Tree 인덱스디스크 기반 스토리지는 하드웨어적 특성으로 I/O의 가장 작은 단위가 페이지가 됩니다. 디스크 기반 자료구조는 디스크 접근 횟수를 최소화 하기 위해 데이터의 지역성을 높여야 합니다. 이렇게 지역성을 높이는 방법은 페이지를 만들 때 비슷하게 참조되는 데이터를 페이지로 만드는 것입니다. 비슷하게 참조되는 데이터를 페이지로 만듦으로써 데이터를 조회할 때 페이지를 넘나드는 포인터를 최소화할 수 있습니다.이렇게 만든 페이지가 B-트리에서 노드가 됩니다. 다시 말해 B-트리는 페이지 기반 자료구조입니다.(비슷하게 참조되는 키값은 사실상 정렬했을 때 인근 키값들)이제 키 값을 기준으로 그 키 값을 가지는 실제 파일 주소를 어떻게 찾는지 예시를 들어 설명해보겠습니다.참고로 여기서 설명하는 B-트리는 엄밀히 B+ 트리입니다 B-트리와 B+ 트리의 차이는 B-트리는 루트 노드, 브랜치 노드, 리프 노드 모든 레벨에 값을 저장하는 것이 가능하고, B+ 트리의 경우 브랜치 노드에 리프노드에 저장된 값을 찾는데 필요한 구분(seperator) 키만 저장합니다.MySQL에서는 MyISAM, InnoDB 모두 B+ 트리 형태로 되어 있고, 편의상 그냥 B-트리라고 하기 때문에 B-트리라고 하더라도 마음속으로는 B+ 트리 형태를 떠올리면 되겠습니다.참고로 이러한 B+ 트리는 리프 노드에만 값을 저장하기 때문에 Insert, Update, Delete 연산은 리프노드에만 영향을 미칩니다. 상위 레벨의 노드는 트리 균형을 위해 분할 혹은 병합이 일어날 때만 영향을 받습니다.  인덱스의 리프 노드는 항상 실제 데이터 레코드를 찾아가기 위한 주소값을 가짐  인덱스의 키 값은 모두 정렬돼 있지만, 데이터 파일의 레코드는 정렬돼 있지 않고 임의의 순서로 저장돼 있음  InnoDB엔진의 데이터 파일 레코드는 클러스터되어 디스크에 저장되므로 기본적으로 프라이머리 키 순서로 정렬되어 저장됨  인덱스는 테이블의 키 컬럼만 가지고 있으므로 나머지 컬럼을 읽으려면 데이터 파일에서 해당 레코드를 찾아야 함MyISAM 스토리지 엔진의 B-Tree 인덱스 구조 및 특성InnoDB 스토리지 엔진의 B-Tree 인덱스 구조 및 특성  InnoDB 엔진에서는 인덱스를 통해 레코드를 읽을 때 파일을 바로 찾아가지 못함  인덱스에 저장돼 있는 프라이머리 키 값을 이용해 프라이머리 키 인덱스를 한 번 더 검색한 후,  프라이머리 키 인덱스의 리프 페이지에 저장돼 있는 레코드를 읽음  즉 InnoDB 엔진에서는 모든 세컨더리 인덱스 검색에서 데이터 레코드를 일기 위해서는,  반드시 프라이머리 키를 저장하고 있는 B-Tree를 다시 한번 검색해야 함  InnoDB 엔진에서 데이터 파일은 프라이머리 키 인덱스 자체한 가지 짚고 넘어갈 점은 위와 같이 인덱스 키 하나에 매칭되는 레코드 하나가 매핑되는 경우는 프라이머리 키 인덱스나 유니크 키 인덱스 같은 특별한 경우이고 보통은 인덱스 키 하나에 여러 개의 레코드가 매핑된다.근데 인덱스를 이용해서 레코드를 찾을 때는 테이블에서 그냥 레코드를 찾는 것보다 5배 정도 더 큰 비용이 들어간다. 여기서 5배라는 거는 복합적인 요인이 합해져서 5배 정도되는 것 같다.- 우선 인덱스 키를 가지고 해당 리프 노드를 찾아가는데 걸리는 시간- 그 리프 노드에서 해당 키를 찾는데 걸리는 시간 (이 시간은 리프 노드의 사이즈에 따라 또 달라짐)- 그리고 키에 매핑되는 데이터의 실제 주소를 알았을 때 데이터 파일의 해당 주소로 찾아가는데 걸리는 시간. (이 경우 한 건 한건이 모두 랜덤 I/O임)- 그리고 조건절에 사용된 인덱스가 = 과 같은 것이 아니라 범위 조건절이면 리프 노드간 이동도 해야함데이터의 Insert, Update, Delete시 인덱스에서의 동작  테이블의 레코드를 저장하거나 변경하는 경우 인덱스 키 추가나 삭제 작업이 발생인덱스 키 추가  B-Tree에 저장될 때는 저장될 키 값을 이용해 B-Tree상의 적절한 위치를 검색해야 함  위치가 결정되면 레코드의 키 값과 대상 레코드의 주소 정보를 B-Tree의 리프 노드에 저장  리프 노드가 꽉 차서 더 저장할 수 없을 때는 리프 노드가 분리돼야 하는데, 이는 상위 브랜치 노드까지 처리의 범위가 넓어짐  이러한 이유로 B-Tree는 상대적으로 쓰기 작업(새로운 키를 추가하는 작업)에 비용이 많이 들어감인덱스 키 삭제  해당 키 값이 저장된 B-Tree의 리프 노드를 찾아서 그냥 삭제 마크만 하면 작업이 완료됨  인덱스 키 삭제로 인한 마킹 작업 또한 디스크 쓰기가 필요하므로 디스크 I/O이 필요한 작업인덱스 키 검색  인덱스를 구축하는 이유는 빠른 검색  인덱스 트리 탐색은 SELECT문 뿐만 아니라, UPDATE나 DELETE를 처리하기 위해 레코드를 검색하는 경우에도 사용됨  함수나 연산을 수행한 결과로 정렬한다거나 검색하는 작업은 B-Tree의 장점을 이용할 수 없음  InnoDB 엔진의 경우, 검색을 수행한 인덱스를 잠근 후 테이블의 레코드를 잠그는 방식으로 구현돼 있음  따라서 UPDATE나 DELETE문이 실행될 때 테이블에 적절히 사용할 인덱스가 없으면 불필요하게 많은 레코드를 잠금B-Tree 인덱스 사용에 영향을 미치는 요소  B-Tree 인덱스는 인덱스를 구성하는 컬럼의 크기와 레코드의 건수, 그리고 유니크한 인덱스 키 값의 개수 등에 의해 검색이나 변경 작업의 성능이 영향을 받음인덱스 키 값의 크기  InnoDB 스토리지 엔진은 디스크에 데이터를 저장하는 가장 기본 단위를 페이지라고 함  페이지: 디스크의 모든 읽기 및 스기 작업의 최소 작업 단위. 또한 InnoDB 엔진의 버퍼 풀에서 버퍼링하는 기본 단위  인덱스도 결국은 페이지 단위로 관리됨. 루트와 브랜치, 리프 노드를 구분하는 기준이 바로 페이지 단위  B-Tree는 자식 노드를 몇 개까지 가질 수 있을까? -&amp;gt; 인덱스의 페이지 크기와 키 값의 크기에 따라 결정됨  InnoDBB 엔진의 페이지 크기를 innodb_page_size 시스템 변수를 이용해 4KB~64KB로 선택 가능 기본값은 16KB  인덱스의 키를 16B, 값에 해당하는 자식 노드 주소를 12B라 하면, 인덱스 페이지당 16*1024/28 = 585개의 키 저장 가능  최종적으로 이 경우에 자식 노드를 585개 가질 수 있는 B-Tree가 됨  인덱스 키 값이 커지면 자식 노드의 수는 줄어듬 -&amp;gt; B-Tree의 탐색 횟수 증가 -&amp;gt; SELECT 쿼리가 느려짐B-Tree 깊이  위에서 키 값의 크기가 커지면 B-Tree의 깊이가 증가함을 확인  B-Tree의 깊이는 MySQL에서 값을 검색할 때 몇 번이나 랜덤하게 디스크를 읽어야 하는지와 직결되는 문제  실제로는 아무리 대용량 데이터베이스라도 B-Tree의 깊이가 5단계 이상까지 깊어지는 경우는 거의 없음선택도(유니크한 값의 수)  인덱스 컬럼 값중 유니크한 값의 수  인덱스 컬럼이 100개의 값을 가지는데 그중에서 유니크한 값의 수가 10개라면 선택도는 10  인덱스는 선택도가 높을수록 검색 대상이 줄어들고 검색 성능이 빨라짐읽어야 하는 레코드의 건수  인덱스를 통해 테이블의 레코드를 읽는 것은 바로 테이블의 레코드를 읽는 것보다 높은 비용이 드는 작업  인덱스를 이용한 읽기의 손익 분기점이 얼마인지 판단할 필요가 있음  일반적으로 인덱스를 통해 레코드 1건을 읽는 것이 직접 읽는 것보다 4~5배 정도 비용이 더 많이 드는 작업  즉 인덱스를 통해 읽어야 할 레코드의 건수가 전체 테이블 레코드의 20~25%를 넘어서면 테이블을 모두 직접 읽어 필요한 레코드만 가려내는(필터링) 방식으로 처리하는 것이 효율적  전체 100만 건의 레코드 가운데 50만 건을 읽어야 하는 작업은 인덱스를 이용하지 않고 직접 읽어서 처리할 것데이터의 Select시 인덱스에서의 동작  스토리지 엔진이 어떻게 인덱스를 이용해 실제 레코드를 읽어내는지 알아보자인덱스 레인지 스캔  인덱스 레인지 스캔은 인덱스의 접근 방법 가운데 가장 대표적인 접근 방식  B-Tree의 필요한 영역을 스캔하는데 어떤 작업이 필요한가  검색해야 할 인덱스의 범위가 결정됐을 때 사용하는 방식  검색하려는 값의 수나 검색 결과 레코드 건수와 관계없이 레인지 스캔이라고 표현  루트 노드에서부터 비교를 시작해 브랜치 노드를 거치고 최종적으로 리프 노드까지 들어가야만 필요한 레코드의 시작 지점을 찾을 수 있음  시작해야 할 위치를 찾으면 그 때부터는 리프 노드의 레코드만 순서대로 읽으면 됨. 이렇게 쭉 읽는 것을 스캔이라고 표현  리프 노드 끝까지 읽으면 노드 간의 링크를 이용해 다음 리프 노드를 찾아 다시 스캔  루트와 브랜치 노드를 이용해 스캔 시작 위치를 검색하고, 그 지점부터 필요한 방향으로 인덱스를 읽어 나감  읽어 나가면서 검색 조건에 일치하는 경우 데이터 파일에서 레코드를 읽어옴  이때 리프 노드에 저장된 레코드 주소로 데이터 파일의 레코드를 읽어오는데 한 건 단위로 랜덤 I/O이 한 번씩 일어남  그래서 인덱스를 통해 데이터 레코드를 읽는 작업은 비용이 많이 드는 작업으로 분류인덱스 풀 스캔  인덱스 레인지 스캔과 달리 인덱스의 처음부터 끝까지 모두 읽는 방식  쿼리의 조건절에 사용된 컬럼이 인덱스의 첫 번째 컬럼이 아닌 경우 인덱스 풀 스캔 방식 사용  예를 들어 인덱스는 (A, B, C) 컬럼의 순서로 만들어져 있지만, 쿼리의 조건절은 B컬럼이나 C컬럼으로 검색하는 경우  일반적으로 인덱스의 크기는 테이블의 크기보다 작으므로 직접 테이블을 읽는 것보다 인덱스만 읽는 것이 효율적  쿼리가 인덱스에 명시된 컬럼만으로 조건을 처리할 수 있는 경우 주로 이 방식이 사용됨테이블 풀 스캔  테이블 자체의 사이즈가 굉장히 작은 경우  조건에 부합하는 레코드가 너무 많은 경우  인덱스를 사용하지 못하는 경우에 해당  WHERE 절이나 ON 절에 인덱스를 이용할 수 있는 적절한 조건이 없는 경우  테이블 전체를 읽어옴루스 인덱스 스캔  오라클의 인덱스 스킵 스캔과 유사한 방식. MySQL에서는 루스(Loose) 인덱스 스캔이라고 함  위에서 봤던 인덱스 레인지 스캔, 인덱스 풀 스캔을 타이트(Tight) 인덱스 스캔으로 분류함  말 그대로 느슨하게 또는 듬성듬성하게 인덱스를 읽는 것을 의미  인덱스 레인지 스캔과 비슷하게 동작하지만 중간에 필요치 않은 인덱스 키 값은 무시(Skip)하고 넘어가는 형태로 처리  일반적으로 GROUP BY 또는 집합 함수 가운데 MAX(), MIN() 함수에 대해 최적화 하는 경우에 사용  아래의 쿼리문이 있을 때, 인덱스가 (dept_no emp_no) 조합으로 정렬되어 있다면, WHERE절을 만족하는 각 dept_no별로 제일 첫 번째 emp_no만 읽어오고 나머지 인덱스는 스킵할 수 있음      SELECT dept_no, MIN(emp_no)  FROM dept_emp  WHERE dept_no BETWEEN &#39;d002&#39; AND &#39;d004&#39;  GROUP BY dept_no;      인덱스 스킵 스캔  데이터베이스에서 인덱스의 핵심은 값이 정렬돼 있다는 사실 -&amp;gt; 이로 인해 인덱스를 구성하는 컬럼의 순서가 매우 중요  (A, B)라는 인덱스가 있을 때 이 인덱스를 사용해 쿼리문의 빠른 처리를 하려면 WHERE 조건절에 A에 관한 조건이 반드시 제일 앞에 등장해야 함. (WHERE A=1000, 또는 WHERE A=1000 AND B=’M’)  나의 쿼리문에 WHERE 조건절에 A랑 B가 있다면 (A, B)라는 인덱스를 생성해주면 쿼리 성능을 향상시킬 수 있음    CREATE INDEX &amp;lt;인덱스명&amp;gt; ON &amp;lt;테이블명&amp;gt; ( 컬럼명1, 컬럼명2, ... );ALTER TABLE &amp;lt;테이블명&amp;gt;ADD INDEX &amp;lt;인덱스명&amp;gt; (컬럼명1, 컬럼명2, ...);CREATE TABLE &amp;lt;테이블명&amp;gt;INDEX &amp;lt;인덱스명&amp;gt; (컬럼명1, 컬럼명2, ...);        MySQL 8.0부터 WHERE B=’M’과 같이 A에 관한 조건절이 없어도 인덱스 (A, B)를 이용해 인덱스 스킵 스캔을 할 수 있도록하는 최적화 기능이 도입됨클러스터링 인덱스  클러스터링: 여러 개를 하나로 묶는다는 의미  InnoDB의 프라이머리 키가 클러스터링 키로 사용되며, 이 값에 의해 레코드의 위치가 결정됨  MySQL 서버에서 클러스터링은 테이블의 레코드를 프라이머리 키를 기준으로 비슷한 것들끼리 묶어서 저장하는 형태로 구현  비슷한 값들을 동시에 조회하는 경우가 많다는 점에 착안한 것  클러스터링 인덱스는 테이블의 프라이머리 키에 대해서만 적용되는 내용  즉 프라이머리 키 값이 비슷한 레코드끼리 묶어서 저장하는 것을 클러스터링 인덱스라고 표현  중요한 것은 프라이머리 키 값에 의해 레코드의 저장 위치가 결정된다는 것  프라이머리 키 값이 변경된다면 그 레코드의 물리적인 저장 위치가 바뀌어야 한다는 것을 의미하기도 함  클러스터링 인덱스는 프라이머리 키 값에 의해 레코드의 저장 위치가 결정되므로 사실 인덱스 알고리즘이라기 보다 테이블 레코드의 저장 방식. 그래서 클러스터링 인덱스와 클러스터링 테이블은 동의어로 사용되기도 함  클러스터링 인덱스로 저장되는 테이블은 프라이머리 키 기반의 검색이 매우 빠름. 대신 저장은 상대적으로 느림  클러스터링 인덱스의 리프 노드에는 레코드의 모든 컬럼이 같이 저장돼 있음 -&amp;gt; 클러스터링 테이블은 그 자체가 하나의 거대한 인덱스 구조로 관리되는 것  프라이머리 키가 없는 경우 InnoDB 엔진이 다음 우선순위대로 프라이머리 키를 대체할 컬럼을 선택          NOT NULL 옵션의 유니크 인덱스 중에서 첫 번째 인덱스      자동으로 유니크한 값을 가지도록 증가되는 컬럼을 내부적으로 추가한 후 클러스터링 키로 선택                  하지만 이 방법은 프라이머리 키가 사용자에게 노출되지 않으며, 쿼리 문장에서 명시적으로 사용할 수 없음(혜택 없음)                    클러스터링 인덱스의 장점과 단점  장점          프라이머리 키로 검색할 때 처리 성능이 매우 빠름      테이블의 모든 세컨더리 인덱스가 프라이머리 키를 가지고 있기 때문에 인덱스만으로 처리될 수 있는 경우가 많음        단점          테이블의 모든 세컨더리 인덱스가 클러스터링 키를 갖기 대문에 전체적으로 인덱스의 크기가 커짐      세컨더리 인덱스를 통해 검색할 때 프라이머리 키로 다시 한 번 검색해야 하므로 처리 성능이 느림      프라이머리 키 사용시 주의 사항  클러스터링 인덱스 키의 크기를 크게 하지 않도록 해야함  AUTO-INCREMENT보다는 가능한 업무적인 컬럼으로 생성          대부분 검색에서 상당히 빈번하게 사용됨 -&amp;gt; 설령 그 컬럼의 크기가 크더라도 해당 레코드를 대표할 수 있다면 그 컬럼을 프라이머리 키로 사용할 것을 권장        프라이머리 키는 반드시 명시할 것참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-13T21:01:35+09:00'>13 Apr 2022</time><a class='article__image' href='/mysql-series9'> <img src='/images/sql_10.png' alt='MySQL Series [Part9] 인덱스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series9'>MySQL Series [Part9] 인덱스</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part8] 트랜잭션과 잠금",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series8",
      "date"     : "Apr 12, 2022",
      "content"  : "Table of Contents  트랜잭션          ACID      트랜잭션 관련 실행 명령어        페이지 캐시          캐싱      캐시 만료      페이지 동기화      페이지 고정      페이지 교체 알고리즘        복구          WAL      언두(Undo) 로그와 리두(Redo) 로그      스틸(Steal)과 포스(Force) 정책        동시성 제어          잠금      격리 수준                  READ UNCOMMITTED          READ COMMITTED          REPEATABLE READ                      참고트랜잭션DBMS에서 트랜잭션이란 하나의 논리적 작업 단위를 의미하며, 여러 작업(DB의 읽기, 쓰기)을 한 단계로 표현하는 방법입니다. 트랜잭션은 어플리케이션 개발에서 부분 업데이트 현상으로 인해 데이터의 정합성이 깨지는 문제를 걱정하지 않도록 해줍니다. 트랜잭션을 사용할 때는 가능한 범위를 최소화하는 것이 좋습니다. 특히 네트워크를 통해 발생하는 작업은 트랜잭션 내에서 제거해야 합니다. 네트워크간 데이터 이동은 그 자체로 높은 비용이 발생할 뿐 아니라, 다양한 이유로 장애가 발생할 수 있는 부분이기 때문입니다. 트랜잭션이 종료되지 않고 유지되는 시간이 길어지면 MySQL서버의 전체적인 성능 하락에 주요 이유가 됩니다.ACID이러한 트랜잭션을 정의하기 위해서는 다음의 4가지 속성이 보장되어야 합니다.원자성(Atomicity)  트랜잭션은 더 작은 단계로 나눌 수 없습니다. 트랜잭션과 관련된 작업은 모두 실행되거나 모두 실패해야 합니다.  All or Nothing  ex) A에서 B로 계좌이체를 할 때, A가 출금이 되면 B도 반드시 입금이 되어야 한다.일관성(Consistency)  트랜잭션이 일어나더라도 데이터베이스의 제약이나 규칙은 그대로 지켜져야 합니다.  사용자가 제어할 수 있는 유일한 속성입니다.  ex) 고객 정보 DB에서 이름을 반드시 입력하도록 제약을 두었다면 트랜잭션 또한 이러한 제약을 가져야 한다.격리성(Isolation)  하나의 트랜잭션은 다른 트랜잭션으로부터 간섭없이 독립적으로 수행되어야 합니다.  동시에 여러 개의 트랜잭션들이 수행될 때, 각 트랜잭션은 연속으로 실행된 것과 동일한 결과를 나타내야 합니다.  ex) A가 만원이 있는 계좌에서 B에게 3천원을 송금하던 도중 자신의 잔액을 확인할 때는 여전히 만원이 있어야 한다.  많은 데이터베이스는 성능상의 이유로 정의에 비해 약한 격리 수준을 사용합니다. (동시성 제어의 격리 수준 참고)지속성(Durability)  커밋된 데이터는 장애가 발생 하더라도 데이터베이스에 저장되어야 한다.  ex) A에서 B로 송금이 완료되어 커밋을 했다면 시스템 중단, 정전으로 장애가 발생해도 DB에 데이터가 그대로 유지되어야 한다트랜잭션 관련 실행 명령어  Commit: A commit ends the current transaction and makes permanent all changes performed in the transaction. The transaction is a sequence of SQL statements that the database treats as a single unit. A commit also erases all savepoints in the transaction and releases transaction locks. After your data is committed, it is visible to other users of the system.Commit means save the cache changes to the databaseA SQL statement that ends a transaction, making permanent any changes made by the transaction. It is the opposite of rollback, which undoes any changes made in the transaction.InnoDB uses an optimistic mechanism for commits, so that changes can be written to the data files before the commit actually occurs. This technique makes the commit itself faster, with the tradeoff that more work is required in case of a rollback.By default, MySQL uses the autocommit setting, which automatically issues a commit following each SQL statement.      Save: A save writes your changes to the database, however at this point these changes are only visible to you within your transaction scope. The database has also generated undo information which contains the old values of your transaction which can be used to rollback your modifications.        Flush: flush() will synchronize your database with the current state of object/objects held in the memory but it does not commit the transaction. So, if you get any exception after flush() is called, then the transaction will be rolled back.  To write changes to the database files, that had been buffered in a memory area or a temporary disk storage area. The InnoDB storage structures that are periodically flushed include the redo log, the undo log, and the buffer pool.  Rollback:페이지 캐시대부분의 데이터베이스는 상대적으로 속도가 느린 영구 저장소(디스크)에 접근하는 회수를 줄이기 위해 페이지(읽고 쓰는 가장 작은 단위)를 메모리에 캐시합니다. 이를 페이지 캐시(page cache)라고 하며 이 때의 메모리 영역을 버퍼 풀(buffer pool)이라고 합니다. 메모리에 있는 페이지에 변경사항이 생겼을 때 아직 디스크로 플러시(flush)되지 않은 페이지를 더티(dirty) 페이지라고 합니다.정리하면 페이지 캐시의 주요 기능은 다음과 같습니다.  페이지를 메모리에 캐시함으로써 빠른 읽기를 지원  쓰기 요청이 발생할때마다 디스크로 플러시하지 않고 버퍼링 후 플러시 할 수 있다캐싱스토리지 엔진이 특정 페이지를 요청하면 우선 캐시된 버전이 있는지 확인합니다. 페이지가 있다면 반환하고 없다면 페이지 번호를 물리적 주소로 변환해 해당 페이지를 메모리로 복사하고 반환합니다.이때 해당 페이지가 저장된 버퍼는 참조상태라고 표현합니다. 작업이 끝나면 스토리지 엔진은 참조 해제해야 합니다.캐시 만료일반적으로 버퍼 풀은 데이터셋보다 크기가 작기 때문에 새로운 페이지를 추가하기 위해 기존 페이지를 만료시키는 작업도 필요하게 됩니다. 페이지가 동기화됐고 고정 또는 참조 상태가 아니라면 바로 제거할 수 있습니다. 페이지를 제거할 때에는 페이지와 관련된 로그도 WAL에서 삭제합니다.페이지 동기화위에서 버퍼 풀의 메모리 용량을 관리하기 위해서는 캐시가 만료된 페이지는 제거해야 한다고 했습니다. 그리고 이때 페이지를 제거하기 위해서는 우선 페이지가 동기화되어야 한다고 했습니다. 페이지 동기화는 더티페이지를 디스크에 반영(flush)하는 것입니다.이렇게 플러시하는 것은 언제 얼마나 자주하는 것이 좋을까요? 변경 사항이 생길 때마다 플러시하게 되면 데이터 손실 가능성을 줄일 수 있겠지만 결국 잦은 디스크 접근을 유발하기 때문에 트레이드 오프가 있습니다. 그래서 데이터베이스에서는 이러한 플러시를 주기적으로 하게 되며 이 시점을 체크포인트(checkpoint)라고 합니다.체크포인트 시점에 플러시가 일어나는데 이 때 플러시는 디스크에 있는 데이터베이스에 데이터가 저장되는 것을 의미하지는 않습니다. 플러시는 메모리에 있는 페이지에 요청된 작업 명령들을 디스크의 WAL(Write Ahead Log)에 남겨두고 페이지와 싱크를 맞추는 것입니다. (보통 커밋이 일어나면 플러시도 그 과정에 포함되어 플러시를 디스크에 저장하는 것으로 정의하기도 함. 애매하네)정리하면  캐시가 만료된 페이지를 삭제하려면 먼저 페이지를 동기화 해야 한다.  동기화된 시점을 체크포인트라고 한다.  동기화는 플러시하는 것이며 플러시는 페이지의 변경시 요청된 작업 명령을 디스크의 WAL에 기록하는 것이다.페이지 고정가까운 시간 내에 요청될 확률이 높은 페이지는 캐시에 가둬 두는 것이 좋습니다. 이를 페이지 고정(pinning)이라고 합니다. 예를 들어 이진 트리 탐색에서 트리의 상위 노드는 접근될 확률이 높기 때문에 이러한 상위 노드는 고정해두면 성능 향상에 도움이 됩니다.페이지 교체 알고리즘저장 공간이 부족한 캐시에 새로운 페이지를 추가하려면 일부 페이지를 만료시켜야 한다고 했습니다. 하지만 빈번하게 요청될 수 있는 페이지를 만료시키면 같은 페이지를 여러 차례 페이징하는 상황이 발생할 수 있습니다. 페이지 교체 알고리즘은 다시 요청될 확률이 가장 낮은 페이지를 만료시키고 해당 위치에 새로운 페이지를 페이징합니다.하지만 페이지의 요청 순서는 일반적으로 특정 패턴이 없기 때문에 어떤 페이지가 다시 요청될지 정확하게 예측하는 것은 불가능 합니다. 그래서 보통은 그 기준을 최근에 요청되었는지 여부, 요청된 빈도수 등으로 합니다. 관련 알고리즘에는 FIFO(First In First Out), LRU(Least Recently Used), LFU(Least Frequently Used), CLOCK-sweep 알고리즘이 있습니다.복구데이터베이스 시스템은 각자 다른 안정성과 신뢰성 문제를 내재한 하드웨어와 소프트웨어 계층으로 구성됩니다. 따라서 여러 지점에서 장애가 발생할 수 있고, 데이터베이스 개발자는 이러한 장애 시나리오를 고려해 데이터를 저장해야 합니다.WAL선행 기록 로그(WAL)는 장애 및 트랜잭션 복구를 위해 디스크에 저장하는 추가 자료 구조입니다. WAL은 페이지에 캐시된 데이터가 디스크로 커밋(책에서는 플러시라고 표기)될 때 까지 관련 작업 이력의 유일한 디스크 기반 복사본입니다.WAL에 있는 각각의 로그에는 단조 증가하는 고유 로그 시퀀스 번호(LSN: Log Sequence Number)가 있습니다.WAL의 주요 기능은 다음과 같습니다.  장애 발생 시 WAL을 기반으로 마지막 메모리 상태를 재구성한다. (undo)  WAL의 로그를 재수행해서 트랜잭션을 커밋한다. (redo)언두(Undo) 로그와 리두(Redo) 로그UNDO는 왜 필요할까?오퍼레이션 수행 중에 수정된 페이지들이 버퍼 관리자의 버퍼 교체 알고리즘에 따라서 디스크에 출력될 수 있다. 버퍼 교체는 전적으로 버퍼의 상태에 따라서 결정되며, 일관성 관점에서 봤을 때는 임의의 방식으로 일어나게 된다. 즉 아직 완료되지 않은 트랜잭션이 수정한 페이지들도 디스크에 출력될 수 있으므로, 만약 해당 트랜잭션이 어떤 이유든 정상적으로 종료될 수 없게 되면 트랜잭션이 변경한 페이지들은 원상 복구되어야 한다. 이러한 복구를 UNDO라고 한다. 만약 버퍼 관리자가 트랜잭션 종료 전에는 어떤 경우에도 수정된 페이지들을 디스크에 쓰지 않는다면, UNDO 오퍼레이션은 메모리 버퍼에 대해서만 이루어지면 되는 식으로 매우 간단해질 수 있다. 이 부분은 매력적이지만 이 정책은 매우 큰 크기의 메모리 버퍼가 필요하다는 문제점을 가지고 있다. 수정된 페이지를 디스크에 쓰는 시점을 기준으로 다음과 같은 두 개의 정책으로 나누어 볼 수 있다.  STEAL: 수정된 페이지를 언제든지 디스크에 쓸 수 있는 정책  No-STEAL: 수정된 페이지들을 최소한 트랜잭션 종료 시점(EOT, End of Transaction)까지는 버퍼에 유지하는 정책STEAL 정책은 수정된 페이지가 어떠한 시점에도 디스크에 써질 수 있기 때문에 필연적으로 UNDO 로깅과 복구를 수반하는데, 거의 모든 DBMS가 채택하는 버퍼 관리 정책이다.REDO는 왜 필요할까?이제는 UNDO 복구의 반대 개념인 REDO 복구에 대해서 알아볼 것인데, 앞서 설명한 바와 같이 커밋한 트랜잭션의 수정은 어떤 경우에도 유지(durability)되어야 한다. 이미 커밋한 트랜잭션의 수정을 재반영하는 복구 작업을 REDO 복구라고 하는데, REDO 복구 역시 UNDO 복구와 마찬가지로 버퍼 관리 정책에 영향을 받는다. 트랜잭션이 종료되는 시점에 해당 트랜잭션이 수정한 페이지들을 디스크에도 쓸 것인가 여부로 두 가지 정책이 구분된다.  FORCE: 수정했던 모든 페이지를 트랜잭션 커밋 시점에 디스크에 반영하는 정책  No-FORCE: 수정했던 페이지를 트랜잭션 커밋 시점에 디스크에 반영하지 않는 정책여기서 주의 깊게 봐야 할 부분은 No-FORCE 정책이 수정했던 페이지(데이터)를 디스크에 반영하지 않는다는 점이지 커밋 시점에 어떠한 것도 쓰지 않는다는 것은 아니다. 어떤 일들을 했었다고 하는 로그는 기록하게 되는데 이 부분은 아래에서 자세히 설명한다.FORCE 정책을 따르면 트랜잭션이 커밋되면 수정되었던 페이지들이 이미 디스크 상의 데이터베이스에 반영되었으므로 REDO 복구가 필요 없게 된다. 반면에 No-FORCE 정책을 따른다면 커밋한 트랜잭션의 내용이 디스크 상의 데이터베이스 상에 반영되어 있지 않을 수 있기 때문에 반드시 REDO 복구가 필요하게 된다. 사실 FORCE 정책을 따르더라도 데이터베이스 백업으로부터의 복구, 즉 미디어(media) 복구 시에는 REDO 복구가 요구된다. 거의 모든 DBMS가 채택하는 정책은 No-FORCE 정책이다.정리해보면 DBMS는 버퍼 관리 정책으로 STEAL과 No-FORCE 정책을 채택하고 있어, 이로 인해서 UNDO 복구와 REDO 복구가 모두 필요하게 된다.스틸(Steal)과 포스(Force) 정책DBMS는 스틸/노스틸 정책과 포스/노포스 정책을 기반으로 메모리에 캐시된 변경 사항을 디스크로 플러시하는 시점을 결정합니다. 이러한 정책들은 복구 알고리즘 선택에 큰 영향을 미칩니다.스틸(Steal)트랜잭션이 완료되지 않은 상태에서 데이터를 디스크에 기록할 것인가?  Steal: 기록한다(Undo 필요)  No-Steal: 기록하지 않는다버퍼 관리자가 트랜잭션 종료 전에는 어떤 경우에도 수정된 페이지들을 디스크에 쓰지 않는다면, UNDO 오퍼레이션은 메모리 버퍼에 대해서만 이루어지면 되는 식으로 매우 간단해질 수 있다. 이 부분은 매력적이지만 이 정책은 매우 큰 크기의 메모리 버퍼가 필요하다는 문제점을 가지고 있다. 수정된 페이지를 디스크에 쓰는 시점을 기준으로 다음과 같은 두 개의 정책으로 나누어 볼 수 있다.  STEAL: 수정된 페이지를 언제든지 디스크에 쓸 수 있는 정책  No-STEAL: 수정된 페이지들을 최소한 트랜잭션 종료 시점(EOT, End of Transaction)까지는 버퍼에 유지하는 정책STEAL 정책은 수정된 페이지가 어떠한 시점에도 디스크에 써질 수 있기 때문에 필연적으로 UNDO 로깅과 복구를 수반하는데, 거의 모든 DBMS가 채택하는 버퍼 관리 정책이다.포스(Force)트랜잭션이 완료된 후 바로 데이터를 디스크에 기록할 것인가?  Force: 바로 기록한다  No-Force: 바로 기록하지 않는다(Redo 필요)성능상의 이유로 때로는 트랜잭션이 완료되기도 전에 디스크에 기록하기도 하고 완료되고 나서도 기록하지 않기도 합니다.스틸과 포스 정책은 트랜잭션 언두와 리두 작업과 관련되기 때문에 매우 중요합니다.동시성 제어  잠금과 트랜잭션, 트랜잭션의 격리 수준이 동시성에 영향을 줌  잠금은 동시성을 제어하기 위한 기능  트랜잭션은 데이터의 정합성을 보장하기 위한 개념잠금  하나의 레코드를 여러 커넥션에서 동시에 변경하려고 할 때 잠금이 없다면 레코드의 값은 예측할 수 없는 상태가 됨  잠금은 동시에 여러 요청이 들어와도 순서대로 한 시점에는 하나의 커넥션만 변경할 수 있게 하는 역할  MySQL 엔진 레벨의 잠금은 모든 스토리지 엔진에 영향을 미치며 종류로는 테이블락, 메타데이터락, 네임드락이 있음  InnoDB 스토리지 엔진 레벨의 잠금은 (테이블 전체가 아닌)레코드 기반의 잠금 방식 -&amp;gt; 뛰어난 동시성 처리를 제공  정확히는 레코드를 잠그는 것이 아니라 인덱스를 잠그는 방식(데이터 접근 기준에 해당하는 인덱스 모두 잠금)  만약 테이블에 인덱스가 하나도 없다면 테이블을 모두 스캔하고 모든 레코드에 잠금이 걸림 -&amp;gt; 인덱스 설계가 중요격리 수준  여러 트랜잭션이 동시에 처리될 때 트랜잭션 간의 작업 내용을 어떻게 공유하고 차단할 것인지 결정하는 레벨  격리 수준은 크게 READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE  READ UNCOMMITTED는 DIRTY READ 문제로, SERIALIZABLE은 동시성 처리 성능 저하로 거의 사용하지 않음  오라클 DB는 READ COMMITTED, MySQL은 REATABLE READ를 주로 사용  격리 수준에 따라 DIRTY READ, NON-REPEATABLE READ, PHANTOM READ와 같은 부정합 문제 발생READ UNCOMMITTED  트랜잭션에서의 변경 내용이 COMMIT이 되었는지 ROLLBACK이 되었는지 관계없이 다른 트랜잭션에서 보임  해당 트랜잭션이 COMMIT되면 상관없지만 ROLLBACK된 경우 다른 트랜잭션은 잘못된 데이터를 읽어간 것이 됨READ COMMITTED  오라클 DB를 포함해 많은 온라인 서비스에서 선택하는 격리 수준  트랜잭션이 데이터를 변경하고 COMMIT 하지 않았다면 다른 트랜잭션은 언두 영역에 백업된 레코드를 읽어감  NON-REPEATABLE READ 문제 발생  어떤 트랜잭션내에서 SELECT 요청이 두 번 발생하는 동안 다른 트랜잭션이 데이터 변경 후 커밋하면 하나의 트랜잭션내에서 같은 SELECT 요청에 대해 다른 결과가 나오게 됨REPEATABLE READ  InnoDB  스토리지 엔진에서 기본적으로 사용되는 격리 수준  READ COMMITTED, REPEATABLE READ 모두 트랜잭션이 커밋되지 않은 경우 MVCC를 이용해 이전에 커밋된 데이터를 보여줌  차이는 언두 영역에 백업된 레코드의 여러 버전 가운데 몇 번째 이전 버전까지 찾아 들어가야 하느냐는 것  모든 트랜잭션은 순차적으로 증가하는 고유한 트랜잭션 번호를 가지며 언두 영역에 백업된 모든 레코드에는 변경을 발생시킨 트랜잭션의 번호가 포함돼 있음  트랜잭션은 자신보다 작은 트랜잭션 번호를 가지는 트랜잭션이 변경한 사항만 볼 수 있음참고  Real MySQL 8.0 (1권) 책  데이터베이스 인터널스 책  Naver D2: DBMS는 어떻게 트랜잭션을 관리할까?  온달의 해피클라우드: ACID 이해하기  노력 이기는 재능 없고 노력 외면하는 결과도 없다: [MySQL Internals] FLUSH  MySQL 공식문서: MySQL Glossary  stackoverflow: SQLAlchemy: What’s the difference between flush() and commit()?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-12T21:01:35+09:00'>12 Apr 2022</time><a class='article__image' href='/mysql-series8'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part8] 트랜잭션과 잠금'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series8'>MySQL Series [Part8] 트랜잭션과 잠금</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part9]: Kafka Connector",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series9",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  Why Connector?  Kafka Connect          Kafka Connect 구성요소      Connect? Connector?      Standalone과 Distributed Workers        Debezium  도커 컴포즈 파일  kafka 컨테이너에서 워커 실행 모드 설정  kafka 컨테이너에서 커넥터 워커 실행  connect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행  기타 커넥터 관련 REST API  커넥터와 백엔드(Java Spring)의 관계  참고Why Connector?커넥터 없이도 프로듀서 컨슈머 사용 가능하지만 커넥터를 이용하면 카프카를 사용하면서 발생할 수 있는 장애에 대한 복구를 비롯한 필요한 기능들을 따로 개발할 필요없이 사용가능Kafka Connect  Kafka Connect는 다른 데이터 시스템을 Kafka와 통합하는 과정을 표준화한 프레임워크  통합을 위한 Connector 개발, 배포, 관리를 단순화Kafka Connect 구성요소  Connector: Task를 관리하여 데이터 스트리밍을 조정하는 jar파일  Task: 데이터 시스템간의 전송 방법을 구현한 구현체  Worker: Connector와 Task를 실행하는 프로세스  Converter: 데이터 포맷을 변환하는데 사용하는 구성요소  Trasform: 데이터를 변환하는데 사용하는 구성요소Connect? Connector?커넥트는 커넥터를 실행시키기 위한 환경(프레임워크)을 제공해줌. 커넥트 위에서 커넥터 설치하고 커넥터(jar파일) 실행하면 됨커넥트 이미지로 인스턴스 띄우고 거기서 각종 커넥터 다운로드 받아서 커넥터를 몽고db, mysql, s3같은데 RESTapi로 등록Standalone과 Distributed WorkersWorker 프로세스를 한 개만 띄우는 Standalone 모드와 여러개 실행시키는 Distributed 모드가 있다.보통 확장성과 내결함성을 이유로 Distributed 모드를 많이 사용한다.DebeziumDebezium은 변경 데이터 캡처를 위한 오픈 소스 분산 플랫폼이다.Debezium 에서 변경된 데이터 캡쳐를 위해 mysql의 경우 binlog, postgresql의 경우 replica slot(logical)을 이용하여 데이터베이스에 커밋하는 데이터를 감시하여 Kakfa, DB, ElasticSearch 등 미들웨어에 이벤트를 전달한다도커 컴포즈 파일version: &#39;3.2&#39;services:  mongodb:    image: mongo:latest    hostname: mongodb    ports:      - &quot;27017:27017&quot;    environment:      MONGO_INITDB_ROOT_USERNAME: root      MONGO_INITDB_ROOT_PASSWORD: root    tty: true    zookeeper:    image: zookeeper:3.7    hostname: zookeeper    ports:      - &quot;2181:2181&quot;    environment:      ZOO_MY_ID: 1      ZOO_PORT: 2181    volumes:      - ./data/zookeeper/data:/data      - ./data/zookeeper/datalog:/datalogco  kafka:    image: wurstmeister/kafka    hostname: kafka    ports:      - &quot;9092:9092&quot;    environment:      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1    tty: true    volumes:      - ./data/kafka/data:/tmp/kafka-logs    depends_on:      - zookeeper    connect:    image: confluentinc/cp-kafka-connect:latest.arm64    hostname: connect1    depends_on:      - kafka    environment:      CONNECT_BOOTSTRAP_SERVERS: kafka:29092      CONNECT_REST_ADVERTISED_HOST_NAME: connect1      CONNECT_GROUP_ID: connect-cluster      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets      CONNECT_STATUS_STORAGE_TOPIC: connect-status      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1      CONNECT_PLUGIN_PATH: /usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/      CONNECT_REST_PORT: 8083    ports:      - 18083:8083    volumes:      - ./connectors/1:/usr/share/confluent-hub-components    command:      - bash      - -c      - |        confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.7.0        /etc/confluent/docker/run &amp;amp;        sleep infinity  producer:    build:      context: ./      dockerfile: Dockerfile_producer    stdin_open: true    tty: true  consumer:    build:      context: ./      dockerfile: Dockerfile_consumer    stdin_open: true    tty: truevolumes:  mongodb:kafka 컨테이너에서 워커 실행 모드 설정cd opt/kafka/configvi connect-distributed.properties# connect 컨테이너에서 커넥터(jar파일)가 설치되어 있는 경로 설정plugin.path=/usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/# 컨버터 설정key.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter=org.apache.kafka.connect.json.JsonConverterkey.converter.schemas.enable=falsevalue.converter.schemas.enable=falsekafka 컨테이너에서 커넥터 워커 실행./bin/connect-distributed.sh ./config/connect-distributed.propertiesconnect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행curl -X POST -H&#39;Accept:application/json&#39; -H&#39;Content-Type:application/json&#39; http://connect1:8083/connectors \  -w &quot;\n&quot; \  -d &#39;{&quot;name&quot;: &quot;mongo-sink&quot;,      &quot;config&quot;: {         &quot;connector.class&quot;:&quot;com.mongodb.kafka.connect.MongoSinkConnector&quot;,         &quot;connection.user&quot;: &quot;root&quot;,         &quot;connection.password&quot;: &quot;root&quot;,         &quot;connection.uri&quot;:&quot;mongodb://root:root@mongodb:27017&quot;,         &quot;database&quot;:&quot;quickstart&quot;,         &quot;collection&quot;:&quot;topicData&quot;,         &quot;topics&quot;:&quot;taxi&quot;,        &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,        &quot;value.converter.schemas.enable&quot;: &quot;false&quot;         }     }&#39;기타 커넥터 관련 REST API# 커넥터 상태 확인(커넥터 등록과 태스크 실행이 RUNNING이면 성공)curl -X GET http://connect1:8083/connectors/mongo-sink/status# 커넥터 삭제curl -X DELETE http://connect1:8083/connectors/mongo-sink커넥터와 백엔드(Java Spring)의 관계커넥터가 있으면 알아서 커넥터가 토픽에서 데이터를 가져와 DB로 잘 반영을하는 것 같다.이런거보면 딱히 스프링부트 같은 걸 이용해서 백엔드 프로그램을 개발하지 않아도 되는 것 같아보인다.하지만 만약 내가 스프링부트 같은 거를 엄청 잘 알아서 직접 개발하는데 불편함이 없다면 왠만한 것들은 스프링 부트를 이용하고 부분적으로 특정 프로듀서/컨슈머는 커넥터를 사용하는 것이 아마 가장 좋은 방법이 아닐까 라는 생각이 든다.나는 지금 스프링부트를 모른다. 심지어 자바 언어도 써본 적이 없다. 커넥터는 아예 러닝 커브가 없는 것은 아니지만 훨씬 쉽다.하지만 백엔드의 중요한 철학들을 공부하는 것은 굉장히 중요해보인다.결론은 지금 당장 구현이 필요한 부분들은 커넥터로 구현을 하고, 백엔드 공부는 스프링 부트를 통해서 계속 하자.백엔드 공부를 스프링 부트로 하기로 한 이유는, 내가 사용하고 있는 언어는 파이썬이지만 데이터 엔지니어링 공부에서 자바 언어는 필요해보인다. (데이터 엔지니어링 분야의 관련 오픈 소스들이 자바로 많이 개발됨)파이썬으로 백엔드를 구현하도록 해주는 장고나 플라스크도 있지만, 아직은 스프링 부트를 사용하는 비중이 더 커보이고 뭔가 공부하는 관점에서는 스프링 부트가 더 도움이 많이 될 것 같다.참고  Confluent 공식문서: Kafka Connect Tutorial on Docker  Connect 도커 이미지  Confluent 공식문서: MongoDB 커넥터  MongoDB 공식문서: MongoDB 커넥터를 위한 Configuration  kudl: CDC - debezium 설정  Confluent 공식문서: 커넥터 관련 강의  Confluent 공식문서: Connect 관련 configuration  sup2is: Kafka Connect로 데이터 허브 구축하기  깃허브: mongodb-university/kafka-edu  Kafka Connect Deep Dive – Converters and Serialization Explained  정몽실이: 카프카 커넥트 실행  Stackoverflow: Connector and Spring Kafka",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/kafka-series9'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part9]: Kafka Connector'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series9'>Kafka Series [Part9]: Kafka Connector</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part10]: MySQL Connector",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series10",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  참고curl -X POST -H&#39;Accept:application/json&#39; -H&#39;Content-Type:application/json&#39; http://connect1:8083/connectors \-w &quot;\n&quot; \-d &#39;{&quot;name&quot;: &quot;mysql-sink&quot;,    &quot;config&quot;: {    &quot;connector.class&quot;:&quot;io.confluent.connect.jdbc.JdbcSinkConnector&quot;,    &quot;connection.user&quot;:&quot;root&quot;,    &quot;connection.password&quot;:&quot;passwd&quot;,    &quot;connection.url&quot;:&quot;jdbc:mysql://root:passwd@mysql:3306&quot;,    &quot;mode&quot;:&quot;upsert&quot;,    &quot;table.name.format&quot;:&quot;taxi&quot;,    &quot;topics.regex&quot;: &quot;jdbc-connector-user&quot;,    &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,    &quot;key.converter.schemas.enable&quot;: &quot;true&quot;,    &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,    &quot;value.converter.schemas.enable&quot;: &quot;true&quot;    }}&#39;create table `taxi` (    `VendorID` FLOAT,    `tpep_pickup_datetime` CHAR(30),    `tpep_dropoff_datetime` CHAR(30),    `passenger_count` FLOAT,    `trip_distance` FLOAT,    `RatecodeID` FLOAT,    `store_and_fwd_flag` CHAR(5),    `PULocationID` INT(10),    `DOLocationID` INT(10),    `payment_type` FLOAT,    `fare_amount` FLOAT,    `extra` FLOAT,    `mta_tax` FLOAT,    `tip_amount` FLOAT,    `tolls_amount` FLOAT,    `improvement_surcharge` FLOAT,    `total_amount` FLOAT,    `congestion_surcharge` FLOAT);프로듀서와 컨슈머를 커넥터로 연결하고 나면 딱히 건드릴게 없다. 근데 생각해보면 프로듀서와 컨슈머 각각 설정할 configuration들이 굉장히 많다. 그러면 그런것들은 각각의 커넥터를 REST API로 등록할 때 바꿀 수 있는 것인가? devidea: [Kafka] Connector-level producer/consumer configuration 글을 보면 그런 것 같은데 아직 Confuent 쪽에서는 커넥터에 관해 이런 Configuration을 커스텀하도록 fully 지원하지는 않는 것 같기도 하다. 아마 예를 들어 MongoDB, MySQL 등 각각의 프로듀서/컨슈머별로 지원해야 하는 특성들을 자기들 생각에는 잘 설정해놓았기 때문에 사용자들이 직접 건드릴 필요가 없다고 생각해서 그런 건가?참고  JDBC Sink Connector Configuration Properties  [Kafka] Sink Connector 생성  JDBC Connector (Source and Sink) for Confluent Platform  [Kafka] Kafka Connect 개념/예제  [Kafka] Kafka Connect - JDBC Connector 예제  How to save a DataFrame to MySQL in PySpark  devidea: [Kafka] Connector-level producer/consumer configuration",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/kafka-series10'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part10]: MySQL Connector'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series10'>Kafka Series [Part10]: MySQL Connector</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part3]: 펀더멘털 분석",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series2",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  참고참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series2'> <img src='/images/finance_logo.jpeg' alt='Finance Series [Part3]: 펀더멘털 분석'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series2'>Finance Series [Part3]: 펀더멘털 분석</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part2]: 연방 준비 시스템의 이해",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series1",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  참고참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series1'> <img src='/images/finance_logo.jpeg' alt='Finance Series [Part2]: 연방 준비 시스템의 이해'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series1'>Finance Series [Part2]: 연방 준비 시스템의 이해</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part1]: 금융시장의 메커니즘",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series0",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  금융시장 메커니즘 이해  이자율 관련 기본 개념          금리(이자율)      단리/복리      연준 기준금리      우대금리      LIBOR 단기 이자율      수익률      물가 상승      실질이자율      순 현재 가치        금융시장 내 자산시장들 간의 관계          채권 시장      주식 시장      외환 시장      커머더티 시장      ETF 시장      선물 시장      옵션 시장        참고금융시장 메커니즘 이해이자율 관련 기본 개념금리(이자율)단리/복리연준 기준금리우대금리LIBOR 단기 이자율수익률물가 상승실질이자율순 현재 가치금융시장 내 자산시장들 간의 관계채권 시장주식 시장외환 시장커머더티 시장ETF 시장선물 시장옵션 시장참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series0'> <img src='/images/finance_1.png' alt='Finance Series [Part1]: 금융시장의 메커니즘'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series0'>Finance Series [Part1]: 금융시장의 메커니즘</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part7] 아키텍처",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series7",
      "date"     : "Apr 9, 2022",
      "content"  : "Table of Contents  MySQL의 전체 구조  MySQL 엔진 아키텍처  InnoDB 스토리지 엔진 아키텍처          InnoDB 버퍼 풀      로그 버퍼      리두 로그                  버퍼 풀과 리두 로그의 관계          버퍼 풀 플러시                    언두 로그      어댑티브 해시 인덱스        참고MySQL의 전체 구조  MySQL 엔진: 요청된 SQL문장을 분석하거나 최적화하는 등 DBMS의 두뇌에 해당하는 처리를 수행  스토리지 엔진: 데이터를 디스크에 저장하거나 디스크로부터 읽어오는 역할MySQL 엔진 아키텍처  커넥션 핸들러: 클라이언트의 접속, 쿼리 요청을 처리  SQL 파서: 실행 이전에 문법성 오류 체크  SQL 옵티마이저: 쿼리의 최적화InnoDB 스토리지 엔진 아키텍처There is a metadata file (ibdata1, which holds, by default, data pages, index pages, table metadata and MVCC information), also known as the InnoDB tablespace file.  You can have more than one ibdata file (see innodb_data_file_path)  There are redo logs (ib_logfile0 and ib_logfile1)  You can have more than two redo logs (see innodb_log_files_in_group)  You can spread data and indexes across multiple ibdata files if innodb_file_per_table is disabled  You can separate data and index pages from ibdata into separate tablespace files (see innodb_file_per_table and StackOverflow Post on how to set this up)InnoDB 버퍼 풀  스토리지 엔진에서 가장 핵심적인 부분  데이터 캐시: 디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간  쓰기 버퍼링: 쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있게 해주는 버퍼 역할  페이지 크기의 조각으로 쪼개어 디스크로부터 읽어온 페이지를 저장  메모리 공간을 관리하기 위해 LRU리스트, 플러시 리스트, 프리 리스트라는 자료구조를 관리로그 버퍼  디스크의 로그 파일에 쓸 데이터를 버퍼링하는 메모리 영역  기본 크기는 16MB  로그 버퍼의 내용은 주기적으로 디스크로 플러시리두 로그  데이터의 변경 내용을 기록하는 디스크 기반 자료구조  ACID의 D(Durable)에 해당하는 영속성과 가장 밀접하게 연관  장애로 데이터 파일에 기록되지 못한 데이터를 잃지 않게 해주는 역할  상대적으로 비용이 높은 쓰기 작업의 성능 향상을 위해 로그 버퍼에 리두 로그를 버퍼링한 후 디스크 영역에 저장  데이터가 데이터 파일에 저장되는 시점보다 리두 로그가 로그 파일에 먼저 저장 -&amp;gt; 리두 로그를 WAL 로그라고도 함버퍼 풀과 리두 로그의 관계  버퍼 풀과 리두 로그의 관계를 이해하면 버퍼 풀 성능 향상에 도움이 되는 요소를 알 수 있음  데이터 변경이 발생하면 버퍼 풀에는 더티 페이지가 생기고, 로그 버퍼에는 리두 로그 레코드가 버퍼링  이 두가지 요소는 체크포인트마다 디스크로 동기화되어야 함  체크포인트는 장애 발생시 리두 로그의 어느 부분부터 복구를 실행해야 할지 판단하는 기준점버퍼 풀 플러시  버퍼 풀을 플러시하면 버퍼 풀 메모리 공간과 리두 로그 공간을 모두 얻을 수 있음  버퍼 풀을 플러시(더티 페이지들을 디스크에 동기화)하면 오래된 리두 로그 공간을 지울 수 있음  이를 위해 InnoDB 스토리지 엔진은 주기적으로 플러시 리스트 플러시 함수를 호출  플러시 리스트에서 오래 전에 변경된 데이터 페이지 순서대로 디스크에 동기화  또한 사용 빈도가 낮은 데이터 페이지를 제거하기 위해 LRU 리스트 플러시 함수를 호출  이 때 InnoDB 스토리지 엔진은 버퍼 풀을 스캔하며 더티 페이지는 동기화 클린 페이지는 프리 리스트로 옮김언두 로그  DML(INSERT, UPDATE, DELETE)로 변경되기 이전 버전의 백업된 데이터를 기록해두는 디스크 기반 자료구조  트랜잭션 보장: 트랜잭션이 롤백될 경우 원래 데이터로 복구하기 위해 언두 로그에 백업해둔 데이터를 이용  격리수준 보장: 특정 커넥션이 변경 중인 레코드에 다른 커넥션이 접근할 경우 격리수준에 맞게 언두 로그의 이전 데이터 제공  대용량 데이터를 변경하거나, 오랜 시간 유지되는 트랜잭션이 증가할 경우 언두 로그의 크기 급격히 증가  언두 로그의 사이즈가 커지면 쿼리 실행시 스토리지 엔진은 언두 로그를 필요한 만큼 스캔해야해서 쿼리의 성능이 감소어댑티브 해시 인덱스참고  Real MySQL 8.0 (1권) 책  MySQL 공식문서: The InnoDB Storage Engine  MyInfraBox: InnoDB Architecture      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-09T21:01:35+09:00'>09 Apr 2022</time><a class='article__image' href='/mysql-series7'> <img src='/images/mysql_3.png' alt='MySQL Series [Part7] 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series7'>MySQL Series [Part7] 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part3]: 해시테이블(Hash Table)",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/hash_table",
      "date"     : "Apr 9, 2022",
      "content"  : "Table of Contents  해시테이블  딕셔너리해시테이블해시 테이블은 키-값을 쌍으로 저장해둔 자료구조로 값(value)에 해당하는 키(key)가 무엇인지만 알면 어디에 위치한 값(value)이든 시간 복잡도 O(1)으로 값을 찾을 수 있습니다.딕셔너리파이썬에서 해시 테이블로 구현된 자료형은 딕셔너리입니다.딕셔너리는 대부분의 연산을 O(1)으로 처리할 수 있다는 점에서 성능이 매우 우수합니다.            연산      시간복잡도              len(a)      O(1)              a[key]      O(1)              key in a      O(1)      파이썬에서는 이외에도 딕셔너리에서 자주 사용되는 성질들을 편하게 사용하는 기능을 제공해주는 다양한 모듈들을 가지고 있습니다. 대표적으로 collections.defaultdict(), collections.Counter()가 있습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-09T21:01:35+09:00'>09 Apr 2022</time><a class='article__image' href='/hash_table'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part3]: 해시테이블(Hash Table)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hash_table'>Coding Test Series [Part3]: 해시테이블(Hash Table)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/stack_queue",
      "date"     : "Apr 8, 2022",
      "content"  : "Table of Contents  스택  큐스택스택은 가장 나중에 들어간 요소가 가장 먼저 나온다는 LIFO(Last In First Out)의 성질을 가지는 자료구조입니다. 이런 특징은 함수 호출과 같은 프로그래밍 세계에서 굉장히 자주 사용되기 때문에 중요합니다.스택에서 가장 중요한 것은 append와 pop을 얼마나 빨리 처리할 수 있냐입니다. 그렇기 때문에 이러한 기능을 O(1)의 시간복잡도로 제공해주는 동적배열(파이썬에서는 리스트)을 통해 스택을 구현할 수 있습니다.큐큐는 가장 먼저 들어간 요소가 가장 먼저 처리되는 FIFO(First In First Out)의 성질을 가지는 자료구조입니다.큐에서는 append와 pop(0)을 얼마나 빨리 처리할 수 있냐가 중요합니다. 동적배열에서 가장 앞 원소를 제거하는 pop(0) 연산의 시간 복잡도가 O(n)이기 때문에 파이썬에서는 리스트가 아닌 deque(데크)라는 별도의 자료형을 사용합니다.🦊 데크(deque)데크는 더블 엔디드 큐(Double-Ended Queue)의 줄임말로, 글자 그대로 양쪽 끝을 모두 추출할 수 있는 추상자료형으로 큐와 스택의 역할을 모두할 수 있습니다. 데크는 이중 연결 리스트로 구현되어 있습니다.큐는 큐 그자체로 보다는 우선순위 큐라는 변형된 방법으로 조금 더 많이 사용됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-08T21:01:35+09:00'>08 Apr 2022</time><a class='article__image' href='/stack_queue'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/stack_queue'>Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part5]: 여러가지 파일 포맷(JSON, BSON, Arrow, Avro, Parquet, ORC)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series5",
      "date"     : "Apr 7, 2022",
      "content"  : "Table of Contents  JSON          JSON 특징      JSON 장점      JSON 단점        BSON          BSON 특징      BSON 장점      BSON 단점        Avro          Avro 특징      Avro 장점      Avro 단점      Avro 예제                  Primitive Data Type          Complex Data Type                      Parquet          Parquet 특징      Parquet 장점        Serialization vs Encoding          Serialization      Encoding        File format          Binary Files      Text Files        참고JSONJSON 특징  JavaScript Object Notation의 약자  서버와 클라이언트 간의 통신에서 일반적으로 많이 사용  자바스크립트 문법과 굉장히 유사하지만 텍스트 형식일 뿐JSON 장점  프로그래밍 모든 분야에 데이터를 표현하는 용도로 널리 사용된다  문법이 쉽고 간단하다  데이터를 쉽게 구조화할 수 있다JSON 단점  텍스트 기반이기 때문에 구문분석이 느리다  BSON에 비해 크기가 크다BSONBSON 특징  JSON을 Binary로 인코딩한 포맷이다  MongoDB 진영에서 처음 등장한 데이터 포맷이다BSON 장점  JSON에 비해 용량이 가벼운 데이터 포맷이다  JSON과 비교해 더 많은 데이터 타입을 사용할 수 있다BSON 단점  아직 JSON만큼 프로그래밍의 다양한 진영에서 지원되지는 않는다AvroAvro 특징  아파치의 하둡 프로젝트에서 개발된 데이터 직렬화 프레임워크이다  데이터 직렬화를 위한 스키마를 뜻하며 JSON 형태로 나타낸다Avro 장점  데이터의 타입을 알 수 있다  데이터를 압축하여 저장한다  Hadoop, Kafka 진영에서 많이 사용된다Avro 단점  바이너리 형태로 직렬화되어 데이터를 쉽게 분석할 수 없다Avro 예제Primitive Data Typetype: null, bool, int, long, float, double, bytes, string{&quot;type&quot;: &quot;string&quot;}{&quot;type&quot;: &quot;int&quot;}{&quot;type&quot;: &quot;boolean&quot;}{&quot;type&quot;: &quot;long&quot;}Complex Data Typetype: recordnamenamespacedocaliasesfields: name, doc, type, default{  &quot;type&quot;: &quot;record&quot;,  &quot;name&quot;: &quot;Students&quot;,  &quot;fields&quot;: [    {&quot;name&quot;: &quot;id&quot;, &quot;type&quot;: &quot;long&quot;},    {&quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;},    {&quot;name&quot;: &quot;majors&quot;, &quot;type&quot;: &quot;array&quot;, &quot;values&quot;: &quot;string&quot;},    {&quot;name&quot;: &quot;phone&quot;, &quot;type&quot;: &quot;string&quot;}  ]}ParquetParquet 특징      열 기반의 데이터 저장 포맷이다        Apache 진영에서 많이 사용된다 (특히 Apache Spark)  Parquet 장점  같은 데이터 타입을 압축하기 때문에 압축률이 Row based에 비해 더 높다  데이터 분석시 필요한 열(Column)만 읽어서 처리할 수 있다  저장용량의 이점, 처리 성능의 이점이 있다Serialization vs EncodingSerializationSerialization generally refers to taking a data structure which exists in memory, and converting it to a string (either binary or text) so that it can be saved to a file, or sent across a network.Serialization is an actual in-memory object that is transformed into an unusable state but the new state can be easily stored or transferred across a network. When it is needed to be used again, it can be deserialized and loaded back into memory in the same state that it was when it was serialized.Serializing is about moving structured data over a storage/transmission medium in a way that the structure can be maintained.EncodingEncoding is a more general term. Encoding means converting to a different format expected by some consumer. So you might encode something to URL format, or JSON, or a binary format, or whatever. You’re right that serialization is a specific instance of encoding.Encoding is more broad, like about how said data is converted to different forms, etc. Perhaps you could think about serializing being a subset of encoding in this example.With regard to a web service, you will probably be considering serializing/deserializing certain data for making/receiving requests/responses - effectively transporting “messages”. Encoding is at a lower level, like how your messaging/web service type/serialization mechanism works under the hood.File formatAll files can be categorized into one of two file formats — binary or text. The two file types may look the same on the surface, but they encode data differently. While both binary and text files contain data stored as a series of bits (binary values of 1s and 0s), the bits in text files represent characters, while the bits in binary files represent custom data.While text files contain only textual data, binary files may contain both textual and custom binary data.Binary FilesBinary files typically contain a sequence of bytes, or ordered groupings of eight bits. When creating a custom file format for a program, a developer arranges these bytes into a format that stores the necessary information for the application. Binary file formats may include multiple types of data in the same file, such as image, video, and audio data. This data can be interpreted by supporting programs, but will show up as garbled text in a text editor. Below is an example of a .PNG image file opened in an image viewer and a text editor.As you can see, the image viewer recognizes the binary data and displays the picture. When the image is opened in a text editor, the binary data is converted to unrecognizable text. However, you may notice that some of the text is readable. This is because the PNG format includes small sections for storing textual data. The text editor, while not designed to read this file format, still displays this text when the file is opened. Many other binary file types include sections of readable text as well. Therefore, it may be possible to find out some information about an unknown binary file type by opening it in a text editor.Binary files often contain headers, which are bytes of data at the beginning of a file that identifies the file’s contents. Headers often include the file type and other descriptive information. For example, in the image above, the “PNG” text indicates the file is a PNG image. If a file has invalid header information, software programs may not open the file or they may report that the file is corrupted.Text FilesText files are more restrictive than binary files since they can only contain textual data. However, unlike binary files, they are less likely to become corrupted. While a small error in a binary file may make it unreadable, a small error in a text file may simply show up once the file has been opened. This is one of reasons Microsoft switched to a compressed text-based XML format for the Office 2007 file types.Text files may be saved in either a plain text (.TXT) format and rich text (.RTF) format. A typical plain text file contains several lines of text that are each followed by an End-of-Line (EOL) character. An End-of-File (EOF) marker is placed after the final character, which signals the end of the file. Rich text files use a similar file structure, but may also include text styles, such as bold and italics, as well as page formatting information. Both plain text and rich text files include a (character encoding, characterencoding) scheme that determines how the characters are interpreted and what characters can be displayed.Since text files use a simple, standard format, many programs are capable of reading and editing text files. Common text editors include Microsoft Notepad and WordPad, which are bundled with Windows, and Apple TextEdit, which is included with Mac OS X.참고  everydayminder: Avro개요  Jaemun Jung, [Avro] Data Encoding과 Avro Format  VCNC Engineering: Apache Spark에서 컬럼 기반 저장 포맷 Parquet(파케이) 제대로 활용하기  Youtube, Differences AVRO vs Protobuf vs Parquet vs ORC, JSON vs XML  What is the difference between binary and text files?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-07T21:01:35+09:00'>07 Apr 2022</time><a class='article__image' href='/data-engineering-series5'> <img src='/images/data_format_logo.jpeg' alt='Data Engineering Series [Part5]: 여러가지 파일 포맷(JSON, BSON, Arrow, Avro, Parquet, ORC)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series5'>Data Engineering Series [Part5]: 여러가지 파일 포맷(JSON, BSON, Arrow, Avro, Parquet, ORC)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part1]: 배열(Array)",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/array",
      "date"     : "Apr 7, 2022",
      "content"  : "Table of Contents  배열  itertools배열자료구조는 크게 메모리 공간 기반의 연속 방식과 포인터 기반의 연결 방식으로 나뉩니다. 배열은 이 중에서 연속 방식의 가장 기본이 되는 자료형입니다. 연결 방식의 경우는 연결 리스트입니다.추상자료형(스택, 큐, 트리, 그래프 등)은 대부분 배열과 연결리스트를 기반으로 구현되었습니다.파이썬에서는 정확히 배열, 연결 리스트라는 자료형은 따로 없습니다. 하지만 더욱 편리한 리스트라는 자료형이 있습니다. 리스트 자료형은 배열과 연결 리스트의 장점을 모두 합쳐놓은 파이썬만의 자료형입니다.그렇기 때문에 저는 지금부터는 코딩테스트에서 리스트를 사용할 때 도움이 될만한 지식들을 여기에 기록해두도록 하겠습니다.파이썬 리스트는 그 자체로 정말 유용한 메소드들을 많이 보유하고 있습니다.a = list()print(dir(a))--------------------------------------------------------------------------------------------------&#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;itertools배열은 대표적인 시퀀스 자료형입니다. 그렇기 때문에 배열안의 원소들을 순회하는 경우가 많습니다. 이 때 파이썬의 itertools 모듈을 적절히 사용하면 코드를 훨씬 더 간결하고 가독성 높게 작성할 수 있습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-07T21:01:35+09:00'>07 Apr 2022</time><a class='article__image' href='/array'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part1]: 배열(Array)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/array'>Coding Test Series [Part1]: 배열(Array)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part0]: Top 10 algorithms in Interview Questions",
      "category" : "",
      "tags"     : "Coding_Test",
      "url"      : "/algo_intro",
      "date"     : "Apr 6, 2022",
      "content"  : "Table of Contents  참고참고  GeeksforGeeks, Top 10 algorithms in Interview Questions",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-06T21:01:35+09:00'>06 Apr 2022</time><a class='article__image' href='/algo_intro'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part0]: Top 10 algorithms in Interview Questions'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/algo_intro'>Coding Test Series [Part0]: Top 10 algorithms in Interview Questions</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part4]: 데이터베이스간 주요 특징 비교",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series4",
      "date"     : "Apr 5, 2022",
      "content"  : "Table of Contents  데이터베이스 주요 특성          데이터베이스 모델                  RDBMS          Document Store          Wide Column Store          Key-Value Store                    쿼리 지원      In-memory 지원      검색엔진 지원      Scale-Up, Scale-Out      Row Based vs Columnar        주요 데이터베이스간 비교          MySQL vs Hive vs Redshift      MongoDB vs Cassandra vs ElasticSearch      DynamoDB vs HBase vs Redis        Youtube  참고데이터베이스 주요 특성데이터베이스 모델RDBMS  MySql  PostreSQL  Apache Hive  AWS RedshiftDocument Store  Schema free한 DBMS와 비슷한 느낌 (NoSQL 중에서 가장 RDBMS와 비슷한 데이터베이스)  데이터가 완전히 Structured 되어 있지 않지만, 최대한 DBMS와 비슷하게 사용하고 싶은 경우  데이터 하나는 JSON과 같은 객체  SQL과 같은 언어로 데이터를 필터링하는 느낌으로 데이터를 읽음 =&amp;gt; 읽어오는게 key-value만큼 빠르지는 않음  쿼리의 성능을 조금 포기하는 대신 범용적인 데이터 수집을 가능하게 함  ex. MongoDB, Elasticsearch 등Wide Column Store  Wide column databases store data in large column-based tables instead of rows. Queries can be run quickly on large amounts of data, making these databases common for retail and IoT data.  A wide-column database is a NoSQL database that organizes data storage into flexible columns that can be spread across multiple servers or database nodes, using multi-dimensional mapping to reference data by column, row, and timestamp.  A wide-column database is a type of NoSQL database in which the names and format of the columns can vary across rows, even within the same table. Wide-column databases are also known as column family databases. Because data is stored in columns, queries for a particular value in a column are very fast, as the entire column can be loaded and searched quickly. Related columns can be modeled as part of the same column family.  Benefits of a wide-column NoSQL database include speed of querying, scalability, and a flexible data model.      A relational database management system (RDBMS) stores data in a table with rows that all span a number of columns. If one row needs an additional column, that column must be added to the entire table, with null or default values provided for all the other rows. If you need to query that RDBMS table for a value that isn’t indexed, the table scan to locate those values will be very slow.    Wide-column NoSQL databases still have the concept of rows, but reading or writing a row of data consists of reading or writing the individual columns. A column is only written if there’s a data element for it. Each data element can be referenced by the row key, but querying for a value is optimized like querying an index in a RDBMS, rather than a slow table scan.  A Columnar data store will store each column separately on disk. A Wide-column database is a type of columnar database that supports a column family stored together on disk, not just a single column.  Key-value와 비슷한데 차이점은 Key-value는 key로 무조건 Value 전체를 읽어야 함  Wide-column은 key로 읽은 결과에서 Column을 통해 더 specific한 value를 얻을 수 있음  ex. Apache Cassandra, Apache HBaseKey-Value Store  해시테이블과 같은 자료구조를 데이터베이스화 한 것  Key를 통해 value를 가져올 수 있다  Key는 유니크해야 한다  Value는 어떤것이든 될 수 있다(숫자, 텍스트, JSON, URI, 이미지 등)  Value의 일부만 읽는 것은 불가능 (이 점이 wide-column과의 차이)  Value를 SQL과 같은 언어가 아니라 key값으로 가져옴 -&amp;gt; 굉장히 빠르다  Key에 해당하는 value 한개  Key-value store + In-memory =&amp;gt; Redis =&amp;gt; Redis가 캐싱 DB 서버로 많이 사용되는 이유  ex. Redis, AWS DynamoDB, Apache HBase쿼리 지원  쿼리를 지원하지 않는 DB: Redis, Apache HBase 등  쿼리를 지원하는 DB: MySQL, PostgreSQL, AWS Redshift, Google Bigquery, Elasticsearch 등In-memory 지원In-memory databases are purpose-built databases that rely primarily on memory for data storage, in contrast to databases that store data on disk or SSDs. In-memory data stores are designed to enable minimal response times by eliminating the need to access disks. Because all data is stored and managed exclusively in main memory, in-memory databases risk losing data upon a process or server failure. In-memory databases can persist data on disks by storing each operation in a log or by taking snapshots.In-memory databases are ideal for applications that require microsecond response times or have large spikes in traffic such as gaming leaderboards, session stores, and real-time analytics.  Redis  AWS ElastiCache  Microsoft SQL Server  (MySQL, MongoDB와 같은 DB도 메모리 캐시를 지원하지만 주요 저장장치는 디스크이기 때문에 In-memory가 아님)검색엔진 지원  ElasticsearchScale-Up, Scale-Out  RDBMS가 Scale-Out이 가능한지에 관한 답은 CAP 이론을 공부해야함  Relational databases are designed to run on a single server in order to maintain the integrity of the table mappings and avoid the problems of distributed computing.  RDBMS가 Scale-out이 힘들다면 Redshift는 뭐지?  Redshift는 Columnar database라서 scale-out이 쉽게 가능  (Columnar databases are like NoSQL databases, in the fact that they are designed to scale “out” using distributed clusters of low-cost hardware to increase throughput)Today, the evolution of relational databases allows them to use more complex architectures, relying on a “master-slave” model in which the “slaves” are additional servers that can handle parallel processing and replicated data, or data that is “sharded” (divided and distributed among multiple servers, or hosts) to ease the workload on the master server.Other enhancements to relational databases such as using shared storage, in-memory processing, better use of replicas, distributed caching, and other new and ‘innovative’ architectures have certainly made relational databases more scalable. Under the covers, however, it is not hard to find a single system and a single point-of-failure (For example, Oracle RAC is a “clustered” relational database that uses a cluster-aware file system, but there is still a shared disk subsystem underneath). Often, the high costs of these systems is prohibitive as well, as setting up a single data warehouse can easily go over a million dollars.The enhancements to relational databases also come with other big trade-offs as well. For example, when data is distributed across a relational database it is typically based on pre-defined queries in order to maintain performance. In other words, flexibility is sacrificed for performance.Additionally, relational databases are not designed to scale back down—they are highly inelastic. Once data has been distributed and additional space allocated, it is almost impossible to “undistribute” that data.NoSQL databases are designed for massive scale on distributed systems (usually hundreds of Terabytes rather than tens of Gigabytes). They can scale-out “horizontally,” meaning that they run on multiple servers that work together, each sharing part of the load.Using this approach, a NoSQL database can operate across hundreds of servers, petabytes of data, and billions of documents—and still manage to process tens of thousands of transactions per second. And it can do all of this on inexpensive commodity (i.e. cheaper) hardware operating in any environment (i.e. cloud optimized!). Another benefit is that if one node fails, the others can pick up the workload, thus eliminating a single point of failure.Massive scale is impressive, but what is perhaps even more important is elasticity. Not all NoSQL databases are elastic. MarkLogic has a unique architecture that make it possible to quickly and easily add or remove nodes in a cluster so that the database stays in line with performance needs. There is not any complex sharding of data or architectural workarounds—data is automatically rebalanced across a cluster when nodes are added or removed. This also makes administration much easier, making it possible for one DBA to manage for data and with fewer headaches.Row Based vs Columnar  Row Based DB는 OLTP에 적합          record단위로 데이터가 발생할 때 이를 하나의 메모리 블럭에 저장 -&amp;gt; 빠른 쓰기 작업      단점은 특정 컬럼값이 모두 필요한 경우 모든 메모리 블럭에 접근해야함 -&amp;gt; 느린 분석      ex. MySQL, PostgreSQL, Oracle 등        Columnar DB는 OLAP에 적합          분석은 특정 컬럼에 있는 모든 값을 이용하는 경우 많음      같은 컬럼에 있는 값을 하나의 메모리 블럭에 저장하면 Random I/O을 줄일 수 있음      또 하나의 메모리 블럭에 같은 타입의 값만 저장하면 DB는 그 타입에 맞는 특별한 압축 방식을 제공해 줄 수 있음      단점은 record단위로 발생하는 데이터를 컬럼별로 따로 저장하기 위해 쓰기 작업시 컬럼 개수만큼 더 많은 I/O 작업 발생      ex. AWS Redshift, Google Big Query, Apache HBase 등        웹 서비스에 붙어있는 DB로는 OLTP를 위한 Row based DB가 적합하고, 이 데이터를 분석팀에게 제공할 때는 Columnar DB에 저장해서 전달해 주는 것이 좋다.  Columnar는 Scale-Out을 쉽게 만들어준다.주요 데이터베이스간 비교MySQL vs Hive vs RedshiftMongoDB vs Cassandra vs ElasticSearchDynamoDB vs HBase vs Redis  AWS whitepapaer: Comparing the Use of Amazon DynamoDB and Apache HBase for NoSQLYoutube참고  DB Engine 비교  AWS, What Is an In-Memory Database?  SCYLLA, Wide-column Database  stackoverflow, What exactly is a wide column store?  stackoverflow, Can relational database scale horizontally  What Keeps Relational Databases From Horizontal Scaling?  MATT ALLEN, Relational Databases Are Not Designed For Scale  JBee, Scale Up, Scale Out, Sharding  HEAVIY.AI, Columnar Database  indicative, What Is A Columnar Database?  sentinelone, Understanding Row- vs Column-Oriented Databases  Youtube: Tech Dummies Narendra L, How row oriented and column oriented db works?  Youtube: Tech Dummies Narendra L, In Memory databases internals for system design interviews                              [Youtube: Tech Dummies Narendra L, Learn System design : Distributed datastores          RDBMS scaling problems          CAP theorem](https://www.youtube.com/watch?v=l9JSK9OBzA4){:target=”_blank”}                      Youtube: Tech Dummies Narendra L, Learn System design : How distributed datastore works(basics)?  IBM, CAP Theorem",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-05T21:01:35+09:00'>05 Apr 2022</time><a class='article__image' href='/data-engineering-series4'> <img src='/images/database_logo.jpeg' alt='Data Engineering Series [Part4]: 데이터베이스간 주요 특징 비교'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series4'>Data Engineering Series [Part4]: 데이터베이스간 주요 특징 비교</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part3]: 비트코인에 관한 기술적 접근",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series2",
      "date"     : "Apr 4, 2022",
      "content"  : "Table of Contents  해시 함수(비트코인 주소)  비대칭 암호화 기법(전자서명)  작업 증명  하드포크와 소프트포크  참고해시 함수(비트코인 주소)비대칭 암호화 기법(전자서명)작업 증명하드포크와 소프트포크참고  블록체인 해설서 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-04T21:01:35+09:00'>04 Apr 2022</time><a class='article__image' href='/blockchain-series2'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part3]: 비트코인에 관한 기술적 접근'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series2'>BlockChain Series [Part3]: 비트코인에 관한 기술적 접근</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part7]: Javascript 객체",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series7",
      "date"     : "Apr 3, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-03T21:01:35+09:00'>03 Apr 2022</time><a class='article__image' href='/javascript-series7'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part7]: Javascript 객체'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series7'>Javascript Series [Part7]: Javascript 객체</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part3]: Elastic 생태계 (Elasticsearch, Logstash, Kibana)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series3",
      "date"     : "Apr 3, 2022",
      "content"  : "Table of Contents  LogstashLogstash  [Logstash] 로그스테이시 사용법 (설정파일) + Elastic  elastic: 데이터 집계, 변환, 저장",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-03T21:01:35+09:00'>03 Apr 2022</time><a class='article__image' href='/data-engineering-series3'> <img src='/images/elastic_logo.jpeg' alt='Data Engineering Series [Part3]: Elastic 생태계 (Elasticsearch, Logstash, Kibana)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series3'>Data Engineering Series [Part3]: Elastic 생태계 (Elasticsearch, Logstash, Kibana)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series1",
      "date"     : "Apr 3, 2022",
      "content"  : "Table of Contents  블록체인 구조          노드      블록        블록체인 동작원리          트랜잭션 요청      블록 생성      검증        비트코인 생태계          블록체인      비트코인 시스템      비트코인 지갑(UTXO)      중개소        참고블록체인 구조노드노드는 일반적으로 어떤 네트워크에 속해있는 기기(컴퓨터, 스마트폰 등) 또는 기기를 사용하는 유저를 일컫습니다. 블록체인 네트워크에서 추구하는 목표는 신뢰할 수 있는 기관(정부, 은행, 핀테크 기업 등) 없이도 사용자(노드)들이 주체가 되어 경제 시스템을 동작하도록 하는 것입니다. 그렇기 때문에 블록체인 기반의 네트워크 구조에서는 노드들의 역할이 굉장히 중요합니다.아래 그림은 기존의 거래를 위한 네트워크 구조와 블록체인 기반의 구조를 비교한 것입니다.기존에 존재하던 네트워크의 경우 A가 B와 거래를 하기 위해서는 자기 자신의 거래내역만 가지고 있으면 됐었습니다. 이게 가능한 이유는 중앙의 신뢰할 수 있는 기관이 거래에서 발생할 수 있는 문제들을 중재하기 때문이었습니다. 하지만 블록체인 기반 네트워크는 이러한 기관을 없애는 것이 목표입니다. 그래서 블록체인에서는 거래내역의 위,변조를 막기 위해 네트워크내 모든 노드들에게 이러한 거래내역을 저장하도록 하고 데이터에 변화가 일어났는지 서로를 감시하도록 합니다.아래 그림은 모든 노드가 블록체인 데이터를 함께 저장(분산 저장이 아니라 중복 저장)하고 블록체인 데이터에 변화가 일어나는지 감시합니다. 이 부분에 대해서는 나중에 블록체인의 동작원리 파트에서 더 자세히 설명하도록 하겠습니다.그러면 지금까지 이렇게 저장된 블록의 용량은 얼마나 될까요? 블록체인 기반 서비스의 조상인 비트코인의 경우 2021년 9월 그동안 생성된 블록의 개수는 70만개를 넘어섰다고 합니다. 블록 하나의 최대 크기는 1MB이므로 평균 0.5MB라고 했을 때 대략 340GB입니다. 저희는 블록체인 기반의 거래 서비스(중 비트코인)을 이용하기 위해 최소한 340GB의 용량이 컴퓨터에 필요하다는 뜻입니다. 이러한 문제 때문에 비트코인의 경우 노드를 설계할 때 두가지의 다른 형태를 두었습니다. 하나는 완전 노드고, 다른 하나는 단순 지급 검증 노드입니다.  완전 노드          블록체인 데이터 전체 다운 (300GB 이상 디스크 공간 요구)      블록 검증 (검증에 대해서는 보상 없음) -&amp;gt; 사실상 인센티브 공학의 결함      블록 생성 (비트코인 보상) -&amp;gt; 채굴자        단순 지급 검증 노드          블록의 헤더 정도만 다운 (100MB 정도의 작은 공간)      트랜잭션을 요청하는 단순 서비스 이용자      블록블록은 마치 거래 장부에서 특정 페이지에 비유할 수 있습니다. 블록 하나에는 2000~3000개 정도의 트랜잭션(거래내역)을 담을 수 있는데 그 이유는 블록 하나의 크기는 1MB로 제한되는데 트랜잭션 하나의 크기가 보통 0.3kB정도이기 때문입니다.비트코인 블록의 구조는 크게 블록의 크기, 블록 헤더, 블록 내 거래내역 개수, 모든 거래내역 이렇게 4가지로 구성됩니다.  블록의 크기          4바이트      블록 하나의 크기 (최대 1메가바이트)        블록 헤더          80바이트      버전 정보, 이전 블록 헤더 해시, 머클 트리 루트, 타임 스탬프, 타깃 난이도 비트, 난스로 구성        블록 내 전체 거래내역 수          1~9바이트        블록 내 모든 트랜잭션          가변 크기      2000~3000개 정도 되는 트랜잭션 저장      블록체인 동작원리여기서는 트랜잭션을 요청한 후 어떻게 블록이 생성되고 그 보상으로 채굴자가 비트코인을 얻게 되는지에 관한 과정을 알아보도록 하겠습니다.트랜잭션 요청블록체인에서는 중앙 서버가 존재하지 않기 때문에 누가 트랜잭션을 처리할 것인지 정해져 있지 않습니다. 그렇기 때문에 블록체인에서는 기본적으로 트랜잭션을 모든 노드에게 브로드캐스팅합니다.이렇게 전달된 트랜잭션들은 각 노드의 대기실에 쌓인 채로 처리되기를 기다립니다. 또한 이 때 처리되는 트랜잭션의 목록과 순서는 노드마다 다릅니다. 노드 A는 a, b, c를 묶어서 처리하고 노드 B는 b, a, d를 처리하는 등 네트워크 상태와 트랜잭션에 책정된 수수료에 따라 달라집니다. 통상 노드는 수수료가 더 높은 트랜잭션을 먼저 처리합니다.블록 생성완전 노드 역할을 하는 노드들은 자신들이 가지고 있는 트랜잭션을 가지고 서로 비동기적으로 블록을 생성하기 위해 경쟁합니다. 노드들이 블록을 만들기 위해 경쟁을 한다고 했는데 어떤 것을 가지고 경쟁하고 있을까요? 그것은 바로 해시 퍼즐입니다. 해시 퍼즐은 공식이 없이 단순 반복을 통해서만 정답을 찾을 수 있습니다. 이러한 단순 반복을 통해 가장 먼저 정답을 찾은 하나의 노드만이 리더로 선출되고 블록을 만들 자격을 얻는 것입니다. 이렇게 블록을 만들기 위해 해시 퍼즐을 푸는 과정을 채굴한다고 합니다. 또한 이렇게 해시 퍼즐을 통해 리더를 선출하는 방법을 작업 증명 방식이라고 합니다. 이외에도 지분 증명이라는 것이 있지만 여기서는 다루지 않도록 하겠습니다.검증위의 그림에서 살펴봤듯이 노드들은 리더가 만든 블록을 자신의 블록체인 데이터에 추가하기 전에 두 가지를 점검합니다. 먼저 리더가 해시 퍼즐의 정답을 제대로 찾았는지 확인하고, 두 번째로 리더가 추가한 트랜잭션에 문제(위조, 변조, 이중 사용 등)가 없는지 검증하는 절차를 가집니다. 이 두 검증 과정은 해시 함수와 전자 서명, 비대칭 암호화 기법을 활용해 순식간에 이뤄집니다.비트코인 생태계블록체인비트코인 시스템비트코인 지갑(UTXO)중개소참고  블록체인 해설서 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-03T21:01:35+09:00'>03 Apr 2022</time><a class='article__image' href='/blockchain-series1'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series1'>BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)</a> </h2><p class='article__excerpt'>블록체인 네트워크에서 추구하는 목표는 신뢰할 수 있는 기관 없이도 사용자들이 주체가 되어 경제 시스템을 동작하도록 하는 것입니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part6]: Javascript 함수와 스코프",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series6",
      "date"     : "Apr 2, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-02T21:01:35+09:00'>02 Apr 2022</time><a class='article__image' href='/javascript-series6'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part6]: Javascript 함수와 스코프'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series6'>Javascript Series [Part6]: Javascript 함수와 스코프</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part5]: Javascript 제어문과 반복문",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series5",
      "date"     : "Apr 1, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-01T21:01:35+09:00'>01 Apr 2022</time><a class='article__image' href='/javascript-series5'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part5]: Javascript 제어문과 반복문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series5'>Javascript Series [Part5]: Javascript 제어문과 반복문</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part2]: RDBMS vs NoSQL",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series2",
      "date"     : "Apr 1, 2022",
      "content"  : "Table of Contents  RDBMS          Relational Data Model        NoSQL          The benefits of NoSQL        RDBMS vs NoSQL  참고RDBMS  RDBMS는 Relational DataBase Management System의 약자  Relational Data Model을 담고있는 데이터베이스를 CRUD할 수 있는 소프트웨어  그 밖에도 데이터의 ACID 특성, 보안과 같은 필요한 기능들을 제공A database back end for your application is vital. It’s likely your database is a relational database. This is because relational databases have dominated the software industry for decades, even as other technologies have radically changed around them.In a relational database management system (RDBMS), data needs to be actively managed and maintained. An RDBMS categorizes data in the form of tables, and the database manager must create a blueprint — otherwise known as a schema — of the relationships between those tables before any data gets stored. The fields in these tables must also have well-defined data types for which values can be stored.When considering implementing a relational database for your data storage needs, it’s important to completely understand what you’re getting into. While RDBMSs are known for being robust, they’re also known for being slow and inflexible.Be sure to consider the query language used to interface with the database you’re interested in. You should also perform a cost-benefit analysis of what it will take to create indices that can speed data operations, as well as the administrative tooling to operate the database.Relational Data Model  1970년 대 E. F. Codd에 의해 처음 제안  테이블과 같은 형태로 데이터를 저장할 수 있도록 모델링한 것  관계형 데이터 모델은 세상을 상호 관계를 가지는 테이블들의 집합으로 묘사  Table: Record의 집합  Attribute: Record의 속성  Record: 관계형 모델로 모델링된 데이터  Degree: 속성(Attribute)의 수  Cardinality: Unique한 Record의 수 (집합의 크기)NoSQL  요즘은 세상의 모든 것들을 데이터로 뽑아낼 수 있다면 수집해서 저장하길 원함  그런데 온갖 종류의 데이터가 다 관계형 데이터 모델로 정의되지 않음  Semi-Structured한 데이터를 저장하기 위한 용도로 NoSQL 등장  RDBMS와 비교해 쿼리의 성능을 조금 포기하는 대신 범용적인 데이터 수집을 가능하게 함  (JOIN과 같은 쿼리 고급 기능이 없거나, 성능이 떨어짐)  NoSQL은 데이터를 표현하는 방법도 다양해서 그 안에서도 Document store, Key-value store, Wide column store 등과 같이 더 세분화됨The benefits of NoSQL“Not only SQL” (NoSQL)  databases were designed to fill the gaps left by relational databases. Consider the core characteristics of a NoSQL database:  Schema-less with no complex relationships  Distributed by replicating data to avoid a single point of failure  Flexible storage of both unstructured and semi-structured data  Highly scalable no matter how much data is enteredRDBMS vs NoSQL  RDBMS도 그 종류가 다양하고, NoSQL은 더 다양함 -&amp;gt; 묶어서 비교하는 것은 정확한 비교 방법은 아님  그래도 최대한 비교 가능한 부분만 가지고 비교해보자참고  khj93, [Database] RDBMS와 NoSQL의 차이점  BCcampus, The Relational Data Model  DB Enigne",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-01T21:01:35+09:00'>01 Apr 2022</time><a class='article__image' href='/data-engineering-series2'> <img src='/images/rdbms_nosql.png' alt='Data Engineering Series [Part2]: RDBMS vs NoSQL'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series2'>Data Engineering Series [Part2]: RDBMS vs NoSQL</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series1",
      "date"     : "Apr 1, 2022",
      "content"  : "Table of Contents  DataBase  Data Warehouse  Data Lake  Data Lakehouse  Summary  참고DataBaseA database is a collection of data or information. Databases are typically accessed electronically and are used to support Online Transaction Processing (OLTP). Database Management Systems (DBMS) store data in the database and enable users and applications to interact with the data. The term “database” is commonly used to reference both the database itself as well as the DBMS.A variety of database types have emerged over the last several decades. All databases store information, but each database will have its own characteristics. Relational databases store data in tables with fixed rows and columns. Non-relational databases (also known as NoSQL databases) store data in a variety of models including JSON (JavaScript Object Notation), BSON (Binary JSON), key-value pairs, tables with rows and dynamic columns, and nodes and edges. Databases store structured and/or semi-structured data, depending on the type.You may also find database characteristics like:  Security features to ensure the data can only be accessed by authorized users.  ACID (Atomicity, Consistency, Isolation, Durability) transactions to ensure data integrity.  Query languages and APIs to easily interact with the data in the database.  Indexes to optimize query performance.  Full-text search.  Optimizations for mobile devices.  Flexible deployment topologies to isolate workloads (e.g., analytics workloads) to a specific - set of resources.  On-premises, private cloud, public cloud, hybrid cloud, and/or multi-cloud hosting options.If your application needs to store data (and nearly every interactive application does), your application needs a database. Applications across industries and use cases are built on databases. Many types of data can be stored in databases, including:  Patient medical records  Items in an online store  Financial records  Articles and blog entries  Sports scores and statistics  Online gaming information  Student grades and scores  IoT device readings  Mobile application informationDatabase examples  Relational databases: Oracle, MySQL, Microsoft SQL Server, and PostgreSQL  Document databases: MongoDB and CouchDB  Key-value databases: Redis and DynamoDB  Wide-column stores: Cassandra and HBase  Graph databases: Neo4j and Amazon NeptuneData Warehouse데이터 웨어하우스는 정보에 입각한 의사 결정을 내릴 수 있도록 분석 가능한 정보의 중앙 repository다트랜잭션 시스템, RDB 및 기타 소스의 데이터들이 데이터웨어하우스에 들어간다(insert)이렇게 데이터웨어하우스에 있는 데이터들을 Business Analyst나 Data Scientist 와 같은 사람들이 BI(Business Inteligence)나 SQL 등을 이용해서 데이터에 액세스 한다하단 티어 즉 최하위에 있는 부분은 Data 부분인 위에서 말했던 트랜잭션 시스템, RDB 등을 비롯한 데이터들이고 이것들이 ETL(Extract Transform Load) 과정을 거쳐서 Data Warehouse에 적재가 된다중간 티어 OLAP Server는 데이터에 액세스하고 분석하는데 사용되는 분석엔진 들이다, 여기서 OLAP란 OnLine Analytical Processing을 말한다.상위 티어는 사용자가 실제로 데이터를 분석하고 마이닝을 하고 또 보고할 때 사용하게 되는 frontend가 존재하는 티어다.이렇게 이루어진 데이터 웨어하우스는 데이터를 정수, 데이터 필드 또는 문자열과 같은 레이아웃 및 유형들을 설명하는 스키마로 구성함으로써 동작하게 된다. 즉 ETL, 데이터를 추출해서 변환해서 스키마에 적재해두는 것이다A data warehouse is a unified data repository for storing large amounts of information from multiple sources within an organization. A data warehouse represents a single source of “data truth” in an organization and serves as a core reporting and business analytics component.Typically, data warehouses store historical data by combining relational data sets from multiple sources, including application, business, and transactional data. Data warehouses extract data from multiple sources and transform and clean the data before loading it into the warehousing system to serve as a single source of data truth. Organizations invest in data warehouses because of their ability to quickly deliver business insights from across the organization.Data warehouses enable business analysts, data engineers, and decision-makers to access data via BI tools, SQL clients, and other less advanced (i.e., non-data science) analytics applications.Note that data warehouses are not intended to satisfy the transaction and concurrency needs of an application. If an organization determines they will benefit from a data warehouse, they will need a separate database or databases to power their daily operations.  Amazon Redshift.  Google BigQuery.  IBM Db2 Warehouse.  Microsoft Azure Synapse.  Oracle Autonomous Data Warehouse.  Snowflake. (YOUR DATA WAREHOUSE AND DATA LAKE)  Teradata Vantage.Data Lake데이터 레이크는 데이터 웨어하우스와는 달리 별도로 정형화나 정규화 등을 하지 않고 데이터를 있는 그대로 원시데이터 상태를 저장한다는 것이다.데이터 레이크로 유입되는 데이터들은 자신의 출처와 시간 같은 메타데이터가 존재하여야 한다.데이터 레이크는 그 크기가 매우 커질것이고 대부분의 저장소는 스키마가 없는 큰 규모의 구조를 지향하기 때문에 일반적으로 데이터 레이크를 구현을 할 때 Hadoop과 HDFS를 비롯한 에코시스템을 사용하는 것이다. (?)A data lake is a centralized, highly flexible storage repository that stores large amounts of structured and unstructured data in its raw, original, and unformatted form. In contrast to data warehouses, which store already “cleaned” relational data, a data lake stores data using a flat architecture and object storage in its raw form. Data lakes are flexible, durable, and cost-effective and enable organizations to gain advanced insight from unstructured data, unlike data warehouses that struggle with data in this format.In data lakes, the schema or data is not defined when data is captured; instead, data is extracted, loaded, and transformed (ELT) for analysis purposes. Data lakes allow for machine learning and predictive analytics using tools for various data types from IoT devices, social media, and streaming data.A data lake is a repository of data from disparate sources that is stored in its original, raw format. Like data warehouses, data lakes store large amounts of current and historical data. What sets data lakes apart is their ability to store data in a variety of formats including JSON, BSON, CSV, TSV, Avro, ORC, and Parquet.Typically, the primary purpose of a data lake is to analyze the data to gain insights. However, organizations sometimes use data lakes simply for their cheap storage with the idea that the data may be used for analytics in the future.You might be wondering, “Is a data lake a database?” A data lake is a repository for data stored in a variety of ways including databases. With modern tools and technologies, a data lake can also form the storage layer of a database. Tools like Starburst, Presto, Dremio, and Atlas Data Lake can give a database-like view into the data stored in your data lake. In many cases, these tools can power the same analytical workloads as a data warehouse.(데이터 레이크는 데이터 분석 단계에서 사용할 목적으로 수집한 저장소는 아니다. 하지만 Presto 같은 걸 사용하면 데이터 레이크에서도 데이터 웨어하우스나 데이터베이스에서 처럼 데이터 분석을 할 수 있다)Data lakes store large amounts of structured, semi-structured, and unstructured data. They can contain everything from relational data to JSON documents to PDFs to audio files.Data does not need to be transformed in order to be added to the data lake, which means data can be added (or “ingested”) incredibly efficiently without upfront planning.The primary users of a data lake can vary based on the structure of the data. Business analysts will be able to gain insights when the data is more structured. When the data is more unstructured, data analysis will likely require the expertise of developers, data scientists, or data engineers.The flexible nature of data lakes enables business analysts and data scientists to look for unexpected patterns and insights. The raw nature of the data combined with its volume allows users to solve problems they may not have been aware of when they initially configured the data lake.Data in data lakes can be processed with a variety of OLAP systems and visualized with BI tools.Data lakes are a cost-effective way to store huge amounts of data. Use a data lake when you want to gain insights into your current and historical data in its raw form without having to transform and move it. Data lakes also support machine learning and predictive analytics.Like data warehouses, data lakes are not intended to satisfy the transaction and concurrency needs of an application.  AWS S3  Azure Data Lake Storage Gen2  Google Cloud StorageOther technologies enable organizing and querying data in data lakes, including:  MongoDB Atlas Data Lake.  AWS Athena.  Presto.  Starburst.  Databricks SQL Analytics.Data LakehouseA data lakehouse is a new, big-data storage architecture that combines the best features of both data warehouses and data lakes. A data lakehouse enables a single repository for all your data (structured, semi-structured, and unstructured) while enabling best-in-class machine learning, business intelligence, and streaming capabilities.Data lakehouses usually start as data lakes containing all data types; the data is then converted to Delta Lake format (an open-source storage layer that brings reliability to data lakes). Delta lakes enable ACID transactional processes from traditional data warehouses on data lakes.SummaryDatabases, data warehouses, and data lakes are all used to store data. So what’s the difference?The key differences between a database, a data warehouse, and a data lake are that:  A database stores the current data required to power an application.  A data warehouse stores current and historical data from one or more systems in a predefined and fixed schema, which allows business analysts and data scientists to easily analyze the data.  A data lake stores current and historical data from one or more systems in its raw form, which allows business analysts and data scientists to easily analyze the data.                   Database      Data Warehouse      Data Lake              Workloads      Operational and transactional      Analytical      Analytical              Data Type      Structured or semi-structured      Structured and/or semi-structured      Structured, semi-structured, and/or unstructured              Schema Flexibility      Rigid or flexible schema depending on database type      Pre-defined and fixed schema definition for ingest (schema on write and read)      No schema definition required for ingest (schema on read)              Data Freshness      Real time      May not be up-to-date based on frequency of ETL processes      May not be up-to-date based on frequency of ETL processes              Users      Application developers      Business analysts and data scientists      Business analysts, application developers, and data scientists              Pros      Fast queries for storing and updating data      The fixed schema makes working with the data easy for business analysts      Easy data storage simplifies ingesting raw data. A schema is applied afterwards to make working with the data easy for business analysts. Separate storage and compute              Cons      May have limited analytics capabilities      Difficult to design and evolve schema. Scaling compute may require unnecessary scaling of storage, because they are tightly coupled      Requires effort to organize and prepare data for use      참고  MongoDB 공식문서: Databases vs. Data Warehouses vs. Data Lakes (추천)  striim: Data Warehouse vs. Data Lake vs. Data Lakehouse  EDUCBA: Redis vs MongoDB  EDUCBA: Data Warehouse vs Data Lake  비투엔: Data Warehouse vs Data Lake  화해블로그, 화해의 Data Warehouse를 소개합니다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-01T21:01:35+09:00'>01 Apr 2022</time><a class='article__image' href='/data-engineering-series1'> <img src='/images/data_warehouse_logo.png' alt='Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series1'>Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Swarm",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series11",
      "date"     : "Mar 22, 2022",
      "content"  : "Table of Contents  참고참고  A look at Docker Swarm  Using placement constraints with Docker Swarm  docker stack deploy  docker service create  Deploy a stack to a swarm  도커 시작하기 10 : 도커 스웜 - 컴포즈 파일과 스택",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-22T21:01:35+09:00'>22 Mar 2022</time><a class='article__image' href='/docker-series11'> <img src='/images/docker_3.svg' alt='Docker Swarm'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series11'>Docker Swarm</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker 명령어 모음",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series10",
      "date"     : "Mar 22, 2022",
      "content"  : "Table of Contents  참고참고  Copying Files To And From Docker Containers          ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-22T21:01:35+09:00'>22 Mar 2022</time><a class='article__image' href='/docker-series10'> <img src='/images/docker_3.svg' alt='Docker 명령어 모음'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series10'>Docker 명령어 모음</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series3",
      "date"     : "Mar 11, 2022",
      "content"  : "Table of Contents  참고참고  nanjangpan.log: Terraform 생존기(1) - 테라폼이란?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-11T21:01:35+09:00'>11 Mar 2022</time><a class='article__image' href='/terraform_series3'> <img src='/images/terraform_logo.jpg' alt='Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series3'>Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series2",
      "date"     : "Mar 10, 2022",
      "content"  : "Table of Contents  HCL 코드  Block          Provider      Resource      Data      Variables      Output      Locals      Module        참고HCL 코드이번 포스트에서는 인프라 관리를 위한 테라폼 코드를 작성하기 위해 알아두면 좋은 몇 가지 구성 요소에 대해 알아보도록 하겠습니다.테라폼 언어는 선언적입니다. 또한 각각의 코드가 목표에 도달하기 위한 단계를 의미하는 것이 아니기 때문에 순서는 크게 중요하지 않습니다. 테라폼은 오직 resource들의 관계만을 고려해 실행 순서를 정합니다.테라폼 코드는 큰 틀에서 몇 가지 Block으로 이루어져 있습니다.&amp;lt;BLOCK TYPE&amp;gt; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; {  # Block body  &amp;lt;IDENTIFIER&amp;gt; = &amp;lt;EXPRESSION&amp;gt; # Argument}resource &quot;aws_vpc&quot; &quot;main&quot; {  cidr_block = var.base_cidr_block}Block블록은 resource와 같은 오브젝트를 나타내기 위해 필요한 설정값을 담고 있습니다. 블록의 종류에는 크게 provider,  resource, data, variables, output, locals, module이 있습니다. 테라폼 코드는 이러한 블록들로 구성되어 있다고 생각해도 무방합니다.Provider  인프라를 제공해주는 주체가 누구인지 설정하는 블럭  예: Local, AWS, Azure, GCP 등  provider를 설정하고 나면 해당 provider 플러그인을 설치하고 필요한 API를 사용할 수 있음  https://registry.terraform.io/browse/providers 참고provider &quot;aws&quot; {  region     = &quot;us-west-2&quot;  access_key = &quot;my-access-key&quot;  secret_key = &quot;my-secret-key&quot;}Resource  테라폼 코드에서 가장 중요한 블럭  각각의 resource 블럭은 인프라스트럭처 오브젝트를 나타냄(컴퓨팅, 스토리지, 네트워크 등)  각각의 resource 종류마다 설정하는 인자값 다르므로 공식 문서 참고          예: aws_network_interface의 경우 subnet_id, private_ips 등이 있고 aws_instance에는 ami, instance_type 등이 있음        resource의 속성값을 다른 resource에서 참조할 수도 있음          예: network_interface_id = aws_network_interface.foo.id      resource &quot;aws_network_interface&quot; &quot;foo&quot; {  subnet_id   = aws_subnet.my_subnet.id  private_ips = [&quot;172.16.10.100&quot;]  tags = {    Name = &quot;primary_network_interface&quot;  }}resource &quot;aws_instance&quot; &quot;foo&quot; {  ami           = &quot;ami-005e54dee72cc1d00&quot; # us-west-2  instance_type = &quot;t2.micro&quot;  network_interface {    network_interface_id = aws_network_interface.foo.id    device_index         = 0  }  credit_specification {    cpu_credits = &quot;unlimited&quot;  }}Data  테라폼 코드 외부에서 설정된 값을 코드로 가져오고 싶은 경우  아래의 예시와 같이 filter, most_recent 와 같은 인자값은 data source마다 달라서 문서 참조해야함                예를 들어 아래와 같이 aws_ami source를 사용하는 경우 다음의 공식문서 참고        resource 블럭과 같은 곳에서 사용하고자 할 때는 data.\&amp;lt;data source&amp;gt;.\&amp;lt;name&amp;gt;.\&amp;lt;attribute&amp;gt;          예: data.aws_ami.web.id      data source별로 속성도 당연히 다르다 공식문서 참고      # AWS AMI 중 state가 available 이고 Component 태그가 web인 것 중 가장 최근 AMIdata &quot;aws_ami&quot; &quot;web&quot; {  # filter에 어떤 설정할 수 있는지는 https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-images.html 참고 (테라폼 공식문서에 링크 다 명시해놨음)  filter {    name   = &quot;state&quot;    values = [&quot;available&quot;]  }  filter {    name   = &quot;tag:Component&quot;    values = [&quot;web&quot;]  }  most_recent = true}# 필터링 거쳐서 얻은 AMI의 id를 사용한다resource &quot;aws_instance&quot; &quot;web&quot; {  ami           = data.aws_ami.web.id  instance_type = &quot;t1.micro&quot;}Variables  Variables는 테라폼 코드를 실행할 때에 파라미터로 값을 동적으로 넘겨줄 수 있도록 함  인자값에는 default, type, description, validation, sensitive, nullable가 있음 (공식문서 참고)  값을 넘겨주는 방법은 다음과 같음          테라폼 명령어 사용시 -var 옵션 사용하기                  terraform apply -var=”image_id=ami-abc123”          terraform apply -var=’image_id_list=[“ami-abc123”,”ami-def456”]’ -var=”instance_type=t2.micro”          apply -var=’image_id_map={“us-east-1”:”ami-abc123”,”us-east-2”:”ami-def456”}’                    .tfvars 파일에 정의하기        # testing.tfvarsimage_id = &quot;ami-abc123&quot;availability_zone_names = [&quot;us-east-1a&quot;,&quot;us-west-1c&quot;,]                # CLIterraform apply -var-file=&quot;testing.tfvars&quot;                    환경변수로 설정하기        export TF_VAR_image_id=ami-abc123                    variable &quot;user_information&quot; {  type = object({    name    = string    address = string  })  sensitive = true}resource &quot;some_resource&quot; &quot;a&quot; {  name    = var.user_information.name  address = var.user_information.address}Output  프로그래밍 언어에서 return과 비슷한 역할을 하는 블럭  그냥 print하는 정도의 역할인가?  필수 인자값에는 value가 있고, 옵셔널 인자값에는 description, sensitive, depends_on이 있다output &quot;instance_ip_addr&quot; {  value = aws_instance.server.private_ip}Locals  반복적인 표현을 줄이고자 할 때 유용한 블럭locals {  service_name = &quot;forum&quot;  owner        = &quot;Community Team&quot;}locals {  # Common tags to be assigned to all resources  common_tags = {    Service = local.service_name    Owner   = local.owner  }}resource &quot;aws_instance&quot; &quot;example&quot; {  # ...  tags = local.common_tags}Module  여러 개의 resource의 묶음  반복적으로 함께 사용되는 resource들을 묶어서 사용할 수 있음  사용자들이 미리 만들어 공유해놓은 Module들이 있다 (공식문서 참고)# AWS VPC modulemodule &quot;vpc&quot; {  # &amp;lt;NAMESPACE&amp;gt;/&amp;lt;NAME&amp;gt;/&amp;lt;PROVIDER&amp;gt;  source = &quot;terraform-aws-modules/vpc/aws&quot;  name = &quot;my-vpc&quot;  cidr = &quot;10.0.0.0/16&quot;  azs             = [&quot;eu-west-1a&quot;, &quot;eu-west-1b&quot;, &quot;eu-west-1c&quot;]  private_subnets = [&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;, &quot;10.0.3.0/24&quot;]  public_subnets  = [&quot;10.0.101.0/24&quot;, &quot;10.0.102.0/24&quot;, &quot;10.0.103.0/24&quot;]  enable_nat_gateway = true  enable_vpn_gateway = true  tags = {    Terraform = &quot;true&quot;    Environment = &quot;dev&quot;  }}참고  Terraform 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-10T21:01:35+09:00'>10 Mar 2022</time><a class='article__image' href='/terraform_series2'> <img src='/images/tf_3.png' alt='Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series2'>Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리</a> </h2><p class='article__excerpt'>이번 포스트에서는 인프라 관리를 위한 테라폼 코드를 작성하기 위해 알아두면 좋은 몇 가지 구성 요소에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part4]: Javascript 데이터 타입과 연산자",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series4",
      "date"     : "Mar 5, 2022",
      "content"  : "Table of Contents  데이터 타입          숫자 타입      문자열 타입      템플릿 리터럴      undefined 타입      심벌 타입        연산자          산술 연산자      비교 연산자      삼항 조건 연산자        논리 연산자데이터 타입ES6 기준으로 자바스크립트에서는 7개의 데이터 타입을 제공한다.            구분      데이터 타입      설명              원시 타입      숫자      숫자, 정수와 실수의 구분 없음                     문자열      문자열                     불리언      논리적 참과 거짓                     undefined      var 키워드로 선언된 변수에 암묵적으로 할당되는 값                     null      값이 없다는 것을 의도적으로 명시할 때 사용되는 값                     symbol      ES6에서 추가된 타입              객체 타입             객체, 함수, 배열 등      숫자 타입자바스크립트는 독특하게 하나의 숫자 타입만 존재한다.ECMAScript 사양에 따르면 숫자 타입의 값은 배정밀도(double precision) 64비트 부동소수점 형식을 따른다.정수, 실수, 2진수, 8진수, 16진수 리터럴은 모두 메모리에 배정밀도 64비트 부동소수점 형식의 2진수로 저장된다. 자바스크립트는 2진수, 8진수, 16진수를 표현하기 위한 데이터 타입을 제공하지 않기 때문에 이들 값을 참조하면 모둗 10진수로 해석된다.var binary = 0b01000001;var octal = 0o101;var hex = 0x41;console.log(binary); // 65console.log(octal); // 65console.log(hex); // 65문자열 타입문자열 타입은 텍스트 데이터를 나타내는 데 사용한다. 문자열은 0개 이상의 16비트 유니코드 문자(UTF-16)의 집합으로 전 세계 대부분의 문자를 표현할 수 있다. 자바스크립트의 문자열은 원시타입이며, 변경 불가능한 값이다.문자열은 작은따옴표(‘’), 큰따옴표(“”) 또는 백틱(``)으로 텍스트를 감싼다. 가장 일반적인 표기법은 작은따옴표를 사용하는 것이다. 따옴표로 감싸는 이유는 키워드나 식별자 같은 토큰과 구분하기 위해서다.var string;string = &#39;apple&#39;;string = &quot;apple&quot;;string = `apple`;템플릿 리터럴ES6부터 템플릿 리터럴이라고 하는 새로운 문자열 표기법이 도입되었다. 템플릿 리터럴은 멀티라인 문자열, 표현식 삽입 등 편리한 문자열 처리 기능을 제공한다. 템플릿 리터럴은 런타임에 일반 문자열로 변환되어 처리된다.템플릿 리터럴은 백틱(``)을 사용해 표현한다.// 멀티라인 문자열var html_code = `&amp;lt;ul&amp;gt;    &amp;lt;li&amp;gt;&amp;lt;a href=&quot;#&quot;&amp;gt;Home&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;`;// 표현식 삽입var first = &#39;Ung-mo&#39;;var last = &#39;Lee&#39;;console.log(&#39;My name is &#39; + first + &#39; &#39; + last + &#39;.&#39;); // ES5console.log(`My name is ${first} ${last}.`); // ES6undefined 타입var 키워드로 선언한 변수는 암묵적으로 undefined로 초기화된다. 다시 말해, 변수 선언에 의해 확보된 메모리 공간을 처음 할당이 이뤄질 때까지 자바스크립트 엔진이 undeefined로 초기화한다. 변수를 참조했을 때 undefined가 반환된다면 참조한 변수가 선언 이후 값이 할당된 적이 없는 변수라는 것을 간파할 수 있다.만약 undefined를 개발자가 의도적으로 변수에 할당한다면 undefined의 본래 취지와 어긋날뿐더러 혼란을 줄 수 있으므로 권장하지 않는다.변수에 값이 없다는 것을 명시하고 싶을 때는 null을 할당하는 것이 좋다.var foo;console.log(foo); // undefined심벌 타입심벌은 ES6에서 추가된 7번째 타입으로, 변경 불가능한 원시 타입의 값이다. 심벌 값은 다른 값과 중복되지 않는 유일무이한 값이다.다른 원시 값은 리터럴을 통해 생성하지만 심벌은 Symbol 함수를 호출해 생성한다. 이때 생성된 심벌 값은 외부에 노출되지 않으며, 다른 값과 절대 중복되지 않는다.var key = Symbol(&#39;key&#39;); // 심벌 생성var obj = {}; // 객체 생성obj[key] = &#39;value&#39;;// 유일한 값을 가지는 심벌을 프로퍼티 키로 사용한다console.log(obj[key]) // value연산자산술 연산자            단항 산술 연산자      ++, –, +, -              이항 산술 연산자      +, -, *, /, %      ++, -- 연산자는 위치에 의미가 있다.var x = 5, result;result = x++; // 선할당 후증가console.log(result, x); // 5 6var x = 5, result;result = ++x; // 선증가 후할당console.log(result, x); // 6 6비교 연산자            비교 연산자      의미      설명              ==      동등 비교      x와 y의 값이 같음              ===      일치 비교      x와 y의 값과 타입이 같음              !=      부동등 비교      x와 y의 값이 다름              !==      불일치 비교      x와 y의 값과 타입이 다름      동등 비교 연산자(==)는 좌항과 우항의 피연산자를 비교할 때 먼저 암묵적으로 타입 변환을 통해 타입을 일치시킨 후 같은 값인지 비교한다.5 == &#39;5&#39;; // true5 === &#39;5&#39;; // false그래서 동등 비교 연산자는 예측하기 어려운 결과를 만들어낸다. 따라서 동등 비교 연산자보다는 일치 비교 연산자를 권장한다.삼항 조건 연산자조건식 ? 조건식이 true일 때 반환할 값 : false일 때 반환할 값var x = 2;var result = x % 2 ? &#39;홀수&#39; : &#39;짝수&#39;;console.log(result); // 짝수삼항 조건 연산자 표현식은 값처럼 사용할 수 있다.논리 연산자            논리 연산자      의미              ||      논리합(OR)              &amp;amp;&amp;amp;      논리곱(AND)              !      부정(NOT)      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-05T21:01:35+09:00'>05 Mar 2022</time><a class='article__image' href='/javascript-series4'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part4]: Javascript 데이터 타입과 연산자'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series4'>Javascript Series [Part4]: Javascript 데이터 타입과 연산자</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part5]: AWS Storage Service: S3",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series5",
      "date"     : "Mar 5, 2022",
      "content"  : "Table of Contents  S3란 무엇인가  S3의 이점  S3의 구성요소          Object      Bucket        S3의 동작원리  S3 특징          Lifecycle Management      Bucket Policy      Data Encryption      Versioning      Cross-Region Replication      Transfer Acceleration        참고S3란 무엇인가object storage service from Amazon that provides industry-leading scalability, data availability, security, and performancesimple management capabilities that allow you to organize your data and fine-tune access controls to match your specific commercial, organizational, and compliance needs.S3의 이점  Performance, scalability, availability, and durability that are unrivalled in the industry  Many cost-effective storage classes are available  Security, compliance, and auditing capabilities that are unrivalled  Manage data and access permissions with ease  Query-in-place and on-demand processing  The most widely used cloud storage serviceS3의 구성요소Object  Objects are fundamental entities stored in Amazon S3  It consists of its data as well as its metadata  S3 cannot access the content or data inside the object  It can only read the metadata ssociated with the object  It allocates a unique version ID for the object when some data is added to the bucketBucket  Buckets are the containers where objects are stored and then uploaded to Amazon S3  There will never be an object without a Bucket  There is no “Windows Explorer” like hierarchy -&amp;gt; just prefix  One can create Buckets in any region of the World which is close to you -&amp;gt; shoud be globally unique nameS3의 동작원리When files are uploaded to the Bucket, the user will specify the type of S3 storage class to be used for those specific objects  S3 Standard          Latency is supposed to be low      Example: Frequently accessed data        S3 Standard for Infrequent Data Access          Where the data is long lived and is not frequently accessed.      Example: Emplyee’s progress report will not be needed daily        Amazone Glacier          Where data can be archived, and high performance is not required      Example: Employee’s old record (profile)        One Zone-IA Storage Class          Where data is infrequently accessed and stored in a particular region      Example: Employee’s performance report is not accessed daily but is stored in a single region        Amazon S3 Standard Reduced Redundancy Storage          Where the data is noncritical and reproduced quickly      Example: External data stored which are not important and can be replaced falls under this category      S3 특징Lifecycle Management  S3 can be configured to move a certain data between various storage classes on a defined schedule  Two types of action: Transition and Expiration[Management Tab] -&amp;gt; [Lifecycle rules: Create lifecycle rule]Bucket Policy  It is an IAM policy which lets you allow and deny permission to your S3 resources  With this, you also define security parameters for more than one file in the desired bucketFirst of all, Unblock all public access[Permission Tab] -&amp;gt; [Bucket policy: Edit] -&amp;gt; [Policy generator] -&amp;gt; Copy policy JSON document -&amp;gt; PasteData Encryption  It intends to protect the data when it is in action or at rest  Two ways of encryption: Client-Side Encryption and Server-Side EncryptionVersioning  It is utilized to preserve and restore early versions of every object stored in your S3 bucket  Unintentional alteration to a file can be easily regained with versioning  파일에 수정이 일어나면 수정 전 내용이 버저닝 된다    [Properties Tab] -&amp;gt; [Bucket Versioning: Edit] -&amp;gt; [Bucket Versioning: Enable] -&amp;gt; [Save changes] -&amp;gt; Being tracked by version Number -&amp;gt; You can check the versions of file by clicking [Versions Tab of any file]      Cross-Region Replication  Cross-Region Replication provides automatic copying of every object uploaded to your buckets(source bucket and destination bucket) in different AWS regionFirst of all, Make another bucket to replicate objectsFrom Source Bucket -&amp;gt; [Management Tab] -&amp;gt; [Replication rules: Create replication rule]Transfer Acceleration  It allows easy and secure transfer of files over long distances between your client and S3 bucket  It takes advantages of the edge location around the world provided by the Amazon CloudFront  It works via carrying the data over an optimized network bridge that runs between your clients’AWS Edge location and your S3 bucket[Properties Tab] -&amp;gt; [Transfer acceleration: Edit] -&amp;gt; [Enable]참고  Youtube Simplilearn: AWS S3 Tutorial",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-05T21:01:35+09:00'>05 Mar 2022</time><a class='article__image' href='/aws-series5'> <img src='/images/aws_logo.png' alt='AWS Series [Part5]: AWS Storage Service: S3'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series5'>AWS Series [Part5]: AWS Storage Service: S3</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series4",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/jenkins_series4'> <img src='/images/jenkins_logo.png' alt='Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series4'>Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part4]: AWS Network Service CloudFront",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series4",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/aws-series4'> <img src='/images/aws_logo.png' alt='AWS Series [Part4]: AWS Network Service CloudFront'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series4'>AWS Series [Part4]: AWS Network Service CloudFront</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series3",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents  1. VPC란          Why VPC?      VPC의 특징      VPC의 구성 요소                  1) IP          2) 서브넷          3) 라우팅 테이블                    VPC의 주요 서비스                  1) Security Group          2) NACL(Network Access Control List)          예시                          Security Group              NACL                                3) IGW (Internet Gateway)          4) NAT (Newtork Address Translation)          5) Bastion host          6) Endpoint                      2. VPC 실습          VPC 생성      서브넷 생성      IGW 생성      라우팅 테이블 설정      NACL 설정      Bastion host 생성      NAT gateway 생성      Endpoint 생성      1. VPC란Amazon VPC는 AWS 클라우드에서 논리적으로 격리된 네트워크 공간을 할당하여 줌으로써 가상 네트워크 내에서 AWS 리소스를 이용할 수 있도록 하는 서비스입니다.Why VPC?  네트워킹 환경을 완벽하게 제어할 수 있다  보안성 높은 클라우드 환경을 구현할 수 있다VPC의 특징  VPC 자체 IP주소 범위, 서브넷, 라우팅 테이블, 게이트웨이 등 네트워킹 환경 제어 가능  VPC는 하나의 Region에만 속할 수 있음(다른 Region으로 확장 불가능)  기존 데이터 센터와의 연결을 통해 하이브리드 환경 구성 가능VPC의 구성 요소1) IP  Public IP: 인터넷을 통해 연결할 수 있는 IP주소  Private IP: 인터넷과 연결되지 않은, VPC 내부에서만 사용할 수 있는 IP주소  Elastic IP: 동적 컴퓨팅을 위해 고안된 고정 Public IP주소2) 서브넷  서비스 목적에 따라 VPC 내부의 네트워크를 IP Block으로 나누기도 하는데 이 때의 IP Block의 모음을 서브넷(Subnet)이라고 합니다  리전(Region)내의 가용 영역(Availability Zone)들은 여러 개의 Subnet을 가질 수 있다  하지만 Subnet이 여러 가용 영역에 확장될 수는 없다.  Public 서브넷: 인터넷 게이트웨이로 라우팅이 되는 서브넷, Public IP나 Elastic IP 주소가 필요  Private 서브넷: 인터넷 게이트웨이로 라우팅 되지 않는 서브넷, 높은 보안성을 필요로 하는 DB(DataBase) 서버 같은 경우 Private 서브넷에 주로 생성  서브넷은 CIDR 블록을 통해 분리/분배할 수 있다          1번째 서브넷 : 211.11.124.0/26 (211.11.124.0 ~ 211.11.124.63)      2번째 서브넷 : 211.11.124.64/26 (211.11.124.64 ~ 211.11.124.127)      3번째 서브넷 : 211.11.124.128/26 (211.11.124.128 ~ 211.11.124.191)      4번째 서브넷 : 211.11.124.192/26 (211.11.124.192 ~ 211.11.124.255)      (26은 IP주소 32자리 중 26자리는 고정, 뒤의 6자리로만 IP주소 구성 -&amp;gt; 2의 6승 -&amp;gt; 하나의 서브넷에 64개의 IP주소가 존재)3) 라우팅 테이블  서브넷 외부로 나가는 아웃바운드 트래픽에 대해 허용된 경로를 알려주는 나침반  서브넷 간의 원활한 통신을 위해 라우팅 테이블을 이용VPC의 주요 서비스1) Security Group  IP와 Port를 기준으로 특정 트래픽만을 허용  Stateful 검문소2) NACL(Network Access Control List)  IP와 Port를 기준으로 특정 트래픽을 허용하거나 거부  Stateless 검문소예시Security Group(Security Group은 Stateful해서 Outbound를 none으로 해도 1025포트로는 내보낼 수 있다)NACL(NACL은 Stateless해서 Outbound를 none으로 하면 1025포트로 내보낼 수 없다)3) IGW (Internet Gateway)  인터넷으로 나가는 통로4) NAT (Newtork Address Translation)  IP Address를 Translation해주는 서비스  나의 Private IP는 알려주고 싶지 않고, 외부 인터넷과는 통신을 하고 싶을 때  Private subnet 안에 있는 private instance가 외부의 인터넷과 통신하기 위한 방법  Private instance가 외부 인터넷과 통신을 하고 싶을 때, Private subnet의 Route Table이 IGW로 라우팅하지 않고, Public subnet에 있는 NAT gateway로 라우팅, NAT gateway에서 IP주소 변환해 IGW로 이동해 외부 인터넷과 통신5) Bastion host  관리자가 private subnet에 접근하고 싶을 때 가능하도록 해주는 서비스  NAT와 반대의 느낌  Public subnet에 있는 EC2를 Bastion host로 사용한다6) Endpoint  AWS의 여러 서비스(S3,dynamodb, ..)들과 VPC를 연결시켜주는 중간 매개체  AWS에서 VPC 바깥으로 트래픽이 나가지 않고 AWS의 여러 서비스를 사용하게끔 만들어주는 서비스  Interface Endpoint : Private ip를 만들어 서비스로 연결해줌(SQS, SNS, Kinesis, Sagemaker 등 지원)  Gateway Endpoint : 라우팅 테이블에서 경로의 대상으로 지정하여 사용(S3, Dynamodb 지원)2. VPC 실습VPC 생성(VPC를 생성하면 기본적인 라우팅 테이블과 NACL을 만들어준다)서브넷 생성IGW 생성라우팅 테이블 설정(VPC가 생성될 때 만들어진 라우팅 테이블이 있지만, 서브넷과 연결된 라우팅 테이블은 없다)(각 서브넷에 라우팅 테이블이 연결되었다)(10.0.0.0/16 이외의 IP주소는 모두 IGW로 보낸다, 순서유효)NACL 설정(인바운드, 아웃바운드의 규칙은 순서가 유효하다)(Public 서브넷에 EC2를 만들어 보았다)Bastion host 생성NAT gateway 생성Endpoint 생성",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/aws-series3'> <img src='/images/aws_logo.png' alt='AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series3'>AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series3",
      "date"     : "Mar 3, 2022",
      "content"  : "Table of Contents  Jenkinsfile  Jenkinsfile의 구조          Sections                  agent          stages          stage          steps          post                    Directives                  environment          options          parameters          triggers          tools          when                      참고JenkinsfileJenkinsfile은 파이프라인을 만들기 위해 작성하는 코드입니다. 이 때 Jenkinsfile을 작성하는 방법에는 두 가지가 있습니다.  Scripted Pipeline2016년까지 Jenkins 코드는 전체적인 틀을 위해서만 Jenkins DSL(Domain-Specific Language)가 약간 사용되었을 뿐 실질적인 프로그램의 흐름은 그루비(Groovy) 언어에 의해 작성되었습니다. 그래서 Jenkinsfile을 작성하기 위해 Jenkins DSL뿐만 아니라 그루비 언어까지 배워야 했습니다. 이렇게 작성된 코드가 바로 Scripted Pipeline입니다.이 때까지는 파이프라인에서 빌드후처리, 파이프라인 에러 상태확인, 알림기능 등 Jenkins에 특화된 내용은 없고 대부분 그루비의 try-catch-finally 구조로 구현해야 했습니다.  Declarative Pipeline그러다 2017년부터 클라우드 비스(CloudBees)에서 선언적 파이프라인을 개발하였습니다. 이 문법을 통하여 파이프라인을 심플하고 독자적인 방법으로 작성할 수 있게 되었습니다.Jenkinsfile의 구조모든 선언적 파이프라인은 반드시 pipeline 블록으로 감싸야 합니다. 그리고 그 안에 Sections과 Directives(지침)로 구성되어 있습니다.SectionsSections에는 agent, stages, steps, post가 있습니다. (stage는 Jenkins DSL에서 Directives로 분류하고 있습니다. 하지만 저는 Sections로 분류하는 것이 편해 여기서 stage도 함께 소개하겠습니다. 공식문서 참고)pipeline {    agent {}    stages {        stage {            steps {}        }    }    post {}}agentagent는 파이프라인 혹은 스테이지를 어디서 실행할지를(파일 경로, 컨테이너 등) 의미합니다.            필수      Yes              값      any, none, label, node, docker, dockerfile, kubernetes              위치      pipeline block, stage block      (공식문서 참고)stagesstage 섹션을 묶는 블럭입니다.            필수      Yes              값      stage 블럭              위치      pipeline block      stagestage는 공식문서에서 Directives(지침)으로 분류하는 블럭이며 파이프라인 내에서 구분하고 싶은 각각의 단계(예를 들어 Build, Test, Deploy)를 나타냅니다.            필수      Yes              파라미터      필수 파라미터: 각 stage의 이름              위치      stages block      stepssteps는 각각의 stage 안에서 실행될 것들을 묶는 블럭입니다.            필수      Yes              값      Basic step: echo, sleep, timeout, AWS step: s3Download, ecrDeleteImage 등              위치      stage block      postpost는 pipeline 또는 각각의 stage가 실행된 후 조건에 따라 실행되는 블록입니다.            필수      No              조건      always, changed, fixed, regression, aborted, failure, success, unstable, unsuccessful, cleanup              위치      pipeline block, stage block      (공식문서 참고)Directivesenvironment환경변수를 지정하기 위한 키-밸류 쌍입니다.            필수      None              값      key-value pairs              위치      pipeline block, stage block      options파이프라인 또는 각 스테이지별로 옵션을 제공합니다. 다양한 종류의 옵션이 있습니다.  newContainerPerStage: docker, dockerfile agent 사용시 가능한 옵션. 각 스테이지마다 새로운 컨테이너 생성  retry: 파이프라인이 실패할 때 다시 실행  timeout: 제한된 시간 동안만 실행되도록 제한이 밖에 많은 옵션들이 있습니다.[공식문서 참고]            필수      No              값      newContainerPerStage, retry, timeout, timestamps, skipDefaultCheckout 등              위치      pipeline block, stage block      parameters파이프라인을 실행할 때 유저가 파라미터를 줄 수 있도록 합니다.            필수      No              값      string, text, booleanParam, choice, password              위치      pipeline block      pipeline {    agent any    parameters {        string(name: &#39;PERSON&#39;, defaultValue: &#39;Mr Jenkins&#39;, description: &#39;Who should I say hello to?&#39;)        text(name: &#39;BIOGRAPHY&#39;, defaultValue: &#39;&#39;, description: &#39;Enter some information about the person&#39;)        booleanParam(name: &#39;TOGGLE&#39;, defaultValue: true, description: &#39;Toggle this value&#39;)        choice(name: &#39;CHOICE&#39;, choices: [&#39;One&#39;, &#39;Two&#39;, &#39;Three&#39;], description: &#39;Pick something&#39;)        password(name: &#39;PASSWORD&#39;, defaultValue: &#39;SECRET&#39;, description: &#39;Enter a password&#39;)    }    stages {        stage(&#39;Example&#39;) {            steps {                echo &quot;Hello ${params.PERSON}&quot;                echo &quot;Biography: ${params.BIOGRAPHY}&quot;                echo &quot;Toggle: ${params.TOGGLE}&quot;                echo &quot;Choice: ${params.CHOICE}&quot;                echo &quot;Password: ${params.PASSWORD}&quot;            }        }    }}triggers파이프라인을 어떤 기준으로 자동화할지 정하는 옵션입니다. 소스코드가 Github 또는 BitBucket이라면  triggers가 필요하지 않을 수 있습니다. 현재 Jenkins에서 제공하는 triggers는 cron, pollSCM, upstream이 있습니다.            필수      No              값      cron, pollSCM, upstream              위치      pipeline block      tools파이프라인 내에서 빌드할 때 필요한 도구들을 참조합니다.            필수      No              값      maven, jdk, gradle              위치      pipeline bloc, stage block      when각각의 스테이지를 실행할지를 설정하는 조건입니다.            필수      No              값      branch, buildingTag, environment, equals, expression, tag, not, allOf, anyOf 등              위치      stage block      pipeline {    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                branch &#39;production&#39;            }            steps {                echo &#39;Deploying&#39;            }        }    }}pipeline {    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                allOf {                    branch &#39;production&#39;                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39;                }            }            steps {                echo &#39;Deploying&#39;            }        }    }}    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                expression { BRANCH_NAME ==~ /(production|staging)/ }                anyOf {                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39;                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;staging&#39;                }            }            steps {                echo &#39;Deploying&#39;            }        }    }}참고  Jenkins 공식문서: Pipeline Syntax  SeungHyeon, [Jenkins] # 선언적(Declarative) 파이프라인",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-03T21:01:35+09:00'>03 Mar 2022</time><a class='article__image' href='/jenkins_series3'> <img src='/images/jenkins_17.png' alt='Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series3'>Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part2]: AWS Computing Service",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series2",
      "date"     : "Mar 3, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-03T21:01:35+09:00'>03 Mar 2022</time><a class='article__image' href='/aws-series2'> <img src='/images/aws_logo.png' alt='AWS Series [Part2]: AWS Computing Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series2'>AWS Series [Part2]: AWS Computing Service</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series2",
      "date"     : "Mar 2, 2022",
      "content"  : "Table of Contents  Jenkins 설치하기  도커 컨테이너로 Jenkins 띄우기          도커 이미지 Pull      Jenkins 컨테이너 실행        Jenkins 컨테이너에서 도커 CLI가 필요한 순간이 온다  참고Jenkins 설치하기  Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed.도커 컨테이너로 Jenkins 띄우기도커 이미지를 이용해서 Jenkins를 띄우는 방법은 간단합니다. 사용하다보면 커스터마이징할 필요가 생겨 이미지를 직접 빌드해야 하는 상황이 오겠지만, 아직 저는 Jenkins를 사용해 본 경험이 없기 때문에 단순히 띄우는 데 의의를 두고 이번 포스트를 작성했습니다.도커 이미지 Pull우선 도커 허브에서 이미지를 다운받습니다. 저는 arm64 아키텍처를 필요로 하기 때문에 그에 맞는 이미지를 다운 받았습니다.docker pull jenkins/jenkins:2.332.1-lts-jdk11Jenkins 컨테이너 실행# p: jenkins 웹 UI를 localhost로 접속하기 위해# u: 내가 사용한 jenkins 이미지 사용자가 root로 안되어 있어 컨테이너 내에서 apt-get update 이런게 안되서 root로 바꿔줌# v: 나중에 뒤에서 사용할 DooD 패턴을 위해 jenkins 컨테이너와 호스트의 소켓을 마운트해야함  docker run -p 8080:8080 -u root -v /var/run/docker.sock:/var/run/docker.sock -it jenkins/jenkins:2.332.1-lts-jdk11 /bin/bash# 웹 브라우저에서 로컬호스트 8080포트로 Jenkins Web UI에 접근localhost:8080그러면 뭔가 비밀번호 같은거를 입력하라고 뜨는데 이는 컨테이너 로그에 남아있어서 복붙하면 됩니다.docker logs &amp;lt;컨테이너명&amp;gt;그러면 플러그인 설치를 어떤 방법으로 할 것인지 묻는데 저는 Jenkins에서 선택해준 것들로 우선 설치하겠습니다.플러그인들을 열심히 설치하고 있습니다. Git이 자주 사용되는지 기본적으로 설치가 되는 모습입니다.설치가 끝나면 계정 설정을 하라고 나오고 간단히 입력하고 나면 Jenkins web UI가 잘 보입니다.Jenkins 컨테이너에서 도커 CLI가 필요한 순간이 온다Jenkins는 CI/CD를 위한 기본적인 툴로써 요즘같은 마이크로서비스 패턴이 트렌드인 시대에서 도커는 반드시 필요해 보입니다. 여기서 문제가 있습니다. 저는 방금 Jenkins 서버를 도커 컨테이너로 띄웠는데 컨테이너에서 도커를 또 설치해도 괜찮을까요?이렇게 도커 컨테이너 안에 도커를 또 설치하는 패턴을 Docker in Docker(DinD)라고 하는데 많은 시니어 개발자들은 이 방식을 권장하지 않는다고 합니다. 다음은 DinD 방식에 대한 장단점을 정리해둔 포스트이니 참고해봐도 좋을 것 같습니다. (~jpetazzo/Using Docker-in-Docker for your CI or testing environment? Think twice. 참고)DinD의 단점을 해결하고자 나온 방식이 Docker out of Docker(DooD)라고 합니다. 이 방법은 컨테이너에 도커 엔진(도커 클라이언트와 도커 호스트)을 설치하지 않고 도커 클라이언트만 설치하는 방식입니다.# 도커 Client 설치  # https://docs.docker.com/engine/install/debian/ 데비안 위에서 도커 클라이언트 설치apt-get updateapt-get install \    ca-certificates \    curl \    gnupg \    lsb-releasecurl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \  $(lsb_release -cs) stable&quot; | tee /etc/apt/sources.list.d/docker.list &amp;gt; /dev/nullapt-get updateapt-get install docker-ce-cli도커 클라이언트가 잘 설치되었는지 확인하기 위해 docker ps 명령어를 실행해봤습니다. 아래의 에러는 위에서 docker run 명령어를 실행할 때 v 옵션으로 마운트하지 않은 경우 발생하는 에러입니다.root@bdab333aab12:/# docker psCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?위에 설명한 대로 잘 따라오셨다면 아마 다음과 같은 결과가 잘 보일겁니다.여기서 핵심은      컨테이너 안의 도커 소켓과 호스트의 도커 소켓을 마운트 한다.     docker run \ -v /var/run/docker.sock:/var/run/docker.sock \ ...            컨테이너 안의 jenkins 유저에게 호스트의 도커 소켓 실행 권한을 준다.    # 아직 되는지 확인해보진 않았다usermod -u &amp;lt;호스트의사용자아이디&amp;gt; jenkins &amp;amp;&amp;amp; \ groupmod -g &amp;lt;호스트의도커그룹아이디&amp;gt; docker &amp;amp;&amp;amp; \ usermod -aG docker jenkins      참고  기억 저장소: DooD (docker-outside-of-docker) 를 통해 Jenkins 컨테이너에서 docker 사용하기  postlude: Jenkins를 docker 컨테이너로 구축하기(Docker in Docker)  아이단은 어디 갔을까: DinD(docker in docker)와 DooD(docker out of docker)  도커 컨테이너에서 permission denied 해결하는 방법: docker run -u root …  도커 공식문서: 데비안 위에 도커 설치하는 방법  Do we need java for jenkins?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-02T21:01:35+09:00'>02 Mar 2022</time><a class='article__image' href='/jenkins_series2'> <img src='/images/jenkins_1.png' alt='Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series2'>Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part1]: What is Terraform?",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series1",
      "date"     : "Mar 1, 2022",
      "content"  : "Table of Contents  Terraform  인프라를 코드로 관리  테라폼 코드  State Driven Workflow Engine  좋은 글귀  참고Terraform  HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share.테라폼은 하시코프에서 오픈소스로 개발 중인 IaC 툴입니다. 테라폼을 통해 인프라 구성에 필요한 파일, 리소스 등을 코드로 정의하여 일관된 워크플로우를 사용해 전체 라이프사이클에 걸쳐 인프라를 프로비저닝하고 관리할 수 있습니다. 쉽게 말해 원하는 인프라를 코드로 작성할 수 있다는 말입니다. 테라폼은 컴퓨팅, 스토리지, 네트워킹 리소스와 같은 저수준의 구성 요소뿐만 아니라 DNS 및 SaaS 기능과 같은 고수준의 구성 요소를 관리할 수도 있습니다.🦊 프로비저닝어떤 프로세스나 서비스를 실행하기 위한 준비 단계를 프로비저닝이라고 이야기합니다. 프로비저닝에는 크게 네트워크나 컴퓨팅 자원을 준비하는 작업과 준비된 컴퓨팅 자원에 사이트 패키지나 애플리케이션 의존성을 준비하는 단계로 나뉘어집니다. 명확한 경계는 불분명하지만 테라폼은 전자를 주로 다루는 도구입니다. (44bits: 테라폼(Terraform) 기초 튜토리얼 참고)인프라를 코드로 관리인프라(컴퓨팅, 스토리지, 네트워킹)를 구성하기 위해 그저 테라폼 코드만 작성하면 된다는 것은 굉장히 편하게 느껴집니다. 제가 AWS를 맛보기로 사용해 봤을 때의 기억으로는 인프라를 구성하기 위해 EC2, S3, VPC 등 AWS 내에서 많은 서비스들을 왔다갔다 하면서 클릭을 했어야 했는데 이제 이러한 작업들을 할 필요 없어진다는 점이 굉장히 편할 것 같습니다. 인프라를 코드로 관리하게 되면 얻게 되는 이점에 대해 한 번 정리해보면 다음과 같은 것들이 있을 것 같습니다.  인프라를 언제 어디서든 일관되게 구성할 수 있다.  인프라 관리를 위한 협업을 코드로 할 수 있다.  인프라를 버전 컨트롤 할 수 있다.  인프라도 지속적 통합/지속적 배포가 가능해진다.테라폼 코드테라폼 코드는 어떻게 생겼길래 인프라를 관리할 수 있는 걸까요? 테라폼 코드 생김새를 한 번 구경하도록 하겠습니다.# main.tfprovider &quot;aws&quot; {  region = &quot;ap-northeast-2&quot;}resource &quot;aws_vpc&quot; &quot;myvpc&quot; {  cidr_block = &quot;10.0.0.0/16&quot;}provider를 통해 인프라를 제공해주는 주체를 정할 수 있습니다. provider는 나의 컴퓨터인 local이 될 수도 있고, aws, gcp, azure와 같은 클라우드 서비스가 될 수도 있습니다.resource를 이용해 내가 생성하고자 하는 인프라를 설정할 수 있습니다. 위에서는 aws_vpc를 생성했습니다.위의 코드를 일반적으로 나타내면 다음과 같습니다.&amp;lt;BLOCK TYPE&amp;gt; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; {  &amp;lt;IDENTIFIER&amp;gt; = &amp;lt;EXPRESSION&amp;gt;}State Driven Workflow Engine  It will parse our HCL configuration/code files.  Using the information in our HCL, Terraform will build up a graph of all the resources we want to provision (desired state) and figure out any dependencies between them to try and decide a logical order they need to be created in.  Terraform will next inspect its State to better understand what it has and hasn’t deployed (if it is our first deployment, the State will be empty). This is known as perceived state. It is perceived state because there is a disconnect between what Terraform “thinks” exists and what “actually” exists.  Terraform next performs a logical delta between our desired state, and what it knows to be our perceived state. It then decides which CRUD actions it needs to perform, and the order to perform them in, in order to bring our perceived state in-line with our desired state.  Terraform next performs all necessary operations to achieve the desired state. The result of this operation will be that resources will likely start to appear in our Azure subscription and this then becomes known as actual state.  Terraform updates the state to reflect what it has done.좋은 글귀Terraform in a nutshellIn its most basic form, Terraform is an application that converts configuration files known as HCL (Hashicorp Configuration Language) into real world infrastructure, usually in Cloud providers such as AWS, Azure or Google Cloud Platform.This concept of taking configuration files and converting them into real resources is known as IaC (Infrastructure as Code) and is the new hotness in the world of Software Engineering. And the reason it is becoming so hot right now, is because this code can live alongside your app code in repos, be version controlled and easily integrated into your CI/CD pipelines.간단히 말해 Terraform은 상태 기반 클라우드 플랫폼 프로비저닝 엔진입니다. 또한 추상화 툴링(provider and backend)을 활용하여 일관성 있고 결정론적인 클라우드 프로바이더별 CRUD API 호출로 해석 및 번역할 수 있는 코드를 작성할 수 있으므로 많은 업무와 스트레스가 제거됩니다.AWS의 CloudFormation참고  44bits: 테라폼(Terraform) 기초 튜토리얼  C:\Dave\Storey, Terraform For Beginners  Jayendra’s Cloud Certification Blog: Terraform Cheat Sheet",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-01T21:01:35+09:00'>01 Mar 2022</time><a class='article__image' href='/terraform_series1'> <img src='/images/tf_1.png' alt='Terraform Series [Part1]: What is Terraform?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series1'>Terraform Series [Part1]: What is Terraform?</a> </h2><p class='article__excerpt'>테라폼을 통해 인프라 구성에 필요한 파일, 리소스 등을 코드로 정의하여 인프라를 프로비저닝하고 관리할 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part1]: What is Jenkins?",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series1",
      "date"     : "Mar 1, 2022",
      "content"  : "Table of Contents  CI/CD  Jenkins  참고CI/CD  CI: 지속적 통합 (Continuous Integration)보통 하나의 서비스를 출시하기 위해 다수의 개발자들이 하나의 저장소에서 코드를 작성하는데 이 때 서로의 작업을 병합하고 에러를 수정하고 테스트하는 일련의 과정을 반복하게 됩니다. 이러한 과정은 보통 비슷한 작업이 반복되는 형태로 진행되기 때문에, 개발자들은 이 과정에서 발생하는 시간 낭비를 줄이고 싶어했습니다.Jenkins는 이러한 요구에 맞춰 개발된 CI 서비스를 제공하는 툴입니다.대표적인 CI 툴은 다음과 같습니다.  CD: 지속적 배포 (Continuous Deploy)CD는 CI 과정을 통해 코드에 문제가 없는 것을 확인하고 서버에 배포하는 과정을 의미합니다.대표적인 CD 툴은 다음과 같습니다.Jenkins  Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software.Jenkins는 파이프라인(Pipeline)을 사용해 거의 모든 언어의 조합과 소스코드 리포지토리에 대한 CI/CD 환경을 구축하기 위한 간단한 방법을 제공합니다.Jenkins가 각각의 단계에 대한 스크립트 작성의 필요성을 없애주지는 않지만, 일반 사용자가 구축할 수 있는 것보다 더 빠르고 더 강력하게 빌드(Build), 테스트, 그리고 배포(deployment) 도구 등 체인 전체를 통합할 수 있는 방법을 제공해 줍니다.Jenkins는 자바 언어로 개발된 툴로써 Jenkins를 실행하기 위해서는 별도의 서버가 존재해야 하며 서버에는 Java가 설치되어 있어야 합니다.Jenkins의 CI 관련 프로세스는 Jenkins의 파이프라인으로 정의됩니다.이러한 파이프라인은 다음의 3 가지 방법으로 정의할 수 있습니다.  Jenkinsfile     pipeline { agent {} environment {} stages {     stage(&quot;Build&quot;) {         environment {}         steps {}     }     stage(&quot;Test&quot;) {         environment {}         steps {}     } } }            Jenkins UI    Blue Ocean (Jenkins 공식문서 참고: Getting started with Pipeline) 참고  IT World: Jenkins란 무엇인가, CI(Continuous Integration) 서버의 이해  DevOps Story, CD를 위한 Jenkins, Argo CD 연계  메쉬코리아 플랫폼실 최제필, CI/CD 도구 및 방법론 도입기  javatpoint: Gradle vs. Jenkins  GitLab vs Jenkins  Katalon: Best 14 CI/CD Tools You Must Know | Updated for 2022",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-01T21:01:35+09:00'>01 Mar 2022</time><a class='article__image' href='/jenkins_series1'> <img src='/images/jenkins_10.png' alt='Jenkins Series [Part1]: What is Jenkins?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series1'>Jenkins Series [Part1]: What is Jenkins?</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part5]: 스파크 SQL",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series5",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents  RDD, Dataframe, Dataset          RDD      Dataframe      Dataset        Dataframe          Dataframe 생성      기본 연산      액션 연산      비타입 트랜스포메이션 연산      함수가 아닌 SQL문        Spark를 이용한 DataFrame 처리 개요  참고RDD, Dataframe, DatasetRDDSpark Core에 RDD가 있다면 Spark SQL에는 Dataframe과 Dataset이 있습니다. 기존의 RDD를 이용해 스파크 애플리케이션 코드를 작성할 때에는 RDD가 가지고 있는 메서드나 특성을 알아야지만 코드를 작성할 수 있었습니다. 그래서 RDD에 대한 이해도가 높아야 분산 환경에서 높은 처리 성능을 이끌어 낼 수 있었습니다.Dataframe그러던 중 Spark 1.3버전에서 Dataframe이라는 새로운 데이터 모델이 공개되었습니다. Dataframe은 개발자들에게 친숙한 SQL과 비슷한 방식으로 작성할 수 있도록하는 API를 제공해 진입 장벽을 낮췄으며 코드의 가독성 또한 높여주었습니다. Dataframe도 마찬가지로 low-level에서는 RDD로 코드가 동작하는데 Spark SQL은 내부적으로 Catalyst Optimizer를 통해 최적의 RDD 코드로 변환됩니다. 따라서 쉬운 코드 작성과 높은 성능을 모두 얻게 되었습니다.그러나 Dataframe에도 아쉬운 점이 있었는데, 바로 RDD에서 가능했던 컴파일 타임 오류 체크 기능을 사용할 수 없다는 점이었습니다.DatasetSpark 1.6버전에서 RDD의 장점과 Dataframe의 장점을 합친 새로운 데이터 모델인 Dataset이 등장했습니다.그리고 Spark 2.0 이후부터는 Dataframe이 Dataset 안에 포함되었습니다.# 데이터셋은 데이터를 처리할 때 데이터의 타입을 있는 그대로 활용할 수 있습니다.데이터셋: Dataset[String], Dataset[Int]# 데이터프레임은 데이터를 처리할 때 데이터 타입을 무조건 org.apache.spark.sql.Row로 감싸줘야 합니다.데이터프레임: DataFrame = Dataset[Row(String)]이렇게 Dataframe은 원래 데이터가 가지고 있던 타입의 특성은 사용하지 않기 때문에 Dataframe API은 비타입 트랜스포메이션 연산(untyped operations)으로 분류됩니다.            데이터 모델      사용 가능한 연산              Dataframe      기본 연산, 액션 연산, 비타입 트랜스포메이션 연산              Dataset      기본 연산, 액션 연산, 타입 트랜스포메이션 연산      저는 파이썬을 주언어로 사용하고 있으며 파이썬 언어는 Dataframe API만 제공하기 때문에 이번 포스트에서는 액션 연산과 비타입 트랜스포메이션 연산에 대해서만 다루도록 하겠습니다.DataframeDataframe 생성Dataframe은 SparkSession을 이용해 생성합니다. 생성 방법은 파일이나 데이터베이스와 같은 스파크 외부에 저장된 데이터를 이용할 수도 있고, 스파크 내에서의 RDD나 Dataframe을 이용해 새로운 Dataframe을 생성할 수도 있습니다.외부 데이터 소스파일이나 데이터베이스같은 외부 저장소의 데이터를 읽어와서 Dataframe을 생성할 때는 SparkSession의 read()메소드를 이용하면 됩니다. read()메소드는 DataFrameReader 인스턴스를 생성하고 이를 이용해 다양한 유형의 데이터를 읽고 Dataframe을 생성할 수 있습니다.from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;sample&quot;).master(&quot;local[*]&quot;).getOrCreate()df = spark.read.format(&quot;json&quot;).option(&quot;allowComments&quot;, &quot;true&quot;).load(&quot;&amp;lt;spark_home_dir&amp;gt;/test.json&quot;)  전체적인 생성 과정은 크게 다음과 같습니다.1. Spark Session의 read() 메소드를 호출해 DataFrameReader 인스턴스 생성2. format() 메소드로 데이터소스의 유형을 지정3. option() 메소드로 데이터소스 처리에 필요한 옵션을 지정4. load() 메소드로 대상 파일을 읽고 데이터프레임을 생성다음은 DataFrameReader가 제공하는 주요 메소드입니다.- format()    읽어들이고자 하는 데이터 소스의 유형을 문자열로 지정(&quot;kafka&quot;, &quot;csv&quot;, &quot;json&quot;, &quot;parquet&quot;, &quot;text&quot; 등)    이 밖에도 지원하지 않는 데이터소스는 라이브러리를 클래스패스에 추가해서 사용할 수 있습니다- option/options()    데이터소스에 사용할 설정 정보를 지정    데이터소스에  따라 다름- load()    데이터소스로부터 실제 데이터를 읽어서 Dataframe을 생성- json()    JSON 형식을 따르는 문자열로 구성된 파일이나 RDD로부터 Dataframe 생성- parquet()    파케이 형식응로 작성된 파일을 읽어서 Dataframe 생성- text()    일반 텍스트 형식으로 작성된 파일을 읽어서 Dataframe 생성- csv()    CSV 파일을 읽어 Dataframe 생성 RDD, Dataframefrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;sample&quot;).master(&quot;local[*]&quot;).getOrCreate()row1 = Row(name=&quot;kim&quot;, age=20, job=&quot;student&quot;)row2 = Row(name=&quot;mike&quot;, age=17, job=&quot;student&quot;)data = [row1, row2]df = spark.createDataFrame(data)기본 연산  Spark 공식문서 참고  persist()  printSchema()  columns  dtypes  createOrReplaceTempView()  explain()액션 연산  Spark 공식문서 참고  show()  head()  take()  count()  describe()비타입 트랜스포메이션 연산Dataframe에서 제공하는 비타입 트랜스포메이션 연산  Spark 공식문서 참고  select()  filter()  agg()  orderBy()  groupBy()  withColumn()org.apache.spark.Column에서 제공하는 비타입 트랜스포메이션 연산  Spark 공식문서 참고  !==, ===  alias()  isin()  when()  like()org.apache.spark.sql.functions에서 제공하는 비타입 트랜스포메이션 연산(왜 트랜스포메이션이지?)  Spark 공식문서 참고  max(), mean(), sum()  count(), countDistince()  explode()  when()  col()  lit()함수가 아닌 SQL문  createOrReplaceTempView(): Creates or replaces a local temporary view with this DataFrame.# DataFrame 생성을 위해서는 SparkSession이 필요함from pyspark.sql import SparkSession# SparkSession 생성spark = SparkSession.builder.master(&quot;local[*]&quot;).appName(&quot;taxi_project&quot;).getOrCreate()# SparkSession을 이용해 DataFrame 생성df = spark \  .read \  .format(&quot;kafka&quot;) \  .option(&quot;kafka.bootstrap.servers&quot;, &quot;kafka:29092&quot;) \  .option(&quot;subscribe&quot;, &quot;test&quot;) \  .option(&quot;startingOffsets&quot;, &quot;earliest&quot;) \  .load()# View 생성df.createOrReplaceTempView(&quot;tripdata&quot;)# SQL문을 이용해 쿼리 작성query = &quot;&quot;&quot;SELECT MONTH(value.tpep_pickup_datetime) AS month, ROUND(AVG(value.trip_distance), 3) AS average_distanceFROM tripdataGROUP BY MONTH(value.tpep_pickup_datetime)ORDER BY month&quot;&quot;&quot;# Returns a DataFrame representing the result of the given query.table = spark.sql(query)# 액션 연산table.show()Spark를 이용한 DataFrame 처리 개요참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  loustler, [Apache Spark] Spark RDD, Dataframe and DataSet  Apache Spark 공식문서: Spark SQL on PySpark  Spark by {Examples}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series5'> <img src='/images/spark_16.jpg' alt='Apache Spark Series [Part5]: 스파크 SQL'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series5'>Apache Spark Series [Part5]: 스파크 SQL</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series4",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents아직 작성 전입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series4'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series4'>Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part3]: 스파크의 클러스터 환경",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series3",
      "date"     : "Feb 21, 2022",
      "content"  : "Table of Contents  클러스터 환경  스파크의 분산처리 아키텍처  스파크 애플리케이션 동작 순서  참고클러스터 환경스파크는 본질적으로 분산처리 프레임워크입니다. 그래서 단순히 테스트를 위한 용도로는 단일 로컬 서버만으로도 가능하지만, 실제 배포 단계에서 스파크를 제대로 활용하기 위해서는 여러 대의 서버를 이용한 클러스터 환경을 구축할 필요가 있습니다.클러스터란 여러 대의 서버가 네트워크를 통해 연결되어 마치 하나의 서버인 것처럼 동작하는 방식을 의미합니다. 하지만 여러 서버들을 이 같은 방식으로 동작시키는 것은 쉬운 일이 아닙니다. 그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다.스파크에서는 자체 구현한 클러스터 매니저도 제공하고 외부 클러스터 매니저를 임포트해서 사용할 수도 있습니다. 이렇게 여러 종류의 클러스터 매니저를 지원하게 되면 선택의 폭이 넓어진다는 장점도 있긴 하지만 클러스터 매니저마다 동작 방식이나 용어가 다르면 혼동이 될 수 있습니다. 스파크에서는 이런 혼란을 없애고자 추상화된 클러스터 모델을 제공함으로써 사용하는 클러스터의 종류에 관계없이 일관된 방법으로 프로그램을 작성하고 클러스터를 관리할 수 있게 해줍니다.내용에 들어가기 전에 한 가지 알아둘 것은 클러스터 환경이라고 해서 로컬 환경에서 사용하던 스파크 애플리케이션 코드를 새로 작성해야 할 필요는 없습니다. 다만 클러스터 환경에서는 여러 서버를 마치 하나의 서버인 것처럼 다뤄야 하기 때문에 하나의 작업을 여러 서버에 분산해서 실행하고 그 결과를 취합할 수 있는 분산 작업 관리 기능이 추가되어야 할 것입니다.따라서 이번 포스트의 목적은 분산처리를 위한 시스템 아키텍처를 이해하고, 이를 구현하기 위해 필요한 설정과 매개변수를 이해하는 것입니다.스파크의 분산처리 아키텍처아래 그림은 분산처리를 위한 스파크의 전형적인 아키텍처입니다.보시다시피 클러스터 매니저는 가운데에서 분산처리를 위한 매니저 역할을 하고 있습니다. 각각의 컴포넌트에 대한 설명은 앞의 포스트에서 다룬 적이 있음으로 여기서는 간단하게만 요약하도록 하겠습니다.  드라이버 프로그램: 스파크 컨텍스트를 생성하고 클러스터 매니저와 연결시켜주는 프로그램  스파크 컨텍스트: 클러스터와 연결되는 객체로 스파크 애플리케이션 코드를 작성하는데 필요한 거의 모든 기능을 제공  클러스터 매니저: 워커 노드를 모니터링하며 최적의 자원(CPU, 메모리) 할당  워커 노드: 분산된 데이터를 할당받고 요청된 작업을 처리하는 서버  익스큐터: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위  잡(Job): 액션 연산의 수  태스크: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위스파크 클러스터는 이와 같이 드라이버, 클러스터 매니저, 워커 노드의 조합으로 구성됩니다. 여기서 실행 모드, 클러스터 매니저의 종류에 따라 약간의 다른 점이 있지만 큰 맥락에서는 같습니다.실행 모드의 경우 두 가지가 있습니다. 클러스터 모드, 클라이언트 모드입니다.두 모드 가운데 어떤 것을 선택하든 수행 결과는 동일합니다. 하지만 클러스터 모드의 경우 드라이버 프로그램과 익스큐터 간의 네트워크 비용이 상대적으로 낮아져서 성능 향상을 기대할 수 있습니다. 하지만 스파크 셸과 같은 인터랙티브 환경을 이용한 디버깅이 어려워서 정형화된 작업에만 주로 사용하고, 클라이언트 모드의 경우 사용성이 편리하지만 드라이버 프로그램과 워커 노드가 네트워크 상에서 너무 많이 떨어져 있으면 전체적인 성능에 영향을 줄 수 있으므로 가급적 동일 네트워크 상에 존재하는 서버로 선택하는 것이 좋습니다.스파크 애플리케이션 동작 순서지금까지 스파크의 클러스터 환경에서 갖게되는 아키텍처와 컴포넌트에 대해 살펴봤습니다. 지금부터는 아키텍처에서 실제로 스파크 애플리케이션이 구동되는 과정을 살펴보도록 하겠습니다.  가장 먼저 스파크 애플리케이션 코드를 작성합니다. 이 때 코드에는 스파크컨텍스트를 생성하는 드라이버 프로그램이 포함돼 있어야 합니다.  작성한 코드를 빌드하고 관련 라이브러리와 함께 jar나 zip 파일 등으로 패키징합니다.  패키지 파일을 스파크에서 제공하는 spark-submit 셸 스크립트를 이용해 클러스터에 배포하고 실행합니다.  코드에 있는 드라이버 프로그램이 실행되고 스파크컨텍스트가 클러스터 매니저와 연동되어 워커 노드에 익스큐터를 생성합니다.  드라이버 프로그램은 작성된 코드에서 액션 연산의 수만큼 잡(Job)을 생성합니다.  잡(Job)은 DAG 스케줄러에게 전달되고 잡을 셔플링이 가장 적게 일어나는 방법으로 스테이지를 나누고 각 스테이지 단계를 여러 개의 태스크로 나누어 DAG를 생성합니다.  생성된 DAG는 클러스터 매니저(마스터 노드)에게 전달됩니다.  클러스터 매니저는 익스큐터에 DAG에 맞게 태스크를 전달합니다.  스파크컨텍스트는 코드에 정의된 연산을 보고 RDD 데이터를 적당한 파티션으로 나눕니다.  이렇게 나누어진 파티션은 태스크마다 1개씩 할당됩니다.  태스크는 결과적으로 다수의 워커 노드에서 실행중인 익스큐터에 의해 분산 처리됩니다.참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  What is SparkContext? Explained",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-21T21:01:35+09:00'>21 Feb 2022</time><a class='article__image' href='/spark-series3'> <img src='/images/spark_13.png' alt='Apache Spark Series [Part3]: 스파크의 클러스터 환경'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series3'>Apache Spark Series [Part3]: 스파크의 클러스터 환경</a> </h2><p class='article__excerpt'>그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part2]: 스파크 개발환경 구축하기",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series2",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  스파크 설치          pyspark                  자바, 파이썬 설치          pyspark 설치                    Spark        로컬 개발 환경  클러스터 환경  참고스파크 설치스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다. 자바가 필요한 이유는 스파크가 JVM 위에서 실행되기 때문입니다.하지만 실무에서는 대부분의 빅데이터 소프트웨어들이 클러스터 환경에서 동작하기 때문에 제대로 활용하기 위해서는 여러 가지 준비할 것도 많고 설정해야 할 것들도 많습니다. 그래서 스파크는 개발/테스트를 위한 용도로 간단하게 사용할 때에는 단독 서버에서 동작하는 로컬 모드를, 배포를 위한 용도로 클라이언트, 클러스터 모드를 지원합니다.스파크 애플리케이션 코드는 자바, 스칼라, 파이썬, R언어로 작성할 수 있습니다.pyspark우선 저는 파이썬을 주언어로 사용하기 때문에 pyspark를 이용해 파이썬으로 스파크 애플리케이션 코드를 작성할 예정입니다. pyspark의 장점은 만약 개발/테스트를 위한 목적으로만 스파크를 사용할 예정이라면 스파크를 설치할 필요가 없다는 것입니다. 스파크를 사용하는데 스파크를 설치할 필요가 없다? 무슨 뜻이냐면 pyspark를 설치하기만 해도 스파크를 실행하기 위해 필요한 최소한의 파일을 함께 설치해줍니다.하지만 여전히 자바는 설치해주어야 합니다.  To run Spark, you only require a Java runtime environment (JRE) but you may also download the Java development kit (JDK) which includes the JRE.저는 파이썬이 설치되어 있는 도커 이미지를 이용해 컨테이너 안에서 실습을 진행해 보았습니다.자바, 파이썬 설치# 파이썬이 설치된 컨테이너 생성docker run -it python:3.8-buster# JDK 설치apt-get updateapt-get install openjdk-11-jdk# JAVA_HOME 변수 설정, 경로 추가export JAVA_HOME=/etc/openjdk-11-jdk     # 본인의 자바 설치 경로export PATH=$JAVA_HOME/bin:$PATH. /etc/profile # bash쉘이면 source /etc/profilepyspark 설치# pyspark 설치pip install pyspark# 잘 설치되었는지 확인import pysparksc = pyspark.SparkContext(appName=&quot;SparkContext&quot;)sc--------------------------------SparkContextVersionv3.2.1Masterlocal[*]AppNameSparkContextSpark이번에는 파이썬에 국한되지 않는 조금 더 일반적인 방법으로 스파크를 설치해보겠습니다. 이번에는 리눅스 운영체제만 가지는 컨테이너 위에서 실습을 진행하도록 하겠습니다.처음에는 우분투 이미지를 바로 컨테이너로 띄우고 그 위에서 자바를 설치하려 했지만, 오라클에서 다운받는 방법을 제한하고 있어서 아래의 방법으로 진행했습니다. (우분투 이미지에 로컬에서 다운받은 자바를 하나의 이미지로 새로 빌드)그래서 사실 위에서 진행한 pyspark만 설치하는 방법에서도 python이미지에 로컬 자바로 한 번 이미지를 빌드한 후 사용하는 것이 좋을 것 같습니다. 저도 아직 본격적으로 사용해보지는 않아서 에러가 있는지는 확인해보지 않았지만 로컬에서 자바를 다운 받고 빌드하는 방법은 확실히 안전합니다.🦊 자바 설치자바 라이센스를 소유하고 있는 오라클에서 2019년 4월부터 자바를 외부의 허용하지 않은 방법으로 다운받는 것을 금지시켰습니다. 그래서 wget과 같은 방식으로 자바8 버전을 더이상 다운받을 수 없게 되고 무조건 오라클에 로그인을 한 후 로컬에 먼저 다운을 받아야합니다. 자바 17은 가능한데 스파크에서 자바 17로 설치하니까 오류가 난다. 구글링에서는 자바를 다운그레이드 하라고 나와있다. 자바8로 해보니까 된다. 그래서 자바8을 지금 다운 받으려고 하는 것이다.그래서 저같은 경우에는 왠만한 작업들은 무조건 도커 컨테이너에서 진행하는 편이라 처음에는 도커허브에서 자바8이 설치되어 있는 이미지를 찾아봤지만 뭔가 세부설정들이 마음에 들지 않게 되어 있어서 이미지를 직접 빌드하기로 결정했습니다. 제가 사용한 방법의 과정은 다음과 같습니다.# 자바를 로컬에 다운로드# 다운로드 페이지 접속https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html# 저는 M1 칩을 사용하고 있어서 ARM64 전용 파일을 다운로드 받았습니다.jdk-8u311-linux-aarch64.tar.gz# 다운로드 받은 폴더에서 압축해제tar -xzvf jdk-8u311-linux-aarch64.tar.gz# Dockerfile 작성# 로컬에 설치한 자바를 컨테이너로 옮기고 스파크까지 설치해주었습니다FROM ubuntu:latestCOPY jdk1.8.0_321 ./jdk1.8.0_321RUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim \&amp;amp;&amp;amp; apt-get -y install wget \&amp;amp;&amp;amp; wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz \&amp;amp;&amp;amp; tar -xzvf spark-3.2.1-bin-hadoop3.2.tgzvim /etc/profile# 환경 변수설정해줍니다. 이부분은 Dockerfile에서 ENV로 설정해 줄 수도 있습니다export JAVA_HOME=/jdk1.8.0_321export PATH=$JAVA_HOME/bin:$PATH. /etc/profilecd spark-3.2.1-bin-hadoop3.2ls--------------------------------------------------------------------------------------------------------------LICENSE  NOTICE  R  README.md  RELEASE  bin  conf  data  examples  jars  kubernetes  licenses  python  sbin  yarn# 스파크에서 제공하는 실행 파일cd binls# 스파크 셸 실행./bin/spark-shell# 셸 종료:q위의 과정은 이미지에서 매번 스파크를 다운받는 방식이기 때문에, 스파크를 다운받은 컨테이너를 다시 한 번 이미지로 만들면 그 다음부터는 새로 만든 이미지를 이용하면 컨테이너를 띄우는 속도가 더 빨라지게 됩니다. 그래서 docker commit 명령어를 이용해 한번 더 이미지를 빌드하는 것을 권장드립니다.# 로컬 터미널에서 docker commit 명령어로 이미지 생성# dockere commit &amp;lt;원하는 컨테이너 이름&amp;gt; &amp;lt;생성할 이미지 이름&amp;gt;docker commit thirsty_galois spark_container로컬 개발 환경위의 설치과정을 완료한 후 스파크의 설정 정보를 확인해 보겠습니다../bin/spark-shell --verbose다른 부분은 일단 신경쓰지 말고 master 부분만 보도록 하겠습니다. 현재 master가 local[*]로 설정되어 있습니다. 이는 현재 드라이버 프로그램을 실행하는 서버를 포함해 워커 노드까지 모두 로컬 서버를 이용하고 있다는 뜻입니다. *는 로컬 서버의 모든 스레드를 사용하겠다는 뜻입니다.따라서 여기까지만 설정하게 되면 로컬에서 테스트 목적으로 사용하기 위한 최소한의 준비는 끝난 것입니다. 이 외에도 여러 가지 설정들을 직접하고 싶을 때에는 ./conf에 설정을 위한 여러가지 파일의 템플릿을 이용할 수 있습니다.클러스터 환경참고  Pyspark 코드는 어디서 실행되는가?  Pyspark만으로 스파크 애플리케이션 실행할 수 있나?  Pyspark의 한계  bin/sh: 1: source: not found  [Linux] 우분투에 자바 설치  Unable to download Oracle JDK 8 using Wget command  자바(JDK, JRE) 모든 버전 다운로드( 6,7,8,9,10,11,12,13,14,15, 16, 17..)  How to Set Up a Multi Node Apache Spark Cluster with Quobyte  Updating the Apache Spark configuration files  [spark] Spark 3 클러스터 설치",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/spark-series2'> <img src='/images/spark_10.png' alt='Apache Spark Series [Part2]: 스파크 개발환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series2'>Apache Spark Series [Part2]: 스파크 개발환경 구축하기</a> </h2><p class='article__excerpt'>스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part1]: 블록체인을 공부하기 전에",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series0",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  블록체인이란  블록체인에 대한 오해  블록체인 용어 바로 알기          분산 vs 탈중앙화      디지털화 vs 블록체인      가상 화폐 vs 암호 화폐      거래소 vs 중개소      트랜잭션 vs 거래내역      디지털 자산        블록체인의 역사          사이퍼펑크      비트코인의 탄생        블록? 블록체인? 비트코인?          블록      블록체인      비트코인        마치며  참고블록체인이란블록체인(blockchain)이란 다수의 거래내역을 묶어 블록을 구성하고, 해시를 이용하여 여러 블록들을 체인처럼 연결한 뒤, 다수의 사람들이 복사하여 분산 저장하는 알고리즘이다.블록체인은 비트코인과 이더리움 등 암호화폐에 사용된 핵심 기술이다. 은행 등 제3의 중개기관이 없더라도 블록체인 기술을 이용하면 누구나 신뢰할 수 있는 안전한 거래를 할 수 있다. 블록체인은 암호화폐뿐 아니라, 온라인 거래내역이 있고 이력관리가 필요한 모든 데이터 처리에 활용할 수 있다. 블록체인 기반의 스마트 계약, 물류관리 시스템, 문서관리 시스템, 의료정보관리 시스템, 저작권관리 시스템 등 다양한 활용이 가능하다.블록체인은 간략히 ‘분산원장’(分散元帳, distributed ledger) 기술이라고 한다. 즉, 거래내역을 기록한 원장을 다수의 사람들에게 분산하여 저장·관리하는 기술이다. 자세히 설명하면, 블록체인이란 다수의 온라인 거래 기록을 묶어 하나의 데이터 블록(block)을 구성하고, 해시(hash) 값을 이용하여 이전 블록과 이후 블록을 마치 체인(chain)처럼 연결한 뒤, 이 정보의 전부 또는 일부를 피투피(P2P) 방식으로 전 세계 여러 컴퓨터에 복사하여 분산 저장·관리하는 기술이다.블록체인에 대한 오해위의 내용은 (해시넷: 블록체인)에서 작성한 글의 일부를 가져온 것입니다.‘제 3자가 필요 없어지고, 이에 따라 수수료도 사라지는 이상적인 플랫폼’. 많은 사람들은 블록체인을 이용하면 그동안의 거래 시스템 전반에 많은 혁신을 가져다 줄 것으로 기대했습니다. 하지만 현실은 그렇지 않았습니다. 블록체인의 등장은 그동안 불필요했던 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다. 또한 독립적인 화폐를 내세웠던 암호 화폐는 선채굴을 악용한 채굴업자들이 암호화폐 대부분을 장악한 후 전 세계 중개소를 통해 일반인들을 선동, 호도하며 내다팔아 막대한 수익을 얻었습니다.저는 이번 BlockChain Series를 준비하며 앞으로 블록체인의 미래가 어떻게 될지 한 번 알아보려고 합니다.블록체인 용어 바로 알기분산 vs 탈중앙화블록체인을 공부하다 보면 분산, 탈중앙화라는 단어가 많이 사용되는 것을 보게 됩니다. 하지만 블록체인과 일반적인 분산 시스템과는 약간 다른 점이 있습니다. 분산 시스템은 데이터를 여러 노드에 분산해 나누어 처리하기 때문에 효율적입니다. 하지만 블록체인은 같은 데이터(거래 내역 등)를 모든 노드가 중복 처리합니다. 이 방법은 데이터의 신뢰도를 높일 수는 있지만 비효율적입니다.            특징      분산      탈중앙화              저장      여러 노드에 분산 저장      여러 노드에 중복 저장              처리      여러 노드가 분산 처리      여러 노드가 중복 처리              장점      높은 효율성      높은 신뢰성      디지털화 vs 블록체인블록체인을 공부하면서 블록체인의 장점으로 시스템의 효율성과 안전한 데이터 저장이라는 글을 본 적이 있습니다. 하지만 이는 디지털화(digitalization)의 장점이며 블록체인은 정확히 이것과 반대입니다. 블록체인은 효율성을 포기하고 데이터의 신뢰성을 높인 것이며, 데이터는 보안되지 않고 모든 사용자에게 공개되어 저장됩니다. 따라서 블록체인은 보안과 효율을 위한 도구가 아니고 신뢰받는 제 3자가 없더라도 거래가 가능한 플랫폼을 만들기 위한 실험적 과정에 있는 개념이라고 생각하면 됩니다.가상 화폐 vs 암호 화폐가상 화폐는 어떤 실물의 가치를 디지털화한 것을 의미하며, 암호 화폐는 가상 화폐의 일부로 블록 체인을 기반으로 만들어진 화폐를 의미합니다. 암호화 되었다는 뜻은 암호 화폐를 보내는 송신인과 수신인이 암호화된 공개키를 통해서만 거래를 하기 때문으로 이는 금융 시스템의 투명성을 해치고 악용될 여지가 많습니다.거래소 vs 중개소거래소는 금융 거래소에서 가져온 단어로 금융 거래소는 거래 시스템에 필요한 환경과 제도가 잘 갖춰져 있습니다. 하지만 암호 화폐 거래소의 경우 암호 화폐의 가격이 단일화 되어 있지도 않고, 거래소 안에 엄격한 규정이 적용되어 있지도 않습니다. 그래서 정확한 명칭은 암호 화폐 중개소 정도가 적합합니다. 그리고 온라인 상에 있는 많은 중개소들은 블록체인과는 별 관련이 없습니다. 중개소는 그저 암호 화폐를 이용하는 지갑이라는 소프트웨어와 온라인 주식 매매에 이용되는 HTS 기능 중 일부를 사용해 거래를 중개하는 브로커일 뿐입니다.트랜잭션 vs 거래내역IT 분야에서는 트랜잭션을 보통 업무 처리의 단위로 얘기 하고 특히 데이터베이스 분야에서는 더 이상 쪼갤 수 없는(또는 더 쪼개면 심각한 오류가 발생할 수 있는)최소한의 업무 처리 단위를 의미합니다. 블록체인에 있어서 트랜잭션은 ‘정의된 이벤트가 발생하는 것’을 의미하며 거래 내역을 포함한 더 포괄적인 의미입니다.디지털 자산암호 화폐는 디지털 자산이 아닙니다. 디지털 자산은 저작권, 소유권 등의 권리를 디지털화 한 것으로 이는 실질적인 가치를 내재하고 있습니다. 하지만 암호 화폐는 내재 가치가 0인 디지털 숫자에 불과합니다. 하지만 내재 가치가 0인 경우에도 가치를 가질 수 있습니다. 그러기 위해 필요한 것이 바로 ‘신뢰’입니다. 따라서 암호 화폐가 실제로 가치를 가지기 위해서는 사람들로부터의 신뢰가 필요합니다. 예를 들어 지폐가 있습니다. 지폐는 종이에 불과하지만 그 뒤에 그 지폐를 발행한 국가의 법령과 신뢰가 뒷받침하기 때문에 실질적인 가치를 가질 수 있는 것입니다. 주식도 온라인 증명서에 불과하지만 주식을 발행한 기관의 신뢰 덕분에 가치를 가지게 됩니다.블록체인의 역사사이퍼펑크블록체인은 사이퍼펑크(cypherpunk) 운동에 뿌리를 두고 있습니다. 사이퍼펑크란 중앙집권화된 국가와 거대 기업들에 대항하여 개인의 프라이버시를 보호하기 위해 암호기술을 이용하여 익명성을 보장하는 탈중앙화 시스템을 만드려는 행동주의자들을 말합니다.비트코인의 탄생2008년 사토시 나카모토란 가명으로 암호화 커뮤니티에 논문이 하나 올라왔습니다. 제목은 비트코인: P2P 전자 캐시 시스템으로 사토시는 이 논문에서 코인을 그 누구의 간섭도 받지 않는 결제 수단이라고 설명했습니다. 그리고 2009년 1월 비트코인의 개념을 실제로 소프트웨어로 구현되어 최초의 블록(제네시스 블록 또는 0번 블록)이 생성되었습니다. 비트코인은 현재까지 10분에 하나꼴로 블록이 만들어지고 있으며 2019년 10월 기준 70만개의 블록이 만들어졌습니다. (참고로 사토시의 정체는 아직까지 밝혀지지 않았다고 합니다)블록? 블록체인? 비트코인?블록체인을 공부하다 보면 블록의 정체가 무엇인지 또 나는 블록체인을 공부하는데 왜 자꾸 나도 모르는 사이에 비트코인이라는 단어가 등장하는 건지 의아했었습니다. (참고로 사토시가 공개한 비트코인에 관한 논문에는 ‘블록’과 ‘체인’이라는 명사가 독립적으로 사용되기는 했지만 ‘블록체인’이라는 단어는 한 번도 등장하지 않았습니다.)블록블록은 전산학에서 보통 한꺼번에 처리되는 논리적 데이터 단위를 일컫습니다. 비트코인에서는 트랜잭션들을 1메가바이트를 넘지 않는 선에서 계속 묶었다가 1메가바이트 직전이 되면 블록으로 만듭니다. 보통 이렇게 하나의 블록이 만들어지는데 10분 정도가 걸립니다.            블록      설명              정의      한꺼번에 처리되는 데이터 단위              크기      최대 1MB              트랜잭션 수      2000~3000개              생성 시간      평균 10분      블록체인트랜잭션 데이터를 포함하는 블록을 앞의 블록의 해시값을 이용해 연결한 것을 블록체인이라고 합니다. 블록은 각각의 노드에서 자신이 가지고 있는 트랜잭션들을 묶다가 블록을 만들 수 있는 시점이 되면 비동기적으로 블록을 만들기 위해 경쟁합니다. 이 때의 경쟁을 채굴(mining)이라고 하며 가장 먼저 블록을 만든 노드의 블록만을 모든 노드의 블록체인에 추가합니다.비트코인비트코인 생태계(네트워크)에서 채굴로 얻게되는 보상을 ‘비트코인’이라고 합니다. 이 보상을 얻기 위해 채굴자들이 그렇게 앞다투어 채굴을 했던 것입니다. 여기서 채굴은 블록을 만들 때 블록에 필요한 해시값을 찾는 과정을 일컫습니다. 채굴에 대한 보상으로 얻게되는 비트코인의 수량은 어떻게 책정될까요? 보상금은 보조금과 수수료의 합으로 이루어지는데, 보조금은 채굴로 얻게되는 고정 수익을 말하고, 수수료는 비트코인 거래 시 송신자로부터 얻게 되는 것으로 수수료는 송신자가 스스로 정하게 됩니다. 그래서 보통 더 높은 수수료를 지불한 트랜잭션이 먼저 블록에 포함되게 됩니다. 보조금의 경우 제네시스 블록이 생성됐을 때에는 50BTC 였으나, 보조금은 블록이 21만개 생성될 때마다 반감되도록 설정되어 현재는 블록을 하나 생성할 때마다 6.25BTC를 얻게 됩니다.블록은 평균 10분 마다 한 개씩 생성되고 보조금은 21만개 마다 계속 반감되기 때문에 시간이 지나면 블록을 생성해도 더 이상 비트코인이 (거의) 발행되지 않게 될 것입니다. 이론적으로 2033년에 거의 포화되는 시점에 다다를 것으로 보며 그 때까지 누적된 비트코인의 양은 2100만BTC라고 합니다. 이렇게 채굴에 대한 보상이 극도로 낮아질 경우, 채굴자가 급격히 줄어들게 될 것이고, 이는 비트코인 시스템의 안전성을 급격하게 저하시키게 됩니다.마치며지금까지 블록체인에 대해 공부하기 전에 알고가면 좋은 지식들에 대해 살펴보았습니다. 다음 포스트에서는 블록체인의 동작원리에 대해 알아보겠습니다.참고  블록체인 해설서 책  해시넷: 블록체인",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/blockchain-series0'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part1]: 블록체인을 공부하기 전에'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series0'>BlockChain Series [Part1]: 블록체인을 공부하기 전에</a> </h2><p class='article__excerpt'>하지만 현실은 블록체인의 등장으로 불필요한 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-memory_allocation",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  메모리 관리          Python Memory Manager      Garbage Collection        메모리 할당          Stack 할당      Heap 할당        참고요즘에는 컴퓨터, 스마트폰을 사용할 때 한가지 프로그램/어플리케이션만 실행하는 사람은 없을 것입니다. 그렇기 때문에 내가 만든 프로그램/어플리케이션이 메모리를 효율적으로 사용하도록 개발하는 것은 중요합니다.메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다. 메모리 할당은 내가 사용하는 프로그래밍 언어, 운영체제, 컴퓨터 아키텍처에 따라 조금씩 다르지만 전체적인 과정은 비슷합니다.파이썬의 메모리 관리는 대부분 Python Memory Manager에 의해 수행되지만, Python Memory Manager를 공부하면 프로그래밍 전반에 대한 이해와 코드 최적화, 디버깅을 더욱 잘 할 수 있게 될 것입니다.메모리 관리파이썬에서의 메모리 관리는 크게 두 가지 레벨로 나누어서 생각할 수 있습니다. 첫 번째로 운영체제 레벨에서는 각 프로세스에 메모리를 얼마나 할당할지를 정하고, 각각의 프로세스가 다른 프로세스에 접근하지 못하도록 관리합니다.두 번째로 파이썬 내에서의 메모리 관리입니다.파이썬에서는 컴파일 단계에서 스택 영역에 메모리를 정적으로 크기를 정하고, 실행 단계에서는 Python memory manager를 이용해 힙 영역에 동적으로 메모리를 할당하고 그 외의 역할(공유, 할당, 제거 등)들을 수행함으로써 메모리를 관리합니다.Python Memory ManagerPython memory manager는 모든 Python objects와 data structures를 포함하는 private 힙을 포함합니다. Python memory manager는 공유(sharing), 분할(segmentation), 사전 할당(preallocation) 또는 캐싱(caching)과 같은 다양한 동적 스토리지 관리 측면을 다루는 다양한 구성 요소를 가지고 있습니다.가장 낮은 수준에서 raw memory allocator는 운영 체제의 메모리 관리자와 상호 작용하여 모든 파이썬 관련 데이터를 저장할 수 있는 충분한 공간을 private 힙에 확보합니다. raw memory allocator 외에도 여러 object-specific allocators가 동일한 힙에서 object의 특성에 맞는 고유한 메모리 관리 정책을 구현합니다.파이썬 힙의 관리는 인터프리터 자체에 의해 수행되며 힙 내부의 메모리 블록에 대한 객체 포인터를 사용자가 직접 제어할 수 없다는 것을 의미합니다. Python objects를 위한 힙 공간 할당은 파이썬/C API 함수를 통해 파이썬 메모리 관리자에 의해 수행됩니다.Garbage Collection가비지 컬렉션은 인터프리터가 프로그램을 사용하지 않을 때 프로그램의 메모리를 비우는 것입니다. 파이썬이 이렇게 할 수 있는 것은 파이썬 개발자들이 백엔드에서 우리를 위해 가비지 컬렉터를 구현했기 때문입니다. 파이썬 가비지 컬렉터는 reference counting 방법으로 객체에 더 이상 참조가 없을 때는 객체가 차지하고 있던 메모리의 할당을 취소하고 메모리를 비우게 됩니다.메모리 할당메모리 할당(memory allocation)은 프로그램이 컴퓨터 메모리의 특정 빈 블록에 할당되거나 할당되는 과정을 의미합니다. 파이썬에서 이 모든 것은 Python memory manager에 의해 수행됩니다.Stack 할당스택 할당은 정적 메모리를 저장하는데, 정적 메모리는 특정 함수나 메서드 호출 내에서만 필요한 메모리입니다. 함수가 호출되면 프로그램의 호출 스택에 추가됩니다. 변수 초기화 같은 특정 함수 내부의 모든 로컬 메모리 할당은 함수 호출 스택에 임시로 저장되며, 함수가 돌아오면 삭제되고 호출 스택이 다음 작업으로 이동합니다. 연속적인 메모리 블록에 대한 이 할당은 미리 정의된 루틴을 사용하여 컴파일러에 의해 처리되기 때문에 개발자들은 이것에 대해 걱정할 필요가 없습니다.(그러면 이 부분은 함수, 메서드, 그리고 함수나 메서드 안에서 사용되는 지역변수들을 컴파일 단계에서 확인하고 이에 알맞은 메모리 크기를 예측해서 정적으로 할당하는 건가?)(그리고 코드 실행 단계에서 함수가 호출될 때마다 스택에 Call stack이 쌓인다?)Heap 할당힙 할당은 프로그램에서 전역 범위로 사용되는 메모리인 동적 메모리를 저장합니다. 이러한 변수들은 특정 메서드나 함수 호출 외부에 필요하거나 전역적으로 여러 함수 내에서 공유됩니다. 스택 할당과 달리 힙 할당은 힙 데이터 구조와 관련이 없습니다. 힙 영역은 단순히 할당하고 어느 정도 임의의 주소에서 자유롭게 사용할 수 있는 큰 메모리 공간이며, 저장되는 객체에 필요한 공간을 기반으로 합니다.(힙 할당은 코드 실행 단계에서 객체의 타입에 맞게 동적으로 메모리 할당된다?)(Python memory manager의 가비지 컬렉션 기능에 의해 더이상 참조되지 않는 객체는 제거되고 메모리 비운다?)참고  How does Memory Allocation work in Python (and other languages)?  Python 공식문서: Memory Management  What’s the difference between a stack and a heap?  RealPython: Memory Management in Python  muchogusto.log: 파이썬 런타임과 메모리 관리  python의 메모리 할당과 관리 (Stack &amp;amp; Heap Memory)  파이썬 메모리 영역",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-memory_allocation'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-memory_allocation'>Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)</a> </h2><p class='article__excerpt'>메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-cpython_pypy",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  파이썬 언어의 특징  파이썬은 인터프린터 언어?  컴파일 언어  인터프리터 언어  파이썬 인터프리터의 종류          CPython      Jython      PyPy        참고파이썬 언어의 특징파이썬(Python)은 1980년대 후반 귀도 반 로섬이 개발하고 1991년에 출시한 high-level 범용 프로그래밍 언어입니다. 동적 타입 언어이므로 변수의 타입을 선언할 필요가 없으며, 코드가 실행되어 메모리 관리가 자동으로 수행됩니다.이번 포스트에서는 파이썬 코드를 실행할 때 어떤 동작이 내부적으로 일어나는지 알아보도록 하겠습니다.저희가 CLI 환경에서 파이썬 코드를 실행하는 경우를 생각해봅시다.python my_code.py여기서 python은 바로 파이썬 인터프리터 프로그램을 의미합니다. 따라서 파이썬 코드는 파이썬 인터프리터를 통해 실행하는 것입니다.  (Python Interpreter - Stanford Computer Science 참고)파이썬은 인터프린터 언어?위에서 파이썬 인터프리터를 통해 파이썬 코드를 실행한다고 했습니다. 그러면 파이썬은 인터프리터 언어일까요? 과거에 C는 컴파일 언어이고, 쉘 프로그래밍 언어는 인터프리터 언어였기 때문에 이 경계가 비교적 명확했지만, 비교적 최근에 나온 언어들은 그 경계가 모호합니다.파이썬 또한 어느 정도의 컴파일 언어적 특성과, 인터프리터 언어의 특성을 모두 가지고 있어서 이분법적으로 나누기가 힘들지만 프로그래밍 언어의 대표인 C언어가 완전한 컴파일 언어이기 때문에 이와 구분하기 위해 그냥 편하게 인터프리터 언어라고 하는 것 같습니다. 정리하면 파이썬은 컴파일 언어이기도 하면서 인터프리터 언어이기도 합니다.컴파일 언어먼저 간단하게 컴파일 언어의 뜻과 컴파일 언어가 코드를 실행하는 과정에 대해 살펴보겠습니다.  컴파일은 소스코드를 다른 타겟 언어(기계어, 자바, C, 파이썬)로 변환하는 과정을 의미  코드를 실행할 때 코드 전체를 인풋으로 사용  코드는 컴파일 단계에서 한 번 기계어로 변환되어 저장되고 나면 언제든 바로 실행가능  컴파일러는 실행하는 역할이 아니고 기계어로 변환하는 역할인터프리터 언어  인터프리터 언어는 소스 코드를 바로바로 실행하게 됩니다.  인터프리터는 코드를 한 줄씩 입력으로 사용해 실행합니다.  인터프리터에는 종류마다 다른 실행 방법이 있습니다.  A. 소스 코드를 파싱해서 설정한 방법에 따라 실행  B. 소스 코드를 먼저 중간 단계의 바이트 언어로 변환하고 A의 과정을 수행  C. 컴파일러에 의해 먼저 변환된 코드를 이용해 1의 과정을 수행파이썬 인터프리터는 이 중 B에 해당합니다.1. 소스 코드를 컴파일러를 이용해 중간 단계의 바이트 언어의 파일(.pyc)로 변환2. Python Virual Machine 위에서 바이트 언어 파일 한 줄씩 실행  파이썬 인터프리터의 종류CPythonJythonPyPyPyPy는 파이썬 언어(RPython: 파이썬의 일부)로 작성된 인터프리터입니다. 디폴트인 CPython과의 호환성을 유지하면서도 CPython 보다 속도가 빠르다고 알려져있습니다.참고  Shalini Ravi, How Does Python Code Run: CPython And Python Difference  elhay efrat, Python » CPython  geeksforgeeks, Difference between various Implementations of Python  파이썬 언어의 특징  파이썬 인터프리터 고르기  Wireframe: 파이썬은 인터프리터언어입니까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-cpython_pypy'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-cpython_pypy'>Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part3]: Javascript 표현식과 문",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series3",
      "date"     : "Feb 6, 2022",
      "content"  : "Table of Contents  값  리터럴  표현식  문  세미콜론 자동 삽입  표현식인 문과 표현식이 아닌 문값값은 표현식이 평가되어 생성된 결과를 말한다. 평가란 식을 해석해서 값을 생성하거나 참조하는 것을 의미한다.10 + 20;x - 1;리터럴리터럴은 사람이 이해할 수 있는 문자 또는 약속된 기호를 사용해 값을 생성하는 표기법을 말한다.            리터럴      예시              정수      100              부동소수점      0.4              2진수      0b010001              문자열      ‘Hello’              불리언      true, false              null      null              undefined      undefined              객체      { name: ‘Lee’, address: ‘Seoul’ }              배열      [1, 2, 3]              함수      function() {}              정규 표현식      /[A-Z]+/g      표현식표현식(expression)은 값으로 평가될 수 있는 문이다. 위에서 살펴본 리터럴은 값으로 평가될 수 있기 때문에 리터럴도 표현식이라 할 수 있다.10&#39;Hello&#39;person.namearr[1]10 + 20sum = 10sum !== 10square()person.getName()x + 3문문(statement)은 프로그램을 구성하는 기본 단위이자 최소 실행 단위다. 문은 여러 토큰으로 구성된다. 토큰이란 문법적인 의미를 가지며, 문법적으로 더 이상 나눌 수 없는 코드의 기본 요소를 의미한다. 예를 들어, 키워드, 식별자, 연산자, 리터럴, 세미콜론, 마침표 등의 특수기호는 모두 토큰이다.문은 선언문, 할당문, 조건문, 반복문 등으로 구분할 수 있다.// 선언문var x;// 할당문x = 5;// 함수 선언문funciton foo() {}// 제어문if (x &amp;gt; 1) { console.log(x); }// 반복문for (var i = 0; i &amp;lt; 2; i++) { console.log(i); }세미콜론 자동 삽입세미콜론(;)은 문의 종료를 나타낸다. 즉 자바슼립트 엔진은 세미콜론으로 문이 종료한 위치를 파악하고 순차적으로 하나씩 문을 실행한다. 단 괄호로 묶은 코드 블록 뒤에는 세미콜론을 붙이지 않는다. 블록은 문의 종료를 의미하는 자체 종결성을 갖기 때문이다.세미콜론은 생략 가능하다. 이는 자바스크립트 엔진이 소스코드를 해석할 때 문의 끝이라고 예측되는 지점에 세미콜론을 자동으로 붙여주는 세미콜론 자동 삽입 기능(ASI:Automatic Semicolon Insertion)이 암묵적으로 수행되기 때문이다.하지만 개발자의 의도와 일치하지 않는 경우도 생기기 때문에 자바스크립트 커뮤니티에서는 세미콜론의 사용을 권장한다.표현식인 문과 표현식이 아닌 문값으로 평가될 수 있으면 표현식인 문이고, 그렇지 않으면 표현식이 아닌 문이다.// 표현식이 아닌 문var x;// 표현식인 문x = 1 + 2;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-06T21:01:35+09:00'>06 Feb 2022</time><a class='article__image' href='/javascript-series3'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part3]: Javascript 표현식과 문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series3'>Javascript Series [Part3]: Javascript 표현식과 문</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part4]: 플링크의 아키텍처",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series4",
      "date"     : "Feb 6, 2022",
      "content"  : "Table of Contents  Flink 생태계 구성  Flink 런타임 아키텍처          Job Manager      Task Manager      Tasks and Operator Chains      Task Slots and Resources        Task Scheduling And Network Shuffle  Flink API  참고Flink 생태계 구성Flink 런타임 아키텍처플링크 런타임은 두 종류의 프로세스로 구성됩니다. 하나는 Job Manager이고 다른 하나는 Task Manger로 Task Manager는 한 개 이상으로 구성할 수 있습니다.플링크 클라이언트는 런타임에 포함되지는 않지만 데이터플로우를 Job Manager로 보내는 역할을 합니다. 이 후 클라이언트는 연결을 끊을 수도 있고 또는 attached mode를 통해 진행 경과를 보고 받을 수도 있습니다. 클라이언트는 자바 위에서 클라이언트 명령어로 실행됩니다.Job Manager잡 매니저는 어플리케이션의 실행을 제어하는 마스터 프로세스입니다. 클라이언트는 잡 매니저로 어플리케이션을 제출합니다. 어플리케이션은 데이터플로우 그래프(또는 잡그래프)와 필요한 클래스, 라이브러리 등을 포함하는 JAR파일로 구성되어 있습니다.잡 매니저는 잡그래프를 실행그래프(ExecutionGraph)라고 불리는 물리적인 그래프로 변환합니다. 잡 매니저는 태스크 매니저내에 사용 가능한 태스크 슬롯에 잡그래프를 태스크 형태로 배포합니다.또한 잡 매니저는 완료된 태스크, 실행 실패, 장애 복구, 체크포인트 조율 등 중앙에서 제어해야 하는 모든 동작에 책임을 집니다.Task Manager태스크 매니저(워커)는 데이터플로우의 태스크들을 실행합니다. 태스크 매니저에 할당하는 가장 작은 작업 단위를 태스크 슬롯이라고 합니다. 태스크 매니저 안에 있는 슬롯의 개수는 동시에 처리되는 작업의 개수를 나타냅니다. 하나의 슬롯안에서 여러개의 연산자가 실행될 수도 있습니다.Tasks and Operator Chains플링크의 태스크는 연산자를 체이닝한 서브태스크의 집합으로 이루어져 있습니다. 각각의 서브태스크는 하나의 스레드에서 실행됩니다. 이렇게 연산자들을 체이닝 함으로써 플링크는 스레드간 핸드오버와 버퍼링으로 인한 오버헤드를 줄입니다. 이는 전체적인 처리량 증가와 지연율 감소를 가능하게 합니다.Task Slots and Resources태스크 매니저는 각각 하나의 JVM 프로세스입니다. 그리고 태스크 매니저는 하나 이상의 서브태스크를 스레드로 분리할 수 있습니다.각각의 태스크 슬롯은 각각의 분리된 자원으로 가장 작은 작업 단위 입니다. 태스크 슬롯으로 분리된 작업들은 자원을 위해 서로 경쟁할 일이 없습니다. 태스크 슬롯에서 분리하는 자원은 메모리 뿐입니다. CPU는 분리되지 않습니다.태스크 매니저는 각각 하나의 JVM 프로세스이기 때문에 태스크 매니저가 여러개의 태스크 슬롯을 가진다면 여러개의 태스크가 하나의 JVM 프로세스 위에서 실행된다는 의미입니다. 하나의 JVM 위에서 실행되는 태스크들은 TCP 연결을 통해 서로 하트비트 메세지를 주고 받습니다. 또한 태스크간 데이터셋을 공유함으로써 태스크별로 발생하는 오버헤드를 줄여줍니다.플링크는 디폴트로 작업의 전체 파이프라인을 하나의 슬롯에 할당합니다.Task Scheduling And Network ShuffleFlink jobs consist of different operations that are connected together in a dataflow graph. The system decides how to schedule the execution of these operations on different processes/machines (TaskManagers) and how data is shuffled (sent) between them.Multiple operations/operators can be chained together using a feature called chaining. A group of one or multiple (chained) operators that Flink considers as a unit of scheduling is called a task. Often the term subtask is used to refer to the individual instances of tasks that are running in parallel on multiple TaskManagers but we will only use the term task here.Task scheduling and network shuffles work differently for BATCH and STREAMING execution mode. Mostly due to the fact that we know our input data is bounded in BATCH execution mode, which allows Flink to use more efficient data structures and algorithms.We will use this example to explain the differences in task scheduling and network transfer:Operations that imply a 1-to-1 connection pattern between operations, such as map(), flatMap(), or filter() can just forward data straight to the next operation, which allows these operations to be chained together. This means that Flink would not normally insert a network shuffle between them.Operation such as keyBy() or rebalance() on the other hand require data to be shuffled between different parallel instances of tasks. This induces a network shuffle.StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStreamSource&amp;lt;String&amp;gt; source = env.fromElements(...);source.name(&quot;source&quot;)	.map(...).name(&quot;map1&quot;)	.map(...).name(&quot;map2&quot;)	.rebalance()	.map(...).name(&quot;map3&quot;)	.map(...).name(&quot;map4&quot;)	.keyBy((value) -&amp;gt; value)	.map(...).name(&quot;map5&quot;)	.map(...).name(&quot;map6&quot;)	.sinkTo(...).name(&quot;sink&quot;);Task1: source, map1, and map2Task2: map3, map4Task3: map5, map6, and sinkAnd we have a network shuffle between Tasks 1 and 2, and also Tasks 2 and 3. This is a visual representation of that job:Flink API      The lowest level abstraction simply offers stateful and timely stream processing. It is embedded into the DataStream API via the Process Function. It allows users to freely process events from one or more streams, and provides consistent, fault tolerant state. In addition, users can register event time and processing time callbacks, allowing programs to realize sophisticated computations.        In practice, many applications do not need the low-level abstractions described above, and can instead program against the Core APIs: the DataStream API (bounded/unbounded streams) and the DataSet API (bounded data sets). These fluent APIs offer the common building blocks for data processing, like various forms of user-specified transformations, joins, aggregations, windows, state, etc. Data types processed in these APIs are represented as classes in the respective programming languages.    The low level Process Function integrates with the DataStream API, making it possible to use the lower-level abstraction on an as-needed basis. The DataSet API offers additional primitives on bounded data sets, like loops/iterations.        The Table API is a declarative DSL centered around tables, which may be dynamically changing tables (when representing streams). The Table API follows the (extended) relational model: Tables have a schema attached (similar to tables in relational databases) and the API offers comparable operations, such as select, project, join, group-by, aggregate, etc. Table API programs declaratively define what logical operation should be done rather than specifying exactly how the code for the operation looks. Though the Table API is extensible by various types of user-defined functions, it is less expressive than the Core APIs, and more concise to use (less code to write). In addition, Table API programs also go through an optimizer that applies optimization rules before execution.    One can seamlessly convert between tables and DataStream/DataSet, allowing programs to mix the Table API with the DataStream and DataSet APIs.        The highest level abstraction offered by Flink is SQL. This abstraction is similar to the Table API both in semantics and expressiveness, but represents programs as SQL query expressions. The SQL abstraction closely interacts with the Table API, and SQL queries can be executed over tables defined in the Table API.  참고  Flink and Kafka Streams: a Comparison and Guideline for Users  Kartik Khare, Here’s How Apache Flink Stores Your State data  stackoverflow: Storage in Apache Flink",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-06T21:01:35+09:00'>06 Feb 2022</time><a class='article__image' href='/flink-series4'> <img src='/images/flink_logo.png' alt='Flink Series [Part4]: 플링크의 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series4'>Flink Series [Part4]: 플링크의 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part2]: Javascript 변수",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  변수란 무엇인가? 왜 필요한가?  변수 선언  변수 호이스팅  값의 할당  변수 네이밍  참고변수란 무엇인가? 왜 필요한가?사람은 계산과 기억을 모두 두뇌에서 하지만, 컴퓨터는 연산과 기억을 수행하는 부품이 나눠져 있다. 컴퓨터는 CPU를 사용해 연산하고, 메모리를 사용해 데이터를 기억한다.메모리는 데이터를 저장할 수 있는 메모리 셀의 집합체다. 메모리 셀 하나의 크기는 1바이트이다. 각 셀은 고유의 메모리 주소를 갖는다. 모든 값은 메모리 상의 임의의 위치에 저장되고 CPU는 이 값을 읽어들여 연산을 수행한다. 연산 결과로 생성된 값도 메모리 상의 임의의 위치에 저장된다. 하지만 문제는 이 값을 재사용하기 위해서는 어디 저장되어야 하는지 알아야 하는데 모른다는 것이다. 설령 안다고 하더라도 개발자가 직접적으로 메모리에 접근하는 것은 위험하다. 그래서 자바스크립트는 직접적인 메모리 제어를 허용하지 않는다.프로그래밍 언어는 기억하고 싶은 값을 메모리에 저장하고, 저장된 값을 읽어 들여 재사용하기 위해 변수라는 메커니즘을 제공한다.변수는 하나의 값을 저장하기 위해 확보한 메모리 공간을 식별하기 위해 붙인 이름을 말한다.변수는 컴파일러 또는 인터프리터에 의해 메모리 공간의 주소로 치환되어 실행된다. 따라서 개발자가 직접 메모리 주소를 통해 값을 저장하고 참조할 필요가 없고 변수를 통해 안전하게 값에 접근할 수 있다.변수 선언변수 선언이란 변수를 생성하는 것을 말한다. 좀 더 자세히 말하면 메모리에는 값을 저장하기 위한 공간을 확보하고, 변수 이름을 메모리 공간의 주소로 연결(name binding)하는 것을 말한다.변수를 사용하려면 반드시 선언이 필요하다. 변수를 선언할 때는 var, let, const 키워드를 사용한다. ES6에서 let, const 키워드가 도입되기 전까지는 var 키워드가 변수 선언을 위한 유일한 키워드였다.🦄 **var 키워드의 단점**  var 키워드의 가장 대표적인 단점은 블록 레벨 스코프를 지원하지 않고, 함수 레벨 스코프만 지원한다는 것이다.  자바스크립트 엔진은 변수 선언을 다음과 같은 2단계에 걸쳐 수행한다.  선언 단계: 변수 이름을 등록해서 자바스크립트 엔진에 변수의 존재를 알린다.  초기화 단계: 값을 저장하기 위한 메모리 공간을 확보하고 암묵적으로 undefined를 할당해 초기화한다.이렇게 초기화 단계를 거침으로써 이전에 다른 애플리케이션이 사용했던 값(garbage value)이 재사용되는 것을 방지해줄 수 있다.변수 호이스팅console.log(score);var score;위의 코드를 보면 변수 선언문보다 변수를 참조하는 코드가 앞에 있다. 그럼에도 결과는 에러가 아닌 undefined다.자바스크립트에서는 변수 선언이 런타임(소스코드가 실행되는 시점)이 아니라 그 이전 단계에서 먼저 실행된다.자바스크립트 엔진은 소스코드를 실행하기에 앞서 먼저 소스코드의 평가 과정을 거치면서 소스코드를 실행하기 위한 준비를 한다. 이 과정에서 엔진은 변수 선언을 포함한 모든 선언문을 소스코드에서 찾아서 먼저 실행한다. 그리고 이 과정이 끝나면 비로소 모든 선언문을 제외한 나머지 코드를 한 줄씩 순차적으로 실행한다.이처럼 모든 선언문(var, let, const, function, function*, class이 코드의 선두로 끌어 올려진 것처럼 동작하는 자바스크립트의 특징을 변수 호이스팅이라 한다.값의 할당변수에 값을 할당할 때는 연산자 =를 사용한다.var score; // 변수 선언score = 80; // 값의 할당var score = 80; // 변수 선언과 값의 할당자바스크립트 엔진은 변수 선언과 값의 할당을 하나의 문으로 단축 표현해도 2개의 문으로 나누어 각각 실행한다. 이 때 주의할 점은 변수 선언과 값의 할당의 실행 시점이 다르다는 것이다. 변수 선언은 값의 할당이 일어나는 런타임 이전에 이루어진다.console.log(score); // undefinedvar score = 80;console.log(score); // 80또한 변수의 선언과 값의 할당을 하나의 문으로 단축 표현해도 자바스크립트 엔진은 나누어 실행하므로, 변수에 먼저 undefined가 할당되어 초기화되는 것은 변함이 없다. 그리고 값이 할당될 때에는 새로운 메모리 공간을 확보하고 그 곳에 할당 값을 저장한다.이렇게 되고나면 undefined는 가비지 컬렉션에 의해 메모리에서 자동 해제된다.변수 네이밍자바스크립트는 변수 네이밍을 할 때에 카멜 케이스와 파스칼 케이스를 권장한다.참고  name-binding",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/javascript-series2'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part2]: Javascript 변수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series2'>Javascript Series [Part2]: Javascript 변수</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Hadoop Series [Part5]: Hadoop ZooKeeper",
      "category" : "",
      "tags"     : "Hadoop",
      "url"      : "/hadoop-series5",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/hadoop-series5'> <img src='/images/hadoop_logo.png' alt='Hadoop Series [Part5]: Hadoop ZooKeeper'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hadoop-series5'>Hadoop Series [Part5]: Hadoop ZooKeeper</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Hadoop Series [Part4]: Hadoop Hive",
      "category" : "",
      "tags"     : "Hadoop",
      "url"      : "/hadoop-series4",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  하이브 소개  하이브 특징  하이브 설치  하이브 원격 하둡 클러스터와 연결  하이브 실행  전통적인 DB와 비교  참고하이브 소개하이브(Hive)는 하둡 기반의 데이터 웨어하우징 프레임워크로, SQL 기술을 가진 분석가가 HDFS에 저장된 대량의 데이터를 분석할 수 있도록 개발되었다. 물론 모든 빅데이터 문제에 SQL이 이상적인 것은 아니다. 예를 들어 복잡한 머신러닝 알고리즘을 구현하기에는 적합하지 않다.하이브 특징  HDFS에 저장된 데이터를 SQL과 유사한 언어로 다루고 싶은 니즈  SQL like한 분석 지원 (HQL)  데이터 분석을 위한 쿼리 엔진 제공  HQL로 작성한 쿼리를 내부적으로 MapReduce 형태로 변환  유사한 기술로 Spark가 있음  Spark는 메모리 기반 소프트웨어이기 때문에 빠르다. 하지만 OOM(Out Of Memory) 자주 발생  Hive는 디스크 기반 소프트웨어이기 때문에 느리지만, 대용량 데이터를 OOM 걱정 없이 다룰 수 있음하이브 설치일반적으로 하이브는 사용자의 워크스테이션에서 실행되고(사용자 워크스테이션과 하둡 클러스터는 네트워크로 연결되어 있어야 함), 작성된 SQL 쿼리는 일련의 맵리듀스 잡으로 변환되어 하둡 클러스터에서 구동된다. 하이브는 HDFS에 저장된 데이터에 스키마를 입히는 방식으로 데이터를 테이블로 구조화시킨다. 테이블 스키마와 같은 메타데이터는 메타스토어라 불리는 데이터베이스에 저장된다.하이브 원격 하둡 클러스터와 연결fs.defaultFS 값이 하이브와 연결하고자 하는 파일시스템. HDFS인 경우 hdfs://, 그냥 운영체제에서 제공하는 파일시스템을 사용할 경우 file://로 시작해 호스트명과 포트명을 명시해주면 된다.다음과 같은 두 가지 방법이 있다.  하이브의 conf 디렉터리에 있는 hive-site.xml 파일에서 fs.defaultFS 값을 설정  하이브 쉘을 실행할 때 hive --hiveconf fs.defaultFS=hdfs://&amp;lt;REMOTE&amp;gt;:8020 이런식으로 설정하이브 실행하이브를 실행하는 가장 보편적인 방법은 하이브 쉘을 이용하는 것이다. 쉘을 이용할 때에도 다음과 같은 3가지 방식을 지원한다.# 대화식 모드hivehive&amp;gt; SHOW TABLES;# 비대화식 모드 - 스크립트 전달hive -f script.q# 비대화식 모드 - 명령줄에 직접 스크립트 작성hive -e &#39;SELECT * FROM dummy&#39;전통적인 DB와 비교  하이브는 HDFS와 맵리듀스를 기반으로 개발되었음  대부분의 DB는 로드 시점에 스키마 검증 -&amp;gt; 쓰기 스키마, 하이브는 쿼리를 실행할 때 스키마 검증 -&amp;gt; 읽기 스키마  쓰기 스키마는 DB가 컬럼 단위의 데이터 색인과 압축을 제공하기 때문에 쿼리가 더 빠름  읽기 스키마는 쿼리가 아직 정해지지 않아 스키마, 인덱스를 정의하기 어렵고 우선 데이터를 로드하고 싶을 때  하이브는 테이블과 파티션 수준의 잠금을 지원참고  ETL 성능 향상을 위한 몇 가지 팁들",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/hadoop-series4'> <img src='/images/hadoop_logo.png' alt='Hadoop Series [Part4]: Hadoop Hive'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hadoop-series4'>Hadoop Series [Part4]: Hadoop Hive</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Hadoop Series [Part3]: Hadoop MapReduce",
      "category" : "",
      "tags"     : "Hadoop",
      "url"      : "/hadoop-series3",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/hadoop-series3'> <img src='/images/hadoop_logo.png' alt='Hadoop Series [Part3]: Hadoop MapReduce'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hadoop-series3'>Hadoop Series [Part3]: Hadoop MapReduce</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Hadoop Series [Part2]: Hadoop HDFS",
      "category" : "",
      "tags"     : "Hadoop",
      "url"      : "/hadoop-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  Hadoop  HDFS          Local File System vs HDFS      구글 플랫폼의 철학      하둡 특성      하둡 클러스터      하둡 블록      네임노드      데이터노드      보조 네임 노드      데이터 읽기 과정      데이터 쓰기 과정      HDFS 커맨드        참고Hadoop  빅데이터 처리를 위한 다양한 소프트웨어 제공  하둡은 개발자 더크 커팅이 처음으로 시작  구글이 발표한 분산 파일 저장을 위한 논문 GFS, 분산 데이터 처리를 위한 MapReduce 논문을 발표  더그 커팅이 구글에서 발표한 논문을 구현해 하둡 프로젝트가 시작됨 -&amp;gt; HDFS와 MapReduce가 하둡 프로젝트에 포함  이 후 빅데이터 처리를 위한 다양한 소프트웨어 Hive, HBase, Cassandra, Yarn 등이 등장하여 하둡 생태계 구성HDFS데이터가 단일 물리 머신의 저장 용량을 초과하게 되면 전체 데이터셋을 분리된 여러 머신에 나눠서 저장할 필요가 있다. 네트워크로 연결된 여러 머신의 스토리지를 관리하는 파일시스템을 분산 파일시스템이라고 한다.하둡은 HDFS(Hadoop Distributed File System)라는 분산 파일시스템을 제공한다. HDFS는 범용 하드웨어로 구성된 클러스터에서 실행되고 스트리밍 방식의 데이터 접근 패턴으로 대용량 파일을 다룰 수 있도록 설계된 파일시스템이다.Local File System vs HDFS  Local File System          In an operating system, file system is the strategy that is used to keep track of files on a disk      The basic file system of Linux operating system is termed as Local file system. It stores any data file as it is in single copy.      It stores data files in Tree format. Here, any user can access data files directly. LFS does not Replicate the data blocks. It always used for storing and processing personal data(small data)      The block size is 4 KB both in Windows and Unix local file systems      multiple disk-seeks in local file system due to its 4KB block size      physically a single unit        HDFS          HDFS will be deployed on top of the existing Operating system to bring its own file system method      the block size in Hadoop HDFS is 64 MB in the initial version and in later versions, it is 128 MB which is configurable      HDFS maintains higher block allocation, the data will be read sequentially after every individual seek -&amp;gt; Data retrieval in DFS is fast      Large files will be split into multiple chunks automatically, distributed and stored across various slave machines (aka. nodes) in HDFS      logically a single unit      구글 플랫폼의 철학  한대의 고가 장비보다 여러 대의 저가 장비가 낫다  데이터는 분산 저장한다  시스템(H/W)은 언제든 죽을 수 있다  시스템 확장이 쉬워야 한다하둡 특성  구글 플랫폼의 철학이 녹아든 빅데이터 처리 소프트웨어  수천대 이상의 서버를 하나의 클러스터로 사용 (Scalable)  범용 하드웨어 클러스터 (Support for Heterogeneous Cluster)  복제를 통한 신뢰성 보장 (Fault Tolerant)  어떠한 대용량 파일도 분산 저장함으로써 적재 가능 (Built for Large Dataset)  데이터 처리의 지역성 보장하둡 클러스터  Name Node: HDFS에 대한 마스터 데몬  Data Node: HDFS에 대한 슬레이브 데몬  Job Tracker: 어플리케이션 관리를 위한 마스터 데몬  Task Tracker: 작업 관리를 위한 슬레이브 데몬하둡 블록  하나의 파일을 여러 개의 Block으로 저장  블럭 크기는 한 번에 읽고 쓸 수 있는 데이터의 크기  블럭의 기본 크기는 128MB. 설정을 통해 변경 가능  128MB와 같이 블록을 크게 잡는 이유는 탐색 비용을 최소화하기 위해(Minimizing seek operation: 시작점 찾는 비용 최소화)  데이터를 조각내고 복제하여 여러 서버에 분산 저장  복제 저장하는 것이 용량 낭비일 수 있지만 이를 통해 얻게 되는 이점이 더 큼  만약 서버 1대에 장애가 발생하면 하트비트가 끊기게 되고, 장애난 서버가 가지고 있던 블록과 같은 데이터를 가지는 서버가 다른 서버와 통신 후 데이터를 복사함으로써 복제 수 유지  블록의 지역성: 데이터를 처리할 때 해당 데이터를 가지고 있는 노드한테 잡을 먼저 할당함  블록 캐싱: 자주 읽는 블록을 데이터 노드의 메모리에 캐싱함으로써 읽기 성능 향상네임노드  네임노드의 관리 대상: 파일시스템 트리, 트리에 포함된 모든 파일과 디렉터리에 대한 메타데이터  이 정보는 네임스페이스 이미지와 에디트 로그라는 두 종류의 파일로 로컬 디스크에 영속적으로 저장됨  데이터노드로부터 블럭 리포트를 받음  데이터 복제 유지를 위한 커맨더 역할을 수행  파일시스템 이미지(fsimage) 관리 (스냅샷)  파일시스템에 대한 Edit Log 관리  네임노드가 없으면 파일시스템은 동작하지 않음          네임노드 장애 복구 방법                  메타데이터를 지속적인 상태로 보존하기 위해 파일로 백업          보조 네임노드 운영: 에디트 로그가 너무 커지지 않도록 주기적으로 이미지와 에디트 로그를 병합해 새로운 이미지를 만듬                    데이터노드  로컬 파일 시스템에 HDFS 데이터 저장  주기적으로 로컬 파일 시스템에 있는 모든 HDFS 블록들을 검사 후 정상적인 블록의 목록을 만들어 네임노드에 전송  데이터 복제시 클라이언트가 네임노드로부터 데이터노드 리스트를 전달 받음  네트워크 트래픽은 클라이언트와 데이터노드 간에만 발생보조 네임 노드  네임노드(NN)와 보조 네임노드(SNN)는 Active/Standby 구조 아님 (각자 역할이 따로 있음)  체크 포인트          fsimage 와 edits 파일을 주기적으로 병합                  1시간 주기로 실행          edits 로그가 일정 사이즈 이상이면 실행                      한계점          네임노드의 SPOF 문제 해결 x      보조 네임노드의 장애 상황 감지 툴 없음      fsimage와 edits 파일  파일 시스템에 관한 스냅샷  스냅샷 이후에 발생된 데이터에 관한 변경사항은 edits 파일에 기록됨  처음 네임노드 데몬이 실행되면 fsimage 스냅샷을 이용해 hdfs 구동  이후 발생되는 변경사항은 fsimage와 병합. 이러한 병합은 보조 네임 노드에서 실행  병합된 fsimage를 주기적으로 네임노드로 가져와서 fsimage를 최신화데이터 읽기 과정1. 클라이언트가 HDFS의 FileSystem 객체의 open() 메서드를 호출해 원하는 파일을 연다2. HDFS는 해당 파일의 첫 번째 블록 위치를 파악하기 위해 RPC를 사용하여 네임노드를 호출한다3. 네임노드는 해당 블록을 가지는 데이터 노드의 주소를 가까운 순으로 정렬하여 리스트로 반환한다4. HDFS는 클라이언트가 데이터를 읽을 수 있도록 파일 탐색을 지원하는 입력 스트림(FSDataInputStream)을 반환한다5. 클라이언트는 스트림을 읽기 위해 read() 메서드를 호출한다6. read() 메서드를 반복적으로 호출하면 데이터 노드에서 클라이언트로 모든 데이터가 전송된다7. 블록의 끝에 도달하면 해당 데이터 노드의 연결을 닫고 다음 블록의 데이터 노드를 찾는다8. 2-7번이 파일의 모든 블록을 읽을 때까지 반복된다9. 클라이언트는 FSDataInputStream의 close() 메서드를 호출한다데이터 쓰기 과정1. 클라이언트는 HDFS의 FileSystem의 create() 메서드를 호출하여 파일을 생성한다2. HDFS는 파일시스템의 네임스페이스에 새로운 파일을 생성하기 위해 네임노드에 RPC 요청을 보낸다3. 네임노드는 요청한 파일과 동일한 파일이 있는지, 클라이언트에 권한이 있는지 등을 검사한다4. 검사가 통과하면 HDFS는 클라이언트에 데이터를 쓸 수 있도록 FSDataOutputStream을 반환한다5. 클라이언트가 데이터를 쓸 때 네임 노드로부터 데이터 노드 리스트를 전달 받는다6. 데이터 노드 사이에 파이프라인이 형성되고 데이터가 복제 수준만큼 복사된다7. 이 때 네트워크 트래픽은 클라이언트와 데이터 노드 간에만 발생한다HDFS 커맨드version, mkdir, ls, put, put, copyFromLocal, copyToLocal, cat, mv, cpmoveFromLocal, moveToLocal, tail, rm, chwon, chgrp, setrep, du, dftouchz, chmod, test, text, count, find, getmergehadoop versionhadoop fs -ls# hadoop fs -copyFromLocal &amp;lt;local source&amp;gt; &amp;lt;hdfs destination&amp;gt;hadoop fs -copyFromLocal ~/test1 /dir1/copytest# hadoop fs -cat &amp;lt;path_to_file_in_hdfs&amp;gt;hadoop fs -cat /dir1/copytest# hadoop fs -get &amp;lt;src&amp;gt; &amp;lt;local destination&amp;gt;hadoop fs -get /testfile ~/copyfromhadoop# hadoop fs -copyToLocal &amp;lt;hdfs source&amp;gt; &amp;lt;local destination&amp;gt;hadoop fs -copyToLocal /dir1/sample ~/copysample# hadoop fs -mv &amp;lt;source&amp;gt; &amp;lt;destination&amp;gt;hadoop fs -mv /dir /dir1# hadoop fs -tail [-f] &amp;lt;file&amp;gt;# hadoop fs -rm &amp;lt;path&amp;gt;# hadoop fs -chown [-R] [owner] [:[group]] &amp;lt;path&amp;gt;hadoop fs -chown newdir /sample# hadoop fs -setrep &amp;lt;replication num&amp;gt; &amp;lt;path&amp;gt;hadoop fs -setrep 2 /dir1# hadoop fs -du -s &amp;lt;file&amp;gt;참고  [토크ON세미나] 아파치 하둡  Simplilearn, What Is HDFS?  Difference between Local File System (LFS) and Distributed File System (DFS)  Hoing: 하둡 프로그래밍(2) – 빅데이터 - 하둡 에코시스템",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/hadoop-series2'> <img src='/images/hadoop_logo.png' alt='Hadoop Series [Part2]: Hadoop HDFS'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hadoop-series2'>Hadoop Series [Part2]: Hadoop HDFS</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Hadoop Series [Part1]: Hadoop Intro",
      "category" : "",
      "tags"     : "Hadoop",
      "url"      : "/hadoop-series1",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/hadoop-series1'> <img src='/images/hadoop_logo.png' alt='Hadoop Series [Part1]: Hadoop Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hadoop-series1'>Hadoop Series [Part1]: Hadoop Intro</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series3",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  State          Operator State      Keyed State        Recovery          Checkpoint      Savepoint      Exactly Once        StateBackend  참고State대부분의 어플리케이션은 상태를 저장하고 활용(stateful)합니다. 만약 각각의 이벤트를 단순히 변환(transformation)하는 용도의 어플리케이션이라면 상태(state)가 필요하지 않을 수도 있습니다. 하지만 단순한 비즈니스 로직조차도 이전의 이벤트나 중간 결과를 사용하는 경우가 대부분이기 때문에 상태를 저장해 활용하는 것은 대부분의 서비스에 중요한 요소입니다.스트림 처리 어플리케이션의 경우 상태(state)의 중요성이 더욱 커지게 됩니다. 스트림 처리 어플리케이션에서 상태가 중요한 이유는 다음과 같습니다.  Low Latency: 스트림 처리는 짧은 지연을 위해 값을 출력 후 갱신을 통해 정확도를 높이는 방식이기에 이전 값을 기억해야함  Recovery: 장애 발생시 저장된 상태를 통해 다시 복구가 가능플링크에서는 상태를 프로그래밍 모델의 일급 시민(first-class citizen)으로 사용하고 있습니다. 플링크에서 상태 핸들링에 관한 주요 특징들에는 다음과 같은 것들이 있습니다.  Multiple State Primitives: Flink provides state primitives for different data structures, such as atomic values, lists, or maps. Developers can choose the state primitive that is most efficient based on the access pattern of the function.  Pluggable State Backends: Application state is managed in and checkpointed by a pluggable state backend. Flink features different state backends that store state in memory or in RocksDB, an efficient embedded on-disk data store. Custom state backends can be plugged in as well.  Exactly-once state consistency: Flink’s checkpointing and recovery algorithms guarantee the consistency of application state in case of a failure. Hence, failures are transparently handled and do not affect the correctness of an application.  Very Large State: Flink is able to maintain application state of several terabytes in size due to its asynchronous and incremental checkpoint algorithm.  Scalable Applications: Flink supports scaling of stateful applications by redistributing the state to more or fewer workers.플링크에서 상태는 단순히 변수에 저장되는 하나의 값일 수도 있고, 파일이나 데이터베이스에 저장되는 데이터일 수도 있습니다. 이런 상태의 다양성을 반영하기 위해 플링크에서는 크게 두 가지 종류의 상태를 제공하며 또 각각의 상태별로 표현할 수 있는 기본 상태(State primitive)도 여러가지 입니다.Operator State연산자 상태(Operator State)는 태스크별로 자신의 상태를 저장하고 있습니다. 카프카에서 컨슈머 그룹 내 컨슈머별로 토픽의 파티션을 공유하지 않고 자신의 파티션을 가지는 것과 비슷한 용도입니다.대부분의 플링크 어플리케이션에서는 연산자 상태가 필요하지 않습니다. 연산자 상태는 대부분 소스나 싱크쪽에서 필요로 하는 상태이기 때문입니다. 이러한 이유로 파이선의 DataStream API에서는 아직 연산자 상태를 지원하지 않습니다.Keyed State키 상태(Keyed State)는 스트림 데이터가 가지는 키별로 접근할 수 있는 상태를 말합니다. 예를 들어 태스크로 들어온 데이터가 ‘Lion’이라는 키를 가지고 있으면 ‘Lion’키와 관련된 상태에만 접근할 수 있습니다. 따라서 스트림의 형태는 기본적으로 KeyedStream이어야 합니다.KeyedState로 사용할 수 있는 기본 상태의 종류는 다음과 같습니다.  ValueState&amp;lt;T&amp;gt; : 키별로 하나의 값만을 저장합니다(ex. 키별 최대값)  ListState&amp;lt;T&amp;gt; : 키별로 리스트를 저장합니다(ex. 키별 회원 이름)  MapState&amp;lt;UK, UV&amp;gt; : 키별로 키-값 쌍을 저장합니다. (ex. 키별 회원 이름과 직업 쌍)Recovery플링크는 중간 결과를 저장하기 위해 상태를 로컬 버퍼에 저장하게 됩니다. 이를 통해 플링크는 짧은 지연으로 결과를 계속 갱신할 수 있습니다. 하지만 로컬 버퍼는 태스크매니저 프로세스가 종료되면 같이 사라지는 휘발성 저장소입니다. 이는 장애 발생시 실행중이던 태스크가 종료될 뿐 아니라 저장해두었던 상태도 함께 사라진다는 뜻입니다. 이를 위해 플링크에서는 상태를 주기적으로 체크포인팅해 원격의 영구저장소로 저장합니다.Checkpoint스트림 처리 어플리케이션은 장비, 네트워크 등 예상치 못한 장애로 상태가 유실되거나 일관성을 잃게될 수 있습니다. 이를 위해 플링크는 주기적으로 원격 영구 저장소로 상태를 체크포인팅해야 합니다.체크포인트의 naive한 알고리즘은 다음과 같습니다.1. 입력 데이터의 인입을 멈춘다2. 어플리케이션에 남아있는 데이터를 처리한다3. 각 태스크의 상태를 복사해 원격 영구저장소로 체크포인팅한다4. 입력 데이터를 다시 받는다플링크에서는 위와같은 naive한 알고리즘을 사용하지는 않습니다. 왜냐하면 위와 같이 체크포인팅을 할 경우 일명 ‘Stop the world’와 같이 체크포인팅을 하는 동안 어플리케이션 전체가 멈춰버리는 현상이 발생하기 때문에 짧은 지연을 요구하는 스트림 처리 어플리케이션에는 적합하지 않습니다.따라서 플링크에서는 전체를 정지하지 않고 체크포인트와 처리간의 결합을 분리하였습니다. 다시 말해 일부 태스크는 상태를 저장하고, 일부 태스크는 데이터를 계속 처리하게 됩니다. 이를 챈디-램포트 알고리즘 기반의 분산 스냅샷 체크포인팅이라고 합니다.플링크의 분산 스냅샷의 핵심 요소는 체크포인트 배리어(barrier)입니다. 배리어는 이전 포스트에서 봤던 워터마크처럼 특별한 용도의 레코드입니다. 레코드이기 때문에 데이터 스트림에 주입되고 데이터 스트림의 일부로 레코드와 함께 흐릅니다.배리어는 데이터 스트림에 주입되어 스트림에 흐르고 있는 레코드들이 어떤 체크포인트에 속하는지 식별하도록 해주는 식별자 역할을합니다. 따라서 배리어 이전에 처리된 레코드가 만든 상태 변경은 배리어의 현재 체크포인트에 포함됩니다.1. 잡 매니저가 모든 소스 태스크에 체크포인트 배리어를 주입한다2. 태스크가 배리어를 수신하면 모든 입력 스트림에서 배리어가 도착할 때 까지 기다린다3. 기다리는 동안 배리어를 전송한 스트림에서 들어온 레코드는 버퍼링 해놓는다4. 기다리는 동안 배리어를 전송하지 않은 스트림에서 들어온 데이터를 계속 처리한다5. 모든 입력 스트림으로부터 배리어가 도착하면 현재 태스크 상태를 백엔드에 체크포인트한다6. 이후 태스크에 연결된 모든 하위 병렬 태스크에 배리어를 브로드캐스팅한다7. 이후 태스크에 버퍼링 되어 있던 레코드를 처리하고 입력 스트림은 체크포인트 시점 위치에서 다시 시작된다8. 이렇게 소스에서부터 시작된 모든 배리어가 싱크 태스크에 도착하면 잡 매니저는 모든 태스크에서 체크포인트가 완료됐음을 기록한다9. 완료된 체크포인트는 장애 복구에 사용된다플링크의 체크포인트 수행은 모두 상태백엔드가 책임지고 있습니다. 상태백엔드의 한종류인 RocksDB는 비동기 체크포인팅을 지원합니다. 따라서 체크포인팅이 시작되면 RocksDB는 상태를 로컬에 복사하고 체크포인팅이 완료되면 이를 태스크 매니저에 알리고 태스크매니저는 이를 잡매니저에 알리며 하던 작업을 별도로 계속 수행하게 됩니다.Savepoint플링크의 세이브 포인트는 기존 데이터베이스 시스템의 복구 로그와 유사한 방식으로 체크 포인트와 다릅니다.체크포인트의 주요 목적은 예기치 않은 작업 실패 시 복구 메커니즘을 제공하는 것입니다. 체크포인트의 라이프사이클은 플링크에 의해 관리됩니다. 즉, 체크포인트는 사용자의 개입 없이 플링크에 의해 생성, 소유 및 해제됩니다.세이브포인트는 체크포인트와 동일한 메커니즘으로 내부적으로 생성되지만 개념적으로 다르기 때문에 생성 및 복원 비용이 다소 비쌀 수 있습니다. 세이브포인트는 주로 Flink 버전 업데이트, 작업 그래프 변경 등에 사용됩니다.세이브포인트는 사용자만 생성, 소유 및 삭제합니다. 즉, Flink는 작업 종료 후나 복원 후에도 저장 지점을 삭제하지 않습니다아래 표는 세이브포인트와 체크포인트를 비교해 놓은 것입니다.Exactly Once플링크에서 정확히 한 번 보장은 스트림 처리에서 상태의 일관성을 의미합니다. 결과를 보장하는 방법 중 가장 엄격한 방식으로 자주 비교되는 보장 방식에는 At Least Once 방식이 있습니다. 정확히 한 번 보장은 스트리밍 시스템이 배치 시스템을 뛰어넘기 위해 반드시 요구되는 조건 중에 하나입니다.At Least Once  데이터 유실 방지에 최우선  결과의 완결만 중요하고 중복은 수용 가능한 조건인 경우(ex. 최대값 구하는 연산)  소스나 어떤 버퍼에서 이벤트를 재생(re-play) 기능을 가지고 있어야함Exactly Once  가장 엄격한 보장 방식  유실 방지 뿐 아니라, 이벤트마다 정확히 한 번씩만 상태를 갱신해야함  At Least Once의 데이터 재생기능은 필수적으로 내포해야함  내부 상태 일관성을 보장하기 위해 트랜잭션 또는 스냅샷 메커니즘을 사용플링크에서는 위에서 설명했듯이 챈디-램포트 기반의 분산 스냅샷 메커니즘을 사용해 내부 상태를 체크포인팅함으로써 일관성을 보장합니다. 여기서 한가지 추가되어야 할 점은 체크포인트 수행 당시 마지막으로 소비했던 위치로 재설정 하여 이벤트를 재생(re-play)할 수 있어야 한다는 것입니다.데이터 소스의 입력 스트림 재생기능은 소스의 구현과 인터페이스에 달려있습니다. 따라서 정확히 한 번 보장을 위해서는 아파치 카프카와 같은 이벤트 로그를 입력 소스로 사용해야 합니다. 카프카는 스트림을 이전 오프셋으로 설정해 과거 레코드를 다시 재생할 수 있도록 구현되어 있습니다.StateBackend상태 백엔드의 역할은 크게 다음과 같습니다.  로컬 상태 관리  원격 저장소에 상태를 체크포인팅플링크에서 제공하는 대표적인 상태 백엔드 두가지는 다음과 같습니다.HashMap StateBackend  Java Heap에 상태를 객체로 저장  해시테이블에 변수와 트리거를 저장  메모리 사용으로 빠른 처리Embedded RocksDB StateBackend  직렬화한 상태 데이터를 로컬 하드 디스크에 저장  디스크와 serialize 사용으로 성능과 처리량간의 트레이드-오프참고  아파치 플링크로 하는 스트림 데이터 처리 책  Flink 공식문서: Stateful Stream Processing  Flink 공식문서: State Backends  Flink 공식문서: What is Apache Flink? — Applications  Flink 공식문서: Working with State  Ververica: 3 important performance factors for stateful functions and operators in Flink  mehmetozanguven: Apache Flink Series 8-State Backend &amp;amp; State Example",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/flink-series3'> <img src='/images/flink_logo.png' alt='Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series3'>Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Container Series [Part2]: 진화하는 컨테이너 표준",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/container-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  도커와 컨테이너 기술  도커 엔진          초기 도커 엔진의 모습      새롭게 바뀐 도커 엔진의 모습                  Client          dockerd          containerd          shim          runc                      참고도커와 컨테이너 기술Docker는 컨테이너 기술을 사용하여 애플리케이션에 필요한 환경을 신속하게 구축하고 테스트 및 배포를 할 수 있게 해주는 플랫폼으로 접근 장벽이 비교적 높았던 컨테이너 기술의 상용화를 앞당겼습니다.컨테이너 기술은 하드웨어와 호스트 OS는 그대로 둔 채 애플리케이션 영역만 캡슐화하여 독립적인 환경을 제공해주는 가상화 방식입니다. 이 덕분에 이전의 가상화 방식에서는 애플리케이션마다 완전한 OS가 필요했지만 컨테이너 기술에서는 이러한 완전한 OS의 불필요한 중복(redundancy)을 제거하였습니다. 이러한 컨테이너 기술 덕분에 CPU, RAM 및 스토리지와 같은 시스템 리소스를 더 많이 확보할 수 있게 되었으며, 라이센스 비용 절감, OS 패치 및 유지보수에 들어가는 오버헤드를 제거할 수 있게 되었습니다.도커 엔진이러한 컨테이너 기술을 실현하기 위해서는 컨테이너를 실행시킬 수 있는 환경을 구축해야 하는데 이를 컨테이너 엔진이라고 하고, 도커에서는 도커 엔진에 해당됩니다.초기 도커 엔진의 모습초기 도커 엔진은 크게 dockerd와 LXC로 이루어져 있었습니다.      dockerd초기 도커 엔진의 dockerd 지금의 dockerd보다 훨씬 많은 역할을 하고 있었습니다. 그 이유는 도커가 처음 등장할 당시 도커 개발자들의 목표는 컨테이너 기술의 대중화였습니다. 그렇기 때문에 최대한 사용성을 간편하게 하고 싶었고 이러한 목적으로 도커 개발자들은 컨테이너 기술을 사용하는데 필요한 많은 기능들을 dockerd에 담아두었었습니다.    이 당시 dockerd에는 현재의 Docker Client, Docker API, Docker Runtime, Image Build와 같은 역할들을 모두 담당하고 있었습니다.        LXCLXC는 단일 호스트 상에서 여러개의 고립된 리눅스 시스템(컨테이너)들을 실행하기 위한 운영 시스템 레벨 가상화 방법입니다. LXC는 dockerd에게 Linux kernel에 존재하는 컨테이너의 기본 building block의 namespaces나 cgroups(control groups)에 대한 접근을 제공했습니다.    namespaces: 운영 시스템을 논리적으로 나누어 고립된 환경을 제공하는 역할cgroups: 고립된 환경에서 사용할 자원을 제한하는 역할  새롭게 바뀐 도커 엔진의 모습도커는 2016년 12월 14일 쿠버네티스, AWS Fargate, Rancher와 같은 컨테이너 기술 기반의 소프트웨어에 dockerd안에 포함되어 있던 containerd라는 컨테이너 런타임을 제공해주기 위해 컨테이너를 모듈화하였습니다. (도커 공식문서 참고)Client도커 클라이언트는 개발자들이 도커를 사용할 때 Docker CLI로 도커 서버에 명령어를 전달하는 역할을 합니다. 흔히 저희가 사용하는 docker run과 같은 명령어가 REST API로 형태로 dockerd에게 전달됩니다.dockerd도커 데몬(dockerd)은 Docker API 요청을 수신하며 이미지 관리, 이미지 빌드, REST API, 인증, 보안, 코어 네트워킹, 오케스트레이션 등과 같은 역할을 담당합니다.containerdcontainerd는 Container의 생명주기를 관리합니다 (= container lifecycle operations).containerd는 원래 작고, 가벼운 Container lifecycle operations으로 설계되었는데, 시간이 지나면서 image pulls, volumes and networks와 같은 기능들이 확장되었습니다.shim앞에서 containerd가 새로운 컨테이너를 만들기 위해 runc를 사용한다고 했는데요. 생성되는 모든 container 당 runc의 새로운 인스턴스를 fork 합니다. 그러나 각 컨테이너가 생성되면, 상위 runc 프로세스가 종료됩니다.수백 개의 runc 인스턴스를 실행하지 않고도 수백 개의 container를 실행할 수 있습니다.컨테이너의 할당된 부모 runc 프로세스가 종료되면, 연결된 containerd-shim 프로세스가 컨테이너의 부모프로세스가 됩니다.이는 containerd에게 컨테이너의 file descriptor(e.g. stdin/out)와 종료 상태를 관리하는 데 필요한 최소한의 코드를 메모리에 남깁니다.runcrunc는 libcontainer용 CLI Wrapper로, 독립된 container runtime입니다.docker가 container 관련된 기능들을 쉽게 사용할 수 있도록 해주는 가볍고 이식가능한 툴입니다.다시 말해, container 동작 환경이 갖추어진 가볍고 이식 가능한 툴입니다.Docker에서 runc는 목적은 단 하나인데요, 바로 Container 생성입니다.참고  Johan Fischer, Comparing Container Runtimes: containerd vs. Docker  tutorialworks: The differences between Docker, containerd, CRI-O and runc  LinkedIn: containerd는 무엇이고 왜 중요할까?  cloud native wiki: 3 Types of Container Runtime and the Kubernetes Connection  pageseo: Docker Engine, 제대로 이해하기 (1)  Devin Jeon, Kubernetes의 Docker container runtime 지원 중단에 대하여  Selecting a container runtime for use with Kubernetes  A Comprehensive Container Runtime Comparison  Don’t Panic: Kubernetes and Docker",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/container-series2'> <img src='/images/container_7.png' alt='Container Series [Part2]: 진화하는 컨테이너 표준'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series2'>Container Series [Part2]: 진화하는 컨테이너 표준</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Airflow Series [Part1]: What is Airflow",
      "category" : "",
      "tags"     : "Airflow",
      "url"      : "/airflow-series1",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/airflow-series1'> <img src='/images/airflow_logo.png' alt='Airflow Series [Part1]: What is Airflow'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/airflow-series1'>Airflow Series [Part1]: What is Airflow</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part2]: 스트림 처리의 핵심(1) 시간",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series2",
      "date"     : "Feb 4, 2022",
      "content"  : "Table of Contents  시간의 중요성  처리 시간과 이벤트 시간  Timely Stream Processing          트리거                  반복 업데이트 트리거          완료 트리거                    워터마크                  워터마크 전파                      참고이 문서는 스트림 처리에 관한 애니메이션을 포함하고 있습니다. 애니메이션이 모바일에서는 제대로 재생되지 않는 문제가 있기 때문에 애니메이션이 궁금하시다면 컴퓨터를 이용한 접속을 권장드리며 또는 아래의 참고 문서를 확인하시면 되겠습니다.  Streaming Systems 시각 자료 참고시간의 중요성스트림 처리에서 핵심적인 개념은 ‘시간’과 ‘상태 관리’입니다. 이번 포스트에서는 그 중 첫 번째인 ‘시간’에 대해 알아보도록 하겠습니다. 네트워크와 통신 채널 같은 현실 세계 시스템은 완벽하지 않기 때문에 데이터는 지연되거나 그로 인해 순서가 바뀌어 도착할 수도 있습니다. 이런 상황에서 스트림 처리가 정확하고 일관된 결과를 생성하는 것은 매우 중요한 문제입니다.처리 시간과 이벤트 시간  이벤트 시간: 이벤트가 발생한 시간  처리 시간: 이벤트가 스트림 처리 연산을 실행 중인 장비에서 처리된 시간만약 이벤트가 발생한 즉시 처리 될 수 있다면 두 시간을 굳이 구분할 필요가 없을 것입니다. 하지만 이러한 두 가지 시간은 일치하지 않을 뿐 아니라, 두 시간의 차이 또한 일정하지 않습니다. 현실 상황에서 일어날 수 있는 예시를 통해 각각의 방법으로 시간을 정의했을 때 어떤 차이가 생기는지 알아보겠습니다.베를린에 살고 있는 앨리스는 출근마다 지하철 안에서 모바일 게임을 합니다. 게임에서는 500개의 풍선을 1분 안에 터트리면 보상을 받는 식입니다. 그런데 베를린 지하철의 네트워크는 자주 끊긴다는 문제가 있습니다. 앨리스가 게임을 하게 되면 발생된 이벤트를 스트림 처리 어플리케이션으로 보내진다고 할 때, 만약 게임 도중 네트워크가 끊기면 어떻게 될까요?위의 그림을 보면, 앨리스의 핸드폰에서는 1분 동안 총 6개의 데이터가 발생했습니다. 하지만 뒤에 2개의 데이터는 지하철이 터널을 지나고 있던 시점이라 아직 스트림 처리 어플리케이션으로 전송이 지연되었습니다. 이러한 상황에서 처리 시간을 기준으로 데이터를 처리한다면 4개의 데이터만을 고려해 결과를 낼 것입니다.하지만 이벤트 시간은 이벤트 내용 안에 포함된 타임스탬프를 기반으로 하기 때문에, 지연되어 도착한 2개의 데이터도 1분 안에 보내진 데이터로 간주됩니다. 그렇기 때문에 스트림 분석 어플리케이션은 지연된 2개의 데이터를 기다릴 것이고 모두 도착한 이후 결과를 계산할 겁니다. 따라서 이벤트 시간은 이벤트의 일부가 지연되더라도 발생했던 일을 제대로 반영할 수 있습니다.위의 예를 통해 알 수 있듯이, 이벤트 시간을 기반으로 하는 연산들은 예측할 수 있고 결과는 항상 결정적(deterministic)입니다. 지연된 이벤트의 처리 뿐만 아니라 데이터의 순서가 바뀌는 문제 또한 이벤트 시간을 사용함으로써 결과의 정확성을 보장할 수 있습니다.만약 시간이나 순서가 중요한 데이터가 아니라면 처리 시간을 기준으로 연산을 해도 됩니다. 하지만 시간에 따른 사용자 행동 분석, 결제 관련 서비스, 이상 탐지 등 대부분의 스트림 처리 어플리케이션은 지연 고려, 순서와 같은 요소들이 중요하기 때문에 이벤트 시간을 기준으로 연산을 제공할 수 있어야 합니다.이벤트 시간을 기준으로 사용하기 위해서는 워터마크라는 추가적인 설정을 해주어야 합니다.Timely Stream Processing실시간 스트림 처리가 어려운 이유 중 하나는 데이터가 Unbounded(무한) Unordered(비정렬) 할 수 있다는 점입니다. Unordered한 특성은 이벤트 시간을 기준으로 타임스탬프를 적용해 해결할 수 있습니다. 그러면 Unbounded는 어떻게 해결할 수 있을까요? Unbounded한 특성은 윈도우를 이용해 해결할 수 있습니다.아래 애니메이션은 Bounded(유한)한 데이터인 경우입니다. 이 경우에는 일정 시간이 지난 후에는 데이터가 더 들어오지 않기 때문에 데이터가 들어온 시간 범위 내에서 전체 데이터를 배치로 처리할 수 있습니다.하지만 Unbounded한 경우에는 데이터가 끝이 없습니다. 일정 시간이 지난 후에 데이터가 들어올지 안들어올지 모르는 상황이기 때문에, 이러한 경우에는 윈도우를 사용해 데이터를 처리해야 합니다.여기까지 오면 이제 실시간 스트림 처리에 조금 더 자신감이 생긴 것 같습니다. 이벤트 시간을 기준으로 타임스탬프를 정의하고, 윈도우를 사용해 무한한 데이터를 유한한 데이터로 나누어 봄으로써 Unbounded Unordered한 데이터가 발생하더라도 스트림 처리가 가능해졌습니다.하지만 한가지 문제점이 남아있습니다. 현실 시스템에서 데이터 지연은 필연적으로 발생하게 되는데, 각 윈도우들에서 지연된 데이터를 포함한 모든 데이터를 받았다는 사실을 어떻게 알며 언제쯤 윈도우에서 결과를 출력할 수 있을까요? 여기서 등장하는 중요한 개념이 바로 ‘트리거’와 ‘워터마크’입니다.트리거  윈도우가 출력되는 시점을 선언하는 방법  결과가 생성되는 시기 제어  윈도우별로 생성되는 각 출력을 윈도우의 패널이라고 함반복 업데이트 트리거  주기적으로 결과를 출력 (새로운 데이터를 만날 때마다, 1분 마다 등)  간단하고 쉽게 구현  언제 결과의 정확성이 신뢰할만한 수준에 다다랐는지 모른다는 한계점 (데이터 만날 때마다 결과를 출력하는 반복 업데이트 트리거)완료 트리거  윈도우 내 입력이 일정 기준 완료됐다고 믿는 시점에 결과를 출력  워터마크가 각 윈도우의 입력 완료 시점을 나타냄  각 윈도우의 입력이 완료되었다고 판단하는 순간 워터마크가 윈도우의 끝으로 이동하고 패널 출력  지연된 데이터 고려해 시점을 결정  패널이 업데이트 되지 않음(지연된 데이터를 알고있는 ‘완벽한 워터마크’와 지연된 데이터의 유무를 모르는 현실적인 ‘휴리스틱 워터마크’)노란색 숫자는 윈도우의 출력인 패널의 값을 나타냅니다. 트리거 설정이 중요한 이유는 어떻게 설정하는가에 따라 정확성과 지연 시간간의 트레이드-오프가 생긴다는 점입니다. 이 부분에 대해서는 뒤에서 다시 자세히 얘기하도록 하겠습니다.완료 트리거가 윈도우 내 입력이 일정 기준 완료되었다고 판단하는 기준이 워터마크입니다. 그렇기 때문에 워터마크에 대해 조금 더 알아보고 완료 트리거에 관한 내용을 정리한 뒤, 트리거 설정에 따른 정확성과 지연 시간 사이의 트레이드-오프 얘기를 마무리 하겠습니다.워터마크  어떤 이벤트 시간을 기준으로 입력이 완료됐음을 표시하는 방법  완료 트리거에서의 핵심  종류별로 이상적인 워터마크, 완벽한 워터마크, 휴리스틱 워터마크가 있음          이상적인 워터마크: 이벤트 시간과 처리 시간이 같다고 가정(이벤트 발생과 동시에 처리), 비현실적      완벽한 워터마크: 시스템이 지연되는 데이터 유무를 알 수 있는 경우, 비현실적      휴리스틱 워터마크: CPU, 분산 시스템, 네트워크 상황을 통해 지연된 데이터를 예측, 현실적      이제 다시 위 애니메이션을 보면 왼쪽 초록색 점선의 완벽한 워터마크는 지연 데이터 9를 알고 워터마크가 X축(이벤트 시간)으로 더 증가하지 않고 있습니다. 오른쪽 초록색 실선의 휴리스틱 워터마크는 지연 데이터 9를 놓쳐서 누락되었습니다.휴리스틱 워터마크에서 지연 데이터 9가 누락된 이유에는 2가지 가능성이 있습니다. 첫 번째는 알고 있었지만 지연 데이터 9를 기다렸다가 패널을 출력하기에는 지연율이 높아져서 데이터를 포기한 것일 수도 있고, 두 번째는 휴리스틱 워터마크의 불완전성으로 인해 놓친 경우일 수도 있습니다.첫 번째 이유를 보면 휴리스틱 워터마크를 사용한 완료 트리거가 지연율을 낮추기 위해 정확성을 조금 포기했습니다. 이를 통해 트리거에 지연율과 정확성의 트레이드-오프가 있음을 알 수 있습니다.이러한 트레이드-오프를 조절하는 것도 가능합니다. 이를 위해 조기/정시/지연 트리거들이 있습니다.조기(early) 트리거  지연 데이터로 휴리스틱 워터마크가 찍히는 시점이 너무 늦어지는 경우에 사용하면 좋음  업데이트 트리거 - 완료 트리거 순으로 조합정시(on-time) 트리거  완료 트리거  정시 트리거는 항상 워터마크가 윈도우 끝에 도달한 순간에만 패널 출력지연(late) 트리거  휴리스틱 워터마크의 불완전성으로 계속 지연 데이터를 놓치는 경우에 사용하면 좋음  완료 트리거 - 업데이트 트리거 순으로 조합위 애니메이션에서 이벤트 시간 12:03 부분을 보게 되면 워터마크는 아직 12:03부분에 머물러있는데 3과 4 데이터를 가진 윈도우가 패널을 출력했습니다. 이렇게 워터마크 이전에 패널을 출력하는 경우가 조기 트리거를 사용한 경우입니다.워터마크 전파지금까지 위에서 워터마크의 용도에 대해 알아보았습니다. 워터마크가 윈도우내 입력 완료 시점을 알려주고 완료 트리거를 보조한다는 점에서 굉장히 유용하다는 것을 깨달았습니다. 여기서는 조금 더 실제적인 측면에 대해 보려고 합니다.  워터마크는 어디서 만들어지는 것인가?  파이프라인 내 여러 컴포넌트들로 어떻게 전파되는가?참고  Flink 공식문서: Timely Stream Processing  Flink and Kafka Streams: a Comparison and Guideline for Users  Flink 공식문서: Windows  Streaming Systems 시각 자료 참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-04T21:01:35+09:00'>04 Feb 2022</time><a class='article__image' href='/flink-series2'> <img src='/images/flink_logo.png' alt='Flink Series [Part2]: 스트림 처리의 핵심(1) 시간'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series2'>Flink Series [Part2]: 스트림 처리의 핵심(1) 시간</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 네트워크 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series9",
      "date"     : "Feb 4, 2022",
      "content"  : "Table of Contents  Bridge Network Driver  Overlay Network Driver  도커 네트워크 실습          도커 네트워크의 몇 가지 특징      Bridge 드라이버 사용해보기      Overlay 드라이버 사용해보기        참고도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다.이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.당연히 다음 질문은 어떤 네트워크 드라이버를 사용해야 하는가 하는 것입니다. 각 드라이버는 트레이드오프를 제공하며 사용 사례에 따라 다른 장점이 있습니다. 도커 엔진과 함께 제공되는 내장 네트워크 드라이버가 있으며 네트워킹 벤더와 커뮤니티에서 제공하는 플러그인 네트워크 드라이버도 있습니다. 가장 일반적으로 사용되는 내장 네트워크 드라이버는 bridge, overlay, macvlan입니다. 이번 포스트에서는 비교적 간단한 드라이버인 bridge와 overlay에 대해서만 살펴보겠습니다.Bridge Network Driverbridge 네트워크 드라이버가 우리 목록의 첫 번째 드라이버입니다. 이해하기 쉽고, 사용하기 쉽고, 문제 해결이 간단하기 때문에 개발자와 Docker를 처음 접하는 사람들에게 좋은 네트워킹 선택이 됩니다. bridge 드라이버는 private 네트워크를 호스트 내부에 생성해 컨테이너들이 생성한 네트워크 안에서 통신할 수 있도록 합니다. 컨테이너에 포트를 노출함으로써 외부 액세스가 허용됩니다. 도커는 서로 다른 도커 네트워크 간의 연결을 차단하는 규칙을 관리하여 네트워크를 보호합니다.내부적으로 도커 엔진은 리눅스 브리지, 내부 인터페이스, iptables 규칙 및 호스트 경로를 만들어 컨테이너 간의 연결을 가능하게 합니다. 아래 강조 표시된 예에서는 도커 브리지 네트워크가 생성되고 두 개의 컨테이너가 이 네트워크에 연결됩니다. 도커 엔진은 별도의 설정 없이 필요한 연결을 수행하고 컨테이너에 대한 서비스 디스커버리를 제공하며 다른 네트워크와의 통신을 차단하도록 보안 규칙을 구성합니다.우리의 애플리케이션은 현재 호스트 8000번 포트에서 서비스되고 있습니다. 도커 브리지는 컨테이너 이름으로 web이 db와 통신할 수 있도록 하고 있습니다. 브릿지 드라이버는 같은 네트워크에 있기 때문에 자동으로 우리를 위해 서비스 디스커버리를 합니다.브리지 드라이버는 로컬 범위 드라이버이므로 단일 호스트에서 서비스 디스커버리, IPAM 및 연결만 제공합니다. 다중 호스트 서비스 검색을 수행하려면 컨테이너를 호스트 위치에 매핑할 수 있는 외부 솔루션이 필요합니다. 이 때 필요한 것이 바로 overlay 드라이버입니다.Overlay Network Driveroverlay 네트워크 드라이버는 multi-host 네트워킹의 많은 복잡성을 획기적으로 단순화합니다. Swarm 스코프 드라이버로, 개별 호스트가 아닌 전체 Swarm 또는 UCP 클러스터에서 작동합니다.overlay 드라이버는 컨테이너 네트워크를 물리적 네트워크와 분리해주는 VXLAN data plane을 사용합니다. 덕분에 다양한 클라우드, 온-프레미스 네트워크 환경 속에서 최고의 이식성을 제공해줍니다.도커 네트워크 실습(라우드 엔지니어 Won의 성장 블로그 참고)도커 네트워크의 몇 가지 특징  도커는 컨테이너에 내부 IP(eth0)를 순차적으로 할당  컨테이너 외부에 노출시킬 엔드포인트로 veth(Virtual Ethernet) 생성  컨테이너마다 veth 네트워크 인터페이스 자동 생성  docker0는 기본 생성되는 디폴트 브리지로 각 veth 인터페이스와 호스트의 기본 네트워크인 eth0와 연결Bridge 드라이버 사용해보기Overlay 드라이버 사용해보기참고  도커 공식문서  MARK CHURCH, Understanding Docker Networking Drivers and their use cases  클라우드 엔지니어 Won의 성장 블로그, 06. 도커 네트워크 포스트  DaleSeo: Docker 네트워크 사용법  Julie의 Tech블로그, 도커 - 네트워킹 / bridge와 overlay  도커 공식문서, Networking with overlay networks  도커 공식문서, Use overlay networks  How To Communicate Between Docker Containers  Using placement constraints with Docker Swarm  Install Docker Engine on Ubuntu",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-04T21:01:35+09:00'>04 Feb 2022</time><a class='article__image' href='/docker-series9'> <img src='/images/docker_network_logo.png' alt='Docker의 네트워크 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series9'>Docker의 네트워크 이해하기</a> </h2><p class='article__excerpt'>이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part6]: 네트워크 프로토콜(3) IP",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series6",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  IP(Internet Protocol)          IP 주소      서브넷 마스크(Subnet Mask)        DNS(Domain Name System) 서버  라우팅          라우팅 테이블      게이트웨이        참고IP(Internet Protocol)IP는 네트워크 상에서 데이터를 목적지로 보내는 역할을 하는 인터넷 계층에서 사용하는 프로토콜IP 주소IP 주소는 인터넷에 연결하고자 하는 디바이스가 가지고 있는 NIC(Network Interface Controller)의 고유한 주소를 뜻합니다. 편지를 주고 받기 위해서는 서로의 주소가 필요한 것처럼 디바이스간 통신을 위해서는 IP주소가 필요합니다. IP주소는 네트워크 번호와 호스트 번호로 이루어진 32비트 숫자입니다.(IPv4 기준)  내부 네트워크에 연결되는 라우터의 포트를 이더넷 인터페이스 (이더넷용 IP 주소는 내부에서 부여받은 IP 주소 중 첫 번째 주소)  외부(인터넷) 쪽으로 연결되는 인터페이스를 시리얼 인터페이스 (시리얼용 IP 주소는 ISP 업체의 라우터가 가지는 시리얼 인터페이스의 IP 주소)  위와 같은 가정에서 우리가 라우터에 부여해야 하는 IP 주소는 두 개가 됨  IP 주소 중 네트워크 부분: 하나의 브로드캐스트 영역          라우터를 거치지 않고 통신이 가능한 영역      라우터가 라우팅할 때 참고하는 부분      라우터는 다른 네트워크로 갈 때만 필요        IP 주소 중 호스트 부분: 각각의 PC 또는 장비  IP 주소의 Class에 따라 어디까지가 네트워크 부분이고, 어디까지가 호스트 부분인지가 나뉨 (네트워크의 크기가 달라짐)          클래스 A는 이진수 중에서 맨 앞쪽 숫자가 항상 0으로 시작되는 것들                  호스트 수가 가장 많은 클래스          앞의 8비트가 네트워크 부분, 나머지 24비트가 호스트 부분          1.0.0.0 ~ 126.0.0.0 까지로 규정 (0 시작과 127 시작은 제외) -&amp;gt; 126개의 네트워크, 각각의 네트워크는 라우터 없이 통신          호스트는 2의 24승 - 2개(모두 0인 경우, 모두 1인 경우) = 16,777,214개 -&amp;gt; 16,777,214개의 호스트가 하나의 네트워크에 연결          따라서 IP 주소를 모두 클래스 A로만 구성한다면 -&amp;gt; 126개의 네트워크 * 16,777,214개의 호스트                    클래스 B는 맨 앞이 10으로 시작                  앞의 16비트가 네트워크 부분          128.0.0.0 ~ 191.255.0.0 까지로 규정                    클래스 C는 맨 앞이 110으로 시작                  앞의 24비트가 네트워크 부분          192.0.0.0 ~ 223.255.255.0 까지로 규정                      기본 게이트웨이(Default Gateway)          내부 네트워크에서는 라우터 없이도 통신이 가능      내부 네트워크에 없는 IP 주소로 갈 때는 이 기본 게이트웨이를 통해 나감      즉 라우터의 이더넷 인터페이스를 의미        라우터는 인터페이스별로 IP 주소 부여. 스위치나 허브는 장비별로 IP 주소 부여서브넷 마스크(Subnet Mask)  주어진 네트워크를 가공할 때 사용  우리가 일단 어떤 IP 주소를 배정받게 되면 보통 이 주소를 그대로 사용하지 않고 서브넷 마스크를 조합하여 사용  우리가 부여받은 net을 여러개의 subnet으로 나눈다는 의미  서브넷마스크를 통해 나누어진 서브넷간의 통신은 라우터를 거쳐야함  모든 IP 주소에는 서브넷 마스크가 따라다님. 쓰지 않더라도. 그래야 지금 IP 주소가 마스킹 된건지 아닌지 알 수 있음  클래스 A의 기본 서브넷 마스크는 255.0.0.0, B는 255.255.0.0, C는 255.255.255.0  서브넷 마스크는 IP주소의 어디까지가 네트워크 부분이고, 어디까지가 호스트 부분인지를 나타내는 역할을 함  서브넷 마스크의 이진수 1은 네트워크 부분, 이진수 0은 호스트 부분을 의미함  즉, 서브넷 마스킹은 기존 IP 주소의 호스트 부분의 일부를 네트워크 부분으로 바꾸는 작업기존 네트워크: 150.150.100.1 =&amp;gt; 1001 0110 1001 0110 0110 0100 0000 0001 =&amp;gt; 클래스 B =&amp;gt; 150.150.0.0이 네트워크를 의미서브넷 마스크: 255.255.255.0 =&amp;gt; 1111 1111 1111 1111 1111 1111 0000 0000 =&amp;gt; 네트워크 자리를 24자리 까지로 늘림 (호스트를 8자리로 줄임)---------------------------------------------------------------------서브넷: 150.150.100.0 =&amp;gt; 1001 01110 1001 01110 01110 0100 0000 0000 =&amp;gt; 최종적으로 서브넷 네트워크가 150.150.100.0가 됨  참고로 호스트 부분이 0인 주소는 호스트 주로로 사용하지 못함. PC에서 사용하는 주소가 아니라 네트워크 자체를 의미  또 호스트 부분이 255인 주소 역시 호스트 주소로 사용할 수 없음. 브로드캐스트 주소 (모든 호스트에게 메시지 보낼 때 사용하는 주소)DNS(Domain Name System) 서버DNS 서버는 도메인 네임을 IP주소로 매핑하여 보관하고 있는 서버입니다. 하지만 모든 도메인 정보를 저장할 수는 없고 저장한다고 해도 IP주소를 가지고 오는데 많은 시간이 소요됩니다. 이를 해결하기 위해 DNS 서버를 계층적으로 구성해 IP 주소를 가져오도록 했으며 한 번 가져온 정보는 캐시에 저장해둡니다. 하지만 캐시에 저장된 후 정보가 변경될 수 있기 때문에 캐시에 저장된 정보는 유효기간이 지나면 캐시에서 삭제됩니다.라우팅목적지 IP 주소를 찾아가는 과정라우팅 테이블MAC addresses are determined for a device when it is manufactured. They don’t change wherever you go. So assume MAC addresses as your name(assume it’s unique).Ip addresses are assigned(by your ISP) when your machine connects to the network. It depends on your location. So assume it as your address.If someone needs to send you a post, if they use your unique name, then routing that post to you will be difficult. Now if they use your address, routing becomes easier.That’s why IP addresses are used. Routing a packet to a IP address is easier than routing it to a MAC address.The routing table is used to find out if a packet should be delivered locally, or routed to some network interface using a known gateway address.MAC addresses are layer-2 addresses, IP addresses are layer-3 addresses, and ports are layer-4 addresses.MAC addresses are not in the packet headers, they are in the frame headers. Only layer-3 addresses are in the packet headers. Ports are in the segment headers.MAC addresses are only significant on a LAN. They are in the frame headers, and frames are stripped at layer-3 boundaries (routers). The routers then use the layer-3 headers with the layer-3 address to forward a packet to the next interface, where게이트웨이  서버든 라우터든 패킷을 보내기 위해서는 일단 라우팅 테이블을 참조함  라우팅 테이블을 보니 라우팅 엔트리는 1.1.1.0/24, Gateway는 없음 -&amp;gt; 목적지 주소가 나와 같은 네트워크에 존재함을 확인  이제 Server1은 목적지 주소 1.1.1.20에 대한 MAC 주소를 알기위해 자신의 ARP 테이블을 참조함  그런데 ARP 테이블이 비어있음 -&amp;gt; ARP miss  Server1은 Server2(1.1.1.20)의 MAC 주소를 알아내기 위해 lan1 포트로 ARP request 패킷을 보냄  Switch1은 이 패킷을 수신하고, 수신 패킷의 Source MAC 주소를 배웁니다          (Switch1의 MAC 테이블에 Server1의 MAC주소와 Switch1의 port를 기록)        Switch1은 이 ARP request 패킷을 보고 Destination MAC이 브로드 캐스팅 주소임을 확인  Switch1은 수신포트 fe1을 제외한 나머지 모든 포트로 flooding  라우터 R1은 패킷의 Target IP 주소를 보고 자기 것이 아닌 것을 확인하고 버림  Server2는 자신의 IP 주소임을 확인 -&amp;gt; 자신의 MAC 주소를 필드에 담아 ARP reply 패킷을 lan1 포트로 보냄  이 패킷을 수신한 Switch1은 Source MAC Learning을 하여 MAC 테이블에 기록  Server1은 자신의 ARP 테이블에 MAC 주소를 기록  이제 Server1은 Server2로 IP 패킷을 보냄  Server1은 목적지 주소 1.1.1.20에 대한 MAC 주소를 ARP 테이블에서 얻어오고 이동해야할 포트 번호를 MAC 테이블을 통해 확인  이 패킷은 fe2 포트를 통해 나가고, Server2가 패킷을 수신참고  스위칭과 라우팅… 참 쉽죠잉~ (1편: Ethernet 스위칭)  스위칭과 라우팅… 참 쉽죠잉~ (2편: IP 라우팅)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/network-series6'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part6]: 네트워크 프로토콜(3) IP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series6'>Network Series [Part6]: 네트워크 프로토콜(3) IP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series1",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  Flink  Use Cases          이벤트 기반 아키텍처      스트림 데이터 처리 및 분석        결론  참고Flink플링크는 분산 스트림 처리 오픈소스 프레임워크입니다 플링크는 상태가 있는 스트림 처리(Stateful Stream Processing) 애플리케이션을 구현하는 데 필요한 다양한 API를 제공합니다. 플링크는 2014년 4월, 아파치 소프트웨어 재단의 인큐베이터 프로젝트로 시작해 2015년 1월, 아파치 최상위 프로젝트가 되었습니다. 플링크는 현재 넷플릭스, 에어비앤비, 우버 등 전 세계 많은 기업의 대규모 스트림 처리 업무에서 핵심적인 역할을 하고 있습니다.플링크가 스트림 처리를 위해 제공하는 스펙은 다음과 같습니다.  Low Latency: ms단위의 지연시간 보장  High Throughput: 초당 100만 단위의 이벤트 처리  In-Memory Computing: 인-메모리 컴퓨팅  Exactly-Once State: 정확히 한 번 상태 보장(데이터의 중복, 유실 방지, 장애 복구 가능)  Out-of-Order Event Handling: 순서가 뒤바뀐 이벤트가 들어오더라도 일관성 있는 결과 제공  Scale-Out Architecture: 수 천대의 클러스터 환경으로 확장 가능(Flink 공식문서 참고)Use Cases플링크는 스트림 처리뿐만 아니라 짧은 지연을 요구하는 다양한 어플리케이션에 사용될 수 있습니다.  이벤트 기반 어플리케이션  스트림 데이터 처리 및 분석 어플리케이션이벤트 기반 아키텍처이벤트 기반 어플리케이션은 이벤트 스트림을 입력으로 받아 특정 비즈니스 로직을 처리합니다. 비즈니스 로직에 따라 알람이나 이메일을 전송할 수도 있고, 다른 어플리케이션이 소비할 수 있도록 외부로 이벤트를 내보낼 수도 있습니다. 일반적으로 실시간 추천, 패턴 감지, 이상 탐지 SNS 웹 어플리케이션 등에 사용됩니다.이벤트 기반 아키텍처는 기존의 트랜잭션 처리 아키텍처와 다음과 같은 차이점이 있습니다.트랜잭션 처리 아키텍처  컴퓨팅 계층과 데이터 스토리지 계층이 분리  원격(외부) 데이터베이스에서 데이터를 읽음  REST 통신 방식으로 서로 연결이벤트 기반 아키텍처  로컬(메모리 내 또는 디스크 내) 데이터 액세스를 통한 빠른 처리량, 낮은 지연율 제공  원격 영구 스토리지에 정기적으로 체크포인트를 기록함으로써 내결함성 달성  이벤트 로그로 서로 연결함으로써 데이터 전송과 수신을 비동기화이벤트 기반 아키텍처로 구현한 어플리케이션을 위해서는 정확히 한 번 수준의 상태 일관성과 수평 확장성을 요구하며 이벤트 시간 지원 여부, 상태 관리의 품질 등이 결정하게 됩니다. 플링크는 이 모든 기능을 포함하고 있기에 이벤트 기반 어플리케이션 구현을 위한 좋은 선택이 될 수 있습니다.스트림 데이터 처리 및 분석스트림 데이터 처리 및 분석 어플리케이션이라고 해서 이벤트 기반 어플리케이션과 크게 다른 것은 아닙니다. 기본적으로 실시간으로 들어오는 스트림 이벤트 또는 데이터를 받으면 그에 맞게 저지연으로 동작하고 상태를 관리하고 결과를 내보내는 것이기 때문에 비슷한 개념입니다.이벤트 기반 어플리케이션이 어떤 비즈니스 로직을 처리한다면, 처리 및 분석 어플리케이션은 처리(Processing)나 분석(Analytics)을 한다는 차이만 있을 뿐입니다.(공식문서에서는 유스케이스를 이벤트 기반 어플리케이션, 데이터 파이프라인, 스트리밍 분석으로 나누어 구분하지만 이것들을 구분하는 것은 내가 느끼기에는 사소하고 괜히 구분지으려고 하는 것이 더 헷갈린다.)결론결론은 플링크는 결국 실시간으로 발생하는 데이터 또는 이벤트를 짧은 지연으로 비즈니스 로직으로 처리, 프로세싱, 분석하기 위한 용도입니다.참고  Flink 공식문서: What is Apache Flink? — Applications  Flink 공식문서: Stateful Stream Processing  High-throughput, low-latency, and exactly-once stream processing with Apache Flink  What makes Flink provides low latnecy, Is Apache Flink the future of Real-time Streaming?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/flink-series1'> <img src='/images/flink_logo.png' alt='Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series1'>Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series5",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  TCP          TCP의 특징      3 way handshake        UDP          UDP의 특징        TCP와 UDP 비교          데이터 전송시 발생할 수 있는 오류        Port  참고TCP와 UDP는 두 대의 컴퓨터를 서로 연결(connection)하는 역할을 하는 전송 계층(Transport layer)에서의 프로토콜TCPTCP의 특징  Transmission Control Protocol  연결 지향 TCP 3 way handshake (논리적 연결)  데이터 전달 보증  순서 보장3 way handshake  클라이언트와 서버를 연결하는 방법UDPUDP의 특징  User Datagram Protocol  기능이 거의 없음 (연결 지향 X, 데이터 전달 보증 X, 순서 보장 X)  단순하고 빠름  애플리케이션에서 추가 작업 필요TCP와 UDP 비교데이터 전송시 발생할 수 있는 오류  패킷의 잘못된 순서  패킷 손실  패킷 내부의 손상된 데이터(헤더, 패킷 내용, IP 주소 등을 기반으로 고정적인 길이의 체크섬 value를 만들어 손상 여부 파악)위 3가지에 대해 TCP는 모두 해결 가능. UDP는 마지막 내부의 손상된 데이터에 대해서만 해결 가능.아래는 TCP와 UDP의 차이를 표로 비교한 것.PortIP 주소를 하나의 공간으로 생각해보자. 여러 서비스에 대해서 구역을 나누지 않고 사용을 하면, 서비스가 겹쳐 Overlap 되면서 서로서로 충돌 및 간섭 현상이 발생할 것이다. 이러한 현상이 발생하면, 통신이 되다가 안되다가를 반복하는 등 트래픽이 불안정해진다. 따라서 이것을 해소하기 위해서는 각각 서비스에 대한 간섭 현상을 없애야 한다. 각 서비스마다 독방 형식의 구역을 나누게 되면 간섭 현상이 사라질 것이다. 따라서 OSI 7계층 중 4계층에서는 하나의 IP 주소를 독방의 개념인 포트(Port)로 나눈다.TCP, UDP는 패킷이 어떤 포트로 이동해야 하는지를 나타낼 수 있습니다. TCP 및 UDP 헤더에는 포트 번호를 표시하는 섹션이 있습니다. 예를 들어 IP(인터넷 프로토콜)와 같은 네트워크 계층 프로토콜은 지정된 네트워크 연결에서 사용 중인 포트를 인식하지 못합니다. 표준 IP 헤더에는 데이터 패킷이 어떤 포트로 이동해야 하는지 나타내는 위치가 없습니다. IP 헤더는 대상 IP 주소만 나타내고 해당 IP 주소의 포트 번호는 표시하지 않습니다.일반적으로 네트워크 계층 프로토콜은 거의 항상 전송 계층 프로토콜과 함께 사용되기 때문에 네트워크 계층에서 포트를 지시할 수 없는 것은 네트워킹 프로세스에 영향을 미치지 않는다.참고  불곰, TCP, UDP 포트 (Port, OSI 4계층)                              [What is a computer port?          Ports in networking](https://www.cloudflare.com/ko-kr/learning/network-layer/what-is-a-computer-port/){:target=”_blank”}                      곰돌이 놀이터: 소켓 통신이란?  아무거나올리는블로그: Socket Programming - Socket  stackoverflow: difference between socket programming and Http programming",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series5'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series5'>Network Series [Part5]: 네트워크 프로토콜(2) TCP, UDP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part4]: 네트워크 프로토콜(1) HTTP",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series4",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  HTTP          HTTP 버전 차이      HTTP의 특징        URL  HTTP Method  HTTP 메세지          시작 라인                  요청 메세지인 경우          응답 메세지의 경우                    HTTP 헤더      HTTP 바디      예시                  요청 메세지          응답 메세지                      웹 브라우저의 동작원리          HTTP 리퀘스트 작성                  URL 입력          HTTP 리퀘스트 작성                    DNS 서버에 웹 서버의 IP주소 조회                  DNS Resolver를 이용해 DNS 서버 조회                    프로토콜 스택에 메시지 송신 요청        참고HTTPHTTP(HyperText Transfer Protocol)는 응용 계층(Application layer)에서 압도적으로 많이 사용되는 프로토콜입니다. 요즘에는 모든 데이터(텍스트, 이미지, 음성, 영상, 파일, JSON 등)를 HTTP 메시지에 담아서 전송합니다.HTTP가 하는 역할은 무엇일까요? 저희는 응용 계층에서 데이터를 주고받기 위해(크롬, 사파리와 같은 웹 브라우저에서 뉴스, 사진, 동영상을 보고 물건을 주문하는 것과 같은 행위) 클라이언트는 요청(request), 서버는 응답(response)하는 방식을 사용합니다.  이 때 응용 계층에 있는 단말기(우리의 핸드폰, 노트북 그리고 구글이 가지고 있는 웹 서버와 같은 것들)들이 서로 일관된 방법으로 데이터를 주고받기 위해 규약이 필요했는데 이때 생긴 규약이 바로 HTTP입니다.이 때 클라이언트는 HTTP 메세지를 작성하기 위해 두 가지를 사용합니다. 바로 URL과 HTTP 메소드입니다.HTTP 버전 차이  HTTP/1.1: 1997년에 등장해서 현재까지 가장 많이 사용하는 버전  HTTP/2:  HTTP/3: UDP기반으로 개발됨HTTP의 특징  클라이언트/서버 구조로 동작          클라이언트가 request를 보내면 서버가 response를 돌려주는 구조        무상태(Stateless) 프로토콜          서버가 클라이언트의 상태를 보존하지 않음 (그래서 클라이언트가 알아서 자신의 상태를 잘 업데이트해서 서버에 전달하게 됨)      장점: 서버 확장성 높음(서버가 중간에 바뀌어도 된다. 어차피 클라이언트가 부담하게 되므로)      장점: 특정 서버에 의존하지 않게 되므로 서버 장애에 강인하다      단점: 클라이언트가 추가 데이터 전송해야함      로그인이 필요한 서비스의 경우 로그인 상태를 서버에 유지해야하므로 이 때는 브라우저의 쿠키와 서버의 세션을 조합해서 보완해야함        비연결성          서버와 클라이언트가 계속 연결을 유지하게 된다면 클라이언트가 늘어날때마다 서버의 리소스 부담 계속 커지게 됨      클라이언트가 request를 보내고 서버가 response를 보낸 후 요청을 끊는다 -&amp;gt; 서버는 최소한의 자원만 사용하게 됨      HTTP는 기본적으로 연결을 유지하지 않는 모델      연결하는데 시간이 별로 소요되지 않나? -&amp;gt; TCP/IP 연결 새로 맺어야함 -&amp;gt; 3-way handshake 시간이 추가된다      그리고 네이버 검색을 예로 들때, 우리가 HTTP 메세지를 보내고 response를 돌려줄 때 검색 결과만 돌려주는게 아니라 그안에 포함된 HTML, CSS, 이미지 등을 함께 돌려줘야 한다 -&amp;gt; 이런 문제를 HTTP Persistent Connection으로 해결      Persistent Connection은 내부 메커니즘에 의해 보통 하나의 웹 페이지를 띄울 동안 연결을 계속 지속시킨다      HTTP/2, HTTP/3 오면서는 HTTP Persistent Connection이 더욱 발전됨        HTTP 메시지URLURL은 Uniform Resource Locator의 약자입니다. URL은 URI(Uniform Resource Identifier)를 표현하기 위한 방법 중 하나입니다. URL말고도 URN이라는 것이 있지만 지금은 거의 URL만 사용하기 때문에 URN은 생략하도록 하겠습니다.인터넷에서 어떤 자원(회원 정보, 주문서, 사진, 동영상 등)을 유일하게 표현하기 위해 URI라는 개념이 등장했고 이를 위한 방법으로 URL을 사용하는 것입니다. URL은 이러한 자원들에게 부여된 고유한 주소를 말합니다.인터넷에서는 모든 자원에 URL이라는 고유한 주소를 부여해 이들을 식별한다URL의 예시를 보겠습니다.https://google.co.kr/search?q=hello&amp;amp;hl=kohttps://order.kyobobook.co.kr/order/orderStepOneURL 문법은 아래와 같습니다.# URL 문법scheme://[userinfo@]host[:port][/path][?query][#fragment]예: https://www.google.com/search?q=hello&amp;amp;hl=ko# scheme예: https- 주로 프로토콜이 사용됩니다.- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 (https, http, ftp)- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트- https는 http에 보안 추가 (HTTP Secure)# host예: www.google.com- 도메인명 또는 IP주소# port예: 8888- 접속 포트# path예: /search- 리소스 경로 (계층적 구조)- 디렉토리명/파일명# query예: ?q=hello&amp;amp;hl=ko- key=value 형태- ?로 시작, &amp;amp;로 추가 가능- query parameter 또는 query string으로 보통 불림# fragment예: #getting-started-introducing-spring-boot- html 내부 북마크 등에 사용- 서버에 전송하는 정보는 아님URL에서 유의할 점은 URL은 자원을 식별하는 용도로만 써야 한다는 것입니다. 예를 들어 어떤 물건을 주문할 때는 주문(order)만을 URL로 표현해야지 주문 확인(order-check), 주문 취소(order-cancel) 이런 행위까지를 포함시키면 안됩니다.HTTP Method이러한 행위를 나타내기 위해 사용하는 것이 바로 HTTP 메소드입니다.인터넷에서 발생하는 행위는 크게 CRUD(Create-Read-Update-Delete)로 나눌 수 있습니다. CRUD를 HTTP에서 제공하는 메소드로 구현할 수 있습니다.            HTTP Method      설명              GET      읽기(리소스 조회)              POST      쓰기(리소스 등록)              PUT      업데이트(리소스 완전 대체)              PATCH      부분 업데이트(리소스 부분 대체)              DELETE      삭제(리소스 삭제)      HTTP 메세지클라이언트와 서버는 URL과 HTTP 메소드를 이용해서 HTTP 메세지를 만들어 통신한다고 했습니다. HTTP 메세지는 바이너리로 표현할 수 있는 모든 데이터를 전송할 수 있습니다. (HTML, TEXT, JSON, XML, 이미지, 영상 파일 등)서버간에 데이터를 주고받을 때에도 대부분 HTTP를 사용한다고 합니다.HTTP 메세지의 구조는 다음과 같습니다.시작 라인요청 메세지인 경우  HTTP 메소드          종류: GET, POST, PUT, DELETE …      서버가 수행해야 할 동작 지정                  GET: 리소스 조회          POST: 요청 내역 처리                      요청 대상          절대경로[?쿼리]      절대경로=”/”로 시작하는 경로        HTTP 버전응답 메세지의 경우      HTTP 버전        HTTP 상태 코드          200: 성공      400: 클라이언트 요청 오류      500: 서버 내부 오류      HTTP 헤더  용도          HTTP 전송에 필요한 모든 부가정보      메세지 바디의 내용, 메세지 바디의 크기, 압축, 클라이언트 정보, ..      HTTP 바디  실제 전송할 데이터  HTML, 이미지, 영상, JSON 등 byte로 표현 가능한 모든 데이터 전송 가능이렇게 HTTP 메세지를 통해서 두 단말기가 응용계층에서 쉽게 통신할 수 있도록 하는 API(Application Program Interface)를 REST(Representational State Transfer) API라고 합니다.예시요청 메세지응답 메세지웹 브라우저의 동작원리우리가 웹 브라우저(크롬, 사파리 등)에서 뉴스 보기를 클릭하거나 유튜브 비디오를 시청할 때 내부적으로 어떤 일들이 일어나는지 한 번 알아보겠습니다.HTTP 리퀘스트 작성우리는 보통 웹 브라우저에서 URL을 입력하거나 어떤 버튼을 클릭하는 식으로 웹 서버와 상호작용 하게 되는데 이 때 웹 브라우저는 내부에서 HTTP 리퀘스트라는 것을 웹 서버에 전송합니다.URL 입력https://www.google.com/search?q=hello&amp;amp;hl=koHTTP 리퀘스트 작성URL을 입력하고 나면 웹 브라우저는 URL을 바탕으로 HTTP 리퀘스트 메시지를 만듭니다.HTTP 리퀘스트 메시지의 형태는 다음과 같습니다.(joie-kim님 블로그 참고)DNS 서버에 웹 서버의 IP주소 조회HTTP 리퀘스트를 작성하고 나면 이제 OS에게 이것을 웹 서버로 전송해달라고 요청합니다. (웹 브라우저가 직접 전송하지 않는 이유는 메시지를 송신하는 기능은 하나의 애플리케이션에만 종속되는 기능이 아니므로 OS에서 전송 기능을 담당하는 것이 더 좋다고 합니다.)OS에서는 리퀘스트 메시지를 전송하기 전에 먼저 도메인 네임을 IP 주소로 변환하는 과정을 거칩니다. 이를 네임 레졸루션(name resolution)이라고 합니다.DNS Resolver를 이용해 DNS 서버 조회네임 레졸루션을 시행하는 것이 DNS 리졸버(DNS Resolver)입니다. 리졸버는 Socket 라이브러리에 들어있는 부품화된 프로그램입니다. Socket 라이브러리는 네트워크 관련 기능을 하는 프로그램을 모아놓은 라이브러리입니다.프로토콜 스택에 메시지 송신 요청DNS Resolver가 IP주소를 찾아오면 이제 진짜 웹 서버로 보낼 준비가 완료되었습니다. 이렇게 준비된 HTTP Request 메시지는 OS의 내부에 포함된 프로토콜 스택을 호출하여 실행을 요청합니다.참고  인프런에서 제공하는 이영한님의 모든 개발자를 위한 HTTP 웹 기본 지식 강의  REST 논문을 정리한 자료  사바라다는 차곡차곡: [REST API] REST에서의 Resource",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series4'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part4]: 네트워크 프로토콜(1) HTTP'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series4'>Network Series [Part4]: 네트워크 프로토콜(1) HTTP</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series3",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  인터넷의 계층화  애플리케이션 계층  전송 계층  인터넷 계층  링크 계층  참고인터넷의 계층화인터넷 프로토콜 스위트(suite: 모음, 세트)(인터넷 프로토콜 스택이라고도 함)는 인터넷에서 컴퓨터들이 서로 정보를 주고 받는데 쓰이는 프로토콜의 모음을 뜻합니다. 이 프로토콜의 모음을 프로토콜의 네트워킹 범위에 따라 4개 또는 7개로 계층화한 것을 TCP/IP-4계층 또는 OSI-7계층 이라고 부릅니다. 여기서 각각의 계층은 특정 계층에서 사용하는 프로토콜이 변경되더라도 다른 계층에 영향을 주지 않도록 설계되었습니다.아래 그림은 각각 OSI-7계층과 TCP/IP-4계층을 나타낸 그림입니다.예전에는 인터넷을 계층화하는 모델로 OSI-7계층을 주로 사용했지만, 요즘에는 이를 좀 더 간소화한 TCP/IP-4계층 모델을 더 많이 사용합니다.애플리케이션 계층  애플리케이션 계층은 FTP, HTTP, SSH, SMTP, DNS 등 응용 프로그램이 사용되는 계층  웹 브라우저와 같은 서비스를 실질적으로 사용자들에게 제공하는 계층  대표적인 프로토콜: FTP, HTTP, SSH, SMTP, DNS  소켓(Socket) 라이브러리전송 계층  전송 계층은 송신자와 수신자를 서로 연결(connection)하는 계층  대표적인 프로토콜: TCP, UDP인터넷 계층  인터넷 계층은 네트워크 패킷을 IP주소를 이용해 목적지로 전송하기 위해 사용되는 계층  인터넷 계층은 상대방이 데이터를 제대로 수신했는지에 대해 보장하지 않는 비연결형적인 특징을 가짐  대표적인 프토로콜: IP, ARP링크 계층  링크 계층은 전선, 광섬유, 무선으로 네트워크 장비(스위치, 라우터)를 연결해 실질적으로 데이터를 전송하는 계층  대표적인 프로토콜: Ethernet, Token-Ring참고  The Secret Security Wiki",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series3'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series3'>Network Series [Part3]: 네트워크 계층 TCP/IP 4계층과 OSI 7계층</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 볼륨 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series8",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  도커에서 데이터 관리하기  마운트 종류  Volume          Volume을 사용하기 좋은 경우        Bind Mount          Bind mount를 사용하기 좋은 경우        tmpfs Mount          tmpfs mount를 사용하기 좋은 경우        사용 방법          Dockerfile      docker run command      docker compose        참고도커에서 데이터 관리하기기본적으로 컨테이너 안에서 생성된 모든 파일은 컨테이너 레이어에 저장됩니다. 그래서, 해당 컨테이너가 삭제되면 데이터도 함께 사라집니다. 따라서 컨테이너의 생명 주기와 관계없이 데이터를 영구적으로 저장하기 위한 방법이 필요합니다. 또한 여러 컨테이너가 데이터를 공유할 수 있으면 데이터를 컨테이너별로 중복 저장할 필요가 없어 컨테이너를 더욱 효율적으로 관리할 수 있게 될 것입니다.이러한 이유로 다음과 같은 기능을 가진 옵션을 도커에서는 제공해주고 있습니다.  데이터 영구 저장  컨테이너간 데이터 공유도커에서 볼륨을 위해 제공해주는 옵션 중 volume 또는 bind mount를 사용하면 컨테이너가 중지된 후에도 파일을 호스트 머신에 파일을 저장함으로써 유지할 수 있습니다.참고로 도커에서 tmpfs mount를 사용하면 호스트의 시스템 메모리에 인-메모리 형식으로 파일을 저장할 수 있습니다.마운트 종류어떤 종류의 마운트를 사용하더라도 컨테이너 안에서 데이터의 모습은 같습니다. 각 마운트 종류의 차이를 이해하는 쉬운 방법은 데이터가 도커 호스트 내에서 어디 존재하는지 생각해보는 것입니다.      Volume을 사용하면 도커에 의해 관리되는 호스트 파일 시스템(Linux기준 /var/lib/docker/volumes/)에 데이터가 저장됩니다. 비-도커 프로세스들은 여기 파일들을 수정해서는 안됩니다.        Bind mount는 호스트 머신 어디든 저장될 수 있습니다. 비-도커 프로세스들도 여기 파일들을 언제든 수정할 수 있습니다.        tmpfs mount는 호스트 메모리 시스템에만 저장됩니다. 호스트 파일 시스템에는 절대 저장되지 않습니다.  VolumeVolume은 완전히 도커에 의해서만 관리되어 호스트 머신의 디렉토리 구조나 OS에 독립적인, 도커에서 데이터를 유지하기 위한 권장되는 메커니즘입니다.또한 볼륨이 컨테이너를 사용하는 컨테이너의 크기를 늘리지 않고 컨테이너의 라이프사이클 외부에 존재하기 때문에 컨테이너 레이어에서 데이터를 유지하는 것보다 더 나은 선택인 경우가 많습니다.Volume은 docker volume create 명령어를 이용해 명시적으로 볼륨을 생성할 수도 있고, 컨테이너를 생성할 때 같이 볼륨을 생성할 수도 있습니다.볼륨은 도커 호스트 내의 디렉토리에 저장됩니다. 컨테이너에 마운트되는 볼륨이 바로 이것 입니다. volume은 bind mount와 비슷하지만 다른 점은 도커에 의해 관리 되고 호스트 머신으로부터 isolated 되었다는 점입니다.주어진 볼륨은 여러 컨테이너에 동시에 마운트 될 수 있습니다. 볼륨을 사용해 실행 중인 컨테이너가 없더라도 볼륨이 저절로 제거되지 않습니다. 만약 사용하지 않는 볼륨을 제거하고 싶다면 docker volume prune 명령어를 사용하면 됩니다.볼륨 드라이버를 사용해 클라우드 또는 리모트 호스트에 데이터를 저장할 수도 있습니다.Volume을 사용하기 좋은 경우  여러 컨테이너에 마운트하고 싶은 경우. 명시적으로 표현한 볼륨이 없으면 자동으로 생성하고 마운트 해줍니다.  도커 호스트의 파일 구조를 모르는 경우. bind mount와 달리 Volume은 볼륨 명으로 관리(bind mount는 디렉토리 경로)  로컬이 아닌 리모트 호스트 또는 클라우드 서버에 저장하고 싶은 경우  높은 성능의 I/O을 요구하는 경우. 볼륨은 호스트가 아닌 Linux VM에 저장. 따라서 읽고 쓰는데 있어 성능이 훨씬 뛰어납니다.  Docker Desktop에서 완전히 네이티브한 파일 시스템이 필요한 경우  백업, 데이터 통합이 필요한 경우Bind Mountbind mount는 volume에 비해 기능이 제한됩니다. bind mount를 사용하면 호스트 머신의 파일 또는 디렉토리가 컨테이너에 마운트됩니다. 파일 또는 디렉토리는 호스트 시스템의 절대 경로에서 참조됩니다. 반대로 volume을 사용하면 호스트 머신의 Docker 스토리지 디렉토리 내에 새 디렉토리가 생성되고 Docker가 해당 디렉토리의 내용을 관리합니다.파일 또는 디렉토리가 도커 호스트에 아직 존재하지 않아도 됩니다. 아직 존재하지 않는 경우 요청 시 생성됩니다. bind mount는 성능이 매우 뛰어나지만 사용 가능한 특정 디렉토리 구조가 있는 호스트 시스템의 파일 시스템에 의존합니다.bind mount를 사용하면 컨테이너에서 실행되는 프로세스를 통해 호스트 파일 시스템을 변경할 수 있습니다. 이는 호스트 시스템에 Docker가 아닌 프로세스에 영향을 미치는 등 보안에 안좋은 영향을 미칠 수 있는 강력한 기능입니다.Bind mount를 사용하기 좋은 경우  호스트 머신에 있는 설정 파일을 컨테이너와 공유하고 싶은 경우  파일 또는 디렉토리 구조가 항상 일관될 수 있는 경우tmpfs Mounttmpfs 마운트는 도커 호스트 또는 컨테이너 내에서 디스크에 유지되지 않습니다. 컨테이너 수명 동안 컨테이너가 비영구 상태 또는 중요한 정보를 저장하는 데 사용할 수 있습니다. 예를 들어, 내부적으로, swarm 서비스는 tmpfs 마운트를 사용하여 서비스의 컨테이너에 비밀을 탑재한다.tmpfs mount를 사용하기 좋은 경우  보안상의 이유 또는 대용량 데이터로 인한 성능 저하 우려로 데이터가 호스트 머신 또는 컨테이너에 유지되길 원하지 않는 경우.사용 방법Dockerfile  Dockerfile에는 볼륨 기능을 위해 VOLUME을 제공합니다.  이미지가 빌드되는 호스트 머신은 사용자에 따라 달라지므로 source는 지정할 수 없습니다.  VOLUME에서 표기하는 것은 오직 컨테이너안에 있는 볼륨의 destination입니다.VOLUME /myvolume  데이터가 사라지지 않도록 저장해두는 source는 컨테이너 생성/실행 시 표기할 수 있습니다.docker run -it -v $(pwd)/src:/src my-image  사실 Dockerfile에서 VOLUME은 사용하지 않는 것이 좋습니다. (어차피 source를 지정할 수 없으므로)docker run command컨테이너를 생성하거나 실행할 때 -v(--volume) or --mount 옵션을 이용해 볼륨을 마운트할 수 있습니다.  -v(--volume)# first field# volume옵션을 사용할 때는 명명된 볼륨이면 볼륨의 이름, 익명 볼륨이면 생략 가능# bind mount옵션의 경우 호스트 머신의 디렉토리 경로# second field는 컨테이너내의 마운트하고자 하는 경로# third field는 옵셔널, 예를 들어 ro(read only라는 의미)-v &amp;lt;first field&amp;gt;:&amp;lt;second field&amp;gt;:&amp;lt;thrid field&amp;gt;# 예시docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  --mount# type=volume, bind, tmpfs# source(src)# target(destination, dst)# readonly--mount &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, ..# 예시--mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app \  nginx:latest  볼륨 생성docker volume create my-voldocker volume ls-------------------------------DRIVER              VOLUME NAMElocal               my-voldocker volume inspect my-vol[    {        &quot;CreatedAt&quot;: &quot;2022-02-02T17:03:46Z&quot;,        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: {},        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;,        &quot;Name&quot;: &quot;my-vol&quot;,        &quot;Options&quot;: {},        &quot;Scope&quot;: &quot;local&quot;    }]docker volume rm my-vol  볼륨과 함께 컨테이너 실행# --mount flag 이용docker run -d \  --name devtest \  --mount source=myvol2,target=/app \  nginx:latest# -v flag 이용docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  읽기 전용 볼륨# --mount flag 이용docker run -d \  -it \  --name devtest \  --mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app,readonly \  nginx:latest# -v flag 이용docker run -d \  -it \  --name devtest \  -v &quot;$(pwd)&quot;/target:/app:ro \  nginx:latestdocker composedocker compose에 관한 포스트는 여기를 참고해주시면 감사드리겠습니다.version: &quot;3.9&quot;services:  frontend:    image: node:lts    volumes:      - myapp:/home/node/appvolumes:  myapp:참고  도커 공식문서: Manage data in Docker  도커 공식문서: Docker-compose volume configuration  DaleSeo: Docker 컨테이너에 데이터 저장 (볼륨/바인드 마운트)  stack overflow: Understanding “VOLUME” instruction in DockerFile",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series8'> <img src='/images/docker_17.png' alt='Docker의 볼륨 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series8'>Docker의 볼륨 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 아키텍처 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series7",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Desktop, Engine, Server, Host, Daemon  Docker Server          Docker Daemon      Docker Driver                  Storage Driver          Network Driver          Execdriver                      참고Desktop, Engine, Server, Host, Daemon도커를 공부하면서 Docker Desktop, Engine, Server, Host, Daemon이라는 용어들의 관계가 조금 헷갈렸었습니다. 지금까지 배웠던 내용을 토대로 다음 용어들을 짧게 정리해볼까 합니다.      Docker DesktopMac, Windows 환경에서의 도커 애플리케이션입니다. 도커는 리눅스 기반의 운영체제에서 동작하는 어플리케이션이지만 Docker Desktop을 통해 Mac, Windows에서도 사용할 수 있도록 해줍니다. 또한 Docker Engine뿐 아니라 Docker Compose도 기본적으로 함께 설치되며 Kubernetes도 클릭 한 번으로 설치가능하도록 해줍니다.        Docker Engine일반적으로 저희가 도커를 생각할 때 가지고 있는 기능들을 곧 Docker Engine이라고 합니다. 다시 말해 도커 컨테이너를 생성하기 위해 요청하는 Client, 실제 컨테이너를 생성하고 관리하는 Server를 포함하는 Client-Server Application을 말합니다.        Docker ServerDocker Client로 부터 REST API 형태로 요청을 받았을 때 그 요청을 토대로 실제로 컨테이너를 생성하고 관리하는 부분을 말합니다.        Docker HostDocker Host는 Docker Engine이 설치된 곳을 말합니다. 제 컴퓨터가 Linux기반의 운영체제였다면 제 컴퓨터 자체가 Host가 되었을 것이고, 만약 클라우드 환경에서 서버를 하나 빌려서 거기에 도커를 설치했다면 빌린 서버가 Host가 될 것입니다.    참고로 제가 지금 사용하고 있는 환경은 Mac입니다. Mac의 운영체제는 Unix 계열의 운영체제로 Linux와는 사용하는 커널이 약간 달라서 결론적으로 도커를 다이렉트로 설치할 수 없습니다. 그래서 macOS 위에 Linux Virtual Machine을 하나 더 띄우고 그 위에서 도커를 설치 사용하게 됩니다. 이렇게 사용하면 기본적으로 Linux VM에 2GB 정도의 메모리가 사용된다고 합니다.  (참고: How much overhead from running Docker on a Mac?)        Docer DaemonDocker Daemon은 Docker Server 안에 있는 핵심 요소 중 하나로 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.  지금까지 도커의 큰 그림에서의 구성요소에 대해 살펴보았습니다. 지금부터는 그 중 Docker Server의 내부에 대해서 조금 더 살펴보려고 합니다. 위의 그림을 보면 Docker Client가 요청을 하면 나머지는 Docker Server에서 실행이 이루어지는데 Docker Server가 요청을 수행하기 위해 내부적으로 어떤 과정을 거치는지 한 번 알아보겠습니다.Docker Server아래 그림은 Docker Server의 아키텍처를 보여주는 좋은 그림입니다. 비록 2014년도에 그려진 그림이어서 최근 버전의 도커와는 차이가 있을 수 있지만 도커의 기본 구성요소를 공부하는 데에는 좋은 자료라고 생각합니다.크게 두 개의 사각형 덩어리가 각각 Docker Daemon과 Docker Driver입니다.(개인적으로 Engine이라고 적힌 부분은 마치 엔진과 같은 역할을 한다는 뜻일 뿐 저희가 위에서 배운 Docker Engine을 뜻하는 건 아니라고 생각합니다.)Docker DaemonDocker daemon 은 docker engine 내에서 주로 client 및 registry, driver 의 중심에서 작업의 분배를 담당하는 중심점이라고 보면 됩니다. client 로부터의 HTTP 요청을 내부 job 단위(가장 기본적인 작업 실행 단위)로 처리할 수 있도록 분배합니다. 즉, HTTP server 의 역할과 함께 client 요청을 분배(route and distribute), scheduling 하고, 요청에 대한 적합한 Handler 를 찾습니다. 요청에 대해 실질적인 처리는 Handler 를 통해 다른 모듈 들에게 전달하여 수행하고 그 결과를 응답으로 작성하여 client 에게 제공합니다.Docker DriverDocker Driver 는 크게 세 가지 범주로 나눌 수 있습니다.  graphdriver : container image 관리  networkdriver : 가상 bridge 등 container 의 network 관리  execdriver : container 생성 관리Storage Drivergraphdriver는 Storage Driver 라고 이해하면 됩니다. /var/lib/docker 내에 저장되어 있는 container, image 관련 정보들을 이용하여 사용자에게 통합된 File System으로 제공하는 드라이버입니다. built-in graphdriver 로는 btrfs, vfs, auts, devmapper, overlay2 등이 있습니다. Storage Driver에 관한 내용은 이 포스트를 참고하시면 됩니다.Network Driver도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다. CNM을 구성하는 요소는 크게 다음과 같이 3가지가 있습니다.  Sandbox: 컨테이너의 Network의 많은 Endpoint를 설정하는 곳으로 Linux network namespace와 비슷한 개념으로 구현  Endpoint: 컨테이너 내의 eth 와 외부의 vth의 페어  Network: 네트워크는 직접적으로 통신을 할 수 있는 엔드포인트를 연결하는 역할2개의 Sandbox 안에 각각 Endpoint 요소를 하나 씩 만들고, 그 Endpoint 둘을 Network 이라는 요소에 연결해 컨테이너 간의 통신을 위한 네트워크를 구현할 수 있습니다. 이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver 입니다.Libnetwork provides the network control and management plane (native service discovery and load balancing). It accepts different drivers to provide the data plane (connectivity and isolation).Some of the network drivers that we can choose are:  bridge: it creates single-host bridge networks. Containers connect to these bridges. To allow outbound traffic to the container, the Kernel iptables does NAT. For inbound traffic, we would need to port-forward a host port with a container port.🦊 **Note**  Every Docker host has a default bridge network (docker0).  All new container will attach to it unless you override it (using --network flag).     MACVLAN: Multi-host network. Containers will have its own MAC and IP addresses on the existing physical network (or VLAN). Good things: it is easy and does not use port-mapping. Bad side: the host NIC has to be in promiscuous mode (most cloud provider does not allow this).  Overlay: it allows containers in different hosts to communicate using encapsulation. It allows you to create a flat, secure, layer-2 network.Note: Docker creates an Embedded DNS server in user-defined networks. All new containers are registered with the embedded Docker DNS resolver so can resolve names of all other containers in the same network.ExecdriverExecdriver는 컨테이너 생성 및 관리에 관한 역할을 담당합니다. 즉, 커널의 격리 기술을 이용하여 컨테이너를 생성하고 실행하는 역할을 합니다. Execdriver의 하위 드라이버인 Runtime driver로는 예전에는 리눅스의 LXC를 이용했지만 최근버전의 도커는 도커내에서 개발한 Docker native runtime driver인 libcontainer나 runc를 이용합니다.Execdriver에서 선택된 LXC 또는 native driver는 Linux Kernel 에서 제공하는 cgroups, namespace 등의 기능을 이용할 수 있는 interface를 제공하고, 이를 통해 도커는 컨테이너 생성 및 관리에 필요한 실질적인 기능들을 제공합니다.docker run 을 실행하면 이는 결국 execdriver -&amp;gt; runtime driver -&amp;gt; cgroups, namespace 등의 기능을 이용하는 인터페이스에 의해 container 환경이 마련되고 기동되는 것이다.참고  Rain.i님의 도커 컨테이너 까보기(4) – Docker Total Architecture 포스트  Maria Valcam, Docker: All you need to know — Containers Part 2",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series7'> <img src='/images/docker_13.png' alt='Docker의 아키텍처 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series7'>Docker의 아키텍처 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker 컨테이너에 저장된 데이터는 어떻게 될까?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series6",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Container Layer  UFS  CoW  Storage Driver  정리  참고도커를 공부하면서 궁금했던 것 중에 하나가 컨테이너에서 생성된 파일은 어디에 저장되어 있는걸까? 였습니다. 그동안 저는 도커에 다른 저장소를 마운트하면 컨테이너에서 생성된 데이터를 저장할 수 있고 그렇지 않다면 컨테이너가 삭제되면서 같이 사라진다라고 알고 있었는데 그러면 컨테이너가 사라지기 전까지는 어디에 저장되어 있는지 궁금해졌습니다.그러던 중 좋은 글을 공유해 놓은 블로그를 알게되어 이와 관련해 정리해보았습니다. (참고: Rain.i 블로그)Container Layer도커 컨테이너는 도커 이미지로부터 만들어진 인스턴스입니다. 도커 이미지를 토대로 여러 개의 컨테이너를 만들 수 있습니다. 예를 들어 우분투 운영체제를 제공하는 이미지를 이용해 어떤 컨테이너에는 파이썬을 설치하고, 어떤 곳에는 nginx를 설치해 웹 서버로 사용할 수도 있습니다. 이렇게 새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다. 이걸 보면 도커는 각각의 서비스를 컨테이너화 했을 뿐 아니라 컨테이너도 또 컨테이너화 한 것 같은 느낌이 드네요.도커가 컨테이너를 이런식으로 구현한 이유는 이미지의 상태를 최대한 그대로 보존하여 컨테이너를 계속 생성하더라도 토대가 변하지 않아 예상치 못한 오류를 예방할 수 있고 관리하기도 편합니다. 사용하는 입장에서도 어차피 컨테이너를 삭제하면 원래 기본 이미지 상태로 돌아가니까 걱정없이 컨테이너를 조작할 수 있을 것 입니다.우선 컨테이너를 생성하고 새로운 데이터를 생성하면 도커 상에서는 Container Layer에 저장된다는 것을 알았습니다. 그런데 Container Layer도 결국 도커를 띄운 호스트의 자원을 이용하기 때문에 제 컴퓨터(로컬이라면 데스크탑이나 노트북, 리모트라면 AWS의 EC2 정도) 어딘가에 저장이 되어 있을 것입니다. 이렇게 컨테이너들이 사용하는 이미지나 변경사항들은 모두 호스트 File system 의 /var/lib/docker 디렉토리 내에 저장된다. 이 영역을 Docker area 또는 Backing Filesystem 이라고 부르기도 한다.만약 컨테이너에서 생성된 파일을 버리지 않고 저장하고 싶다면 다음의 두 가지 방법을 사용할 수 있습니다.  Commit: 컨테이너 상에서 변경을 수행한 후 새로운 이미지로 만들어둔다.  Volume: 변경사항을 로컬 또는 외부 볼륨에 저장하도록 한다.UFS위의 내용을 읽다보면 이러한 의문이 생길 수 있습니다. ubuntu 이미지가 가지고 있던 Filesystem이 아닌 별도의 Filesystem에 Container Layer의 데이터가 저장이 되는데 왜 우리는 컨테이너를 사용할 때 이러한 사실을 몰랐을까? 그 이유는 바로 도커에서는 UFS(Union File System)라는 방식을 이용해 Image Layer와 Container Layer의 Filesystem을 하나로 통합해서 저희에게 제공해줍니다.이러한 UFS 방식의 장점은 무엇일까요? 가장 큰 장점은 Image Layer의 데이터를 여러 컨테이너가 공유할 수 있다는 점입니다. 공유한다는 것은 여러 개의 컨테이너를 띄우더라도 Image Layer의 데이터 용량은 단 1개만큼만 저장된다는 말입니다.CoW위의 그림과 같이 Image Layer의 a라는 파일을 a&#39;으로 수정할 때 Image Layer에서 파일이 수정되지 않고 Container Layer 위에서 새로 파일을 복사한 후 수정하는 것을 CoW(Copy on Write)라고 합니다. 이러한 기법을 통해 기존의 이미지에 대한 변경을 막을 수 있습니다. 하지만 Copy-on-Write 기법은 그 동작 구조 상 다음의 단점이 있습니다.  Performance Overhead: data 를 먼저 복제(Copy)한 후 변경을 수행해야함  Capacity Overhead: 원본 데이터 뿐 아니라, 변경된 데이터도 저장해야함따라서 되도록이면 중복 사용되고 수정되지 않을만한 데이터들을 이미지 레이어로 구성하는 것이 좋습니다.Storage Driver위에서 그동안 배운 UFS와 CoW 방식을 도커에서 쉽게 이용할 수 있는 것은 도커의 Storage Driver 덕분입니다. Storage Driver는 컨테이너 내에서의 파일 I/O 처리를 담당하는 드라이버입니다. Storage Driver는 Pluggable한 구조로 되어 있고 특성도 다릅니다. 또한 리눅스 배포판마다 지원하는 드라이버도 다르므로 자신의 workload에 맞는 Storage Driver를 선택해아 합니다.Storage Driver의 종류(참고: 도커 공식문서)리눅스 배포판별 지원하는 Storage Driver(참고: 도커 공식문서)Storage Driver와 Backing File SystemStorage Driver는 Container Layer의 데이터를 Backing filesystem(/var/lib/docker)으로 저장하고 사용자에게 layered filesystem으로 제공해 줍니다.   (참고로 볼륨 마운트는 이러한 Storage Driver의 도움없이 직접 Host의 Filesystem에 접근 가능합니다.)참고로 Storage Driver와 Backing filesystem 간에도 종속성이 있습니다.(참고: 도커 공식문서)Storage Driver와 graphDBStorage Driver는 사용자에게 최적의 통합된 파일 시스템을 제공하기 위해서는 layer 별 관계를 조회하고 key를 통해 특정 image를 검색하는 등, 이러한 일련의 정보 검색 및 관리하는 데이터베이스가 필요합니다. 이런 정보를 저장하고 있는 데이터베이스를 graphDB라고 합니다. (graphDB는 Storage Driver의 뇌와 같은 역할?)정리  UFS: Container Layer와 Image Layer의 파일이 통합되어 보인다  CoW: Image Layer 내의 파일을 원본은 유지하는 방향으로 파일을 수정할 수 있다  Storage Driver: 위의 기능들을 실제로 수행하는 드라이버  graphDB: Storage Driver가 최적의 실행을 하는데 필요한 정보를 저장하고 있는 SQLite기반 DB참고  도커 공식문서 About Storage Driver  Rain.i님의 도커 컨테이너 까보기(2) – Container Size, UFS 포스트  Davaom’s Tech Blog, [Docker] 컨테이너의 구조 포스트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series6'> <img src='/images/docker_5.png' alt='Docker 컨테이너에 저장된 데이터는 어떻게 될까?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series6'>Docker 컨테이너에 저장된 데이터는 어떻게 될까?</a> </h2><p class='article__excerpt'>새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part2]: 네트워크 장비",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series2",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  랜카드  허브  스위치  라우터랜카드랜카드는 유저의 데이터를 케이블에 실어서 허브나 스위치 혹은 라우터 등으로 전달해주고, 자신에게 온 데이터를 CPU에게 전달해주는 역할을 합니다. 랜카드는 어떤 환경에서 사용하는가에 따라 이더넷용 랜카드, 토큰링용 랜카드, FDDI, ATM용 랜카드 등으로 구분하지만 요즘은 90% 이상 이더넷용 랜카드를 사용합니다. 랜카드는 데스크탑의 메인보드에 기본적으로 붙어있고, 추가적으로 랜카드를 부착할 수도 있습니다.허브허브는 한마디로 정의하면 멀티포트(Multiport) 리피터(Repeater)라고 할 수 있습니다. 멀티포트는 포트가 많이 붙어있다는 뜻이고, 리피터는 들어온 데이터를 그대로 재전송한다는 의미입니다. 따라서 허브는 특정 포트에서 들어온 데이터를 나머지 포트로 데이터를 뿌려주는 역할을 합니다.허브를 이용한 네트워크의 예시를 하나 들어보겠습니다. 허브에도 이더넷 방식의 허브와 토큰링 방식의 허브가 있는데, 보통 이더넷 방식이 더 빠르기 때문에 이더넷 허브를 기준으로 얘기하도록 하겠습니다.허브로 연결된 컴퓨터 5대가 있을 때, 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하려고 합니다. 데이터를 허브로 보내주게 되면 허브는 일단 이 데이터를 허브내의 나머지 컴퓨터에 모두 전송합니다. 그러면 각각의 컴퓨터는 랜카드를 통해 들어온 데이터가 자신의 데이터가 맞는지 확인하고 자기의 것이 아니면 버리게 됩니다.여기서 중요한 점은 위에서 구성한 네트워크가 이더넷 네트워크이기 때문에 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하는 동안 다른 컴퓨터들은 데이터를 주고 받을 수 없다는 점입니다. 따라서 이렇게 같은 허브에 연결된 컴퓨터들은 모두 같은 콜리전 도메인에 있다고 말합니다. 그래서 만약 허브만으로 약 100대의 컴퓨터를 연결할 수 있는 네트워크를 연결했다면 1대의 컴퓨터가 통신하는 동안 나머지 99대의 컴퓨터들은 모두 기다리고 있어야 합니다. 이는 네트워크의 속도를 저하시키는 치명적인 원인이 됩니다.스위치위에서 살펴본 허브의 단점은 아주 명확합니다. 네트워크의 규모를 확장하기 위해 아무리 허브의 개수를 늘려도 하나의 콜리전 도메인이기 때문에 네트워크의 속도를 느리게 만든다는 점입니다. 이를 해결하기 위해서는 네트워크의 규모를 확장시켜도 콜리전 도메인이 커지지 않아야 하는데 이 때 등장한 것이 바로 스위치입니다. 사실 스위치 이전에 브릿지라는 것이 있었지만 요즘은 브릿지를 스위치가 대체하였기 때문에 스위치에 대해서 알아보도록 하겠습니다.스위치는 콜리전 도메인을 나눠준다고 했습니다. 예시를 보도록 하겠습니다.보시다시피 콜리전 도메인은 스위치에 의해 분리되었습니다. 이렇게 되면 왼쪽 콜리전 도메인 내의 컴퓨터들이 통신하는 동안 오른쪽에서도 이와 상관없이 통신이 가능합니다. (왼쪽 콜리전 도메인 내의 컴퓨터와 오른쪽 콜리전 도메인에 있는 컴퓨터가 통신하는 경우는 제외. 이런 경우에 해당하는지는 스위치에서 저장하고 있는 맥 주소 테이블을 바탕으로 판단합니다)라우터라우터가 필요한 이유는 또 스위치로는 해결하지 못하는 문제가 있기 때문입니다. 바로 브로드캐스트 도메인 분할 문제입니다. 브로드캐스트 도메인은 무엇이고 콜리전 도메인이랑 차이는 무엇인지 보도록 하겠습니다. 콜리전 도메인 영역은 A라는 컴퓨터가 B라는 컴퓨터에 데이터를 보내고 싶은데 어디로 보내야 할지 몰라 콜리전 도메인 영역 내의 모든 컴퓨터에 데이터를 보내는 영역입니다. 이 때는 B 컴퓨터를 제외한 나머지 컴퓨터는 데이터를 받아도 본인 것이 아니기 때문에 랜카드가 자신의 CPU까지 데이터를 보내지 않기 때문에 컴퓨터 성능에 영향을 주지 않습니다.콜리전 도메인은 컴퓨팅 성능에는 영향을 주지 않는다. 다만 네트워크 속도를 저하시킬 뿐이다. 브로드캐스트 도메인은 컴퓨팅 성능에도 영향을 주는 영역입니다. A라는 컴퓨터가 만약 어떤 데이터를 브로드캐스팅하면 브로드캐스트 도메인 내의 모든 컴퓨터는 이 데이터의 목적지가 됩니다. 그렇기 때문에 이 데이터는 브로드캐스트 도메인 내의 모든 컴퓨터에 도착하고 랜카드는 이 데이터를 CPU에 전달해주게 됩니다. 이런 일이 자주 발생하게 되면 모든 컴퓨터의 컴퓨팅 성능에 영향을 미칠 것입니다.라우터는 브로드캐스트 도메인을 분할해줍니다. 이 말은 네트워크를 분리해 준다는 말입니다. 브로드캐스팅은 하나의 네트워크 내에서만 일어납니다. 그래서 라우터를 이용하면 네트워크를 분할할 수 있고 브로드캐스트 도메인 영역을 분할할 수 있습니다.라우터는 네트워크를 분할한다.  🦊🐱 VLAN(Virtual LAN)위에서 라우터를 이용해 네트워크를 분할했습니다. 이런 역할을 스위치가 할 수도 있습니다. 스위치의 VLAN 기능을 이용하면 네트워크 영역을 분할할 수 있습니다. 하지만 딱 그 뿐입니다. 두 네트워크 간의 통신은 스위치로 할 수 없습니다. 두 네트워크가 통신하기 위해서는 라우터가 필요합니다.스위치의 VLAN을 이용하면 네트워크를 분할할 수 있다.라우터는 네트워크를 분할하고 두 네트워크 간의 통신을 가능하게 해준다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series2'> <img src='/images/net_7.png' alt='Network Series [Part2]: 네트워크 장비'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series2'>Network Series [Part2]: 네트워크 장비</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part1]: 네트워크 용어",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series1",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  네트워크  인터넷  LAN  이더넷  참고네트워크네트워크는 서로를 연결시켜 놓은 망이라고 할 수 있습니다. 그렇다고 연결만 해놓고 끝내면 되는 것은 아니고, 연결된 장비들끼리 대화(통신)를 할 수 있어야 합니다. 왜 이런 개념이 등장했을까요? 그 이유는 바로 자원을 공유하고 싶어서 였습니다.지금과 같이 개인마다 컴퓨터가 보급되기 이전에는 보통 터미널이라고 불리는 단말기 여러 대를 호스트 컴퓨터에 붙여서 사용했습니다. 그러다가 프린트기를 공유하기도 하고, 나중에는 하나의 호스트를 공유하는 것이 아니라 여러 대의 호스트를 공유하고자 하는 요구가 생겨나면서 지금의 네트워크로 발전하게 되었습니다.인터넷그럼 인터넷은 무엇일까요? 인터넷의 인터(Inter)는 연결을 의미합니다. 따라서 인터넷이란, 여러 개의 네트워크를 연결한 것이라는 의미를 가지고 있습니다.위의 네트워크에서 설명했듯이, 네트워크는 자원의 공유를 위해 등장하게 되었습니다. 이 당시에는 물론 그 범위가 회사 또는 특정 단체 내에서의 자원 공유였을 겁니다. 그러다가 사람들이 이제 다른 네트워크와도 정보를 공유하고 싶어졌습니다. 이 때 등장하게 된 개념이 바로 인터넷인 것입니다.인터넷의 특징은 하나의 프로토콜만을 사용한다는 것입니다. 프로토콜은 쉽게 말해 무엇인가를 하기 위해 정해놓은 규약을 의미합니다. 인터넷은 네트워크간의 통신을 의미하기 때문에, 인터넷에서 사용하는 프로토콜은 네트워크간의 통신을 위한 규약이라고 할 수 있습니다. 전세계에서 수많은 네트워크가 생성될텐데 이들간의 통신을 위한 규약을 정해놓지 않는다면, 이들을 연결하기란 불가능할 것입니다. 이를 위해 인터넷에서 모든 네트워크가 사용하는 프로토콜이 있는데 이를 TCP/IP라고 합니다.LANLAN은 Local Area Network의 약자로, 즉 한정된 공간에서 구성한 네트워크를 의미합니다.보통 집, 사무실, 학교, 회사 내에서 사용하는 네트워크를 LAN이라고 합니다. 반면 멀리 떨어진 지역과도 통신할 수 있도록 구성한 네트워크를 WAN(Wide Area Network)라고 합니다. 보통 네트워크내에서 인터넷을 사용할 수 있다면 이러한 네트워크를 WAN이라고 할 수 있습니다.이더넷네트워크를 공부하다 보면 이더넷(Ethernet)이라는 용어를 자주 듣게 됩니다. 이더넷은 네트워크에서 통신을 하는 방식 중 하나라고 생각하면 될 것 같습니다. 대표적으로 토큰링 방식과 이더넷 방식이 있는데 요즘에는 이더넷 방식이 성능적으로 훨씬 우수하기 때문에 대부분의 네트워크는 이더넷 방식을 사용하고 있다고 보면 됩니다.  토큰링: 네트워크내의 장비들 중 토큰을 가지는 장비만 데이터 전송, 전송 후 토큰 넘기는 방식  이더넷: 정해진 순서없이 모든 장비들이 데이터 전송, 데이터 간 충돌 발생하면 기다렸다 또 전송위에서 설명했듯이 이더넷은 네트워크 통신 방식의 일종으로 네트워크 내 장비들이 자유롭게 데이터를 전송하고, 충돌이 발생하면 랜덤한 시간을 기다린 후 다시 전송하는 방식으로 이를 CSMA/CD (Carrier Sense Multiple Access / Collision Detection)라고 합니다. 초기에는 이러한 방식보다 토큰링이 더 안정된 방식이라는 인식이 있었지만, 이더넷과 관련한 기술들이 계속 발전을 거듭하게 되면서 이제는 대부분 이더넷 방식으로 네트워크를 구성합니다.참고  성공과 실패를 결정하는 1%의 네트워크 원리 책  양햄찌가 만드는 세상: 맥 어드레스란 무엇인가?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series1'> <img src='/images/net_6.png' alt='Network Series [Part1]: 네트워크 용어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series1'>Network Series [Part1]: 네트워크 용어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series8",
      "date"     : "Jan 31, 2022",
      "content"  : "Table of Contents  Connection Client To Broker          Scenario 0: Client and Kafka running on the same local machine      Scenario 1: Client and Kafka running on the different machines      Scenario 2: Kafka and client running in Docker      Scenario 3: Kafka in Docker container with a client running locally                  Adding a new listener to the broker                    Scenario 4: Kafka running locally with a client in Docker container      원문: Confluent블로그Connection Client To Broker클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.  브로커와의 초기 연결. 연결이 되면 브로커는 클라이언트에게 연결 가능(resolvable and accessible from client machine)한 브로커의 엔드포인트 제공(advertised.listeners)  클라이언트와 연결 가능한 브로커와의 연결초기 연결은 producer = KafkaProducer(bootstrap_servers=[&quot;localhost:9092&quot;]) 와 같이 bootstrap_servers 중 하나의 서버와 초기 연결된다. 그러면 연결된 서버는 클라이언트에게 advertised.listeners를 노출해 연결되도록 한다.예시로 클라이언트와 카프카가 서로 다른 머신에 있는 경우를 보자.연결이 성공되는 경우는 다음과 같다.연결이 실패되는 경우는 다음과 같다.이러한 경우에는 advertised.listeners를 localhost:9092로 설정하면 안된다.Scenario 0: Client and Kafka running on the same local machinebootstrap_servers = &#39;localhost:9092&#39;advertised_listeners = &#39;localhost:9092&#39;  잘 동작한다.클라이언트에 전달되는 메타데이터는 192.168.10.83이다. 이 값은 로컬 머신의 IP 주소이다.Scenario 1: Client and Kafka running on the different machines카프카 브로커가 다른 머신에서 동작하는 경우를 살펴보자. 예를 들면 AWS, GCP와 같은 클라우드에서 생성한 머신여기 예제에서 클라이언트는 나의 노트북이고 카프카 브로커가 동작하고 있는 머신의 LAN은 asgard03이라고 해보자.초기 연결은 성공한다. 하지만 메타데이터에서 돌려주는 노출된 리스너는 localhost이다. 하지만 클라이언트의 localhost에는 카프카 브로커가 없으므로 연결은 실패한다.이 문제를 해결하기 위해서는 server.properties에서 advertised.listeners 값을 수정해 클라이언트에서 접근 가능한 올바른 호스트네임과 포트를 제공해주어야 한다.# advertised.listeners 수정 전advertised.listeners=PLAINTEXT://localhost:9092listeners=PLAINTEXT://0.0.0.0:9092# advertised.listeners 수정 후advertised.listeners=PLAINTEXT://asgard03.moffatt.me:9092listeners=PLAINTEXT://0.0.0.0:9092Scenario 2: Kafka and client running in Docker도커를 이용할 때 기억해야할 점은 도커는 컨테이너를 통해 그들만의 작은 세상을 만든다는 것이다. 컨테이너는 자체적인 호스트네임, 네트워크 주소, 파일 시스템을 가지고 있다. 따라서 컨테이너를 기준으로 localhost는 더이상 나의 노트북이 아니다. 도커 컨테이너에서 localhost는 컨테이너 자기 자신이다.여기서는 카프카와 클라이언트를 모두 각각 도커 호스트 위에 컨테이너로 만들어 본다.클라이언트를 컨테이너로 만들어주는 Dockerfile이다.FROM python:3# We&#39;ll add netcat cos it&#39;s a really useful# network troubleshooting toolRUN apt-get updateRUN apt-get install -y netcat# Install the Confluent Kafka python libraryRUN pip install confluent_kafka# Add our scriptADD python_kafka_test_client.py /ENTRYPOINT [ &quot;python&quot;, &quot;/python_kafka_test_client.py&quot;]위의 메니페스트를 이용해 클라이언트 이미지를 만든다.docker build -t python_kafka_test_client .카프카 브로커를 생성하자.docker network create rmoff_kafkadocker run --network=rmoff_kafka --rm --detach --name zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:5.5.0docker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0쥬키퍼와 카프카 브로커가 컨테이너로 돌아가고 있다.$ docker psIMAGE                              STATUS              PORTS                          NAMESconfluentinc/cp-kafka:5.5.0        Up 32 seconds       0.0.0.0:9092-&amp;gt;9092/tcp         brokerconfluentinc/cp-zookeeper:5.5.0    Up 33 seconds       2181/tcp, 2888/tcp, 3888/tcp   zookeeper위에서 우리는 우리만의 도커 네트워크를 만들었고 이제 이 네트워크를 통해 클라이언트와 브로커가 통신하도록 해보자$ docker run --network=rmoff_kafka --rm --name python_kafka_test_client \        --tty python_kafka_test_client broker:9092결과를 보면 초기 연결은 성공하지만, 메타데이터로 localhost를 돌려주기 때문에 프로듀서와 클라이언트의 연결은 실패된다.이를 해결하려면 advertise.listeners의 호스트네임을 컨테이너 이름으로 바꿔줘야 한다.# 수정 전-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \# 수정 후 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \최종적으로 브로커 설정을 다음과 같이 고칠 수 있다.docker stop brokerdocker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0Scenario 3: Kafka in Docker container with a client running locally위의 Scenario 2와 비교하여 클라이언트가 컨테이너화 되어 있다가 여기서는 따로 컨테이너화 되지 않고 로컬 머신 위에 있다. (이러한 차이로 위에서 하던 방식이 왜 안되는 건지 모르겠다…)로컬에 실행하는 클라이언트는 따로 네트워크가 구성되어 있지 않다. 그렇기 때문에 따로 특정 트래픽을 받기 위해서는 로컬의 포트를 열어 이를 통해 통신해야 한다. 아래 그림과 같이 9092:9092 포트를 열었다고 해보자. 클라이언트가 로컬의 9092포트 엔드포인트로 접근하기 위해서는 bootstrap_servers=&#39;localhost:9092&#39;로 해야 한다. advertised.listeners는 broker:9092로 해야 한다(클라이언트와 localhost관계가 아니므로).문제는 클라이언트 입장에서 broker:9092는 resolvable하지 않다.Adding a new listener to the broker이 문제를 해결하는 방법은 다수의 리스너를 만드는 것이다....    ports:      - &quot;19092:19092&quot;    environment:      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT...Scenario 4: Kafka running locally with a client in Docker container이런 상황이 잘 있지는 않지만, 어쨋든 이런 경우에 대한 해결책은 있다. 다만 좀 임시방편적일 뿐이다.만약 맥에서 도커가 동작하고 있다면, host.docker.internal을 이용할 수 있다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-31T21:01:35+09:00'>31 Jan 2022</time><a class='article__image' href='/kafka-series8'> <img src='/images/kafka_54.png' alt='Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series8'>Kafka Series [Part8]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]</a> </h2><p class='article__excerpt'>클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part7]: Kafka Listeners – Explained[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series7",
      "date"     : "Jan 30, 2022",
      "content"  : "Table of Contents  Kafka Listeners  Why can I connect to the broker, but the client still fails?  HOW TO: Connecting to Kafka on Docker  HOW TO: Connecting to Kafka on IaaS/Cloud          Option 1: External address is resolvable locally      Option 2: External address is NOT resolvable locally        Exploring listeners with Docker원문: Confluent블로그읽어보면 좋은 포스트Kafka Listeners카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners(또는 도커 이미지를 사용할 경우 KAFKA_ADVERTISED_LISTENERS)를 external IP 주소로 설정해야 합니다.아파치 카프카는 분산 시스템입니다. 데이터는 리더 파티션으로부터 쓰고 읽어지며 리더 파티션은 어떤 브로커에도 있을 수 있습니다. 그래서 클라이언트가 카프카에 연결되기 위해서는 해당 리더 파티션을 가지고 있는 브로커가 누구인지에 대한 메타데이터를 요청합니다. 이 메타데이터에는 리더 파티션을 가지는 브로커의 엔드포인트 정보를 포함하고 있으며 클라이언트는 이 정보를 이용해 카프카와 연결될 것입니다.만약 카프카가 도커와 같은 가상머신이 아닌 bare metal 위에서 동작한다면 이 엔드포인트는 그저 hostname이나 localhost 정도가 될 것입니다. 하지만 조금 더 복잡한 네트워크 환경 또는 멀티 노드 환경으로 오게 되면 조금 더 주의가 필요하게 됩니다.초기에 브로커가 연결되면 실제로 리더 파티션을 가지는 브로커의 host와 IP의 정보를 돌려줍니다. 이러한 과정은 단일 노드 환경에서도 마찬가지입니다.KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXTKAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOBserver.properties  KAFKA_LISTENERS: 카프카가 리스닝하기 위해 노출하는 host/IP와 port  KAFKA_ADVERTISED_LISTENERS: 클라이언트에게 알려주는 리스너의 host/IP와 port 리스트  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 각 리스너들이 사용하는 security protocol  KAFKA_INTER_BROKER_LISTENER_NAME: 브로커들 간의 통신을 위해 사용하는 리스너브로커에 연결되면 연결된 리스너가 반환됩니다. kafkacat은 이러한 정보를 알아보는 유용한 툴입니다. -L을 이용하면 연결된 리스너에 관한 메타데이터를 얻을 수 있습니다.# 9092포트로 연결시, localhost:9092 리스너가 반환$ kafkacat -b kafka0:9092 \           -LMetadata for all topics (from broker -1: kafka0:9092/bootstrap):1 brokers:  broker 0 at localhost:9092# 29092포트로 연결시, kafka0:29092 리스너가 반환$ kafkacat -b kafka0:29092 \           -LMetadata for all topics (from broker 0: kafka0:29092/0):1 brokers:  broker 0 at kafka0:29092Why can I connect to the broker, but the client still fails?초기 브로커 연결에 성공했다고 하더라도, 브로커가 반환하는 메타데이터 안에 있는 주소로 여전히 클라이언트가 접근하지 못하는 경우가 있습니다.      AWS EC2 인스턴스에 브로커를 만들어 로컬 머신에서 EC2에 있는 브로커로 메세지를 보내보려고 합니다. external hostname은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com입니다. 로컬 머신과 EC2가 포트포워딩을 통해 연결되었는지 확인해보겠습니다.        우리의 로컬 머신은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com을 54.191.84.122으로 성공적으로 리졸브(resolve) 합니다.  $ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -LMetadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):1 brokers:  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092      hostname이 ip-172-31-18-160.us-west-2.compute.internal인 리스너를 반환합니다.        하지만 인터넷을 통해 ip-172-31-18-160.us-west-2.compute.internal은 not resolvable해서 클라이언트는 브로커에 메세지 전송을 실패합니다.  $ echo &quot;test&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time  브로커가 설치된 서버의 클라이언트로는 문제없이 동작한다.$ echo &quot;foo&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginningfoo이러한 일이 발생하는 이유는 9092포트로 연결하는 리스너가 내부 리스너이기 때문이라고 한다. 그래서 브로커가 설치된 서버의 내부에서만 resolvable한 hostname인 ip-172-31-18-160.us-west-2.compute.internal을 리턴한다.HOW TO: Connecting to Kafka on Docker도커에서 동작하기 위해서는 카프카의 두 개의 listener를 지정해야 한다.      도커 네트워크 내에서의 통신: 이것은 브로커간의 통신 또는 도커 안의 다른 컴포넌트와의 통신을 의미한다. 이를 위해서는 도커 네트워크 안에 있는 컨테이너의 호스트네임을 사용해야 한다. 각각의 브로커는 컨테이너의 호스트네임을 통해 서로 통신하게 될 것이다.        도커가 아닌 네트워크로부터의 트래픽: 이것은 도커를 실행하는 서버에서 로컬로 동작하는 클라이언트가 될 수 있다. 이러한 경우 도커를 실행하는 서버(localhost)에서 컨테이너의 포트에 연결할 수 있다. 아래의 도커 컴포즈 스니펫을 한 번 보자.  […]kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      […]      # For more details see See https://rmoff.net/2018/08/02/kafka-listeners-explained/      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB[…]      도커 네트워크 내에 클라이언트가 있다면 클라이언트는 호스트네임 kafka0, 29092 포트를 이용한 BOB 리스너를 통해 브로커와 통신할 것입니다. 각각의 컨테이너(클라이언트, 브로커)는 kafka0를 도커 내부 네트워크를 통해 resolve합니다.        도커를 실행하는 호스트 머신(VM)에 있는 외부 클라이언트의 경우, 호스트 네임 localhost, 9092 포트를 이용한 FRED 리스너를 통해 브로커와 통신한다.        도커를 실행하는 호스트 머신(VM) 밖에 있는 외부 클라이언트는 위의 리스너를 통해 통신할 수 없다. 왜냐하면 kafka0도 localhost도 모두 resolvable하지 않기 때문이다.  HOW TO: Connecting to Kafka on IaaS/Cloud도커와의 차이점은, 도커에서 외부의 연결은 단순히 localhost에서 이루어진 반면, 클라우드 호스트 기반의 카프카는 클라이언트가 localhost에 존재하지 않는다는 것이다.더 복잡한 것은 도커 네트워크가 호스트의 네트워크와는 크게 분리되어 있지만 IaaS에서는 외부 호스트 이름이 내부적으로 확인 가능한 경우가 많기 때문에 이러한 문제가 실제로 발생할 경우 호스트 이름이 잘못될 수 있다.브로커에 연결할 외부 주소가 브로커에게 로컬로 확인할 수 있는지 여부에 따라 두 가지 방법이 있다.Option 1: External address is resolvable locallyEC2 인스턴스의 IP 주소는 기본적으로 External IP. 만약 local에서 resolvable하다면, 로컬 내의 클라이언트, 외부 클라이언트 모두 이를 통해 통신 가능. 다만 외부 클라이언트는 밑의 설정만 추가해주면 된다.advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092Option 2: External address is NOT resolvable locally만약 로컬 내에서 resolvable하지 않다면, 두 가지 리스너가 필요하다.  VPC 내에서의 통신을 위해 local에서 resolvable한 Internal IP를 통해 내부에서 리슨한다  VPC 밖, 예를 들어 나의 노트북에서 접속하려는 경우 인스턴스의 External IP가 필요하다listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTadvertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092inter.broker.listener.name=INTERNALExploring listeners with Docker  Listener BOB (port 29092) for internal traffic on the Docker network  Listener FRED (port 9092) for traffic from the Docker host machine (localhost)  Listener ALICE (port 29094) for traffic from outside, reaching the Docker host on the DNS name never-gonna-give-you-up---version: &#39;2&#39;services:  zookeeper:    image: &quot;confluentinc/cp-zookeeper:5.2.1&quot;    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000  kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      KAFKA_BROKER_ID: 0      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot;      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100  kafkacat:    image: confluentinc/cp-kafkacat    command: sleep infinity",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-30T21:01:35+09:00'>30 Jan 2022</time><a class='article__image' href='/kafka-series7'> <img src='/images/kafka_36.png' alt='Kafka Series [Part7]: Kafka Listeners – Explained[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series7'>Kafka Series [Part7]: Kafka Listeners – Explained[번역]</a> </h2><p class='article__excerpt'>카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners를 external IP 주소로 설정해야 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part2]: 컴퓨터의 기본 구조",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series2",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  컴퓨터의 기본 구성          하드웨어의 구성      폰 노이만 구조      하드웨어 사양 관련 용어        CPU          CPU의 기본 구성      CPU의 명령어 처리 과정        컴퓨터 성능 향상 기술          버퍼      캐시      인터럽트        참고컴퓨터의 기본 구성하드웨어의 구성컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다. 이 중에 CPU와 메모리는 필수장치로 구분되고 나머지는 주변장치로 구분됩니다.폰 노이만 구조오늘날 대부분의 컴퓨터는 CPU, 메모리, 저장장치, 입출력장치가 버스로 연결되어 있는 폰 노이만 구조를 따르고 있습니다. 폰 노이만 구조 이전에는 컴퓨터가 하드와이어링(hard wiring) 형태로 용도에 맞게 매번 컴퓨터의 전선을 새로 연결해야 했습니다. 이러한 문제를 해결하기 위해 수학자 존 폰 노이만(John von Neumann)은 프로그램만 교체하여 메모리에 올리는 방법을 제안했습니다. 이러한 폰 노이만 구조 덕분에 오늘날에는 프로그래밍 기술을 이용해 컴퓨터로 다양한 작업을 할 수 있게 되었습니다.폰 노이만 구조의 가장 중요한 특징은, 모든 프로그램은 메모리에 올라와야 실행할 수 있다는 것입니다. 예를 들어 하드디스크에 워드 프로그램과 문서가 저장되어 있어도 실행을 하기 위해서는 메모리에 올라와야 합니다. 운영체제 또한 프로그램이기 때문에 메모리에 올라와야 실행이 가능합니다.하드웨어 사양 관련 용어  CPU 클럭(clock): 초당 CPU내의 트랜지스터가 열고 닫히는 횟수(사이클 수) 하나의 사이클에 여러 개의 명령어가 완료되는 경우도 있고, 하나의 명령어가 여러 사이클에 걸쳐서 완료되기도 함      바이트(byte): 저장장치의 기억 용량 단위                            용량 단위          용량                          1B          1byte                          1KB          2^10byte = 1024B                          1MB          2^20byte = 1024KB                          1GB          2^30byte = 1024MB                          1TB          2^40byte = 1024GB                      버스(bus): 시스템 버스는 메모리와 주변장치를 연결하는 버스로 메인보드의 클럭속도를 나타내는 지표이며, CPU 내부 버스는 CPU 내부 부품들을 연결하는 버스로 CPU 클럭 속도와 같음. CPU 버스 속도가 시스템 버스의 속도보다 훨씬 빠름CPUCPU(Central Processing Unit)은 중앙처리장치라고 하며 메모리에 올라온 프로그램의 명령어를 해석하여 실행하는 장치입니다. 따라서 중앙 처리 장치(CPU)는 컴퓨터 부품과 정보를 교환하면서 컴퓨터 시스템 전체를 제어하는 장치로, 모든 컴퓨터의 작동과정이 중앙 처리 장치(CPU)의 제어를 받기 때문에 컴퓨터의 두뇌에 해당한다고 할 수 있습니다.🦊 32bit CPU흔히 CPU를 얘기할 때 32bit CPU, 64bit CPU라고 하는데 이 때 32bit는 CPU가 메모리에서 데이터를 읽거나 쓸 때 한 번에 처리할 수 있는 데이터의 최대 크기를 말합니다.CPU의 기본 구성  산술논리 연산장치(ALU): 산술 연산(덧셈, 뺄셈 등)과 논리 연산(AND, OR 등)을 수행하는 부분  제어장치(control unit): 명령어를 해석해 제어 신호를 보냄으로써 작업을 지시하는 부분  레지스터(resister): CPU 내에 데이터를 임시로 보관하는 부분CPU의 명령어 처리 과정CPU는 메모리에 올라온 프로그램을 실행하기 위해서는 컴파일러를 이용해 코드를 기계어로 바꿔줘야 합니다. 이 기계어를 사람이 이해하기 쉽게 일대일 대응시켜 기호화한 어셈블리어가 있는데 어셈블리어를 살펴보면 CPU가 어떤 식으로 명령어를 내리고 처리하는지 볼 수 있습니다.# C언어int D2 = 2, D3 = 3, sum;sum = D2 + D3;# 어셈블리어LOAD mem(100), register 2; # 메모리 100번지에 있는 값을 레지스터2에 로드LOAD mem(120), register 3; # 메모리 120번지에 있는 값을 레지스터3에 로드ADD register 5, resister 2, register 3; # 레지스터2와 레지스터3에 저장된 값을 더해 레지스터5에 저장MOVE register 5, mem(160); # 레지스터5에 저장된 값(5)을 메모리 160번지로 이동위의 명령어는 명령어 레지스터에 저장되고 제어장치는 저장된 명령어를 해석하고 알맞은 제어 신호를 보냄으로써 동작을 수행합니다. 이러한 제어 신호는 제어버스를 통해 메모리와 주변장치에 전달합니다.컴퓨터 성능 향상 기술현재 컴퓨터 구조의 가장 큰 문제는 CPU와 다른 장치간의 작업 속도가 다르다는 것입니다. CPU 내부 버스의 속도가 시스템 버스의 속도보다 빠르기 때문에, 메모리를 비롯한 주변장치의 속도가 CPU의 속도를 따라가지 못하고 있습니다. 여기서는 이러한 속도 차이를 개선하기 위해 개발된 기술 중 운영체제와 관련된 기술을 살펴보겠습니다.버퍼버퍼(buffer)는 속도에 차이가 있는 두 장치 사이에서 그 차이를 완화하는 역할을 합니다. 예를 들어 저장장치에서 메모리로 데이터를 읽어올 때 데이터를 하나씩 전송하는 것보다 일정량의 데이터를 모아서 한꺼번에 전송하면 속도를 향상시킬 수 있습니다. (일상생활에서 물건을 하나씩 나르는 것보다 바구니에 물건을 일정량 담아서 옮기는 것이 더 빠릅니다. 특히 거리가 먼 경우에는 그 차이가 더 클 것입니다.) 버퍼는 이러한 바구니 역할을 합니다.캐시캐시(cache)는 메모리와 CPU간의 속도 차이를 완화하기 위한 용도로 메모리의 데이터를 미리 가져와 저장해두는 임시 장소입니다. 캐시 또한 버퍼의 일종으로 CPU가 앞으로 사용할 것으로 예상되는 데이터를 미리 가져다 놓습니다(prefetch).캐시는 CPU 안에 있으며 CPU 내부 버스의 속도로 동작합니다. 캐시는 메모리의 내용 중 일부를 미리 가져오고, CPU는 메모리에 접근하기 전에 캐시를 먼저 방문해 원하는 데이터가 있는지 찾아봅니다. 캐시에서 원하는 데이터를 찾은 경우를 캐시 히트(cache hit)라고 합니다. 일반적인 컴퓨터의 캐시 적중률은 약 90%입니다.캐시 적중률을 높이기 위해 캐시는 내부적으로 현재 위치와 가까이 위치한 데이터를 가져옵니다. 캐시 용량이 높은 캐시를 구매할 수도 있지만 가격이 비쌉니다.인터럽트초기의 컴퓨터 시스템에는 주변장치가 많지 않아 CPU가 직접 입출력장치에서 데이터를 가져오거나 보냈는데 이러한 방식을 폴링(polling)이라고 합니다. 오늘날에는 주변장치가 많아 CPU가 모든 입출력에 관여하면 작업 효율이 현저하게 떨어집니다. 이러한 문제를 해결하기 위해 등장한 것이 인터럽트(interrupt) 방식입니다.CPU는 데이터를 가져오거나 보낼 때 직접하지 않고, 입출력 관리자에게 명령을 보냅니다. 입출력 관리자가 메모리에 가지고 오거나 메모리의 데이터를 저장장치로 옮기는 동안 CPU는 계속 다른 작업을 할 수 있습니다. 입출력 관리자가 데이터 전송을 완료하고 나면 완료 신호를 CPU에 보내는데 이를 인터럽트라고 합니다.인터럽트 방식을 이용하면 데이터의 입출력이 이루어지는 동안 CPU는 다른 작업을 하고 있을 수 있습니다.참고  쉽게 배우는 운영체제 책 참고  i’m developer, not coder블로그 참고  인텔 홈페이지  위키백과",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/os-series2'> <img src='/images/os_4.png' alt='OS Series [Part2]: 컴퓨터의 기본 구조'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series2'>OS Series [Part2]: 컴퓨터의 기본 구조</a> </h2><p class='article__excerpt'>컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part7]: StatefulSet과 Headless의 조합",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series7",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series7'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part7]: StatefulSet과 Headless의 조합'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series7'>Kubernetes Series [Part7]: StatefulSet과 Headless의 조합</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part6]: ConfigMap과 Secret",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kubernetes-series6'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part6]: ConfigMap과 Secret'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series6'>Kubernetes Series [Part6]: ConfigMap과 Secret</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents  Kafka as a Datastore  Kafka as a Data Loss Prevention Tool  참고Kafka as a DatastoreKafka can be used for storing data. You may be wondering whether Kafka is a relational or NoSQL database. The answer is that it is neither one nor the other.Kafka, as an event streaming platform, works with streaming data. At the same time, Kafka can store data for some time before removing it. This means that Kafka is different from traditional message queues that drop messages as soon as they are read by the consumer. The period during which the data is stored by Kafka is called retention. Theoretically, you can set this period to forever. Kafka also can store data on persistent storage, and replicates data over the brokers inside a cluster. This is just another trait that makes Kafka look like a database.Why then isn’t Kafka used widely as a database, and why aren’t we addressing the idea that this might be a data storage solution? The simplest reason for this is because Kafka has some peculiarities that are not typical for general databases. For example, Kafka also doesn’t provide arbitrary access lookup to the data. This means that there is no query API that can be used to fetch columns, filter them, join them with other tables, and so on. Actually, there is a Kafka StreamsAPI and even an ksqlDB. They support queries and strongly resemble traditional databases. But they are like scaffolds around Kafka. They act as consumers that process data for you after it’s consumed. So, when we talk about Kafka in general and not its extensions, it’s because there isn’t a query language like SQL available within Kafka to help you access data. By the way, modern data lake engines like Dremio can solve this issue. Dremio supports interactions using SQL with data sources that don’t support SQL natively. So, for example, you can persist data from Kafka streams in AWS S3, and then access it using Dremio AWS edition.Kafka is also focused on the paradigm of working with streams. Kafka is designed to act as the core of applications and solutions that are based on streams of data. In short, it can be seen as a brain that processes signals from different parts of the body and allows an organ to work by interpreting those signals. The aim of Kafka is not to replace more traditional databases. Kafka lives in a different domain, and it can interact with databases, but it is not a replacement for databases. Kafka can be easily integrated with databases and cloud data lake storage such as Amazon S3 and Microsoft ADLS with the help of Dremio.Keep in mind that Kafka has the ability to store data and the data storage mechanism is quite easy to understand. Kafka stores the log of records (messages) from the first message up till now. Consumers fetch data starting from the specified offset in this log of records. This is the simplified explanation of what it looks like:The offset can be moved back in history which will force the consumer to read past data again.Because Kafka is different from traditional databases, the situations where it can be used as a data store are also somewhat specific. Here are some of them:  To repeat the processing of the data from the beginning when the logic of processing changes;  When a new system is included in the processing pipeline, and it needs to process all previous records from the very beginning or from some point in time. This features helps avoid copying the full dump of one database to another;  When consumers transform data and save the results somewhere, but for some reason, you need to store the log of data changes over time.Later in this article, we will look at an example of how Kafka can be used as a data store in a use case similar to the first one described above.Kafka as a Data Loss Prevention ToolA lot of developers choose Kafka for their projects because it provides a high level of durability and fault-tolerance. These features are achieved by saving records on disk and replicating data. Replication means that the same copies of your data are located on several servers (Kafka brokers) within the cluster. Because the data is saved on disk, the data is still there even if the Kafka cluster becomes inactive for some period of time. Thanks to the replication, the data stays protected even when one or several of the clusters inside the broker are damaged.After data is consumed, it is often transformed and/or saved somewhere. Sometimes data can become corrupt or lost during data processing. In such cases, Kafka can help restore the data. If needed, Kafka can provide a way to execute operations from the beginning of the data stream.You should be aware that the two main parameters used to control the data loss prevention policy are the replication factor and the retention period. The replication factor shows how many redundant copies of data for the given topic are created in the Kafka cluster. To support fault-tolerance you should set the replication factor to a value greater than one. In general, the recommended value is three. The greater the replication factor, the more stable the Kafka cluster. You can also use this feature to place Kafka brokers closer to the data consumers while having replicas on geographically remote brokers at the same time.The retention period is the time during which Kafka saves the data. It is obvious that the longer the period, the more data you will save, and the more data you will be able to restore in case something bad happens (for example, the consumer goes down due to power failure, or the database loses all data as the result of an accidental wrong database query or hacker attack, etc.).참고  Towards Data Science, Using Kafka as a Temporary Data Store and Data-loss Prevention Tool in The Data Lake",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kafka-series6'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series6'>Kafka Series [Part6]: 데이터 파이프라인에서 카프카를 사용하는 목적</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part1]: 운영체제의 개요",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series1",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  CPU 스케줄링과 프로세스 관리: CPU 소유권을 어떤 프로세스에 할당할지, 프로세스의 생애주기를 관리  메모리 관리: 한정된 메모리를 어떤 프로세스에 얼마나 할당할지 관리  파일시스템 관리: 파일을 디스크에 어떤 방법으로 보관할지 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/os-series1'> <img src='/images/os_1.png' alt='OS Series [Part1]: 운영체제의 개요'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series1'>OS Series [Part1]: 운영체제의 개요</a> </h2><p class='article__excerpt'>운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series5",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  카프카 CLI          토픽 생성      토픽 리스트      토픽 삭제        처리량을 높이고 싶은 경우  지연율을 낮추고 싶은 경우  순서 보장, 정확히 한 번 전송  토픽 내 데이터를 주기적으로 삭제  참고카프카 CLI토픽 생성토픽 리스트토픽 삭제처리량을 높이고 싶은 경우지연율을 낮추고 싶은 경우순서 보장, 정확히 한 번 전송토픽 내 데이터를 주기적으로 삭제참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/kafka-series5'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series5'>Kafka Series [Part5]: 실습을 통한 카프카 명령어와 옵션</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose를 이용해 데이터 파이프라인 구축하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series5",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  참고Elasticsearch 6.x는 arm 아키텍처 지원 X (도커 기준)나는 m1이라서 Elasticsearch 7.x 이상 써야됨Elasticsearch-Hadoop은 어떤 버전에서든 7.x 정식 지원 안함.MongoDB로 생각해보자……………ES와 Spark 컨테이너 실행version: &#39;3.2&#39;services:  elasticsearch:    image: elasticsearch:7.16.2    hostname: elasticsearch    ports:      - &quot;9200:9200&quot;      - &quot;9300:9300&quot;    environment:      ES_JAVA_OPTS: &quot;-Xmx256m -Xms256m&quot;      ELASTIC_PASSWORD: changeme      ELASTIC_USERNAME: elastic      discovery.type: single-node    spark:    image: kimziont:spark    hostname: spark    ports:      - &quot;4040:4040&quot;    tty: trueSpark에서 ElasticSearch를 쓰려면 커넥터가 필요한 것 같다. Elastic에서 이를 위해 elasticsearch-hadoop를 제공하는데 우선 다운을 받아야 한다.wget https://artifacts.elastic.co/downloads/elasticsearch-hadoop/elasticsearch-hadoop-6.4.1.zipunzip elasticsearch-hadoop-6.4.1.zipmkdir -p /app/spark/jars/ext/elasticsearch-hadoopmv elasticsearch-hadoop-6.4.1 /app/spark/jars/ext/elasticsearch-hadoop/6.4.1설치가 완료되었으면 스파크를 실행할 때 설치한 경로를 실행 인자로 주면되는 것 같다.pyspark에서 사용할 때는 아래와 같다.# pyspark 실행 커맨드pyspark --driver-class-path=/app/spark/jars/ext/elasticsearch-hadoop/6.4.1/dist/elasticsearch-hadoop-6.4.1.jardf = spark.read.format(&quot;org.elasticsearch.spark.sql&quot;).option(&quot;es.read.field.as.array.include&quot;, &quot;NerArray&quot;).option(&quot;es.nodes&quot;,&quot;localhost:9200&quot;).option(&quot;es.nodes.discovery&quot;, &quot;true&quot;).load(&quot;index명&quot;) df.registerTempTable(&quot;ner&quot;) spark.sql(&quot;show tables&quot;).show() spark.sql(&quot;select * from ner&quot;).show()spark-shell을 사용할 때는 아래와 같다.spark-shell --jars /app/spark/jars/ext/elasticsearch-hadoop/6.4.1/dist/elasticsearch-hadoop-6.4.1.jar참고      Docker 공식문서    Spark와 ElasticSearch 연동하기  elasticsearch-hadoop  Jason Heo’s Blog: Spark 3.0에서 elasticsearch hadoop 사용하기",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T00:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/docker-series5'> <img src='/images/datapipeline_logo.png' alt='Docker Compose를 이용해 데이터 파이프라인 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series5'>Docker Compose를 이용해 데이터 파이프라인 구축하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose 속성 알아보기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series4",
      "date"     : "Jan 26, 2022",
      "content"  : "Table of Contents  도커 컴포즈(Docker Compose)  Top-level keys  services  참고도커 컴포즈(Docker Compose)보통 하나의 프로젝트에는 여러 가지 서비스를 포함하고 있습니다. 예를 들어 웹 서버 기반의 프로젝트를 진행한다고 할 때 프로젝트의 기본적인 아키텍처는 다음과 같습니다.(Netflix Tech Blog 참고)위의 그림을 보면 웹 서버뿐만 아니라 데이터 웨어하우스, 데이터베이스, 검색 서비스 등과 같은 것들이 서로 유기적으로 연결되어 있습니다. 이런 경우 프로젝트를 상용화 하기 위해서는 어떻게 해야 할까요? 각각의 서비스를 도커를 이용해 컨테이너로 띄우고 서로를 연결시켜주어야 합니다. 이는 앞에서 배웠던 도커 이미지를 만들기 위한 Dockerfile 단계에서는 할 수 없고 도커 컨테이너를 실행(docker run)하는 단계에서 컨테이너를 띄우는 순서, 데이터 공유, 환경변수 등을 설정해서 실행해야 합니다.우리가 도커를 배우는 이유는 서비스를 컨테이너화하고 간결한 코드로 관리하여 어디서든 배포가능 하도록 하기 위한 것인데 컨테이너 실행 단계에서 이렇게 복잡한 과정이 필요하다면 생각보다 힘이 빠질 것 같습니다.이러한 문제를 해결하기 위해 도커에서는 도커 컴포즈라는 도구를 제공하였습니다. 도커 컴포즈는 기존의 여러 서비스를 연결하기 위해 컨테이너 실행 단계에서 복잡한 옵션을 주는 상황에서 벗어나 처음부터 YAML 파일 형태로 관리하여 docker-compose.yml 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜주는 것입니다. 다음은 웹 서버와 데이터베이스를 연결해 하나의 프로젝트로 배포하도록 해주는 도커 컴포즈 파일의 예시입니다.version: &#39;3&#39;services:  db:    image: postgres    volumes:      - ./docker/data:/var/lib/postgresql/data    environment:      - POSTGRES_DB=sampledb      - POSTGRES_USER=sampleuser      - POSTGRES_PASSWORD=samplesecret      - POSTGRES_INITDB_ARGS=--encoding=UTF-8  django:    build:      context: .      dockerfile: ./compose/django/Dockerfile-dev    environment:      - DJANGO_DEBUG=True      - DJANGO_DB_HOST=db      - DJANGO_DB_PORT=5432      - DJANGO_DB_NAME=sampledb      - DJANGO_DB_USERNAME=sampleuser      - DJANGO_DB_PASSWORD=samplesecret      - DJANGO_SECRET_KEY=dev_secret_key    ports:      - &quot;8000:8000&quot;    command:       - python manage.py runserver 0:8000    volumes:      - ./:/app/(44bits 블로그 참고)지금부터 도커 컴포즈 파일의 구성요소를 한 번 살펴보도록 하겠습니다.Top-level keys도커 컴포즈 파일의 최상위 키값에는 version, services, networks, volumes과 같은 항목이 있습니다.  version: 도커 컴포즈 파일 포맷의 버전을 지정 (도커 엔진과의 호환성 체크 문서)  services: 컨테이너 각각에 대한 세부사항 설정으로 도커 컴포즈에서 가장 많은 비중 차지  networks: 컴포즈가 제공하는 디폴트 네트워크가 아닌 커스텀 네트워크 생성을 원할 때 지정 (참고)  volumes: 볼륨에 이름을 지정할 수 있으며, 여러 컨테이너에서 공유하는 볼륨을 제공services  image원하는 컨테이너의 이미지를 이미지 저장소에서 불러와 설정합니다. 불러오는 방법은 다음과 같은 방법이 있습니다.# 이미지 이름image: redis# 이미지 이름:태그image: ubuntu:18.04# 이미지 작성자/이름image: tutum/influxdb# 이미지 urlimage: example-registry.com:4000/postgresql  buildbuild를 사용하면 이미지를 불러오지 않고, Dockerfile을 이용해 빌드할 수 있습니다.          context: Dockerfile이 위치한 디렉토리 또는 깃 레포지토리의 url      dockerfile: context에 Dockerfile이 없을 경우 사용할 대체 Dockerfile (단독으로 못쓰고 context필요)      args: 이미지 빌드 단계에서만 사용되는 환경변수의 값 (Dockerfile ARG 명령어에 미리 명시되어야함)      ARG buildnoRUN echo &quot;Build number: $buildno&quot;services:  webapp:    build:      context: ./dir      dockerfile: Dockerfile-alternate      args:        buildno: 1build와 image가 함께 표기된 경우 build로 이미지를 만들고 이미지의 이름에 image 값을 사용합니다.build: ./dirimage: webapp:tag  environment컨테이너에 환경변수를 추가해줍니다. 아래의 SESSION_SECRET와 같이 value를 표기하지 않으면 컴포즈가 실행중인 머신 안에서 정의된 value로 해석가능 합니다. 이것은 value를 secret하게 할 수 있으며 또한 host-specific value로 사용할 수 있습니다.environment:  RACK_ENV: development  SHOW: &#39;true&#39;  SESSION_SECRET:  ports포트를 노출해줍니다. 간단한 short syntax와 추가적으로 필드를 추가할 수 있는 long syntax가 있습니다.          Short Syntax 표기법보통 HOST PORT: CONTAINER PORT 방법으로 표기합니다번호 하나만 작성되면 CONTAINER PORT를 의미합니다IP 주소를 표기하여 해당 IP 주소의 트래픽만 허용할 수도 있습니다 (default: 0.0.0.0)쌍따옴표를 안쓰면 시간으로 인식되기 때문에 반드시 쌍따옴표로 감싸줍니다          ports:    - &quot;3000&quot;    - &quot;3000-3005&quot;    - &quot;8000:8000&quot;    - &quot;9090-9091:8080-8081&quot;    - &quot;49100:22&quot;    - &quot;127.0.0.1:8001:8001&quot;    - &quot;127.0.0.1:5000-5010:5000-5010&quot;    - &quot;127.0.0.1::5000&quot;    - &quot;6060:6060/udp&quot;    - &quot;12400-12500:1240&quot;                    Long Syntax 표기법                  target: the port inside the container          published: the publicly exposed port          protocol: the port protocol (tcp or udp)          mode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.                  ports:    - target: 80      published: 8080      protocol: tcp      mode: host                      volumes호스트 경로의 디렉토리 또는 네임드 볼륨을 컨테이너에 마운트합니다. 하나의 서비스를 위한 볼륨으로는 호스트 경로를 사용해도 괜찮지만, 여러 서비스가 공유하는 볼륨이 필요하다면 top-level volumes에 네임드 볼륨을 정의해야 합니다.          Short Syntax 표기법                              일반적인 표기방법: [SOURCE:]TARGET[:MODE]              volumes:      # Just specify a path and let the Engine create a volume      - /var/lib/mysql      # Specify an absolute path mapping      - /opt/data:/var/lib/mysql      # Path on the host, relative to the Compose file      - ./cache:/tmp/cache      # Named volume      - datavolume:/var/lib/mysql                                          Long Syntax 표기법                  type: 마운트 타입. volume, bind, tmpfs, npipe (참고)                          volume: 도커에 의해 관리되는 볼륨을 마운트하는 경우              bind: 호스트 머신의 파일 또는 디렉토리를 컨테이너에 마운트하는 경우                                source: 마운트 하고자 하는 호스트 경로의 디렉토리 또는 네임드 볼륨                      target: 볼륨이 마운트 될 컨테이너에서의 경로              version: &quot;3.9&quot;  services:    web:      image: nginx:alpine      ports:        - &quot;80:80&quot;      volumes:        - type: volume          source: mydata          target: /data          volume:            nocopy: true        - type: bind          source: ./static          target: /opt/app/static  networks:    webnet:  volumes:    mydata:                                            depends_on서비스간의 실행순서를 통해 디펜던시를 지키도록 해줍니다. 이 설정은 종속되는 다른 서비스가 실행되기만을 기다릴 뿐 준비(ready)가 되었는지 까지는 고려하지 않습니다. 준비 상태가 필요하다면 추가적인 설정이 필요합니다. (참고)아래 예시는 web 서비스 컨테이너가 실행되기 전에 db와 redis의 실행을 기다립니다. 보통 데이터베이스, 주키퍼와 같은 선행되어야 하는 서비스가 있는 경우 많이 사용합니다.version: &quot;3.9&quot;services:  web:    build: .    depends_on:      - db      - redis  redis:    image: redis  db:    image: postgres  command기본 커맨드 명령어를 오버라이딩합니다.command: bundle exec thin -p 3000command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;thin&quot;, &quot;-p&quot;, &quot;3000&quot;]참고  Docker Compose 공식문서  44bits님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-26T21:01:35+09:00'>26 Jan 2022</time><a class='article__image' href='/docker-series4'> <img src='/images/docker_8.jpeg' alt='Docker Compose 속성 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series4'>Docker Compose 속성 알아보기</a> </h2><p class='article__excerpt'>도커 컴포즈는 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part4]: 카프카의 데이터 저장",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series4",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  카프카의 데이터 저장 방식          Partition      Segment        저장된 데이터의 포맷(Kafka messages are just bytes)  성능 향상을 위한 파티션 수  장애 복구를 위한 복제  로그 설정을 통해 효율적으로 보관하기(Log Retention)          Role of Indexing within the Partition      Rolling segments      Impact of increasing/decreasing the segment size      Log retention - The records may persist longer than the retention time      Conclusion        참고카프카의 데이터 저장 방식Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a Distributed, Replicated Commit Log. This, I think, clearly represents what Kafka does, as all of us understand how logs are written to disk. And in this case, it is the messages pushed into Kafka that are stored to disk.  Kafka는 커밋 로그를 분산 복제하는 시스템  여기서 로그는 우리가 디스크에 저장한 메세지를 의미  (우리의 메세지를 로그로 표현하려고 하는 이유는 아마 메세지 안에 보통 데이터 뿐만 아니라 다른 메타데이터도 들어 있어서?)카프카의 데이터는 다음과 같은 구조로 이루어져 있다.  Topic: namespace처럼 논리적으로 구분하는 기준. 데이터를 구분하는 가장 큰 구분 기준  Partition: 실제로 컨슈머가 담당하는 작업 단위(컨슈머 그룹내에서 파티션은 하나의 컨슈머에게만 할당 가능). 폴더로 구분  Segment: 여러 메세지를 묶어놓은 하나의 파일. 파티션 한 개에 여러 개의 세그먼트가 저장되어 있음.  Message: 우리가 실제로 보내는 데이터 + 생성된 타임스탬프 + 프로듀서 ID + …로 이루어져 있음Partition3개의 파티션을 가지는 토픽을 우선 한 개 만들어보자.kafka-topics.sh --create --topic freblogg --partitions 3 --replication-factor 1 --zookeeper localhost:2181파티션이 저장되는 위치로 이동해 토픽 이름으로 시작하는 파티션을 검색해보면 3개의 폴더가 보인다.$ tree freblogg*freblogg-0|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpointfreblogg-1|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpointfreblogg-2|-- 00000000000000000000.index|-- 00000000000000000000.log|-- 00000000000000000000.timeindex`-- leader-epoch-checkpoint다음과 같은 명령어를 실행해 브로커로 메세지를 보내보자.kafka-console-producer.sh --topic freblogg --broker-list localhost:9092$ ls -lh freblogg*freblogg-0:total 20M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpointfreblogg-1:total 21M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpointfreblogg-2:total 21M- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint두 개의 메세지를 보냈다. 결과를 확인해보면 두 개의 파티션이 가지는 00000000000000000000.log 라는 세그먼트 파일의 용량이 증가했다. 파일을 열어보면 다음과 같은 내용이 적혀있다.$ cat freblogg-2/*.log@^@^BÂ°Â£Ã¦Ãƒ^@^K^XÃ¿Ã¿Ã¿Ã¿Ã¿Ã¿^@^@^@^A&quot;^@^@^A^VHello World^@브로커에 저장된 메세지는 바이트 형태로 저장되기 때문에 제대로 디코딩하지 않으면 이상하게 읽힌다. 하지만 Hello World라고 적힌 것을 보아 .log라는 파일에 우리가 보낸 메세지가 저장된다는 것을 알 수 있다.메세지가 파티션에 하나씩 저장된 이유는 라운드 로빈 방식으로 메세지를 파티션에 할당하기 때문이다. 메세지 할당 방식은 카프카에서 제공하는 다른 방식을 사용할 수도 있고, 만약 메세지에 키를 설정해줬다면 키마다 파티션을 다르게 할당하도록 커스터마이징할 수도 있다.세그먼트는 여러 메세지를 하나로 묶어 저장하고 있고, 각각의 메세지는 1씩 증가하는 offset을 가진다. 각 세그먼트는 자신이 가지고 있는 메세지의 가장 처음 오프셋을 이름으로 한다.위와 같은 랜덤한 문자열들을 읽고 싶으면 Kafka 툴을 사용할 수 있다.kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files logs\freblogg-2\00000000000000000000.logThis gives the outputDumping logs\freblogg-2\00000000000000000000.logStarting offset: 0offset: 0 position: 0 CreateTime: 1533443377944 isvalid: true keysize: -1 valuesize: 11 producerId: -1 headerKeys: [] payload: Hello Worldoffset: 1 position: 79 CreateTime: 1533462689974 isvalid: true keysize: -1 valuesize: 6 producerId: -1 headerKeys: [] payload: amazonCreateTime과 같은 값은 컨슈머로 가져와서 사용할 수 있는 값이 아니다. 카프카 내부적으로 가지고 있는 메타데이터이다. 그렇기 때문에 데이터의 타임스탬프가 필요하다면, 데이터를 생성할 때 내부적으로 메세지에 명시적으로 담아서 브로커에 담아야 한다.You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.Segment위에서 봤던 .log, .index, .timeindex을 모두 세그먼트 파일이라고 한다. 세그먼트 파일을 하나로 하지 않고, 나누어 저장하는 이유는 여러가지가 있다.그중에서도 데이터를 삭제할 때 이점이 있다는 것이다. Kafka는 구조적 특성으로 메세지마다 데이터를 삭제하는 것이 불가능하다. 유일하게 메세지를 삭제하는 방법은 바로 세그먼트 파일을 삭제하는 것이다. 보통 세그먼트 파일 삭제는 카프카 configuration을 통해 삭제하는 Retention policy 방법을 사용한다. (정책을 통해 주기적으로 삭제)세그먼트 파일의 의미는 다음과 같다.  .index file: This contains the mapping of message offset to its physical position in .log file.  .log file: This file contains the actual records and maintains the records up to a specific offset. The name of the file depicts the starting offset added to this file.  .index file: This file has an index that maps a record offset to the byte offset of the record within the** .log **file. This mapping is used to read the record from any specific offset.  .timeindex file: This file contains the mapping of the timestamp to record offset, which internally maps to the byte offset of the record using the .index file. This helps in accessing the records from the specific timestamp.  .snapshot file: contains a snapshot of the producer state regarding sequence IDs used to avoid duplicate records. It is used when, after a new leader is elected, the preferred one comes back and needs such a state to become a leader again. This is only available for the active segment (log file)  .leader-epoch-checkpoint: It refers to the number of leaders previously assigned by the controller. The replicas use the leader epoch as a means of verifying the current leader. The leader-epoch-checkpoint file contains two columns: epochs and offsets. Each row is a checkpoint for the latest recorded leader epoch and the leader’s latest offset upon becoming leaderAn index file for the log file I’ve showed in the ‘Quick detour’ above would look something like this:If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.저장된 데이터의 포맷(Kafka messages are just bytes)Kafka messages are just bytes. Kafka messages are organized into topics. Each message is a key/value, but that is all that Kafka requires. Both key and value are just bytes when they are stored in Kafka. This makes Kafka applicable to a wide range of use cases, but it also means that developers have the responsibility of deciding how to serialize the data.There are various serialization formats with common ones including:  JSON  Avro  Protobuf  String delimited (e.g., CSVThere are advantages and disadvantages to each of these—well, except delimited, in which case it’s only disadvantages 😉Choosing a serialization format  Schema: A lot of the time your data will have a schema to it. You may not like the fact, but it’s your responsibility as a developer to preserve and propagate this schema. The schema provides the contract between your services. Some message formats (such as Avro and Protobuf) have strong schema support, whilst others have lesser support (JSON) or none at all (delimited string).  Ecosystem compatibility: Avro, Protobuf, and JSON are first-class citizens in the Confluent Platform, with native support from the  Confluent Schema Registry, Kafka Connect, ksqlDB, and more.  Message size: Whilst JSON is plain text and relies on any compression configured in Kafka itself, Avro and Protobuf are both binary formats and thus provide smaller message sizes.  Language support: For example, support for Avro is strong in the Java space, whilst if you’re using Go, chances are you’ll be expecting to use Protobuf.데이터를 브로커에 저장할 때는 전송된 데이터의 포맷과는 상관없이 원하는 포맷으로 브로커에 저장할 수 있다. 예를 들어 프로듀서가 JSON으로 보냈다고 하더라도 브로커에 저장할 때 포맷은 Avro, Parquet, String 뭘 하든 상관없다. 다만 중요한 것은 Serializer로 Avro를 선택했다면, Deserializer도 반드시 Avro를 선택해야 한다. 그러고 나면 컨슈머에서 전달 받는 데이터의 포맷은 자연스럽게 다시 JSON 형태를 얻게 된다.Remember, Kafka messages are just pairs of key/value bytes, and you need to specify the converter for both keys and value, using the key.converter and value.converter configuration setting. In some situations, you may use different converters for the key and the value.Here’s an example of using the String converter. Since it’s just a string, there’s no schema to the data, and thus it’s not so useful to use for the value:&quot;key.converter&quot;: &quot;org.apache.kafka.connect.storage.StringConverter&quot;,Some converters have additional configuration. For Avro, you need to specify the Schema Registry. For JSON, you need to specify if you want Kafka Connect to embed the schema in the JSON itself. When you specify converter-specific configurations, always use the key.converter. or value.converter. prefix. For example, to use Avro for the message payload, you’d specify the following:&quot;value.converter&quot;: &quot;io.confluent.connect.avro.AvroConverter&quot;,&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,Common converters include:# Avroio.confluent.connect.avro.AvroConverter# Protobufio.confluent.connect.protobuf.ProtobufConverter# Stringorg.apache.kafka.connect.storage.StringConverter# JSONorg.apache.kafka.connect.json.JsonConverter# JSON schemaio.confluent.connect.json.JsonSchemaConverter# ByteArrayorg.apache.kafka.connect.converters.ByteArrayConverterJSON의 경우 스키마가 설정을 안하는 것이 디폴트다. 하지만 스키마를 고정하고 싶은 경우 두 가지 방법을 사용할 수 있다.  JSON schema io.confluent.connect.json.JsonSchemaConverter를 쓴다 (with 스키마 레지스트리)    &quot;value.converter&quot;: &quot;io.confluent.connect.json.JsonSchemaConverter&quot;,&quot;value.converter.schema.registry.url&quot;: &quot;http://schema-registry:8081&quot;,        비효율적이지만 매번 메시지에 스키마를 담아서 전송/저장한다.    value.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=true      2번 방식을 사용하면 메세지가 다음과 같이 schema 부분과, payload 부분이 함께 저장된다.{  &quot;schema&quot;: {    &quot;type&quot;: &quot;struct&quot;,    &quot;fields&quot;: [      {        &quot;type&quot;: &quot;int64&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;registertime&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;userid&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;regionid&quot;      },      {        &quot;type&quot;: &quot;string&quot;,        &quot;optional&quot;: false,        &quot;field&quot;: &quot;gender&quot;      }    ],    &quot;optional&quot;: false,    &quot;name&quot;: &quot;ksql.users&quot;  },  &quot;payload&quot;: &quot;Hello World&quot;}이렇게 하면 메세지 사이즈가 커지기 때문에 비효율적이다. 그래서 스키마가 필요한 경우에는 스키마 레지스트리를 사용하는 것이 효율적이다.만약 컨버터에 JSON serializer를 사용했고 스키마를 따로 설정하지 않을거라면,value.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter.schemas.enable=false이렇게 schema를 찾을 필요 없다고 명시해주자. (디폴트가 false인데 왜 해줘야하는거지..?)아래 표는 serializer와 deserializer의 싱크를 어떻게 맞춰야 에러가 안나는지 알려준다. 기본적으로 serializer는 메세지나 상황에 맞게 원하는 것을 선택하고, deserializer는 serializer와 같은 포맷을 사용하도록 하면 된다.성능 향상을 위한 파티션 수To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.파티션 내에서는 메세지의 순서가 지켜진다. 그래서 토픽을 이루는 파티션이 1개라면 메세지의 순서를 걱정할 필요가 없다. 하지만 파티션의 개수를 2개 이상으로 하면 메세지의 순서가 보장되지 않는다.병렬 처리를 통해 성능을 높이고자 할 때, 파티션의 개수와 컨슈머의 개수를 늘려준다.  파티션의 수 &amp;gt;= 컨슈머 수  병렬 정도 = MIN(파티션의 수, 컨슈머 수)  파티션의 개수는 늘릴수만 있고 줄일 수는 없음장애 복구를 위한 복제복제는 특정 브로커 서버에 장애가 났을 경우를 대비하기 위한 용도다. 만약 브로커가 1대라면 복제는 아무 의미가 없다. 복제는 브로커의 개수만큼 설정하면 된다. 더 크게 더 적게 해도 되지만, 같게 하는 것이 제일 합당한 선택이다.복제수는 토픽마다 다르게 설정할 수 있다. 복제 수는 늘리는 만큼 성능이 약간 떨어진다. 그래서 토픽의 중요도에 따라 다르게 설정하는 것이 좋다.복제에 관해 이해하려면 리더/팔로워, 커밋과 같은 것들을 배워야 한다. 컨슈머는 리더 파티션만 가져갈 수 있다. 복제는 리더가 장애가 났을 경우를 대비하기 위한 용도다.Say for the freblogg topic that we’ve been using so far, we’ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this.Even when you have a replicated partition on a different broker, Kafka wouldn’t let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition.A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.로그 설정을 통해 효율적으로 보관하기(Log Retention)Apache Kafka is a commit-log system. The records are appended at the end of each Partition, and each Partition is also split into segments. Segments help delete older records through Compaction, improve performance, and much more.Kafka allows us to optimize the log-related configurations, we can control the rolling of segments, log retention, etc. These configurations determine how long the record will be stored and we’ll see how it impacts the broker’s performance, especially when the cleanup policy is set to Delete.For better performance and maintainability, multiple segments get created, and rather than reading from one huge Partition, Consumers can now read faster from a smaller segment file. A directory with the partition name gets created and maintains all the segments for that partition as various files.The active segment is the only file available for reading and writing while consumers can use other log segments (non-active) to read data. When the active segment becomes full (configured by log.segment.bytes, default 1 GB) or the configured time (log.roll.hours or log.roll.ms, default 7 days) passes, the segment gets rolled. This means that the active segment gets closed and re-opens with read-only mode and a new segment file (active segment) will be created in read-write mode.Role of Indexing within the PartitionIndexing helps consumers to read data starting from any specific offset or using any time range. As mentioned previously, the .index file contains an index that maps the logical offset to the byte offset of the record within the .log file. You might expect that this mapping is available for each record, but it doesn’t work this way.How these entries are added inside the index file is defined by the log.index.interval.bytes parameter, which is 4096 bytes by default. This means that after every 4096 bytes added to the .log file, an entry gets added to the .index file. Suppose the producer is sending records of 100 bytes each to a Kafka topic. In this case, a new index entry will be added to the .index file after every 41 records (41*100 = 4100 bytes) appended to the log file.(모든 레코드가 인덱싱되기는 하는데, 레코드 한 개 넣을때마다 인덱싱되는 것은 아니고 .log 파일 하나가 다 차고나면 해당 .log 파일의 레코드를 인덱싱해서 .index 파일을 만든다)If a consumer wants to read starting at a specific offset, a search for the record is made as follows:  Search for the .index file based on its name. For e.g. If the offset is 1191, the index file will be searched whose name has a value less than 1191. The naming convention for the index file is the same as that of the log file  Search for an entry in the .index file where the requested offset falls.  Use the mapped byte offset to access the .log file and start consuming the records from that byte offset.As we mentioned, consumers may also want to read the records from a specific timestamp. This is where the .timeindex file comes into the picture. It maintains a timestamp and offset mapping (which maps to the corresponding entry in the .index file), which maps to the actual byte offset in the .log file. (특정 타임스탬프로 레코드 읽는 방법: .timeindex -&amp;gt; .index -&amp;gt; .log)Rolling segmentsAs discussed in the above sections, the active segment gets rolled once any of these conditions are met-  Maximum segment size - configured by log.segment.bytes, defaults to 1 Gb  Rolling segment time - configured by log.roll.ms or log.roll.hours, defaults to 7 days  Index/timeindex is full - The index and timeindex share the same maximum size, which is defined by the log.index.size.max.bytes, defaults to 10 MB(보통 1번 크기를 늘리면, 3번 크기도 늘려야 한다)Impact of increasing/decreasing the segment sizeGenerally you don’t want to increase/decrease the log.segment.bytes and keep it as default. But let’s discuss the impact of changing this value so that you can make an informed decision if there’s a need.Log retention - The records may persist longer than the retention timeKafka, with its feature of retaining the log for a longer duration rather than deleting it like traditional messaging queues once consumed, provides many added advantages. Multiple consumers can read the same data, apart from reading the data it can also be sent to data warehouses for further analytics.How long is the data retained in Kafka? This is configurable using the maximum number of bytes to retain by using the log.retention.bytes parameter. If you want to set a retention period, you can use the log.retention.ms, log.retention.minutes, or log.retention.hours (7 days by default) parameters.The following things may impact when the records get deleted-  If the producer is slow and the maximum size of 16 Kb is not reached within 10 minutes, older records won’t be deleted. In this case, the log retention would be higher than 10 mins.  If the active segment is filled quickly, it will be closed but only get deleted once the last inserted record persists for 10 mins. So in this case as well, the latest inserted record would be persisted for more than 10 mins. - Suppose the segment is getting filled in 7 mins and getting closed, the last inserted record will stay for 10 mins so the actual retention time for the first record inserted into the segment would be 17 mins.  The log can be persisted for an even longer duration than the last added record in the segment. How? Because the thread which gets executed and checks which log segments need to be deleted runs every 5 mins. This is configurable using log.retention.check.interval.ms configurations. - Depending on the last added record to the segment, this cleanup thread can miss the 10 min retention deadline. So in our example above instead of persisting the segment for 17 mins, it could be persisted for 22 mins.  Do you think that this would be the maximum time the record is persisted in Kafka? No, the cleaner thread checks and just marks the segment to be deleted. The log.segment.delete.delay.ms broker parameter defines when the file will actually be removed from the file system when it’s marked as “deleted” (default, 1 min) - Going back to our example the log is still available even after 23 mins, which is way longer than the retention time of 10 mins.So The usual retention limits are set by using log.retention.ms defines a kind of minimum time the record will be persisted in the file system.Consumers get records from closed segments but not from deleted ones, even if they are just marked as “deleted” but not actually removed from the file system.Conclusion참고  Data types for Kafka connector  Kafka Connect Deep Dive – Converters and Serialization Explained  dol9, Kafka 스키마 관리, Schema Registry  A Practical Introduction to Kafka Storage Internals  Here’s what makes Apache Kafka so fast  stackoverflow: Which directory does apache kafka store the data in broker nodes  Abhishek Sharma, How kafka stores data  Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals  towardsdatascience, Log Compacted Topics in Apache Kafka  conduktor, Understanding Kafka’s Internal Storage and Log Retention  What is a commit log and why should you care?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/kafka-series4'> <img src='/images/kafka_78.png' alt='Kafka Series [Part4]: 카프카의 데이터 저장'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series4'>Kafka Series [Part4]: 카프카의 데이터 저장</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series3",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  WORKDIR  VOLUME  COPY  ADD  참고Dockerfile instruction  COPY  ADD  VOLUME  WORKDIRWORKDIRWORKDIR 명령은 Docker 파일에서 이어지는 모든 RUN, CMD, ENTRIPOINT, COPY 및 ADD 명령에 대한 작업 디렉토리를 설정합니다. WORKDIR이 존재하지 않으면 이후 Dockerfile 명령어에 사용되지 않더라도 생성됩니다.WORKDIR 명령은 Docker 파일에서 여러 번 사용할 수 있습니다. 상대 경로가 제공되는 경우 이전 WORKDIR 명령의 경로에 상대적입니다. 예를 들어 다음 명령어의 결과는 /a/b/c입니다.WORKDIR /aWORKDIR bWORKDIR cRUN pwd또한 ENV를 이용해 Dockerfile에서 명시한 환경 변수의 경우 WORKDIR 명령어에서 해석할 수 있습니다. 아래 예를 보면 DIRPATH는 Dockerfile에서 정의를 했기 때문에 /path로 인식되고, DIRNAME은 해석되지 않아 /path/$DIRNAME과 같은 결과가 나옵니다.ENV DIRPATH=/pathWORKDIR $DIRPATH/$DIRNAMERUN pwdVOLUMEVOLUME 명령은 지정된 이름으로 마운트 지점을 생성하고 네이티브 호스트 또는 다른 컨테이너와 마운트됩니다.docker run 명령어를 실행하면 기본 이미지 내의 디렉토리 중 명시된 디렉토리에 있는 파일들로 마운트된 디렉토리를 초기화합니다.VOLUME 명령어로 볼륨을 생성한 뒤 이후의 빌드과정에서 생기는 볼륨의 변경값은 모두 무시됩니다.호스트 디렉터리는 컨테이너를 생성하거나 실행할 때 지정해야 합니다.  호스트 디렉토리(마운트 지점)는 본질적으로 호스트에 종속됩니다. 이는 지정된 호스트 디렉토리를 모든 호스트에서 사용할 수 있다고 보장할 수 없기 때문에 이미지 이식성을 유지하기 위한 것입니다. 따라서 Dockerfile 내에서 호스트 디렉토리를 마운트할 수 없습니다.COPYThe COPY instruction copies new files or directories from  and adds them to the filesystem of the container at the path .The  is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.# t로 시작하는 모든 txt파일을 &amp;lt;WORKDIR&amp;gt;/relativeDir/ 로 복사한다COPY t*.txt relativeDir/# test.txt, teso.txt, tesi.txt과 같은 파일을 /absoluteDir/ 로 복사한다COPY tes?.txt /absoluteDir/ADD참고  Docker 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series3'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series3'>Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series2",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  RUN  CMD  ENTRYPOINT  CMD vs ENTRYPOINT  참고Dockerfile instruction  RUN  CMD  ENTRYPOINTRUNRUN 명령어 작성요령은 다음과 같이 2가지 형태가 있습니다.  shell form: RUN &amp;lt;command&amp;gt; (the command is run in a shell, Linux default: /bin/sh -c)  exec form: RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]RUN instruction은 어떠한 명령어든 최근 이미지에 새로운 레이어에서 실행됩니다. 그리고 실행 결과는 이미지에 커밋됩니다. 커밋된 새로운 이미지는 Dockerfile의 다음 단계에 계속 사용됩니다.원한다면 RUN 명령어 중 만들어지는 커밋된 이미지를 이용해 컨테이너를 생성할 수 있습니다.  shell form: RUN &amp;lt;command&amp;gt;shell form의 기본 shell은 /bin/zsh -c echo Test 과 같이 직접 표기를 통해 바꿀 수 있습니다. 또한 \를 통해 여러 개의 RUN 명령어를 하나로 압축할 수 있습니다.RUN /bin/zsh -c echo $HOMERUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim  exec formexec form은 /bin/sh -c 가 필요하지 않은 경우 사용 가능한 형태입니다.RUN pip install -r requirements.txtCMDCMD 명령어 작성요령은 다음과 같이 3가지 형태가 있습니다.  CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec form, this is the preferred form)  CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT)  CMD command param1 param2 (shell form)CMD 명령어는 오직 한 개의 명령어만 효과가 있습니다. 만약 아래와 같이 여러 번에 걸쳐서 작성하면 마지막 명령어 CMD echo &quot;B&quot;만 실행됩니다.CMD echo &quot;A&quot; CMD echo &quot;B&quot; CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다. 예를 들어 설명해보겠습니다.  exec form: CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]# executable과 params의 조합이 하나의 디폴트CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;Hello&quot;] --------------# 컨테이너를 실행할 때 별다른 명령어를 입력하지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello# 명령어를 입력하면 CMD의 디폴트는 실행되지 않습니다docker run -it --rm &amp;lt;image-name&amp;gt; echo &quot;Good morning&quot;-&amp;gt; Good morning참고로 exec form은 shell processsing을 지원하지 않습니다. 그래서 CMD [ &quot;echo&quot;, &quot;$HOME&quot; ]은 $HOME을 대체해서 출력하지 않습니다.🦊shell processing이 필요한 경우 두 가지 방법이 있습니다.# shell을 직접 실행한다CMD [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]# shell form을 사용한다CMD echo $HOME  only params: CMD [&quot;param1&quot;,&quot;param2&quot;]이 경우에는 반드시 ENTRYPOINT 명령어를 명시해줘야 합니다. 왜냐하면 인자값만 줬을 뿐 아무런 실행 가능한 것도 표기하지 않았기 때문입니다. 이 방법은 ENTRYPOINT 명령어에 디폴트 파라미터를 제공하기 위한 것입니다.ENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]--------------  # 컨테이너를 실행할 때 디폴트 인자값 주지 않아 CMD 명령어가 실행된 경우 docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# 실행 시 인자 값을 주어 CMD 명령어가 실행되지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello ME  shell form: CMD command param1 param2shell form을 사용하면 command가 /bin/sh -c 를 통해 실행되게 됩니다. 그래서 만약 .py와 같은 파이썬 파일을 실행할 때는 shell form이 아닌 exec form을 사용해야 합니다.CMD echo &quot;Hello&quot;--------------docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hellodocker run -it --rm &amp;lt;image-name&amp;gt; echo Bye-&amp;gt; ByeCMD는 보다시피 컨테이너 실행 시 디폴트 값을 줄 뿐 반드시 실행된다는 보장을 할 수 없다. 항상 실행을 보장하고 싶을 때에는 ENTRYPOINT를 사용하면 된다.ENTRYPOINTENTRYPOINT에도 2가지 표현 방법이 있습니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]  shell form: ENTRYPOINT command param1 param2ENTRYPOINT 명령어는 docker run --entrypoint을 사용하는 경우를 제외하고는 오버라이딩 되지 않고 반드시 실행된다는 특징이 있습니다. 예를 들어 만약 docker run &amp;lt;image&amp;gt; -d 식으로 컨테이너를 실행했다면 -d는 ENTRYPOINT의 exec form 뒤에 붙게 됩니다.shell form은 어떠한 CMD 명령어나 run 커맨드라인 인자값도 사용되지 않도록 합니다. 단점은 CMD의 경우와 마찬가지로 무조건 /bin/sh -c로 시작할 수 밖에 없다는 점입니다.ENTRYPOINT 명령어도 마지막 것만 실행됩니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]FROM ubuntuENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]----------------------# ENTRYPOINT, CMD 모두 실행docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# ENTRYPOINT, run argument 실행docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello MECMD vs ENTRYPOINT  CMD, ENTRYPOINT 명령어는 마지막 하나만 실행된다  CMD 명령어는 도커 컨테이너 실행할 때 디폴트 값을 주기 때문에 오버라이딩 될 수 있다  항상 실행되는 명령어를 원한다면 ENTRYPOINT를 사용하자  항상 실행되는 명령어와 오버라이딩 되는 인자를 원한다면 CMD와 ENTRYPOINT를 함께 써보자  CMD와 ENTRYPOINT의 조합 결과는 다음과 같다참고  Docker 공식문서  스뎅(thDeng)님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series2'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series2'>Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series1",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  Dockerfile  Dockerfile Instructions          FROM                  Multi-Stage Builds                    LABEL      ARG      ENV        참고Dockerfile instruction  FROM  LABEL  ARG  ENVDockerfileDockerfile InstructionsFROMFROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다. 그렇기 때문에 유효한 Dockerfile은 반드시 FROM 명령어로부터 시작해야 합니다.  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt; [AS &amp;lt;name&amp;gt;]  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt;[:&amp;lt;tag&amp;gt;] [AS &amp;lt;name&amp;gt;]# python:3.8-buster 이미지를 기본 이미지로 만듭니다FROM python:3.8-buster# 현재 Dockerfile이 있는 위치에 있는 모든 파일을 새로 만든 이미지의 디렉토리 위치로 복사합니다COPY . .# 복사된 requirements.txt 파일에 있는 라이브러리를 설치RUN pip install -r requirements.txt# deeplearning:pytorch 라는 새로운 이미지를 만듭니다docker build -t deeplearning:pytorch .Multi-Stage Builds이미지를 빌드할 때 가장 중요한 것은 이미지의 사이즈를 줄이는 것입니다. Dockerfile에서 각각의 명령어는 이미지의 layer를 하나씩 늘려나가게 됩니다. 이를 경량화하는 방법으로 RUN 명령어 사용시 Bash에서 &amp;amp;&amp;amp; 연산자를 사용할 수 있습니다.또한 만약 여러 개의 이미지로부터 새로운 이미지를 불러와야 하는 상황이라면 FROM 과 COPY를 사용해 이미지를 경량화 할 수 있습니다. 이를 이용하면 각각의 이미지에서 원하는 파일만 선택적으로 복사해 다음 이미지로 전달시키고 필요없는 파일(다운로드 과정에서 필요한 코드와 같은 부수적인 파일들)은 제거할 수 있습니다. 이 방법을 Multi-Stage Builds라고 하는데 이 방법은 여러 개의 이미지로 부터 새로운 이미지를 생성할 때 여러 개의 Dockerfile이 필요없이 하나의 파일에 관리할 수 있다는 장점도 있습니다.Multi Stage Builds 방법으로 이미지를 만드는 코드의 형태는 다음과 같습니다.# &amp;lt;image&amp;gt; 를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# &amp;lt;image2&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image2&amp;gt;...# &amp;lt;image&amp;gt;의 빌드 결과로 생성된 파일 중 원하는 파일만 복사COPY --from=apple /dir/you/want/from/apple /dir/of/image2# 이 방법은 기본 이미지는 &amp;lt;image&amp;gt; 하나만 필요하지만 원하는 파일만 가져오고 싶은 경우 사용하는 것 같다# &amp;lt;image&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# apple로 생성된 이미지를 기본 이미지로 한다FROM apple AS apple_juice...# &amp;lt;image&amp;gt;에서 필요한 파일만 복사COPY /dir/you/want/from/apple /dir/of/apple_juice# 사용 예시FROM golang:1.16 AS builderWORKDIR /go/src/github.com/alexellis/href-counter/RUN go get -d -v golang.org/x/net/html  COPY app.go    ./RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROM alpine:latest  RUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=builder /go/src/github.com/alexellis/href-counter/app ./CMD [&quot;./app&quot;]  LABELLABEL 명령어는 이미지에 메타데이터를 추가하기 위해 사용됩니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot;LABEL com.example.label-with-value=&quot;foo&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;This text illustrates \that label-values can span multiple lines.&quot;하나의 명령어에 여러 데이터를 추가하면 이미지 크기를 줄일 수 있습니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; \com.example.label-with-value=&quot;foo&quot; \version=&quot;1.0&quot; \ description=&quot;This text illustrates \that label-values can span multiple lines.&quot;ARG빌드 단계에서만 사용하기 위한 변수입니다. 밑에서 배울 ENV와 같은 변수를 지정하게 되면 ENV가 ARG를 오버라이딩합니다.ENVENV 명령어는 환경 변수를 키:밸류 형태로 지정하도록 해줍니다. 설정된 환경 변수는 설정 이후의 모든 빌드 단계와 런타임 단계에서 사용됩니다.만약 밸류로 띄어쓰기가 필요하다면 쌍따옴표로 감싸주면 됩니다.ENV MY_NAME=&quot;John Doe&quot;ENV MY_CAT=fluffy참고  Docker 공식문서  Docker 공식문서2  stack overfolw Nagev 답변  EARTHLY 블로그: Docker Multistage Builds  geeksforgeeks 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series1'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series1'>Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Fault tolerance in Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series3",
      "date"     : "Jan 24, 2022",
      "content"  : "Table of Contents  Fault tolerance in Kafka          카프카 리플리케이션(Replication)      리더(Leader)와 팔로워(Follower)      컨트롤러(Controller)      리플리케이션 과정        참고자료Fault tolerance in Kafka카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.카프카 리플리케이션(Replication)카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, 다른 브로커에 파티션을 복제해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 리더와 팔로워가 있습니다.(shwitha B G 블로그 참고)리더(Leader)와 팔로워(Follower)카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.컨트롤러(Controller)리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 컨트롤러라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.(shwitha B G 블로그 참고)컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. 주키퍼(Zookeeper) 개념이 잠깐 등장합니다.(Zookeeper is the centralized service for storing metadata of topic, partition, and broker)  주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.  브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다  컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.  컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.리플리케이션 과정마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 하이워터마크(high water mark)라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 Message3을 읽어갔지만, 이제는 더이상 Message3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.참고자료  Hackernoon 블로그  Ashwitha B G 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-24T21:01:35+09:00'>24 Jan 2022</time><a class='article__image' href='/kafka-series3'> <img src='/images/kafka_15.png' alt='Kafka Series [Part3]: Fault tolerance in Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series3'>Kafka Series [Part3]: Fault tolerance in Kafka</a> </h2><p class='article__excerpt'>이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part5]: Kubernetes networking for developers [번역]",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series5",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Basics  Localhost (IP address 127.0.0.1)  Pod network  Service network원문: Kubernetes networking for developers - IBM developers불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.Kubernetes Basics컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 파드(Pod)로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)Localhost (IP address 127.0.0.1)같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 공유되는 네트워크 네임스페이스를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.Pod network파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.사용되는 IP 주소는 파드 네트워크라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.Service network쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해 트래픽을 전송하도록 해줍니다.kind: ServiceapiVersion: v1metadata:  name: web  namespace: my-appspec:  selector:    app: web-server  ports:  - name: web    protocol: TCP    port: 80    targetPort: 80위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.  ClusterIP  NodePort  LoadBalancer  ExternalName",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series5'> <img src='/images/kube_35.png' alt='Kubernetes Series [Part5]: Kubernetes networking for developers [번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series5'>Kubernetes Series [Part5]: Kubernetes networking for developers [번역]</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series4",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  로컬환경          미니큐브(minikube)      Docker Desktop      kind(Kubernetes in Docker)        클라우드환경          GKE(Google Kubernetes Engine)      EKS(Elastic Kubernetes Service)        참고자료로컬환경쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 테스트, 개발 환경에 적합합니다.미니큐브(minikube)미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)kind(Kubernetes in Docker)minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)brew install kindkind version--------------------kind v0.11.1 go1.17.2 darwin/arm64잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)# kind로 클러스터 구축을 위한 kind.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦 ✓ Configuring the external load balancer ⚖️ ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✗ Joining worker nodes 🚜 실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드환경GKE(Google Kubernetes Engine)EKS(Elastic Kubernetes Service)참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series4'> <img src='/images/kube_24.png' alt='Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series4'>Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기</a> </h2><p class='article__excerpt'>쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part3]: Kubernetes Service",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series3",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Network  ClusterIP  NodePort  LoacBalancer  Ingress          클러스터 외부의 로드 밸런서만을 이용한 Ingress      클러스터 내부의 Ingress 파드를 곁들인 Ingress        참고자료Kubernetes Network쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 localhost로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 파드의 IP 주소로 통신하면 됩니다. 또한 노드 간의 통신은 VXLAN이나 L2 Routing을 이용할 수 있습니다.이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러나 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.  로드 밸런싱  서비스 디스커버리  클러스터 내부 DNSClusterIPClusterIP는 서비스의 기본 타입입니다. ClusterIP 서비스를 생성하면 클러스터 내부에서만 통신 가능한 가상 IP가 할당됩니다. kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송합니다.NodePortNodePort는 모든 노드의 IP주소:포트에서 수신한 트래픽을 컨테이너에 전송하는 형태로 외부와 통신할 수 있습니다. NodePort는 전체 노드 N개 중 임의의 노드의 IP주소를 외부에 노출합니다. 그럼에도 ClusterIP를 통해 다른 노드의 파드로 통신하는데에는 문제 없습니다. 그러나 노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)이 되기 때문에 NodePort만을 이용해 외부와 통신하는 것은 분명한 한계점이 있습니다. 또한 NodePort는 쿠버네티스에서 지정한 범위(30000~32767) 안에서만 지정할 수 있기 때문에 서비스로 활용하기에는 포트 번호가 예쁘지는 않습니다. 노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양합니다.LoacBalancerLoadBalancer에서는 NodePort와 다르게 별도로 외부 로드 밸런서를 사용하기 때문에 노드 장애가 발생해도 크게 문제가 되지 않습니다. 노드에 장애가 발생한 경우 해당 노드를 목적지에서 제외 처리하고 트래픽을 전송하지 않게됩니다. LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 할당됩니다. 실제 서비스 운영 환경에서는 외부로부터 요청을 수신하는 External IP 주소를 DNS 설정 등의 이유로 고정하는 것을 선호하고 LoadBalancer 서비스는 이를 지원합니다.Ingress인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, kind: Ingress타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.클러스터 외부의 로드 밸런서만을 이용한 Ingress  GKE 인그레스외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.클러스터 내부의 Ingress 파드를 곁들인 Ingress  Nginx 인그레스클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서  NodePort vs LoadBalancer stackoverflow  Google Kubernetes Engine 가이드  Confluent 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series3'> <img src='/images/kube_26.png' alt='Kubernetes Series [Part3]: Kubernetes Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series3'>Kubernetes Series [Part3]: Kubernetes Service</a> </h2><p class='article__excerpt'>쿠버네티스에서는 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part2]: Main elements of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series2",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kafka의 주요 구성요소  Topic, Partition, Segment  Producer          레코드 전송과정      레코드 파티셔닝 전략                  라운드 로빈(Round-Robbin) 방식          스티키 파티셔닝(Sticky Partitioning) 방식                    적어도 한 번 전송      정확히 한 번 전송                  중복 없는 전송          트랜잭션 API                      Broker          컨트롤러      데이터의 저장      데이터의 삭제      데이터 복제        Consumer          컨슈머 오프셋 관리      그룹 코디네이터      파티션 리밸런싱                  라운드 로빈 파티션 할당 전략          스티키 파티션 할당 전략          협력적 스티키 파티션 할당 전략                      마치며  참고자료Kafka의 주요 구성요소Kafka는 크게 3가지로 이루어 있습니다.  Producer: Kafka로 메시지를 보내는 모든 클라이언트  Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버  Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트(참고: cloudkarafka)Topic, Partition, SegmentKafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.  Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)  Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)  Segment: 메시지가 실제로 저장되는 파일. 기본적으로 1GB를 넘을 때마다 파일이 새로 생성(참고: cloudkarafka)카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.Producer프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.(Dzone 블로그 참고)레코드 전송과정프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 ProducerRecord라고 하는 형태로 전송되며, Topic과 Value는 필수값이며, Partition과 Key는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은  프로듀서에서 send() 메소드 호출  Serializer는 JSON, String, Avro 등의 object를 bytes로 변환  ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달  Partition이 지정되지 않았을 때, Key값이 지정되었다면 Partitioner가 Key값을 바탕으로 해당 파티션에 전달  Partition, Key값이 모두 없으면 라운드 로빈(Round-Robbin)방식 또는 스티키 파티셔닝(Sticky Partitioning) 방식으로 메세지를 파티션에 할당  파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 배치로 전송할 수도 있음레코드 파티셔닝 전략라운드 로빈(Round-Robbin) 방식프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.스티키 파티셔닝(Sticky Partitioning) 방식라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)적어도 한 번 전송카프카에서 적어도 한 번 전송은 프로듀서와 브로커간 주고 받는 ACK로 구현됩니다. 프로듀서는 메시지를 브로커에게 전송하고 브로커로부터 메시지를 잘 받았다는 ACK를 받으면 다음 메시지를 전송합니다.만약 브로커가 메시지를 못 받았다면 프로듀서는 ACK를 못 받게 되고 프로듀서는 다시 메시지를 전송합니다. 또한 브로커가 메시지를 받았다고 하더라도 네트워크 장애로 ACK를 브로커에게 돌려 주지 못하면 프로듀서는 브로커가 메시지를 못 받은 것으로 간주하고 다시 같은 메시지를 보내게 됩니다. 이러한 특징으로 인해 이를 적어도 한 번 전송이라고 합니다.카프카는 기본적으로 이와 같은 적어도 한 번 전송 방식을 기반으로 동작합니다.정확히 한 번 전송데이터 처리나 가공 작업을 하는 대부분의 사람들은 데이터 파이프라인에서 메세지 손실 뿐만 아니라 중복도 발생하지 않기를 원할겁니다. 이러한 방식을 정확히 한 번 전송 방식이라고 합니다.하지만 카프카에서 정확히 한 번 전송 방식은 기준이 조금 더 엄격합니다. 카프카에서 정확히 한 번 전송은 트랜잭션과 같은 전체적인 프로세스 처리를 의미하며, 중복 없는 전송은 정확히 한 번 전송의 일부 기능이라 할 수 있습니다.중복 없는 전송중복 없는 전송은 브로커가 중복된 메세지를 받을 경우 중복 저장하지 않고, 메세지를 받았다는 ACK만 돌려보내는 방식입니다. 프로듀서는 메세지를 보낼 때 누가(Producer ID), 어떤 메세지(시퀀스 번호)를 보내는지 정보를 추가해서 브로커에게 보내고, 브로커는 이 정보를 메모리와 리플리케이션 로그에 저장해 놓습니다.(kafka-logs/토픽-파티션 폴더/세그먼트 시작 오프셋.snapshot) 그리고 프로듀서가 보낸 메세지의 시퀀스 번호와 비교해 브로커가 자신이 저장해 놓은 시퀀스 번호보다 정확하게 하나 큰 경우에만 메세지를 저장합니다.트랜잭션 API프로듀서가 카프카로 정확히 한 번 방식으로 메세지를 전송할 때, 프로듀서가 보내는 메세지들은 원자적으로 처리되어 전송에 성공하거나 실패하게 됩니다. 이렇게 메세지를 관리하며 커밋 또는 중단 등을 표시하는 것은 트랜잭션 코디네이터에 의해 수행됩니다.정확히 한 번 전송을 위해서는 트랜잭션 API를 이용해 다음과 같은 동작을 단계별로 수행합니다.      트랜잭션 코디네이터 찿기    트랜잭션 코디네이터는 __transaction_state 토픽의 리더 파티션을 가지고 있는 브로커입니다. 트랜잭션 코디네이터의 주 역할은 PID와 transactional.id를 매핑하고 해당 트랜잭션 전체를 관리하는 것입니다.        프로듀서 초기화    프로듀서가 InitPidRequest(프로듀서 초기화 요청)를 트랜잭션 코디네이터로 보내면 트랜잭션 코디네이터는 TID와 PID를 매핑하고 해당 정보를 트랜잭션 로그에 기록합니다.        메시지 전송    프로듀서는 토픽의 파티션으로 메세지를 전송합니다.        트랜잭션 종료 요청    메세지 전송을 완료한 프로듀서는 commitTransaction()메소드 또는 abortTransaction()메소드 중 하나를 호출해 트랜잭션이 완료되었음을 트랜잭션 코디네이터에게 알립니다.        사용자 토픽에 표시    메세지를 전송한 토픽의 파티션에 트랜잭션 커밋 표시를 기록합니다. 이렇게 파티션에 기록한 커밋 표시를 ‘컨트롤 메세지’라고 합니다. 이 메세지는 해당 메세지가 제대로 전송됐는지 여부를 컨슈머에게 나타내는 용도로, 커밋되지 않은 트랜잭션에 포함된 메세지는 컨슈머에게 반환하지 않게 됩니다.        트랜잭션 완료    마지막으로 트랜잭션 코디네이터는 트랜잭션 로그에 완료됨(Committed)이라고 기록합니다.  Broker브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.컨트롤러클러스터의 다수 브로커중 한 대가 컨트롤러의 역할을 합니다. 컨트롤러는 다른 브로커들의 상태를 체크하고 브로커가 장애로 클러스터에서 빠지는 경우 해당 브로커에 존재하던 리더 파티션을 다른 브로커로 재분배합니다. 컨트롤러 역할을 하는 브로커에 장애가 생기면 다른 브로커가 컨트롤러 역할을 합니다.데이터의 저장카프카의 데이터는 config/server.properties의 log.dir 옵션에 정의한 디렉토리(보통 /tmp/kafka-logs)에 저장됩니다. 데이터는 토픽 이름과 파티션 번호의 조합으로 디렉토리(토픽명-파티션 번호)를 생성하여 데이터를 저장합니다.디렉토리(토픽명-파티션 번호) 아래를 확인하면 .index, .log, .timeindex 파일이 존재합니다.000000000.index: 메시지의 오프셋을 인덱싱한 정보를 담은 파일000000000.log: 메시지와 메타데이터를 저장한 파일(대표적으로 offset, key, value 저장)000000000.timeindex: 메시지에 포함된 타임스탬프 값을 기준으로 인덱싱한 정보를 담은 파일(생성 시간 또는 적재된 시간)(000000000은 각 세그먼트가 가지는 레코드중 시작 오프셋)데이터의 삭제카프카는 다른 메세징 플랫폼과 다르게 컨슈머가 데이터를 가져가더라도 토픽의 데이터는 삭제되지 않습니다. 또한 컨슈머나 프로듀서가 데이터 삭제를 요청할 수 없으며 오직 브로커만이 데이터를 삭제할 수 있습니다. 하지만 카프카에 저장한 데이터는 다른 일반적인 데이터베이스처럼 특정 데이터를 선별해서 삭제할 수 없습니다. 카프카에서 데이터를 삭제하는 단위는 로그 세그먼트 파일 단위입니다. 참고로 수정 또한 불가능하기 때문에 프로듀서는 데이터를 브로커에 전송하기 전에 검증하는 것이 좋습니다.데이터 복제Consumer컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.  파티션 할당 전략  프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가  컨슈머의 개수가 파티션보다 많지는 않은가  컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가컨슈머 오프셋 관리컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 __consumer_offsets라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, 컨슈머가 다음으로 읽어야 할 위치를 말합니다.참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.그룹 코디네이터컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.이렇게 컨슈머 그룹내의 변화를 감지하기 위해 트래킹하는 것이 바로 그룹 코디네이터입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.파티션 리밸런싱이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.라운드 로빈 파티션 할당 전략라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.스티키 파티션 할당 전략스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.협력적 스티키 파티션 할당 전략이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.(컨플루언트 블로그 참고)마치며이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 장애 발생에 대처 가능한 안정적인 서비스를 제공해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 높은 처리량, 데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활하게 해주는 것이 좋습니다.참고자료  실전 카프카 개발부터 운영까지 책  Dzone 블로그  CodeX 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kafka-series2'> <img src='/images/kafka_30.png' alt='Kafka Series [Part2]: Main elements of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series2'>Kafka Series [Part2]: Main elements of Kafka</a> </h2><p class='article__excerpt'>프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker란 무엇인가?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series0",
      "date"     : "Jan 22, 2022",
      "content"  : "Table of Contents  도커 소개  도커의 장점  도커의 구조  도커의 구성요소          도커 데몬      도커 클라이언트      도커 오브젝트                  이미지          컨테이너                      참고도커 소개Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다. 컨테이너 기술은 가상화를 위한 방법 중 하나인데 이에 관한 더 자세한 내용은 추후에 다루어 보도록 하겠습니다.개발자들에게 있어 골칫거리 중 하나는 새로 만든 애플리케이션을 개발 환경에서 테스트 환경으로, 테스트 환경에서 운영환경으로 옮길 때마다 온갖 이상한 오류를 만난다는 것입니다. 그 이유는 인프라 환경마다 네트워크 기술과 보안 정책, 스토리지가 모두 제각각이어서 그렇습니다. 그래서 ‘소프트웨어를 한 컴퓨팅 환경에서 다른 컴퓨팅 환경으로 이동하면서도 안정적으로 실행하는 방법이 없을까?’라는 고민이 커졌고 그 대답이 바로 컨테이너였습니다.개념은 간단합니다. 애플리케이션과 그 실행에 필요한 라이브러리, 바이너리, 구성 파일 등을 패키지로 묶어 배포하는 것입니다. 이렇게 하면 노트북-테스트 환경-실제 운영환경으로 바뀌어도 실행에 필요한 파일이 함께 따라다니므로 오류를 최소화할 수 있습니다. 운영체제를 제외하고 애플리케이션 실행에 필요한 모든 파일을 패키징한다는 점에서 운영체제 위에서 구현된 가상화, 즉 ‘운영체제 레벨 가상화’라고 부르기도 합니다.참고로 도커 이전에도 컨테이너 기술을 이용한 운영체제 레벨의 가상화는 있었습니다. 구글에서는 도커가 등장하기 전부터 이러한 기술을 회사 내부적으로 이용하고 있었다고 합니다. 그러나 기술적으로 높은 진입 장벽 때문에 대중화되지 않았던 것 뿐입니다.이러한 상황 속에서 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되었고, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 필요한 모든  애플리케이션들을 도커파일을 이용해 이미지를 만들고 컨테이너로 배포하는 게 흔한 개발 프로세스가 되었습니다.도커의 장점  애플리케이션을 인프라 환경에 상관없이 빠르게 배포할 수 있습니다.  어플리케이션을 실행하기 위한 독립적인 컨테이너 환경을 제공해 서비스간 디펜던시 오류를 해결해줍니다.  별다른 운영체제 소프트웨어가 필요없어 가볍습니다.도커의 구조  도커의 아키텍처는 클라이언트-서버 아키텍처입니다.  도커 클라이언트(docker)는 도커 (REST) API를 사용해 도커 데몬(dockerd)에게 요청 메시지를 보냅니다.  dockerd은 요청을 받으면 이미지, 컨테이너, 네트워크, 볼륨과 같은 도커 오브젝트를 생성하고 관리합니다.  도커 레지스트리는 public한 곳(docker hub)도 있고, private(AWS의 ECR)한 곳도 있습니다.도커의 구성요소도커 데몬도커 데몬(dockerd)은 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.도커 클라이언트도커 클라이언트(docker)는 사용자가 도커 데몬과 통신하는 주요 방법입니다. docker run과 같은 명령을 사용하면 도커 클라이언트는 해당 명령을 도커 데몬에게 전송하고 도커 데몬은 명령을 수행하게 됩니다.도커 오브젝트(출처: 오웬의 개발 이야기)이미지도커 이미지는 도커 컨테이너를 생성하기 위한 읽기 전용 템플릿입니다. 기본 이미지 위에 원하는 커스터마이징을 통해 새로운 이미지를 만들 수도 있으며, 이렇게 만들어진 이미지는 Docker Registry에 Push하여 공유할 수 있습니다. 이미지를 만들때에는 Dockerfile에 필요한 명령어를 정의하여 만들 수 있습니다. Dockerfile에 정의된 각각의 명령어들은 이미지의 Layer를 생성하며, 이러한 Layer들이 모여 이미지를 구성합니다. Dockerfile을 변경하고 이미지를 다시 구성하면 변경된 부분만 새로운 Layer로 생성됩니다. 이러한 Image의 Layer구조는 Docker가 타 가상화 방식과 비교할 때, 매우 가볍고 빠르게 기동할 수 있는 요인이 됩니다.컨테이너컨테이너는 Docker API 사용하여 생성, 시작, 중지, 이동 또는 삭제 할 수 있는 이미지의 실행가능한 인스턴스를 나타냅니다. 컨테이너를 하나 이상의 네트워크에 연결하거나, 저장 장치로 묶을 수 있으며, 현재 상태를 바탕으로 새로운 이미지를 생성할 수도 있습니다. 기본적으로 컨테이너는 Host 또는 다른 컨테이너로부터 격리되어 있습니다. 컨테이너가 제거될 때는 영구 저장소에 저장되지 않은 변경 사항은 모두 해당 컨테이너와 같이 사라집니다.도커 내부에 대해 조금 더 알아보고 싶으시다면 [Docker의 아키텍처 이해하기]포스트를 참고하셔도 좋습니다.참고  도커 공식문서  Rain.i님의 도커 컨테이너 까보기(1) – Protocol, Registry 포스트  yjs0997님의 [Docker 기본(2/8)] Docker’s Skeleton 포스트  ITWorld 용어풀이: 컨테이너(container), IT World",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-22T21:01:35+09:00'>22 Jan 2022</time><a class='article__image' href='/docker-series0'> <img src='/images/docker_3.svg' alt='Docker란 무엇인가?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series0'>Docker란 무엇인가?</a> </h2><p class='article__excerpt'>Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part1]: What is Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series1",
      "date"     : "Jan 17, 2022",
      "content"  : "Table of Contents  카프카의 탄생  카프카의 구조  카프카의 장점          높은 처리량      낮은 지연      확장성      영속성      고가용성        카프카의 핵심 기능          순서 보장      적어도 한 번 전송 방식      강력한 파티셔닝      비동기 방식        정리  카프카 생태계  참고자료카프카의 탄생2011년 링크드인에서는 파편화된 데이터 수집 및 분배를 위한 데이터 파이프라인을 운영하는 데에 어려움을 겪었습니다.초기 운영시에는 데이터를 생성하고 적재하는데 큰 어려움이 없었지만, 서비스 사용자가 증가함에 따라 데이터량이 폭발적으로 증가하기 시작했고 아키텍처도 점점 복잡해지기 시작했습니다.이를 해결하기 위해 기존에 나와있던 데이터 프레임워크와 오픈소스를 활용해 파이프라인의 파편화를 개선하려고 노력했지만 결국 한계를 느끼고 링크드인의 데이터팀은 데이터 파이프라인을 위한 새로운 시스템을 만들기로 결정했습니다.그 결과물로 등장한 것이 카프카(Kafka)입니다. 카프카는 이후 아파치 재단의 최상위 프로젝트로 등록되었습니다. 그리고 2014년 카프카 개발의 핵심 멤버였던 제이 크랩스(Jay Kreps)는 컨플루언트(Confluent)라는 회사를 창업해 카프카를 더욱 편리하게 운영할 수 있도록 몇 가지 추가적인 기능을 추가해 현재 카프카 관련 다양한 서비스를 제공하고 있습니다.카프카의 구조카프카는 더이상 각각의 애플리케이션을 서로 연결해서 데이터를 처리하는 것이 아니라 한 곳에 모아 처리할 수 있도록 중앙 집중화 하였습니다.기존에 있던 데이터 파이프라인은 소스와 타겟의 직접적인 연결로 한쪽의 이슈가 다른 쪽의 애플리케이션에 영향을 미치곤 했지만 카프카는 이러한 높은 의존성 문제를 해결했습니다. 이제 소스 애플리케이션은 어느 타겟 애플리케이션으로 보낼지 고민하지 않고 일단 카프카로 넣으면 됩니다.카프카 내부에 데이터가 저장되는 파티션의 동작은 FIFO 방식의 큐 자료구조와 유사합니다. 카프카에서는 이러한 큐를 브로커(broker), 소스 애플리케이션을 프로듀서(producer), 타겟 애플리케이션을 컨슈머(consumer)라고 합니다.카프카를 소개하는 좋은 문장이 있어 가져와봤습니다.  Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.  실시간 데이터를 스트리밍하는 분산환경의 publish-subscribe 메시지 플랫폼  복잡한 데이터 파이프라인 구조를 간단하게 해주며 파이프라인의 확장성을 높여준다  Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.  데이터를 소스에서 목적지로 직접 전달하는 것이 아니다  프로듀서가 데이터를 생성(발행)해서 브로커(잡지)에 저장한다  컨슈머는 브로커(잡지)에 저장된 데이터를 읽어간다(구독)카프카의 장점높은 처리량  배치 처리 제공 -&amp;gt; 네트워크 비용 낮춤 -&amp;gt; 처리량 증가  데이터를 파티션 단위로 나눔 -&amp;gt; 컨슈머 개수를 파티션 수 만큼 늘림 -&amp;gt; 컨슈머 수만큼 처리량 증가  데이터를 압축하여 브로커에 전송 가능 -&amp;gt; 처리량 증가낮은 지연  높은 처리량에 초점이 맞춰져 있지만 설정값을 통해 낮은 지연율을 얻을 수 있음  (카프카에 관한 글을 읽으면 카프카가 제공하는 큰 장점 중 하나가 낮은 지연율이라고 생각이 들지만 직접 써보게 되면 낮은 지연율 보다는 높은 처리량에 초점이 맞춰져있다. 하지만 설정값을 바꿔줌으로써 낮은 지연율을 얻을 수 있다)확장성  카프카는 분산 시스템 -&amp;gt; 트래픽량에 따라 스케일 아웃 가능 -&amp;gt; 높은 확장성영속성  다른 메세징 플랫폼과 다르게 데이터를 파일 시스템에 저장 -&amp;gt; 종료되더라도 데이터가 남아있다 -&amp;gt; 영속성  (또한 페이지 캐시 영역을 메모리에 따로 생성해 데이터 읽는 속도도 느려지지 않음)고가용성  여러 대의 브로커에 데이터 복제 -&amp;gt; 브로커가 고장나도 장애 복구 가능 -&amp;gt; 높은 가용성카프카의 핵심 기능순서 보장이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.적어도 한 번 전송 방식분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(idempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다. 최근에는 ‘정확히 한 번’ 전송 방식이 도입되어 카프카내에서 중복성을 제거하는 방법이 많이 사용되고 있습니다.Idempotent: Characteristic that we can apply operation multiple times without changing the result beyond the initial application.카프카에서 정확히 한 번 전송 방식을 지원하는데 멱등적인 방법은 아님. De-duplication 방식임.In the de-duplication approach, we give every message a unique identifier, and every retried message contains the same identifier as the original. In this way, the recipient can remember the set of identifiers it received and executed already. It will also avoid executing operations that are executed.It is important to note that in order to do this, we must have control on both sides of the system: sender and receiver. This is because the ID generation occurs on the sender side, but the de-duplication process occurs on the receiver side.it’s impossible to have exactly-once delivery in a distributed system. However, it’s still sometimes possible to have exactly-once processing.(정확히 한 번 전송은 사실 Producer는 중복 전송하더라도, 브로커에서 저장하는 과정을 한 번만 수행함으로서 보장된다)강력한 파티셔닝파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.비동기 방식데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)정리  Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.  데이터를 분산처리하여 높은 처리량을 제공하고, 선택적으로 낮은 지연율을 얻을 수 있다  심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.카프카 생태계참고자료  실전 카프카 개발부터 운영까지 책  CodeX 블로그  How Does Kafka Perform When You Need Low Latency?  Here’s what makes Apache Kafka so fast  Apache Kafka Architecture: What You Need to Know",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-17T21:01:35+09:00'>17 Jan 2022</time><a class='article__image' href='/kafka-series1'> <img src='/images/kafka_70.png' alt='Kafka Series [Part1]: What is Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series1'>Kafka Series [Part1]: What is Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part1]: What is Apache Spark?",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series1",
      "date"     : "Jan 15, 2022",
      "content"  : "Table of Contents  Spark Introduction  RDD          파티션(Partition)      불변성(Immutable)      게으른 연산(Lazy Operation)        Spark Architecture          드라이버 프로그램      클러스터 매니저      (마스터 노드)      워커 노드        알아두면 좋은 것들          Shuffling      Passing Functions to Spark        질문  참고Spark Introduction스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다. 쉽게 말해 대용량 데이터를 여러 컴퓨터에 나누어서 동시에 처리한다고 할 수 있습니다. 이런 방법이 스파크 이전에 없었던 것은 아닙니다. 스파크 이전에 하둡(Hadoop)이 이와 유사한 기능을 제공했었습니다. 참고로 하둡은 더그 커팅(Doug Cutting)이라는 사람이 구글이 발표했던 두 개의 논문(The Google File System_2003, MapReduce: simplified data processing on large clusters_2008)을 직접 구현해 만든 프레임워크입니다. 이처럼 구글에서는 예전부터 대용량의 데이터를 고속 분산처리 하기 위해 노력했었고, 현재 스파크는 대부분의 기업들이 사용하고 있는 소프트웨어입니다.지금부터는 스파크와 하둡을 비교하며 스파크의 특징에 어떤 것이 있는지 알아보겠습니다.            차이점      하둡      스파크              기반      디스크 기반      메모리 기반              처리방식      Map-Reduce      RDD              프로그래밍언어      자바      스칼라, 자바, 파이썬, R              라이브러리      -      다양한 라이브러리(Spark streaming, MLlib, GraphX 등) 제공        Hadoop의 MapReduce는 기본적으로 Disk I/O를 많이 발생시켰기 때문에 Batch 처리에만 적합  Spark는 머신러닝과 같이 반복 작업을 많이 하는 알고리즘이 메모리 상에서 바로 수행될 수 있게 해주는 프레임워크처리방식에 대해 조금 더 이야기 해보겠습니다. 맵리듀스(MapReduce)는 2004년 구글에서 대용량 데이터 처리를 분산환경에서 처리하기 위한 목적의 소프트웨어 프레임워크입니다. 맵리듀스는 함수형 프로그래밍에서 일반적으로 사용되는 Map과 Reduce라는 함수를 기반으로 만들어졌습니다. Map은 각각의 분산된 환경에서 독립적으로 실행되는 함수의 일종, Reduce는 분산 환경에서의 데이터를 하나로 모으는 함수라고 생각할 수 있습니다.맵리듀스는 분산된 환경에서 데이터가 처리되는데 필요한 많은 함수들을 제공해주지만, 현업에서 필요한 기능들을 모두 커버하기에는 무리가 있었습니다. 그래서 이러한 단점을 보완하기 위해 2009년 UC Berkeley 대학에서 연구를 시작해 2012년 미국 NSDI 학회에서 스파크의 핵심 개념인 RDD(Resilient Distributed Dataset) 에 대한 논문을 발표하였습니다.RDD  RDD is a fault-tolerant collection of elements that can be operated on in parallel.(아파치 스파크 공식문서 참고)다시 말하면 RDD란 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소로 구성되며 데이터를 처리하는 과정에서 장애가 발생하더라도 스스로 복구할 수 있는 능력을 가진 데이터 모델이라는 뜻입니다. RDD는 분산 데이터에 대한 모델로서 단순히 값으로 표현되는 데이터만 가리키는 것이 아니고, 분산된 데이터를 다루는 방법까지 포함하는 일종의 클래스와 같은 개념입니다.RDD에서 중요한 특징은 다음과 같습니다.  파티션(Partition): 분산 처리 기능 제공  불변성: 장애 복구 기능 제공  게으른 연산(Lazy operation): 연산 최적화 기능 제공파티션(Partition)RDD는 분산 데이터 요소로 구성된 데이터 집합입니다. 여기서 분산 데이터 요소를 파티션이라고 합니다. 스파크는 작업을 수행할 때 바로 이 파티션 단위로 나눠서 병렬로 처리합니다. 여기서 제가 헷갈렸던 것은 파티션이 분산처리와 병렬처리 중 어떤 것을 기준으로 나뉘어진 단위인가 라는 것 이었습니다. 공식문서(아파치 스파크 공식문서 참고)를 살펴본 결과 파티션은 병렬 처리가 되는 기준이었습니다. 여러 서버에 분산할 때 보통 하나의 서버 당 2~4개 정도의 파티션을 설정합니다. 이 기준은 개인의 클러스터 환경에 따라 기본 설정 값이 다르며 이 값은 원하는 값으로 바꿀 수 있습니다. 구글에서 이미지를 살펴보았을 때는 다들 task당 한개의 파티션이라고 합니다.불변성(Immutable)한 개의 RDD가 여러 개의 파티션으로 나뉘고 다수의 서버에서 처리되다 보니 작업 도중 일부 파티션 처리에 장애가 발생해 파티션 처리 결과가 유실될 수 있습니다. 하지만 스파크에서 RDD는 불변성이기 때문에 생성 과정에 사용되었던 연산들을 다시 실행하여 장애를 해결할 수 있습니다. 여기서 불변성이라는 말은 RDD에서 어떤 연산을 적용해 다른 RDD가 될 때 무조건 새로 RDD를 생성합니다(RDD는 불변이다). 이러한 방식 덕분에 장애가 발생해도 기존의 RDD 데이터에 다시 연산을 적용해 장애를 해결할 수 있는 것입니다(RDD는 회복 탄력성이 좋다(resilient)).게으른 연산(Lazy Operation)RDD의 연산은 크게 트랜스포메이션 과 액션이라는 두 종류로 나눌 수 있습니다.  트랜스포메이션: RDD1 -&amp;gt; RDD2 이런식으로 새로운 RDD를 만들어내는 연산, 대표적으로 map 함수  액션: RDD -&amp;gt; 다른 형태의 데이터를 만들어내는 연산, 대표적으로 reduce 함수(아파치 공식문서 참고)트랜스포메이션 연산은 보통 분산된 서버 각각에서 독립적으로 수행할 수 있는 연산입니다. 그리고 액션은 분산된 서버에 있는 데이터가 서로를 참조해야 하는 연산입니다. 그래서 액션은 서버 네트워크간의 이동이 발생하게 됩니다. 이런 현상을 셔플링(Shuffling)이라고 하고, 보통 네트워크에서 읽어오는 연산은 메모리에 비해 100만배 정도 느립니다.그렇기 때문에 셔플링이 발생하는 연산을 할 때에는 그 전에 최대한 데이터를 간추리는 것이 중요한데 스파크의 중요한 특징 중 하나가 바로 게으른 연산을 한다는 것입니다. 게으른 연산이라는 말은 RDD가 액션연산을 수행할 때에 비로소 모든 연산이 한꺼번에 실행된다는 것입니다. 이러한 방식의 장점은 데이터를 본격적으로 처리하기 전에 어떤 연산들이 사용되었는지 알 수 있고, 이를 통해 최종적으로 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그중에서 가장 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다. 이렇게 되면 셔플링이 최대한 작은 사이즈로 발생할 수 있도록 합니다.스파크는 RDD를 사용함으로써 처리 속도도 높이고, 장애 복구도 가능해졌다Spark ArchitectureSpark는 여러 모듈로 구성되어 있습니다. 크게 두 부분으로 나누어 보면, 컴퓨터 Cluster의 리소스를 관리하는 Cluster Manager와 그 위에서 동작하는 사용자 프로그램인 Spark Application(Driver Program과 여러 Executor 프로세스로 구성)으로 구분됩니다.  드라이버 프로그램  클러스터 매니저  워커 노드드라이버 프로그램  The process running the main() function of the application and creating the SparkContext  드라이버 프로그램의 역할은 다음과 같습니다.          클러스터 매니저(마스터 노드)와의 connection을 위한 스파크 컨텍스트 객체를 생성      SparkContext를 이용해 RDD 생성      SparkContext를 이용해 연산 정의      정의된 연산은 DAG 스케줄러에게 전달되고 스케줄러는 연산 실행 계획 수립 후 클러스터 매니저에 전달        드라이버 프로그램은 어떤 노드에도 위치할 수 있습니다.          클라이언트 모드: 클라이언트 노드로 따로 빼서 실행하는 경우, 클러스터 매니저가 있는 마스터 노드에서 실행하는 경우      클러스터 모드: 워커 노드에서 실행하는 경우 (–deploy-mode cluster 옵션 필요)        SparkContext는 스파크 어플리케이션 코드를 작성하기 위한 환경, 도구를 제공해주는 클래스입니다클러스터 매니저  An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN, Kubernetes)  Standalone: Standalone은 마스터 노드에서 ./sbin/start-master.sh 명령을 실행하면 된다  YARN: Hadoop이 설치된 노드가 클러스터 매니저가 된다  Kubernetes: Kubernetes가 설치된 노드가 클러스터 매니저가 된다클러스터 매니저의 종류마다 지원하는 범위가 세부적으로 다르지만 대략적인 역할은 다음과 같습니다.  스파크 컨텍스트 생성시 설정된 Executer의 개수, Executer의 메모리를 바탕으로 자원을 할당  이용 가능한 워커에 태스크를 할당하기 위해 노드를 모니터링(마스터 노드)스파크 공식문서를 보면 스파크 아키텍처에서 마스터 노드에 대한 별다른 언급은 없다. 그 이유가 그냥 클러스터 매니저가 실행되는 노드를 지칭하기 때문이다. 위에서 Standalone을 클러스터 매니저로 사용할 때 실행한 명령어가 start-master.sh인 것만 봐도 클러스터 매니저와 마스터 노드는 차이가 없다.  Standalone일 때 마스터 명시: --master spark://&amp;lt;클러스터 매니저를 실행 중인 노드의 호스트&amp;gt;:&amp;lt;포트&amp;gt;  Kubernetes일 때 마스터 명시: --master k8s://https://&amp;lt;k8s-apiserver-host&amp;gt;:&amp;lt;k8s-apiserver-port&amp;gt;워커 노드스파크 컨텍스트는 워커 노드에 Executer를 생성하도록 클러스터 매니저에 요청을 하고 클러스터는 그에 맞춰 Executer를 생성합니다. Executer가 생성되면 드라이버 프로그램은 정의된 연산을 수행합니다.이 때 작업을 실제로 수행하는 것은 아니고 액션 연산의 수만큼 잡(Job)을 생성하고 잡은 셔플링이 최대한 적게 일어나는 방향으로 스테이지(Stage)를 나눕니다. 나누어진 스테이지는 다시 여러 개의 태스크(Task)로 나누어진 후 워커 노드에 생성된 Executer에 할당됩니다. 워커 노드는 Executer를 이용해 태스크를 처리하고, 데이터를 나중에 재사용 할 수 있도록 메모리에 저장도 합니다.  Executer: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위, 하나의 노드에 여러 개 Executer 가능  Job: 액션 연산의 수  Task: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위알아두면 좋은 것들ShufflingPassing Functions to Spark질문  RDD가 파티션으로 나뉘어지는 시점은 RDD가 생성되는 순간일까 아니면 연산이 실행되는 순간일까?  어떤 기준으로 RDD를 파티셔닝할까?  셔플링은 액션 연산에서만 발생할까?참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  stackoverflow: Understand Spark: Cluster Manager, Master and Driver nodes (훌륭한 질문과 훌륭한 대답)  How Applications are Executed on a Spark Cluster",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-15T21:01:35+09:00'>15 Jan 2022</time><a class='article__image' href='/spark-series1'> <img src='/images/spark_6.png' alt='Apache Spark Series [Part1]: What is Apache Spark?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series1'>Apache Spark Series [Part1]: What is Apache Spark?</a> </h2><p class='article__excerpt'>스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Github Actions Series [Part1]: Understanding GitHub Actions",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/github_action_series1",
      "date"     : "Jan 13, 2022",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-13T21:01:35+09:00'>13 Jan 2022</time><a class='article__image' href='/github_action_series1'> <img src='/images/github-actions_logo.png' alt='Github Actions Series [Part1]: Understanding GitHub Actions'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action_series1'>Github Actions Series [Part1]: Understanding GitHub Actions</a> </h2><p class='article__excerpt'>GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part2]: Kubernetes Resource",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series2",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  쿠버네티스의 리소스          Workload Resources                  Pod          ReplicaSet          Deployment                    Service관련 리소스                  Service          Ingress                    Config and Storage관련 리소스                  ConfigMap          Volume                      참고자료쿠버네티스의 리소스Workload Resources  Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.(Rancher문서 참고)PodPod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다. Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다. 컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.Pod가 생성되는 과정은 다음과 같습니다.Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 할당합니다. 그러면 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.🐨 오브젝트 생성을 위한 YAML파일Pod를 포함해 쿠버네티스의 오브젝트를 만들기 위해서는 YAML파일이 필요합니다. YAML파일에 오브젝트를 위한 설정들을 작성할 수 있는데, 이 때 필수적으로 사용되는 key값들이 있습니다.            Key      설명      예              apiVersion      오브젝트 버전      v1, app/v1, ..              kind      오브젝트 종류      Pod, ReplicaSet, Deployment, ..              metadata      메타데이터      name, label, ..              spec      오브젝트 별 상세 설정      오브젝트마다 다름      apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1Pod의 spec에는 containers, volumes, restartPolicy, hostname, hostNetwork 등이 있습니다.(Pod공식문서 참고)ReplicaSetReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트입니다. 단일 노드 환경이면 Pod는 모두 단일 노드에서 생성되고, 여러개의 노드를 가지고 있는 상황이면, Pod는 노드에 각각 분산되어 배포됩니다.(노드 장애 대비) 이 때 Pod를 어떤 노드에 배치할지는 스케줄러가 결정하게 됩니다. 보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많습니다.ReplicaSet은 다음과 같이 동작합니다.ReplicaSet controller가 desired state에 맞춰 Pod를 생성합니다. 그러면 Scheduler는 생성된 Pod를 노드에 할당해줍니다.apiVersion: apps/v1kind: ReplicaSetmetadata:  name: echo-rsspec:  replicas: 3  selector:    matchLabels: # app: echo이고 tier: app인 label을 가지는 파드를 관리      app: echo      tier: app  template: # replicaset이 만드는 pod의 템플릿    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v1ReplicaSet의 spec에는 replicas, selector, template, minReadySeconds가 있습니다.(ReplicaSet 공식문서 참고)DeploymentDeployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트입니다. ReplicaSet을 이용하여 Pod을 업데이트하고 이력을 관리하여 롤백Rollback하거나 특정 버전revision으로 돌아갈 수 있습니다.Deployment 오브젝트가 Pod의 버전을 관리하는 과정은 다음과 같습니다.Deployment Controller가 Deploy 조건을 체크하면서 원하는 버전에 맞게 Pod의 버전을 맞춥니다. 이 때 ReplicaSet에 있는 Pod들을 보통 한 번에 바꾸지 않고 조건에 맞게(예를 들어, 25%씩) 바꿔나감으로써 버전을 바꾸더라도 중간에 서비스가 중단되지 않도록 합니다. (무중단배포)apiVersion: apps/v1kind: Deploymentmetadata:  name: echo-deployspec:  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 0  replicas: 4  selector:    matchLabels:      app: echo      tier: app  template:    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v2spec에는 replicas, selector, template, strategy  등이 있습니다.(Deployment 공식문서 참고)Service관련 리소스  In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.ServiceService는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod와 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.  ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할  NodePort: 외부에서 접근가능하도록 하는 포트 역할  LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.# ClusterIP# redis라는 Deployment 오브젝트에 IP할당apiVersion: v1kind: Servicemetadata:  name: redisspec:  ports:    - port: 6379 # clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)      protocol: TCP  selector: # 어떤pod로 트래픽을 전달할지 결정    app: counter    tier: db# NodePortapiVersion: v1kind: Servicemetadata:  name: counter-npspec:  type: NodePort  ports:    - port: 3000 # ClusterIP, Pod IP의 포트      protocol: TCP      nodePort: 31000 # Node IP의 포트  selector:    app: counter    tier: app# LoadBalancerapiVersion: v1kind: Servicemetadata:  name: counter-lbspec:  type: LoadBalancer  ports:    - port: 30000      targetPort: 3000      protocol: TCP  selector:    app: counter    tier: appIngressIngress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 nginx, haproxy, traefik, alb등이 있습니다.minikube를 이용할 경우 다음 명령어로 설치할 수 있습니다.# nginx ingress controllerminikube addons enable ingressapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: echo-v1spec:  rules:    - host: v1.echo.192.168.64.5.sslip.io      http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: echo-v1                port:                  number: 3000# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라spec에는 rules, defaultBackend(어느 rule에도 속하지 않을 경우) 등이 있습니다.(Ingress 공식문서 참고)Config and Storage관련 리소스ConfigMapConfigMap은 설정, 환경 변수들을 담는 오브젝트입니다. 예를 들어 개발/운영에 따라 환경 변수값이 다른 경우, ConfigMap 을 활용해 Pod 생성시 넣어줄 수 있습니다.ConfigMap을 다양한 방법으로 만들 수 있습니다.  ConfigMap yaml 파일로 오브젝트 생성  환경 변수 설정을 담고 있는 yaml파일을 ConfigMap 오브젝트로 생성  그냥 환경 변수를 담고 있는 임의의 파일을 ConfigMap 오브젝트로 생성# ConfigMap yaml파일apiVersion: v1 # 참고로 v1이면 core API groupkind: ConfigMapmetadata:  name: my-configdata:  hello: world  kuber: neteskubectl apply -f config-map.yml# 환경 변수 설정을 담고 있는 yaml파일global:  scrape_interval: 15sscrape_configs:  - job_name: prometheus    metrics_path: /prometheus/metrics    static_configs:      - targets:          - localhost:9090# yaml 파일로 ConfigMap 파일 생성kubectl create cm my-config --from-file=config-file.yml# ConfigMap 적용kubectl apply -f my-config.yml# config-env.yml파일 (yml파일 아니지만 그냥 확장자 yml로 해놓아도됨)hello=worldhaha=hoho# 임의의 파일로 ConfigMap 파일 생성kubectl create cm env-config --from-env-file=config-env.yml# ConfigMap 적용kubectl apply -f env-config.yml여러 가지 방법으로 ConfigMap을 Pod에 적용할 수 있습니다.  디스크 볼륨 마운트  환경변수로 사용# ConfigMap yaml파일이 있는 볼륨 마운트apiVersion: v1kind: Podmetadata:  name: alpinespec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      volumeMounts:        - name: config-vol          mountPath: /etc/config  volumes:    - name: config-vol      configMap:        name: my-config# ConfigMap yaml파일 직접 환경변수로 설정apiVersion: v1kind: Podmetadata:  name: alpine-envspec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      env:        - name: hello          valueFrom:            configMapKeyRef:              name: my-config              key: helloVolumeVolume은 저장소와 관련된 오브젝트입니다. 지금까지 만들었던 컨테이너는 Pod을 제거하면 컨테이너 내부에 저장했던 데이터도 모두 사라집니다. MySQL과 같은 데이터베이스는 데이터가 유실되지 않도록 반드시 별도의 저장소에 데이터를 저장하고 컨테이너를 새로 만들 때 이전 데이터를 가져와야 합니다.저장소를 호스트 디렉토리를 사용할 수도 있고 EBS 같은 스토리지를 동적으로 생성하여 사용할 수도 있습니다. 사실상 인기 있는 대부분의 저장 방식을 지원합니다.저장소의 종류에는 다음과 같은 것들이 있습니다.  임시 디스크          emptyDir                  Pod 이 생성되고 삭제될 때, 같이 생성되고 삭제되는 임시 디스크          생성 당시에는 아무 것도 없는 빈 상태          물리 디스크(노드), 메모리에 저장                      로컬 디스크          hostpath                  노드가 생성될 때 이미 존재하고 있는 디렉토리                      네트워크 디스크          awsElasticBlockStore, azureDisk 등      # emptydirapiVersion: v1kind: Podmetadata:  name: shared-volumes spec:  containers:  - name: redis    image: redis    volumeMounts:    - name: shared-storage      mountPath: /data/shared  - name: nginx    image: nginx    volumeMounts:    - name: shared-storage      mountPath: /data/shared  volumes:  - name : shared-storage    emptyDir: {}# hostpathapiVersion: v1kind: Podmetadata:  name: host-logspec:  containers:    - name: log      image: busybox      args: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]      volumeMounts:        - name: varlog          mountPath: /host/var/log  volumes:    - name: varlog      hostPath:        path: /var/log참고자료  subicura님의 kubenetes안내서  하나씩 점을 찍어나가며 블로그  Kubernetes 공식문서  Rancher 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/kubernetes-series2'> <img src='/images/kube_22.png' alt='Kubernetes Series [Part2]: Kubernetes Resource'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series2'>Kubernetes Series [Part2]: Kubernetes Resource</a> </h2><p class='article__excerpt'>Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/container-series1",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  물리서버(Bare Metal)  가상머신(Virtual Machine)          호스트 가상화      하이퍼바이저 가상화        컨테이너(Container)  참고이번 글에서는 IT업계에서 그동안 어떤 방법으로 컴퓨팅 자원을 사용해서 애플리케이션을 개발해왔는지에 대해서 살펴보겠습니다. 순서는 크게 물리서버, 가상머신, 컨테이너 순으로 알아보도록 하겠습니다.물리서버(Bare Metal)물리서버는 서버가 가지고 있는 컴퓨팅 자원을 오직 하나의 사용자(여기서 사용자는 서비스 사용자가 아니라 자원을 사용하는 사람)에게 할당하는 것입니다. 그렇기 때문에 물리서버에서는 자원을 전혀 분리해서 사용하지 않습니다.자원을 분리하지 않고 사용하는 것에는 장단점이 있습니다.먼저 장점으로는 하드웨어간 네트워크 이동이 발생하지 않기 때문에 네트워크 지연율을 최소화할 수 있고, 보안적인 측면에서도 훨씬 안전하다고 할 수 있습니다. 또한 별도의 하드웨어 에뮬레이션이 없기 때문에 하드웨어에 루트 레벨의 접근이 가능하고 이를 통해 뛰어난 커스터마이징을 할 수 있습니다. 그래서 보안을 최우선으로 생각하는 뱅킹시스템이나 데이터베이스 시스템은 이러한 Bare Metal 형태로 컴퓨팅 자원을 사용합니다.단점으로는 서비스 개발에 있어 여러 개의 애플리케이션을 독립적으로 개발하기가 힘듭니다. 그 이유는 자원이 분리되어 있지 않고 그로 인해 각각의 애플리케이션에 필요한 라이브러리 종속성도 해결하기가 어려워집니다. 한마디로 디펜던시 지옥에 빠질 수 있습니다 또한 사용자가 가지고 있는 Bare Metal 서버에 최적화된 방식으로 서비스를 개발하다보니 다른 서버에 배포가 힘들어집니다. 이러한 단점은 Scale Out 방식이 아닌 Scale Up으로 밖에 확장성을 가지지 못한다는 점으로 이어집니다.가상머신(Virtual Machine)약 1990년까지 하드웨어는 급속도로 성장해 왔습니다. 인텔의 설립자인 고든 무어가 1965년에 무어의 법칙을 얘기했다는 사실을 생각해보면 대략 30년이 지났으므로 2년간 2배씩 성장한다고 했을 떄 약 3만배(2의 15승) 가까이 성장을 한 것입니다. 하지만 그당시 소프트웨어의 발전은 한참 뒤쳐져 있는 상황이었습니다. 그래서 1990년대 중순에 처음으로 가상화 기술에 대한 요구가 생겨나기 시작했습니다.호스트 가상화처음 시중에 등장한 가상화 기술은 호스트 가상화(Type2 Hypervisor)였습니다. 호스트가상화는 Host OS위에 컴퓨팅 자원이 격리된 가상 머신을 띄우고 그 안에 Guest OS가 구동되는 방식입니다. 종류로는 VM Workstation, VMware Server, VMware Player, MS Virtual Sever, Virtual Box 등이 있습니다. 호스트 가상화는 일반 사용자들이 자신의 컴퓨터 위에 가상 머신을 실행시키는 방식으로 간단하게 가상화할 수 있지만, 하드웨어 에뮬레이팅을 위한 하이퍼바이저(Hypervisor)와 Host OS라는 두 개의 소프트웨어를 추가로 실행시켜야 하는 오버헤드가 있습니다.하이퍼바이저 가상화그다음 등장한 것이 하이퍼바이저 가상화(Type1 Hypervisor)입니다. 현재 서버 가상화 기술에서는 주류 방식으로 사용되고 있습니다. 종류로는 Xen, KVM 등이 있습니다. 이러한 방식의 가상화는 Host OS 없이 사용하는 가상화 방식이기 때문에 불필요한 오버헤드를 줄여줍니다. 아마존 AWS와 같은 클라우드의 컴퓨팅 서비스가 대표적으로 이러한 방식의 가상화 기술을 사용합니다. 하지만 Host OS가 없다는 사실에서 생기는 문제는 각각 다른 Guest OS가 하드웨어에 접근할 수 있어야 한다는 것입니다. 이를 가능하게 하는 방법에는 2가지가 있습니다.첫 번째는 하이퍼바이저가 구동될 때 DOM0라고 하는 관리용 가상머신을 하나 실행시켜 DOM0가 중개하는 전가상화 방식입니다. DOM0의 역할은 각각의 Guest OS에서 발생하는 요청을 하이퍼바이저가 해석할 수 있도록 컴파일해서 하이퍼바이저에 전달하는 것입니다. 이 방법은 호스트 OS보다는 가벼운 DOM0를 실행한다는 점에서 오버헤드가 줄게되지만 여전히 성능상의 단점이 있습니다.두 번째는 반가상화 방식(Bare Metal Hypervisor)입니다. 반가상화는 DOM0를 없애고, 각각의 Guest OS가 하이퍼바이저에게 직접 요청(Hypercall)할 수 있도록 Guest OS의 커널을 수정하는 방법입니다. 이 방법은 별도의 레이어가 필요없기 때문에 가장 오버헤드가 적게 발생합니다. 하지만 이 방법은 OS의 커널을 수정해야하기 때문에 오픈 소스의 OS에서만 가능하고 macOS나 windows같은 운영체제에서는 불가능합니다.하지만, 어디까지나 분류는 분류일 뿐 Type 1 하이퍼바이저들이 모두 전가상화에만 속하거나 반가상화에만 속하는 것은 아닙니다. 최근에는 하이퍼바이저에서 전가상화와 반가상화의 경계가 별 의미가 없어졌습니다.  VMware나 KVM이 대표적인 전가상화 제품에 속하고, Xen이 대표적인 반가상화 제품에 속했다(과거형).  전가상화는 모든 CPU 명령어를 가상화(애뮬레이션)하므로 아키텍처에 제한을 받지 않지만 느리다.  반가상화는 꼭 필요한 CPU 명령어만 가상화한다.          꼭 필요한 명령어만을 가상화 요청(Hyper Call)하도록 커널 수정 필요        Xen은 반 가상화 하이퍼바이저로 등장했지만 오래 전부터 전 가상화도 지원한다.  VMWare이나 KVM도 전 가상화 하이퍼바이저이지만 반 가상화 기능을 제공한다.  전 가상화와 반 가상화 하이퍼바이저의 경계가 거의 없어짐.컨테이너(Container)컨테이너 기술은 가상화의 꽃이라고 할 수 있습니다. 컨테이너는 애플리케이션 가상화로, VM과 달리 OS를 포함하지 않습니다. 즉, 하드웨어와 호스트 OS는 그대로 둔 채 애플리케이션 영역만 캡슐화하여 격리하는 방식입니다. VM에 비해 가볍고, 배포가 빠르며, 자원을 효율적으로 사용할 수 있다는 장점이 있어 최근에 많이 활용되고 있습니다. 이 컨테이너 기술이 정말 대중적인 가상화 기술로 자리잡게될 수 있었던 데에는 2013년 발표된 도커(Docker) 덕분일 것입니다.참고로 도커 이전에도 컨테이너 기반의 가상화는 있었습니다. 컨테이너의 역사에 대해 간략히 살펴보겠습니다.  2000년, Unix OS 인 FreeBSD 에서 OS 가상화 기능인 FreeBSD Jail를 발표합니다.  2001년, Linux에서 커널에 Linux-Vserver 라는 기능을 추가하여 OS 가상화 환경을 이용할 수 있게 되었습니다.  2006년, Google은 cgroup는 프로세스 자원 이용량을 제어하는 기능을 발표합니다.  2008년, Red Hat 에서 논리적으로 시스템 자원을 분할하는 Namespace를 발표합니다.  비슷한 시기에 IBM에서 LXC (LinuX Containers)를 발표합니다.  LXC가 cgroup 과 Namespace를 사용하여 구현한 최초의 Linux 컨테이너 엔진입니다.  2013년, 도커라는 회사에서 LXC를 아주 잘 활용할 수 있도록 도커( Docker) 라는 기술을 오픈소스로 발표합니다.  도커는 Dockerfile이란 메니페스트를 만들고, Container Hub를 만들면서, Container기술은 급속히 발전하게 됩니다.  2015년, Google에서 컨테이너를 통합하여 오케스트레이션하는 쿠버네티스라는 프로젝트를 오픈소스로 발표합니다.  2016년, 구글이 쿠버네티스를 CNCF 재단에 기증하면서 클라우드네이티브 시대의 서막을 알리게 됩니다.  이후 Containerd 와 CRI-O 그리고 PODMAN 등 컨테이너는 표준기술 중심으로 발전하고 있습니다.  이외에도 rht, OCI, CRI-O 등 표준 기술들이 발전하였고, 레드햇은 Kubernetes 기반으로 OpenShift를 개발했습니다.다음 포스트에서는 도커에 대해 공부해보도록 하겠습니다.참고  KT Cloud: Cloud 인프라 Intro - 물리서버와 가상서버  opennaru: 물리서버 , 가상화 , 컨테이너 기술 진화의 역사  phoenixnap: The Definitive Guide to Bare Metal Servers for 2021  phoenixnap: Bare Metal Vs VM: What Performs Better  IT Opening: Xen Kvm 반가상화 전가상화 차이 비교 분석  NDS: [소개] 가상화의 종류3가지  하드웨어 가상화(Virtualization) 뜻, 가상화 기술 종류 4가지, 가상머신(Virtual Machine)의 단점 3가지  Rain.i: 하이퍼바이저(Hypervisor)의 종류  openmaru: 컨테이너 기술의 발전과 역사",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/container-series1'> <img src='/images/container_5.png' alt='Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series1'>Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container</a> </h2><p class='article__excerpt'>도커 이전에도 컨테이너 기반의 가상화는 있었습니다. 컨테이너의 역사에 대해 간략히 살펴보겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Django Series [Part1]: ",
      "category" : "",
      "tags"     : "Django",
      "url"      : "/django-series0",
      "date"     : "Jan 9, 2022",
      "content"  : "",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/django-series0'> <img src='/images/django_logo.png' alt='Django Series [Part1]: '> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/django-series0'>Django Series [Part1]: </a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part1]: Kubernetes Intro",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  들어가기 전에          도커의 등장        Kubernetes          쿠버네티스 소개      쿠버네티스 아키텍쳐      Desired State        마치며  참고자료들어가기 전에도커의 등장2013년 도커가 등장하기 전까지 서버 관리는 굉장히 어렵고 컨트롤하기 어려운 것으로 여겨졌습니다. 하나의 서비스를 제공하기 위해서는 보통 수십에서 수백개의 애플리케이션이 서로 연결되어 동작하는데, 이 때 오류가 나게 되면 어디서 문제가 생긴건지 파악하기가 쉽지 않았습니다.이러한 문제를 해결하기 위해 사람들은 가상화 기술을 이용해 서버를 애플리케이션별로 격리시키고자 하였습니다. 이 때 크게 두가지 방법으로 접근할 수 있는데, 하나는 가상머신을 이용해 컴퓨팅 리소스를 따로 분리하여 사용하도록 하는 것이었습니다. 하지만 이 방법은 컴퓨팅 성능을 떨어트립니다.두 번째 방법은 LXC(LinuX Containers)라는 리눅스 커널 기술로 기존의 하드웨어 레벨에서 하던 방식을 운영체제 레벨에서 해결하도록 했습니다. 이렇게 하면 컴퓨팅 성능도 떨어트리지 않으면서, 파일시스템, 리소스(CPU, 메모리, 네트워크)를 분리할 수 있습니다. 하지만 이 방법은 사용하기에는 운영체제에 대한 깊은 이해를 필요로 해서 많은 개발자들이 쉽게 쓰기는 힘들었습니다.이 때 등장한 것이 바로 도커입니다. 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되자, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 수많은 애플리케이션이 컨테이너로 배포되고 도커파일을 만들어 이미지를 빌드하고 컨테이너를 배포하는 게 흔한 개발 프로세스가 되었습니다.(도커에 관한 더 자세한 내용은 여기를 참고하시기 바랍니다)이제 모든 것들을 컨테이너화하기 시작하면서 우리의 서비스는 다음과 같은 모습을 가지게 되었습니다.이렇게 서비스 하나를 배포하기 위해 수많은 컨테이너를 띄우고, 연결하고, 버전업을 해야하는 상황이 생긴겁니다. 그래서 개발자들은 이제 컨테이너들을 동시에 띄우고 관리까지 해주는 컨테이너 오케스트레이션기술이 필요해지게 되었습니다.Kubernetes쿠버네티스 소개쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 합니다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작합니다.쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화합니다.쿠버네티스 아키텍쳐  마스터: 전체 클러스터를 관리하는 서버  노드: 컨테이너가 배포되는 서버쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.  컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 Desired 상태를 유지, API서버 요청 처리  스케줄러: 새로 생성되는 Pod(컨테이너와 비슷)가 있는지 계속 체크, 생성되면 가장 적절한 노드 선택컨트롤러는 자신이 맡고 있는 오브젝트의 상태를 계속 체크하고 상태를 유지합니다. 또한 API 서버에서 어떤 새로운 상태를 요구할 경우, 맞춰서 또 상태를 바꿔서 유지하고 이 때 새롭게 Pod가 생성되거나 삭제되면 스케줄러가 그에 맞춰서 노드에서 삭제, 할당합니다.Desired State쿠버네티스에서 가장 중요한 것은 desired state(원하는 상태)라는 개념입니다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말합니다.쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있습니다.이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청합니다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 됩니다.마치며쿠버네티스는 여러 컨테이너를 자동으로 배포해주고 관리해준다는 점에서 정말 좋은 기술입니다. 그리고 마이크로서비스, 클라우드 환경과도 정말 잘 어울리기 때문에 배워두면 정말 쓸모가 많을 것 같습니다. 하지만 쿠버네티스는 많은 영역을 커버하다보니 배워야할 것들이 굉장히 많습니다. 그리고 컨테이너들을 띄우는 서버를 관리하기 위한 서버를 더 사용하게 되는 것이기 때문에, 컴퓨팅 자원이 충분하지 않다면 사용하는 것이 적절하지 않을 수도 있습니다. (쿠버네티스를 운영환경에 설치하기 위해선 최소 3대의 마스터 서버와 컨테이너 배포를 위한 n개의 노드 서버가 필요)다음 포스트에서는 서버가 넉넉하지 않은 상황에서 사용할 수 있는 minikube를 설치, 그리고 쿠버네티스에 명령어를 전달할 때 사용하는 kubectl 설치해보겠습니다.그리고 도커에서는 컨테이너를 띄우지만 쿠버네티스에서는 컨테이너를 관리할 수 있도록 조금 더 패키징한 다양한 오브젝트를 띄우게 되는데 이 때 어떠한 오브젝트들이 있는지도 배워보도록 하겠습니다.참고자료  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/kubernetes-series1'> <img src='/images/kube_23.svg' alt='Kubernetes Series [Part1]: Kubernetes Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series1'>Kubernetes Series [Part1]: Kubernetes Intro</a> </h2><p class='article__excerpt'>쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part1]: Javascript Intro",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  자바스크립트의 탄생  자바스크립트의 표준화  자바스크립트의 역사          Ajax      jQuery      자바스크립트 엔진 V8      Node.js      SPA        ECMAScript  자바스크립트의 특징  자바스크립트 실행 환경자바스크립트의 탄생1995년 웹 브라우저 시장을 지배하고 있던 넷스케이프 커뮤니케이션즈는 웹 페이지의 보조적인 기능을 수행하기 위해 브라우저에서 동작하는 경량 프로그래밍 언어를 도입하기로 결정한다. 그래서 탄생한 것이 브렌던 아이크(Brendan Eich)가 개발한 자바스크립트다.자바스크립트의 표준화1996년 마이크로소프트에서 마이크로소프트에서 인터넷 익스플로러의 점유율을 높이고자 자바스크립트의 파생 버전인 Jscript를 인터넷 익스플로러에 탑재했다. 이로 인해 브라우저에 따라 웹페이지가 정상적으로 동작하지 않는 크로스 브라우징 이슈가 발생하기 시작했다.넷스케이프 커뮤니케이션즈는 컴퓨터 시스템의 표준을 관리하는 비영리 표준화 기구인 ECMA 인터내셔널에 자바스크립트의 표준화를 요청한다.1997년 ECMA-262라 불리는 표준화된 자바스크립트 초판이 완성되었고, 상표권 문제로 자바스크립트는 ECMAScript로 명명되었다.            버전      출시연도      특징              ES1      1997      초판              ES2      1998      ISO/IEC 16262 국제 표준과 동일한 규격을 적용              ES3      1999      정규표현식, try … catch              ES5      2009      HTML5와 함께 출연한 표준안, JSON, strict mode, 접근자 프로퍼티, 프로퍼티 어트리뷰트 제어, 향상된 배열 조작 기능(forEach, map, filter, reduce, some, every)              ES6(ECMAScript 2015)      2015      let/const, 클래스, 화살표 함수, 템플릿 리터럴, 디스트럭처링 할당, 스프레드 문법, rest파라미터, 심벌, 프로미스, Map/Set, 이터러블, for…of, 제너레이터, Proxy, 모듈 import/export      자바스크립트의 역사초창기 자바스크립트는 웹페이지의 보조적인 기능을 수행하기 위한 한정적인 용도로 사용되었다. 이 시기에 대부분의 로직은 주로 웹 서버에서 실행되었고, 브라우저는 서버로부터 전달받은 HTML과 CSS를 단순히 렌더링하는 수준이었다.Ajax1999년, 자바스크립트를 이용해 서버와 브라우저가 비동기방식으로 데이터를 교환할 수 있는 통신 기능인 Ajax(Asynchoronous JavaScript and XML)가 XMLHttpRequest라는 이름으로 등장했다.이전의 웹페이지는 html 태그로 시작해서 html 태그로 끝나는 완전한 HTML 코드를 서버로부터 다시 전송받아 웹페이지 전체를 렌더링하는 방식으로 동작했다. 이러한 방식은 변경할 필요가 없는 부분까지 서버로부터 코드를 다시 전송받기 때문에 성능면에서 부족한 점이 있었다.Ajax의 등장 이후, 웹 페이지에서 변경할 필요가 없는 부분은 다시 렌더링하지 않고, 필요한 부분만 렌더링하는 방식이 가능해졌다. 이로써 웹 브라우저에서도 데스크톱 애플리케이션과 유사한 빠른 성능과 부드러운 화면 전환이 가능해졌다.jQuery2006년 jQery의 등장으로 다소 번거로웠던 DOM(Document Object Model)을 더욱 쉽게 제어할 수 있게 되었고, 크로스 브라우징 이슈도 어느 정도 해결되었다. jQuery는 많은 사용자 층을 확보하게 되었고, 다소 배우기 까다로웠던 자바스크립트보다 jQuery를 더 선호하는 개발자가 양산되기도 했다.자바스크립트 엔진 V8그동안 웹 애플리케이션은 데스크톱 애플리케이션에 비해 성능상의 한계점이 있다는 인식이 있어왔지만 Ajax의 등장으로 웹 애플리케이션의 가능성을 확인하게 되었고, 이 후 자바스크립트로 웹 애플리케이션을 구축하려는 시도가 늘면서 자바 스크립트를 구동하는 자바스크립트 엔진의 성능을 더 높이고자 하는 요구가 생기게 되었다.이에 구글은 2008년 V8이라는 자바스크립트 엔진을 개발하였고 V8의 등장으로 자바스크립트를 이용해 개발한 웹 애플리케이션이 기존의 데스크톱 애플리케이션과 유사한 UX를 제공할 수 있게 되었다.Node.jsNode.js는 라이언 달(Ryan Dahl)이 2009년 개발한 자바스크립트 엔진 V8로 빌드된 자바스크립트 런타임 환경이다.Node.js는 브라우저의 자바스크립트 엔진에서만 동작하던 자바스크립트를 브라우저 이외의 환경에서도 동작할 수 있도록 했다.Node.js는 다양한 플랫폼에 적용할 수 있지만 서버 사이드 애플리케이션 개발에 주로 사용되며, 이에 필요한 모듈, 파일 시스템, HTTP 등 빌트인 API를 제공한다.프론트엔드와 백엔드 영역을 모두 자바스크립트로 개발할 수 있다는 동형성(isomorphic)은 개발 속도를 향상시켰다.그동안 브라우저에서만 동작하는 반쪽짜리 프로그래밍 언어 취급을 받았지만 Node.js의 등장으로 서버 사이드 애플리케이션 개발에도 사용할 수 있게 됨에따라 현재는 프론트영역 백엔드 영역을 아우르는 웹 프로그래밍 언어의 표준으로 자리잡았다.Node.js는 비동기 I/O을 지원하며, 단일 스레드 이벤트 루프 기반으로 동작함으로써 요청 처리 성능이 좋다. 따라서 Node.js는 데이터를 실시간으로 처리하기 위해 I/O이 빈번하게 발생하는 SPA(Single Page Application)에 적합하다.SPA자바스크립트의 발전으로 웹 어플리케이션을 이용한 개발이 활발해지다보니 복잡한 규모의 개발에 점점 대처하기가 어려워졌다. 이러한 요구에 발맞춰 여러 기업에서는 CBD(Component Based Development) 방법론을 기반으로 하는 SPA가 대중화 되면서 Angular, React, Vue.js등 다양한 프레임워크/라이브러리가 등장하게 되었다ECMAScriptECMAScript는 자바스크립트의 표준 사양인 ECMA-262를 말합니다. 각 브라우저 제조사는 ECMAScript 사양을 준수해 브라우저의 자바스크립트 엔진을 구현한다.자바스크립트는 ECMAScript와 브라우저가 별도 지원하는 클라이언트 사이드 Web API(DOM, XMLHttpRequest, fetch 등)을 아우르는 개념이다.자바스크립트의 특징자바스크립트는 웹브라우저에서 동작하는 유일한 프로그래밍 언어다.자바스크립트는 개발자가 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어다.자바스크립트는 명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어다.자바스크립트 실행 환경자바스크립트를 실행하기 위해서는 자바스크립트 엔진이 필요한데 이는 브라우저와 Node.js에만 있다.자바스크립트를 개발/테스트할 때는 주로 크롬의 개발자 도구, Node.js, 비주얼 스튜디오 코드의 Live Server 확장 플러그인을 사용한다. Live Server를 사용하면 별도의 가상 서버가 기동되고 서버에 있는 브라우저에 HTML 파일을 로딩한다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/javascript-series1'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part1]: Javascript Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series1'>Javascript Series [Part1]: Javascript Intro</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part4]: ElasticSearch 검색",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-search",
      "date"     : "Jan 7, 2022",
      "content"  : "Table of Contents  검색 API  Query DSL검색 APIQuery DSL",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/elasticsearch-search'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part4]: ElasticSearch 검색'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-search'>ElasticSearch Series [Part4]: ElasticSearch 검색</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part1]: AWS Intro",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series1",
      "date"     : "Jan 7, 2022",
      "content"  : "Table of Contents  클라우드 컴퓨팅  AWS의 정의와 특징클라우드 컴퓨팅  Cloud computing is distribution of computing services via the Internet, including server, storage, database, networking and software to provide faster innovation, flexible resources and economics of scaleAWS의 정의와 특징      Cloud computing platform used to manage and maintain hareware and infrastructure of resources        It is comprehensive and simple-to-use computing platform offered by Amazon        It reduces the expense and complexity of purchasing and running resources on-site for businesses and indivisuals        IaaS, PaaS, SaaS are used to build it  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/aws-series1'> <img src='/images/aws_logo.png' alt='AWS Series [Part1]: AWS Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series1'>AWS Series [Part1]: AWS Intro</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part3]: ElasticSearch Modeling",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-modeling",
      "date"     : "Jan 6, 2022",
      "content"  : "Table of Contents  Elasticsearch 모델링          데이터 타입                  Keyword 데이터 타입          Text 데이터 타입                    매핑 파라미터      Elasticsearch 모델링엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.  필드에 지정할 데이터 타입  필드별 매핑 파라미터예를 들어, 영화 정보를 엘라스틱서치를 이용해 저장하고 싶다면,            필드명      필드 타입              movieTitle      text              jenre      keyword              nation      keyword              produceYear      integer              releaseDate      date              actor      keyword      이렇게 필드별로 text, keyword, integer, date 등의 데이터 타입을 설정할 수 있습니다.데이터 타입# 대표적인 데이터 타입- 문자열 관련한 데이터 타입: keyword, text- 일반적인 데이터 타입: integer, long, double, boolean- 특수한 데이터 타입: date, ip, geo_point, geo_shapeKeyword 데이터 타입Keyword 데이터 타입은 문자열 데이터를 색인할 때 자주 사용하는 타입 중 하나로, 별도의 분석기 없이 원문 그대로가 저장된다는 것이 특징입니다. 예를 들어 ‘elastic search’라는 문자열을 keyword 타입으로 저장한다면, ‘elastic’이나 ‘search’로는 검색이 되지 않고 정확히 ‘elastic search’라고해야만 검색됩니다. 이러한 데이터 타입은 주로 카테고리형 데이터의 필드 타입으로 적절하며, 문자열을 필터링, 정렬, 집계할 때는 keyword타입을 이용해야 합니다.Text 데이터 타입반지의 제왕 영화 시리즈에는 ‘반지의 제왕: 반지 원정대’, ‘반지의 제왕: 두 개의 탑’, ‘반지의 제왕: 왕의 귀환’이 있습니다. 근데 저는 부제목까지는 기억이 안나고 ‘반지의 제왕’만 기억이 납니다. 그래서 저는 ‘반지의 제왕’이라고만 검색해도 위의 영화들이 나왔으면 좋겠습니다. 이럴 때는 text데이터 타입을 이용합니다. Text타입은 전문 검색이 가능하다는 점이 가장 큰 특징입니다. Text타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 역색인(inverted index)됩니다.더 자세한 내용은 공식문서를 참고해주시면 좋을 것 같습니다. (엘라스틱서치 공식문서 참고)매핑 파라미터문서를 색인하는 과정은 당연 엘라스틱서치에서 가장 중요한 부분입니다. 그렇기 때문에 엘라스틱서치에서는 매핑 파라미터를 통해 색인 과정을 커스텀하도록 도와줍니다. 예를 들어 어떤 영화 데이터는 장르가 없다고 하면 색인할 때 필드를 생성하지 않습니다. 이럴 때 null_value를 ‘데이터 없음’이라고 설정했다면, 필드가 생성 되고 값에 ‘데이터 없음’이라는 값이 들어갑니다. 또 다른 예시는 특정 필드를 색인에 포함할지 말지를 결정하는 enabled, index 파라미터도 있습니다. (둘의 차이는 stack overflow 참고)# 매핑 파라미터- 문자열에 자주 사용: analyzer, search_analyzer, similarity, term_vector, normalizer- 저장 관련: enabled, index, store, copy_to, doc_values- 색인 방식과 관련: ignore_above, ignore_malformed, coerce, dynamic, null_value- 필드 안의 필드를 정의할 때 사용: properties, fields- 그 밖: position_increment_gap, format(엘라스틱서치 공식문서 참고)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-06T21:01:35+09:00'>06 Jan 2022</time><a class='article__image' href='/elasticsearch-modeling'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part3]: ElasticSearch Modeling'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-modeling'>ElasticSearch Series [Part3]: ElasticSearch Modeling</a> </h2><p class='article__excerpt'>엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-element",
      "date"     : "Jan 5, 2022",
      "content"  : "Table of Contents  Elasticsearch를 구성하는 개념          인덱스      샤드      타입      문서      필드      매핑        Elasticsearch에서 제공하는 주요 API          인덱스 관련 API      문서 관련 API      검색 API      집계 API      Elasticsearch를 구성하는 개념“Elasticsearch에서 데이터는 인덱스 안의 특정 타입 에 문서 로 저장되는데 이 때 문서는 필드 를 가지고 있으며, 이러한 필드는 매핑 프로세스로 정의된 이름과 속성을 보통 따른다. 그리고 이 때 모든 문서들은 정의된 샤드 의 개수에 각각 골고루 분배되어 분산처리된다. 또한 장애를 대비해 레플리카 의 개수만큼 복제해 놓기도 한다.”위의 내용은 Elasticsearch의 데이터 저장방식에 관한 글입니다. 하지만 Elasticsearch가 익숙하지 않은 사람에게는 낯선 용어들도 있고 익숙하지만 이해하기 힘든 용어들도 있습니다. 그래도 Elasticsearch에서 사용하는 용어와 RDBMS에서 사용하는 용어를 비교하며 살펴보고 다시 한번 읽어보면 조금 더 이해가 갈 것입니다.            Elasticsearch      RDBMS              인덱스      데이터베이스              타입      테이블              문서      행              필드      열              매핑      스키마              샤드      파티션      인덱스인덱스는 논리적 데이터 저장 공간을 뜻하며, 하나의 물리적인 노드에 여러 개의 인덱스를 생성할 수도 있습니다. 이는 곧 멀티테넌시를 지원한다는 뜻이기도 합니다. 만약 Elasticsearch를 분산 환경으로 구성했다면 하나의 인덱스는 여러 노드에 분산 저장되며 검색 시 더 빠른 속도를 제공합니다.샤드분산 환경으로 저장되면 인덱스가 여러 노드에 분산 저장된다고 했는데, 이렇게 물리적으로 여러 공간에 나뉠 때의 단위를 샤드라고 합니다. 이 때 샤드는 레플리카의 단위가 되기도 합니다.타입타입은 보통 카테고리와 비슷한 의미로 노래를 K-pop, Classic, Rock처럼 장르별로 나누는 것과 같습니다. 하지만 6.1 버전 이후 인덱스 당 한 개의 타입만 지원하고 있습니다.문서한 개의 데이터를 뜻하며, 기본적으로 JSON 형태로 저장됩니다.필드필드는 문서의 속성을 나타내며 데이터베이스의 컬럼과 비슷한 의미입니다. 다만 컬럼의 데이터 타입은 정적이고, 필드의 데이터 타입은 좀 더 동적이라고 할 수 있습니다.매핑매핑은 필드와, 필드의 타입을 정의하고 그에 따른 색인 방법을 정의하는 프로세스입니다.Elasticsearch에서 제공하는 주요 APIfrom elasticsearch import ElasticsearchES_URL = &#39;localhost:9200&#39;ES_INDEX = &#39;first_index&#39;DOC_TYPE = &#39;_doc&#39;es = Elasticsearch(ES_URL)인덱스 관련 API# 인덱스 메타데이터, 매핑 정의index_settings = {    &#39;settings&#39;: {        &#39;number_of_shards&#39;: 2,        &#39;number_of_replicas&#39;: 1    },    &#39;mappings&#39;: {         &#39;properties&#39;: {            &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},            &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}        }            }}# 인덱스 생성es.indices.create(index=ES_INDEX, **index_settings)--------------------------------------------------------{&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;first_index&#39;}# 인덱스 메타 데이터 확인es.indices.get_settings()--------------------------------------------------------{&#39;first_index&#39;: {&#39;settings&#39;: {&#39;index&#39;: {&#39;routing&#39;: {&#39;allocation&#39;: {&#39;include&#39;: {&#39;_tier_preference&#39;: &#39;data_content&#39;}}},    &#39;number_of_shards&#39;: &#39;2&#39;,    &#39;provided_name&#39;: &#39;first_index&#39;,    &#39;creation_date&#39;: &#39;1641728644368&#39;,    &#39;number_of_replicas&#39;: &#39;1&#39;,    &#39;uuid&#39;: &#39;3QtIZXthRcGtCdV40WmUCg&#39;,    &#39;version&#39;: {&#39;created&#39;: &#39;7160299&#39;}}}}}# 인덱스 매핑 확인es.indices.get_mapping(index=ES_INDEX)--------------------------------------------------------{&#39;first_index&#39;: {&#39;mappings&#39;: {&#39;properties&#39;: {&#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},    &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}}}}}# 인덱스 삭제es.indices.delete(ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}# 인덱스 존재 유무es.indices.exists(ES_INDEX)--------------------------------------------------------True# 매핑 업데이트new_field = {  &quot;properties&quot;: {    &quot;school&quot; : {      &quot;type&quot;: &quot;text&quot;    }  }}es.indices.put_mapping(new_field, index=ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}문서 관련 API# 문서 삽입unit_document = {    &#39;name&#39;: &#39;Jay Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,    &#39;phone&#39;: &#39;010-1234-5678&#39;}es.index(index=ES_INDEX, doc_type=DOC_TYPE, id=1, document=unit_document)--------------------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 4, &#39;_primary_term&#39;: 1}# 문서 정보 확인es.get(index=ES_INDEX, doc_type=DOC_TYPE, id=1)-----------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;_seq_no&#39;: 0, &#39;_primary_term&#39;: 1, &#39;found&#39;: True, &#39;_source&#39;: {&#39;name&#39;: &#39;Jay Kim&#39;,  &#39;age&#39;: 28,  &#39;gender&#39;: &#39;male&#39;,  &#39;email&#39;: &#39;abc@gmail.com&#39;,  &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,  &#39;phone&#39;: &#39;010-1234-5678&#39;}}# 문서를 가지는 인덱스 생성 (이미 있으면 삽입)unit_document = {    &#39;name&#39;: &#39;Jae yeong Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;경북 구미 형곡동&#39;,    &#39;phone&#39;: &#39;010-3321-5668&#39;}es.create(index=ES_INDEX, id=2, body=unit_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 1, &#39;_primary_term&#39;: 1}# 문서 삭제es.delete(index=ES_INDEX, id=2)---------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;deleted&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 2, &#39;_primary_term&#39;: 1}# query로 삭제body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.delete_by_query(index=ES_INDEX, body=body)----------------------------------------------{&#39;took&#39;: 23, &#39;timed_out&#39;: False, &#39;total&#39;: 1, &#39;deleted&#39;: 1, &#39;batches&#39;: 1, &#39;version_conflicts&#39;: 0, &#39;noops&#39;: 0, &#39;retries&#39;: {&#39;bulk&#39;: 0, &#39;search&#39;: 0}, &#39;throttled_millis&#39;: 0, &#39;requests_per_second&#39;: -1.0, &#39;throttled_until_millis&#39;: 0, &#39;failures&#39;: []}# 문서 수정# &quot;&quot;doc&quot;&quot; is essentially Elasticsearch&#39;s &quot;&quot;_source&quot;&quot; fieldupdate_document = {&#39;doc&#39;: {         &#39;address&#39;: &#39;경북 구미 송정동&#39;,         &#39;age&#39;: 28,         &#39;email&#39;: &#39;ziont0510@gmail.com&#39;,    }}es.update(index=ES_INDEX, id=1, body=update_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;updated&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 5, &#39;_primary_term&#39;: 1}검색 API# 매칭되는 문서 개수body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.count(body=body, index=ES_INDEX)---------------------------------------------------------------{&#39;count&#39;: 2, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}}# 문서 검색body = {    &#39;size&#39;:10,    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}    es.search(body=body, index=ES_INDEX)-------------------------------------------------------{&#39;took&#39;: 6, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 0, &#39;relation&#39;: &#39;eq&#39;},  &#39;max_score&#39;: None,  &#39;hits&#39;: []}}집계 API",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-05T21:01:35+09:00'>05 Jan 2022</time><a class='article__image' href='/elasticsearch-element'> <img src='/images/elastic_4.png' alt='ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-element'>ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드</a> </h2><p class='article__excerpt'>Elasticsearch에서 데이터는 인덱스 안의 특정 타입에 문서로 저장되는데..</p></div></div></div>"
    } ,
  
    {
      "title"    : "MongoDB Series [Part1]: MongoDB Intro",
      "category" : "",
      "tags"     : "MongoDB",
      "url"      : "/mongodb-intro",
      "date"     : "Jan 4, 2022",
      "content"  : "Table of Contents  From MongoDB to S3  From Kafka to S3  참고From MongoDB to S3boto3 라이브러리 사용하면 내가 저장할 파일의 경로를 이용해서 s3에 저장 가능한데 몽고db는 뭔가 저장 포맷이 디코딩이 쉽지 않아 보인다. 그래서 다른 방법을 써야할 것 같다.(https://towardsdatascience.com/data-lake-in-s3-from-mongodb-addd0b9f9606)mongodb to s3 로 옮기는 좋은 방법은 airflow의 Operator 쓰는 것 같다(https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/operators/transfer/mongo_to_s3.html)(https://apache.googlesource.com/airflow/+/refs/tags/1.10.15/docs/installation.rst)그 밖에 가운데 스파크를 사용하는 방법도 있는 것 같다 (mongodb - spark - s3)(https://medium.com/@akash.kumar_5441/migrate-data-from-mongodb-into-s3-64838b2fc46e)From Kafka to S3sink connector참고  프로그래머 YD: Docker - 도커로 MongoDB 컨테이너 설치하는 방법을 알아보자  프리킴: [MongoDB] 몽고DB 기본 명령어  Confluent hub: Debezium MongoDB CDC Source Connector",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-04T21:01:35+09:00'>04 Jan 2022</time><a class='article__image' href='/mongodb-intro'> <img src='/images/mongodb_logo.png' alt='MongoDB Series [Part1]: MongoDB Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-intro'>MongoDB Series [Part1]: MongoDB Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part1]: ElasticSearch Installation",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-intro",
      "date"     : "Jan 3, 2022",
      "content"  : "Table of Contents  Elasticsearch 소개          Elasticsearch를 사용하는 이유                  Elasticsearch는 빠릅니다          Elasticsearch는 본질상 분산적입니다.          Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.          그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.                      Elasticsearch 설치          Docker를 이용한 설치      github + Docker를 이용한 설치      Linux에 직접 설치      마치며      Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64 OS architecture을 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-03T21:01:35+09:00'>03 Jan 2022</time><a class='article__image' href='/elasticsearch-intro'> <img src='/images/elastic_1.png' alt='ElasticSearch Series [Part1]: ElasticSearch Installation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-intro'>ElasticSearch Series [Part1]: ElasticSearch Installation</a> </h2><p class='article__excerpt'>Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진</p></div></div></div>"
    } ,
  
    {
      "title"    : "Pytorch Series [Part1]: torch",
      "category" : "",
      "tags"     : "Pytorch",
      "url"      : "/pytorch-torch",
      "date"     : "Jan 2, 2022",
      "content"  : "1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-02T21:01:35+09:00'>02 Jan 2022</time><a class='article__image' href='/pytorch-torch'> <img src='/images/pytorch_logo.webp' alt='Pytorch Series [Part1]: torch'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/pytorch-torch'>Pytorch Series [Part1]: torch</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part5]: 자바 조금 더 알아보기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series5",
      "date"     : "Jul 8, 2021",
      "content"  : "Table of Contents  1. 상속  2. 캐스팅과 제네릭          1) 캐스팅      2) 제네릭        3. 인터페이스와 추상 클래스          1) 인터페이스      2) 추상 클래스      1. 상속클래스간 공통된 속성과 기능이 많이 있을 경우 만약 이 공통된 부분을 클래스마다 다시 쓴다면 프로그래밍의 중요한 법칙 중 하나인 ‘DRY(Don’t Repeat Yourself; 중복 배체)’를 어기게 되는 것입니다. 자바의 ‘클래스 상속(Class Inheritance)’ 기능이 이 문제를 해결해줍니다.public class 자식클래스 extends 부모클래스 {    ...}접근 제어자가 private이 아니면 자식클래스에서도 변수와 메소드를 그대로 사용할 수 있습니다.자식 클래스가 부모 클래스가 가지고 있는 메소드를 덮어 쓰고 싶을 때는(기존 부모 클래스의 메소드와는 독립적인 메소드로 만들고 싶을 때) ‘메소드 오버라이딩(Method Overriding)’을 해줘야 합니다. 메소드 정의 위에 써져있는 @Override가 메소드 오버라이딩을 표시해줍니다. @Override와 같이 골뱅이(@)가 붙어있는 문법을 ‘어노테이션(Annotation)’이라고 합니다. 주석(Comment)과 어느정도 비슷하지만, 어노테이션은 자바에서 추가적인 기능을 제공합니다. 예를 들어서 @Override를 써줬는데 부모 클래스에 같은 이름의 메소드가 없는 경우, 오류가 나오게 됩니다.public class MinimumBalanceAccount extends BankAccount {    private int minimum;    @Override    public boolean withdraw(int amount) {        if (getBalance() - amount &amp;lt; minimum) {            System.out.println(&quot;적어도 &quot; + minimum + &quot;원은 남겨야 합니다.&quot;);            return false;        }        setBalance(getBalance() - amount);        return true;    }}이번에는 부모 클래스가 가지고 있는 메소드에서 몇 가지를 추가해서 쓰고 싶은 경우에는 super를 사용하면 됩니다.public class TransferLimitAccount extends BankAccount {    private int transferLimit;    @Override    boolean withdraw(int amount) {        if (amount &amp;gt; transferLimit) {            return false;        }        return super.withdraw(amount);    }}  2. 캐스팅과 제네릭1) 캐스팅ArrayList&amp;lt;BankAccount&amp;gt; accounts = new ArrayList&amp;lt;&amp;gt;();accounts.add(ba);accounts.add(mba);accounts.add(sa);for (BankAccount account : accounts) {    account.deposit(1000);}이렇게 하면 각 계좌가 BankAccount 타입으로 ‘캐스팅(Casting)’되고, 한꺼번에 묶어서 다룰 수 있습니다.sa에게는 이자를 붙여주고 싶은데, BankAccount 클래스에는 addInterest 메소드가 없습니다. 만약 여기서 SavingsAccount만 골라서 addInterest 메소드를 쓰고 싶으면 instanceof 키워드를 사용하면 됩니다.for (BankAccount account : accounts) {    account.deposit(1000);    if (account instanceof SavingsAccount) {        ((SavingsAccount) account).addInterest();    }}2) 제네릭아래 꺽쇠 기호(&amp;lt;&amp;gt;) 사이에 있는 T를 ‘타입 파라미터’라고 부릅니다. 그리고 이와 같이 타입 파라미터를 받는 클래스를 ‘제네릭 클래스(Generic Class)’라고 합니다.public class Box&amp;lt;T&amp;gt; {    private T something;    public void set(T object) {         this.something = object;    }    public T get() {        return something;    }}아래처럼 타입 파라미터로 String을 넘겨주면,Box&amp;lt;String&amp;gt; box = new Box&amp;lt;&amp;gt;();클래스에 있던 모든 T가 String으로 대체된다고 생각하면 됩니다.public class Box&amp;lt;String&amp;gt; {    private String object;    public void set(String object) {        this.object = object;    }    public String get() {        return object;    }}지금까지는 타입 파라미터로 아무 클래스나 넘길 수 있었는데요. extends 키워드를 이용하면 타입을 제한할 수도 있습니다.public class PhoneBox&amp;lt;T extends Phone&amp;gt; extends Box&amp;lt;T&amp;gt; {    public void handsFreeCall(String numberString) {        object.call(numberString);    }}3. 인터페이스와 추상 클래스1) 인터페이스클래스가 생성될 때, 특정 빈 메소드를 강제로 가지도록 하고 싶을 때 인터페이스를 이용합니다// 인터페이스public interface Shape {  // 빈 메소드  double getArea();  double getPerimeter();}// 특정 인터페이스를 따라야 하는 클래스public class Circle implements Shape {  ...  ...  public double getArea() {    return PI * radius * radius;  }  public double getPerimeter() {    return 2 * PI * radius;  }}2) 추상 클래스// 추상클래스public abstract class Shape {  // 변수  public double x, y;  // 메소드  public void move(doulbe x, double y) {    ...  }  // 빈 메소드 (추상 메소드)  public abstract double getArea();  public abstract double getPerimeter();}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-08T21:01:35+09:00'>08 Jul 2021</time><a class='article__image' href='/java-series5'> <img src='/images/java_logo.png' alt='Java Series [Part5]: 자바 조금 더 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series5'>Java Series [Part5]: 자바 조금 더 알아보기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part4]: 자바의 자료형 특징",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series4",
      "date"     : "Jul 7, 2021",
      "content"  : "Table of Contents  1. 기본형 참조형  2. null  3. final  4. 클래스 변수와 클래스 메소드          1) 클래스 변수      2) 클래스 메소드        5. Wrapper class  6. ArrayList  7. HashMap1. 기본형 참조형// 참조형의 경우 == 연산자는 같은 인스턴스를 가리키는지를 물어봄String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase() == &quot;abc&quot;);-----------------------------------------------------------------false// 인스턴스가 가지는 값이 같은지를 확인하고 싶으면 equals() 메소드를 사용해야함String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase().equals(&quot;abc&quot;));----------------------------------------------------------------true2. null어떤 언어들에서는 ‘비어있음’을 None으로 표현하고, 또 어떤 언어들에서는 nil로 표현합니다. 자바에서는 ‘비어있음’이 null이라는 값으로 표현됩니다. 단, null은 참조형 변수(Reference Type)만 가질 수 있는 값입니다.3. final변수를 정의할 때 final을 써주면, 그 변수는 ‘상수’가 됩니다. 즉, 한 번 정의하고 나서 다시 바꿀 수 없다는 것이죠.public final double pi = 3.141592;4. 클래스 변수와 클래스 메소드1) 클래스 변수클래스 변수는 인스턴스가 생성될 때마다 값이 초기화 되는 것이 아니라, 모든 인스턴스들이 함께 공유하는 변수입니다. 클래스 변수를 정의하기 위해서는 static이라는 키워드를 붙여주면 됩니다.자주 접하게 되는 클래스 변수는 바로 상수입니다. final을 공부할 때 상수를 보긴 했지만, 상수를 더 상수답게 쓰려면 static과 함께 쓰는 것이 좋습니다. 상수는 인스턴스에 해당되는 것이 아니며, 여러 복사본 대신 한 값만 저장해두는 것이 맞기 때문입니다. 상수 이름은 보통 모두 대문자로 쓰고, 단어가 여러 개인 경우 _로 구분 짓습니다.public class CodeitConstants {    public static final double PI = 3.141592653589793;    public static final double EULERS_NUMBER = 2.718281828459045;    public static final String THIS_IS_HOW_TO_NAME_CONSTANT_VARIABLE = &quot;Hello&quot;;    public static void main(String[] args) {        System.out.println(CodeitConstants.PI + CodeitConstants.EULERS_NUMBER);    }}2) 클래스 메소드마찬가지로, 클래스 메소드는 인스턴스가 아닌 클래스에 속한 메소드입니다. 클래스 메소드는 언제 사용할까요? 인스턴스 메소드는 인스턴스에 속한 것이기 때문에, 반드시 인스턴스를 생성해야 사용할 수 있습니다. 하지만 클래스 메소드는 클래스에 속한 것이기 때문에, 인스턴스를 생성하지 않고도 바로 실행할 수 있습니다.예를 들어,  Math.abs(), Math.max() 등을 사용하면, 자바에서 미리 만들어 둔 수학 관련 기능을 활용할 수 있습니다. 하지만 우리는 Math 클래스의 인스턴스를 생성하지는 않습니다. 필요하지 않기 때문이죠. 단지 Math 클래스의 기능(메소드)만 활용하면 됩니다.사실 우리가 가장 먼저 접한 ‘클래스 메소드’는 바로 main 메소드입니다. main은 자바 프로그램의 시작점이라고 했습니다. 첫 번째로 실행되는 코드이니, 어떤 인스턴스도 생성되어 있지 않습니다. 따라서 main 메소드 역시 인스턴스를 생성하지 않고 실행하는 ‘클래스 메소드’입니다. 클래스 메소드도 동일하게 static이라는 키워드로 정의할 수 있습니다.5. Wrapper class‘Wrapper 클래스’는 기본 자료형을 객체 형식로 감싸는 역할을 합니다. Integer 클래스는 int형을, Double 클래스는 double을, Long 클래스는 long을, Boolean 클래스는 boolean을 wrapping할 수 있습니다. 그런데 이런 Wrapper 클래스가 왜 필요할까요?기본형 자료형(Primitive Type)을 참조형(Reference Type)처럼 다루어야할 때 Wrapper 클래스를 사용하면 됩니다. 예를 들어서 ArrayList같은 컬렉션을 사용할 때는 꼭 참조형을 사용해야 합니다.// 생성자로 생성하는 방법Integer i = new Integer(123);// 리터럴로 생성하는 방법Integer i = 123;6. ArrayListimport java.util.ArrayList// ArrayList&amp;lt;안에 넣은 객체의 클래스&amp;gt;ArrayList&amp;lt;String&amp;gt; nameList = new ArrayList&amp;lt;&amp;gt;();nameList.add(&quot;Jay&quot;);nameList.add(&quot;Mike&quot;);nameList.remove(1);nameList.contains(&quot;Jay&quot;);for (String name : nameList) {  System.out.println(name)}7. HashMapHashMap&amp;lt;String, Integer&amp;gt; check = new HashMap&amp;lt;&amp;gt;();check.put(&quot;사과&quot;,new Integer(1))check.get(&quot;사과&quot;)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-07T21:01:35+09:00'>07 Jul 2021</time><a class='article__image' href='/java-series4'> <img src='/images/java_logo.png' alt='Java Series [Part4]: 자바의 자료형 특징'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series4'>Java Series [Part4]: 자바의 자료형 특징</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part3]: 자바와 객체지향 프로그래밍",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series3",
      "date"     : "Jul 5, 2021",
      "content"  : "Table of Contents  1. 객체 만들기  2. 객체 설계하기          1) 접근 제어자(Access Modifier)                  a) private          b) protected          c) public final                    2) 메소드 오버로딩(Method Overloading)      3) 생성자      1. 객체 만들기자바는 객체 단위로 동작하는 객체 지향 프로그래밍이기 때문에, Hello world 한 줄을 출력하더라도 클래스로 작성해야 합니다.클래스는 변수와 메소드를 갖는 객체(인스턴스)를 만드는 설계도라고 생각하시면 됩니다.public class BankAccount {    // 변수    private int balance;    private Person owner;        // 메소드    public void setBalance(int newBalance){        this.balance = newBalance;    }    public int getBalance(){        return balance;    }    public void setOwner(Person newOwner){        this.owner = newOwner;    }    public Person getOwner(){        return this.owner;    }        boolean deposit(int amount){        if (amount &amp;lt; 0 || owner.getCashAmount() &amp;lt; amount){            System.out.println(&quot;입금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance += amount;            owner.setCashAmount(owner.getCashAmount()-amount);            System.out.println(amount + &quot;원 입금하였습니다. 잔고: &quot; + balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;                    }    }    boolean withdraw(int amount){        if ( amount &amp;lt; 0 || getBalance() &amp;lt; amount) {            System.out.println(&quot;출금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance -= amount;            owner.setCashAmount(owner.getCashAmount()+amount);            System.out.println(amount + &quot;원 출금하였습니다. 잔고: &quot;+ balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;        }    }    }2. 객체 설계하기1) 접근 제어자(Access Modifier)a) privatepublic class Person {    private int age;    public void setAge(int newAge) {        if (newAge &amp;gt; 0) {            this.age = newAge;        }    }    public int getAge() {        return this.age;    }}age 변수를 public으로 하면 외부에서 Person 객체를 생성했을 때 age 변수에 직접 접근이 가능하다 (p1.age = -10 이런 식으로) 따라서 외부에서 무분별하게 접근하는 것을 막고자 접근 제어자를 public이 아닌 private으로 작성합니다.b) protected접근 제어자에는 public, private 그리고 protected가 있습니다. protected를 사용하면 자식 클래스에 한해서 변수에 직접적으로 접근이 가능합니다. 그렇게 함으로써 private을 사용했을 때 setter, getter 메소드를 작성해야하는 불편함을 해소해줍니다.public class Person {    protected int age;}public class Student extends Person {  public void olderAge() {    age = age + 1  }}c) public final자식 클래스 뿐 아니라 다른 곳에서도 사용되지만 수정은 안되도록 하는 ( 좀 더 일반적인) 방법은 public final입니다.public final double pi = 3.142) 메소드 오버로딩(Method Overloading)‘메소드 오버로딩(Method Overloading)’은 클래스 내에 같은 이름의 메소드를 2개 이상 정의할 수 있게 해주는 기능입니다.public class Calculator {    int add(int a, int b) {        return a + b;    }    int add(int a, int b, int c) {        return a + b + c;    }    double add(double a, double b) {        return a + b;    }}지금까지 써왔던 System.out.println()도 메소드오버로딩되어 있는 메소드입니다.3) 생성자‘생성자(Constructor)’는 크게 두 가지 역할이 있습니다  인스턴스를 만들고,  인스턴스의 속성(인스턴스 변수)들을 초기화시켜줍니다.생성자를 한 개도 정의 안 했을 경우에는 자바에서 자동으로 기본 생성자를 제공해줍니다.Person p1 = new Person();생성자를 하나라도 정의하면 위의 기본 생성자는 사용할 수 없습니다.public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public static void main(String[] args) {    Person p1 = new Person(&quot;Jay&quot;, 27);    }}  생성자 오버로딩도 가능합니다.// 생성자 오버로딩public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public Person(String pName) {    this.name = pName;    this.age = 12;    // 12살을 기본 나이로 설정    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-05T21:01:35+09:00'>05 Jul 2021</time><a class='article__image' href='/java-series3'> <img src='/images/java_logo.png' alt='Java Series [Part3]: 자바와 객체지향 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series3'>Java Series [Part3]: 자바와 객체지향 프로그래밍</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part2]: 자바 시작하기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series2",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  Hello World  변수와 연산  조건문  반복문  배열Hello Worldpublic class HelloWorld {    /*main: 프로그램을 실행하면 가장 먼저 실행되는 메소드    void: 리턴값 없음    String[] args: args라는 이름의 문자열 변수가 메소드에 전달*/    public static void main(String[] args) {        // System: 클래스, out: 클래스 변수, println: 메소드        System.out.println(&quot;Hello World&quot;);    }}변수와 연산public class Variables {    public static void main(String[] args) {        // 선언 방법        // 1)        int age;        age = 27;        // 2)        double num = 12.5;        // 자료형        // primitive type        int myInt = 123;        long myLong = 12345678910L;        double myDouble = 3.14; // double이 더 높은 정밀도, 소수형의 기본 타입        float f = 3.14f;        char a = &#39;a&#39;; // 쌍따옴표로 감싸면 String으로 인식함        char aPrime = 97; // 아스키 값 97 == &#39;a&#39;        char b = &#39;가&#39;;        boolean myBoolean = true;        // 객체형 type        String myString = &quot;jay kim&quot;;    }}조건문public class IfElse {    public static void main(String[] args) {        int temp = 15;        if (temp &amp;lt; 0) {            System.out.println(&quot;오늘의 날씨는 영하입니다: &quot; + temp +&quot;도&quot;);        } else if (temp &amp;lt; 5){            System.out.println(&quot;오늘의 날씨는 0도 이상 5도 미만입니다: &quot; + temp +&quot;도&quot;);        } else {            System.out.println(&quot;오늘의 날씨는 5도 이상입니다: &quot; + temp +&quot;도&quot;);        }    }}public class Switch {    public static void main(String[] args) {        int score = 80;        String grade;        switch (score / 10) {            case 10:                grade = &quot;A+&quot;;                break;            case 9:                grade = &quot;A&quot;;                break;            default:                grade = &quot;F&quot;;                break;        }        System.out.println(&quot;학점은 &quot; + grade + &quot;입니다.&quot;);    }}반복문public class For {    public static void main(String[] args) {        int sum = 0;        // i++는 실행 부분이 실행되고 나서 실행된다        for (int i = 1; i &amp;lt;= 5; i++) {            sum += i;            System.out.println(i);        }    }}public class While {    public static void main(String[] args ) {        int i = 1;        int sum = 0;        while (i &amp;lt;= 3) {            sum = sum + i;            i = i + 1;        }        System.out.println(sum);    }}배열public class Array {    public static void main(String[] args) {        // 배열 생성하는 첫 번째 방법        int[] intArray = new int[5];        intArray[0] = 2;        intArray[1] = 3;        intArray[2] = 5;        intArray[3] = 7;        intArray[4] = 11;        // 배열 생성하는 두 번째 방법        int[] arr1 = {1, 2, 3, 4, 5};        int[] arr2 = arr1;        int[] arr3 = arr1.clone();        arr1[0] = 100;        System.out.println(arr2[0]);        System.out.println(arr3[0]);        for (double i : intArray) {            System.out.println(i);        }        int[][] multiArray = new int[3][4];        int[][] multiArray2 = { {1 ,2, 3, 4},                            {5, 6, 7, 8},                            {9, 10, 11, 12}                            };        System.out.println(multiArray2[0][1]);    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series2'> <img src='/images/java_logo.png' alt='Java Series [Part2]: 자바 시작하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series2'>Java Series [Part2]: 자바 시작하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part1]: 자바와 가상머신",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series1",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  자바와 가상머신자바와 가상머신  한 번만 작성하면, 어디서든 동작한다. (Write Once, Run Anywhere.)어떤 언어는 운영체제에 따라 동작이 달라집니다.분명히 윈도우즈에서는 잘 동작했는데, 맥에서 동작하지 않는 일이 발생합니다.그래서 우리가 개발할 때는 항상 운영체제를 신경써야 합니다.중간 중간 테스트도 해주어야 하고요.만약 휴대폰 애플리케이션을 개발한다면 어떨까요?최악의 경우, 모든 휴대폰 기종을 모아서 매번 테스트를 해봐야겠네요.자바는 이런 ‘호환성’문제를 해결해 줍니다.‘자바 가상머신’이라는 것만 설치되면, 어느 운영체제이든, 어느 디바이스이든, 동일하게 동작합니다.(자바 가상머신은 영어로 Java Virtual Machine, 줄여서 JVM 이라고 부릅니다.)이러한 자바의 높은 호환성은 애플리케이션의 특징과도 잘 맞아떨어지기 때문에, 애플리케이션 개발에 활발히 사용되고 있죠.JVM을 사용해서 마음껏 개발할 수 있는 환경을 JRE (Java Runtime Environment) 라고 부르며, 내 컴퓨터에 이런 환경을 만들기 위해서는 JDK (Java Development Kit) 라는 것을 설치하면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series1'> <img src='/images/java_logo.png' alt='Java Series [Part1]: 자바와 가상머신'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series1'>Java Series [Part1]: 자바와 가상머신</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Intermediate Series [Part1]: Iterable, Iterator, Generator",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-mid-context_manager",
      "date"     : "Jun 1, 2021",
      "content"  : "Table of Contents  Python tricks we MUST all use",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-06-01T21:01:35+09:00'>01 Jun 2021</time><a class='article__image' href='/python-mid-context_manager'> <img src='/images/python_intermediate_logo.png' alt='Python Intermediate Series [Part1]: Iterable, Iterator, Generator'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-mid-context_manager'>Python Intermediate Series [Part1]: Iterable, Iterator, Generator</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part11]: Advanced RNN",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_rnn",
      "date"     : "Apr 6, 2021",
      "content"  : "Table of Contents  Advanced RNN          바닐라 RNN의 한계점      LSTM (Long Short Term Memory)      GRU  (Gated Recurrent Unit)                  참조                    Advanced RNN바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다.LSTM (Long Short Term Memory)바닐라 RNN의 한계를 극복하기 위해 다양한 RNN의 변형이 나왔습니다. LSTM과 GRU가 대표적인 예입니다.긴 시퀀스를 다룰 때 LSTM이 바닐라 RNN과 비교해 어떤 점에서 더 좋은지 알기위해 우선 LSTM의 구조에 대해 먼저 살펴보겠습니다.꽤나 복잡하게 생겨서 머리가 아플 수 있지만 하나하나 살펴보면 충분히 가능하기 때문에, 천천히 살펴보도록 하겠습니다.우선 위로 지나가는 Cell state 부분부터 한번 보겠습니다.Cell state의 역할은 중요한 정보는 그대로 넘겨주고, 중요하지 않은 정보는 약하게 함으로써 중요한 정보만 계속 흘러갈 수 있도록 해줍니다. 이걸 가능하게 하는 것이 바로 게이트입니다. i(t) 게이트를 통해 중요한 정보는 흘러가고, f(t) 게이트를 통해 중요하지 않은 정보를 약하게 만듭니다. 그러면 어떤 정보가 중요하고 중요하지 않은지는 어떤 기준으로 정해지고 어떻게 설정해야 할까요? 그 기준은 바로 이전 셀의 h(t-1)의 값과 현재 층의 입력 x(t) 으로 정해지며, 설정하는 것은 우리의 몫이 아닌 신경망의 역할입니다. 신경망은 학습을 통해 알아서 중요한 정보와 중요하지 않은 정보를 잘 선택할 수 있도록 학습됩니다.위 과정을 통해 C(t)를 만듭니다.정보의 중요도에 따라 크기가 달라진 C(t)의 값을 tanh에 넣어서 이 값을 다시 -1과 1사이의 값으로 만들어 준 후 o(t)에 곱해 줌으로써 h(t)를 계산합니다.C(t)와 h(t)를 다음 셀에 전달해줍니다.그래서 이러한 LSTM이 어떤 점에서 바닐라 RNN이 가지는 한계를 극복하게 된걸까요? 그것은 바로 C(t)가 셀에서 tanh함수를 거치지 않기 때문에 중요한 정보가 셀을 거듭하더라도 약해지지 않고  정보를 잘 전달할 수 있다는 것입니다.GRU  (Gated Recurrent Unit)GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수님이 집필한 논문에서 제안되었습니다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰습니다.LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했습니다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재합니다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있습니다.데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있습니다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문입니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-06T21:01:35+09:00'>06 Apr 2021</time><a class='article__image' href='/advanced_rnn'> <img src='/images/LSTM_2.png' alt='Deep Learning Series [Part11]: Advanced RNN'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_rnn'>Deep Learning Series [Part11]: Advanced RNN</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part10]: RNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_rnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  RNN          시퀀스 데이터 vs 시계열 데이터      시퀀스 데이터      바닐라 RNN      평가      깊은 RNN 모델      양방향 RNN 모델      바닐라 RNN의 한계점                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.          참조                    RNN시퀀스 데이터 vs 시계열 데이터  시퀀스 데이터는 순서만 중요한 데이터 (문장, 음성)  시계열 데이터는 순서뿐 아니라 데이터가 발생한 시간도 중요한 데이터 (주식, 센서 데이터)시퀀스 데이터  시퀀스 데이터는 IID가정을 대체로 위배하기 때문에, 순서를 바꾸면 데이터의 확률 분포도 바뀌게 됩니다.  이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률 분포를 다루기 위해 조건부 확률을 이용할 수 있습니다.  다음과 같은 문제를 해결하기 위한 모델을 만든다고 생각해 봅시다.이미지에 대한 설명문 달기주가 예측하기한국어 영어로 번역하기다음과 같은 문제는 입력과 출력이 시퀀스 형태를 가지고 있습니다. 이러한 시퀀스 데이터를 처리하기 위해 고안된 모델을 시퀀스 모델이라고 합니다. 그 중에서도 RNN은 딥러닝에서 가장 기본적인 시퀀스 모델입니다.one to one: 비 시퀀스 데이터를 다루는 경우one to many: 이미지 캡셔닝many to one: 주가 예측, 텍스트 분류 many to many: 번역바닐라 RNN그동안 신경망들은 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했습니다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 합니다. 그런데 그렇지 않은 신경망들도 있습니다. RNN(Recurrent Neural Network)이 그 중 하나입니다.RNN은 해당 층의 입력 데이터와 이전 층에서의 출력을 함께 입력으로 사용합니다.그리고 이전 층의 출력과 해당 층의 입력은 다음과 같이 결합되게 됩니다.(참고로 W_d와 W_h를 concatenation해서 쓸 수도 있습니다.)이를 모델에 적용해 다시 한 번 살펴보면 다음과 같습니다.이를 식으로 표현하면 다음과 같습니다.평가Loss함수 식을 보면 변수 t에 대해 theta값은 변하지 않는다 -&amp;gt; 펼쳐져 있지만 W_h와 W_d는 같은 레이어입니다.깊은 RNN 모델양방향 RNN 모델양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다.즉, RNN이 과거 시점(time step)의 데이터들을 참고해서, 찾고자하는 정답을 예측하지만 실제 문제에서는 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에 힌트가 있는 경우도 많습니다. 그래서 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다.바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다.만약 tanh가 아니라 relu를 쓴다면, 반대로 맨 앞에서 받았던 정보가 층을 거듭할수록 값이 너무 커져서 시퀀스 뒤에 위치한 데이터의 학습을 방해할 수 있습니다.이를 해결하기 위해 RNN의 advanced 버전인 LSTM과 GRU에 대해서는 다음 포스트에서 살펴보도록 하겠습니다.실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/basic_of_rnn'> <img src='/images/basic_of_rnn_5.png' alt='Deep Learning Series [Part10]: RNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_rnn'>Deep Learning Series [Part10]: RNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part9]: Advanced CNN  ",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_cnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  Advanced CNN          1. GoogLeNet                  1) GoogLeNet의 특징          2) Inception Module          3) GoogLeNet Network Structure                    2. ResNet                  1) Residual Learning          2) Residual Block          3) ResNet Architecture                    Advanced CNN1. GoogLeNet1) GoogLeNet의 특징  커널의 적절한 사이즈를 찾기 위해 고민하기 보다, 여러 가지 사이즈의 커널을 병렬로 이용함으로써 보다 풍부한 Feature Extraction 수행한다  1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시킨다.  1 X 1 이라서 Feature map dimension이 줄어든 것이 아니라, 1 X 1 필터의 채널 수를 작은 사이즈를 썼기 때문이다.  참고로 컨볼루션 커널의 사이즈, padding, striding이 Feature map의 사이즈를 결정한다.  컨볼루션 커널의 채널의 개수가 Feature map의 개수를 결정한다.2) Inception Module  위 그림과 같이 1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시켰다.  Inception모듈을 보면 여러 필터가 병렬적으로 연산되고 모듈 끝에서 결과들이 Concatenation됩니다.  따라서 Feature map의 개수는 달라도 괜찮지만, Feature map의 사이즈는 같아야 합니다.3) GoogLeNet Network Structure  다음과 같이 Inception Module은 총 9개로 구성되어 있다  Concatenation까지가 Inception Module에 포함되고 그 후에 보통 Pooling layer를 거친 뒤 다시 Inception Module로 들어가는 것으로 반복된다.  Inception Module을 2개 거친 후 Pooling하기도 하고, 5개 거치고 Pooling 하기도 한다.2. ResNet  VGG 모델이 나온 이후 깊은 Network가 좋은 성능을 낸다는 인식이 생겼다.  하지만 비슷한 방식으로 Network를 더 깊게 만들었을 때 오히려 성능이 저하되었다.  그 원인으로는 Gradient Vanishing과 파라미터 수 증가에 따른 학습 속도 저하가 있다.  파라미터 수 증가는 앞에서와 같이 1 X 1 컨볼루션으로 해결하였다.  Gradient Vanishing 문제는 Residual Learning을 통해 해결하였다.1) Residual Learning  처음 Residual Learning이 나오기 전에 시도되던 방법은 Identity mapping이다.  Identity mapping은 층은 더 깊게 만들되, Gradient vanishing은 생기지 않도록 하기 위해 이전 값을 그대로 다시 통과시키는 방법이다.  비선형성은 있어야 층을 깊게 쌓는 의미가 있으므로 Relu()정도가 있어야 하는데, 이렇게 되면 identity한 mapping이 되기 어려워진다.  그래서 좀 더 쉬운 방법으로 제안된 것이 Residual Learning이다.  H(x)가 x가 되도록 하는 것이 아니라, F(x)가 0이 되도록 학습하는 것이 쉽다.2) Residual Block  ResNet 50을 포함한 이보다 깊은 네트워크(50/101/152)에서는 1 X 1 Conv를 이용해 파라미터 갯수를 줄였다.  Residual Block 내에서는 Feature map 사이즈는 동일하고 Filter수만 변함3) ResNet Architecture",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/advanced_cnn'> <img src='/images/googlenet_1.png' alt='Deep Learning Series [Part9]: Advanced CNN  '> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_cnn'>Deep Learning Series [Part9]: Advanced CNN  </a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part8]: CNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_cnn",
      "date"     : "Apr 4, 2021",
      "content"  : "Table of Contents  CNN의 기초          CNN의 발단      CNN 구조                  1. 컨볼루션 연산          2. 커널(채널), 필터          3. 스트라이드, 패딩                          스트라이드(stride)              패딩(padding)                                4. 풀링 연산                    특성맵의 크기 구하기      CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.                    CNN의 기초CNN의 발단그동안 앞에서 설명한 신경망은 모두 기본 전제가 노드가 서로 완전 연결되어 있는 경우였습니다. 이를 완전 연결층(Fully connected layer)라고 합니다. 이는 목적한 바를 잘 이룰 수 있도록 특징 공간 갖는 모델을 만들 수 있지만, 가중치가 너무 많아 복잡도가 너무 높습니다. 이는 모델의 학습 속도를 더디게 하며 또한 과잉적합에 빠질 가능성을 높이게 됩니다. 컨볼루션 신경망(CNN)은 부분 연결 구조로 이러한 문제를 잘 해결하며 또한 특징을 잘 추출하도록 해줍니다. 이러한 이유로 CNN은 이미지, 비전 분야에서 매우 뛰어난 성능을 발휘했으며, 음성인식이나 자연어 처리같은 다른 작업에도 사용됩니다.실제로 1958년에 데이비드 허블의 연구에서는 인간의 시각 뉴런은 부분연결 구조를 가진다는 사실을 밝혀냈습니다. 이를 조금 더 자세히 설명하면, 눈의 시각 피질 안의 많은 뉴런은 작은 국부 수용장(Local receptive field)를 가지며 이는 시야의 영역에서 작은 특정 패턴에 뉴런이 반응한다는 것입니다. 예를 들어,눈이 처음 무언가를 봤을 때는 국부 수용장에 해당하는 작은 영역이 가지는 패턴에 먼저 반응을 하고, 시각 신호가 연속적으로 뇌의 뉴런들을 통과하며 다음 뉴런들은 점점 더 큰 수용장에 있는 복잡한 패턴에 반응을 합니다. 이러한 점에서 CNN은 사람의 눈과 비슷한 원리를 가지고 있다고 할 수 있습니다.CNN은 눈의 원리와 비슷하다.부분 연결 구조로 되어 있어 비교적 구해야 하는 가중치의 갯수가 적다.학습 속도가 훨씬 빠르며, 과잉적합에 빠질 가능성 또한 낮아진다.CNN 구조보통 이미지 분류를 위한 CNN의 구조는 다음과 같습니다.1. 컨볼루션 연산사실 CNN에서의 합성곱은 실제의 합성곱과는 다릅니다. 실제의 합성곱은 필터를 뒤집어야 하지만 CNN에서는 필터를 뒤집지 않습니다. 그 이유는 어차피 필터의 가중치 값은 처음에 보통 랜덤으로 초기화하게 됩니다. 따라서 가중치를 굳이 뒤집지 않아도 상관이 없습니다. 어쨋든 CNN에서의 컨볼루션 연산은 필터가 옆으로 움직이면서 데이터와 각각 원소별 곱셈을 진행하고 그 곱셈의 결과들을 하나의 값으로 합하면 됩니다.밑의 예시를 살펴보면 3×1 + 1×(-1) + 1×1 + 7×(-1) + 2×1 + 5×(-1) = -7 이 됩니다.다음의 컨볼루션 연산은 필터의 사이즈, 스트라이드의 크기, 패딩 여부 등에 따라 결과(특성 맵)이 달라집니다.2. 커널(채널), 필터여기서 CNN에서 정말 헷갈리지만 또 중요한 개념이 등장합니다. 바로 커널(채널)과 필터입니다. 느낌이 비슷해서 헷갈릴 수 있지만 엄연히 구분되어 사용되어야 하기 때문에 여기서 한 번 짚고 넘어가도록 하겠습니다.커널은 채널과 비슷한 의미로 데이터의 커널의 수(RGB채널의 경우 3)와 필터의 커널 수는 항상 같아야 합니다.필터는 카메라 필터와 비슷하게 shape을 위한 필터, curve를 위한 필터와 같은 필터를 의미합니다.예를 들어 채널이 1(Gray scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.만약 채널이 3(RGB scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.따라서 필터의 커널의 수는 항상 데이터의 커널의 수를 따르게 되고,결과(특성 맵)의 커널 수는 항상 필터의 수를 따르게 됩니다.CNN의 목표는 특징을 잘 추출해주는 필터의 가중치를 찾는 것입니다.3. 스트라이드, 패딩스트라이드(stride)  스트라이드는 필터의 미끄러지는 간격을 조절하는 것을 말합니다.  기본은 1이지만, 2를(2pixel 단위로 Sliding window 이동) 적용하면 입력 특성 맵 대비 출력 특성 맵의 크기를 대략 절반으로 줄여줍니다.  stride 를 키우면 공간적인 feature 특성을 손실할 가능성이 높아지지만, 이것이 중요 feature 들의 손실을 반드시 의미하지는 않습니다.  오히려 불필요한 특성을 제거하는 효과를 가져 올 수 있습니다. 또한 Convolution 연산 속도를 향상 시킵니다.패딩(padding)  패딩은 데이터 양 끝에 빈 원소를 추가하는 것을 말합니다.      패딩에는 밸리드(valid) 패딩, 풀(full) 패딩, 세임(same) 패딩이 있습니다. 패딩 각각의 역할은 다음과 같습니다.                            패딩          역할                                      밸리드          평범한 패딩으로 원소별 연산 참여도가 다르다                          풀          데이터 원소의 연산 참여도를 갖게 만든다                          세임          특성 맵의 사이즈가 기존 데이터의 사이즈와 같도록 만든다                      세임패딩을 적용하면 Conv 연산 수행 시 출력 특성 맵 이 입력 특성 맵 대비 계속적으로 작아지는 것을 막아줍니다.4. 풀링 연산  풀링층은 특성 맵의 사이즈를 줄여주는 역할을 합니다.  보통 최대 풀링 또는 평균 풀링을 많이 사용합니다.    보통은 Conv연산, ReLU activation함수를 적용한 후에 풀링을 적용합니다.    풀링은 비슷한 feature 들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화 시켜 줍니다.  일반적으로는 Sharp 한 feature 가 보다 Classification 에 유리하여 최대 풀링이 더 많이 사용됩니다.  풀링의 경우 특정 위치의 feature 값이 손실 되는 이슈로 인해 최근 Advanced CNN에서는 Stride만 이용하여 모델을 구성하는 경향입니다.특성맵의 크기 구하기  5×5 입력, 3×3 필터가 스트라이드가 1이고 패딩이 없는 경우, 특성 맵의 크기는 3×3이 됩니다.CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-04T21:01:35+09:00'>04 Apr 2021</time><a class='article__image' href='/basic_of_cnn'> <img src='/images/cnn_3.png' alt='Deep Learning Series [Part8]: CNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_cnn'>Deep Learning Series [Part8]: CNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-heapq",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/python-component-heapq'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-heapq'>Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/regulation",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝의 규제를 통한 일반화          1. 모델의 일반화      2. 규제의 필요성과 원리                  규제의 정의                    3. 규제 기법                  1) 데이터 증가(Data Augmentation)                          a) Salt &amp;amp; Pepper Noise              b) Rotation, Flipping Shifting              c) Dropping, Exchanging for Text augmentation              d) 새로운 데이터 생성                                2) 가중치 벌칙                          L2놈                                3) 드롭아웃(Dropout)          4) 조기 멈춤(Early stopping)                    딥러닝의 규제를 통한 일반화1. 모델의 일반화모델의 일반화란 무엇일까요? 모델이 특정 데이터에만 잘 맞는 것이 아니라 전례가 없는 데이터에 대해서도 높은 정확도를 유지하도록 하는 것을 모델의 일반화라고 합니다. 그러기 위해서는 우선  우리가 해결하고자 하는 문제가 가지고 있는 데이터의 특징 공간을 모델이 충분히 잘 수용할 수 있어야 합니다.모델의 용량은 데이터를 나타낼 수 있는 모델의 차원이라고 생각합니다. 따라서 모델이 데이터의 특징을 잘 찾아내기 위해서는, 모델의 차원이 데이터의 특징 공간의 차원 보다는 높아야 합니다. 그럼 무조건 모델의 용량이 크면 다 해결되는 걸까요? 그렇지는 않습니다. 왜냐하면 데이터에는 우리가 정말로 원하는 성분 말고도 우리가 원치 않는 노이즈가 함께 존재하는 경우가 대부분이기 때문입니다. 용량이 너무 크면 모델은 노이즈에 대한 잘못된 특징도 수용하기 때문에 성능이 떨어지게 됩니다. 이러한 현상을 모델이 데이터에 과잉적합되었다고 합니다. 따라서 현대 기계 학습은 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 노이즈에 민감하지 않도록 하기 위해 여러 가지 규제 기법을 적용하는 접근방법을 채택합니다.용량이 충분히 큰 모델에 여러 가지 규제 기법을 적용해 일반화 능력을 높일 수 있다.2. 규제의 필요성과 원리따라서 우리는 모델이 주어진 문제에 대해 최소한의 역할을 하기 위해 용량을 크게 하는 것이 좋으며(과소 적합 방지), 전례가 없는 데이터에 대해서도 높은 정확도를 가지는 성능이 좋은 모델을 만들기 위해 모델을 잘 일반화 시켜야 합니다(과잉 적합 방지).그리고 모델을 일반화 시키기 위한 방법을 우리는 규제라고 합니다.규제의 정의  일반화 오류를 줄이기 위해 학습 알고리즘을 수정하는 모든 방법3. 규제 기법현대 기계 학습은 아주 다양한 규제 기법을 사용합니다.  데이터를 통해          Data Augmentation -&amp;gt; Noise injection        Loss 함수를 통해          Weight Decay -&amp;gt; L1, L2 규제        Neural network layer를 통해          Dropout        학습 방식, 추론 방식을 통해          Early Stopping      Bagging &amp;amp; Ensemble      1) 데이터 증가(Data Augmentation)핵심 특징을 간직한 채, noise를 더하여 데이터를 확장하는 방법으로 보통 핵심 특징을 보존하기 위해 휴리스틱한 방법을 사용합니다.이를 통해 더욱 noise robust한 모델을 얻을 수 있습니다. 규칙을 통해 데이터를 증가시키려고 하면 모델이 그 규칙을 배우게 되기 때문에규칙이 아닌 Randomness를 통해 데이터를 증가시켜야 합니다.a) Salt &amp;amp; Pepper Noise  Adding RGB(255, 255, 255) noise  Adding RGB(0, 0, 0) noiseb) Rotation, Flipping Shiftingc) Dropping, Exchanging for Text augmentation  임의의 단어를 생략한다  임의로 특정 단어를 주변 단어와 위치를 바꾼다d) 새로운 데이터 생성  Autoencoder, GAN을 통해 데이터를 학습 후 새로운 데이터 생성2) 가중치 벌칙가중치 𝛉가 커지게 되면 R항이 커지게 되고 그러면 손실 함수 J가 증가하게 됩니다. 우리의 학습 알고리즘은 손실 함수가 작아지도록 하므로 R항은 가중치의 크기에 제약을 가하는 역할을 한다고 볼 수 있습니다. 규제 항 R은 가중치를 작은 값으로 유지하므로 모델의 용량을 제한하는 역할을 한다고 볼 수 있습니다. 𝜆는 층마다 다르게 할 수도 있고 같게 할 수도 있습니다.하지만 실제로 사용하게 되면 성능이 오히려 떨어져 잘 사용하진 않습니다.L2놈규제 항 R로 가장 널리 쓰이는 것은 L2놈이며 이를 가중치 감쇠 기법이라고 합니다.목적 함수가 달라졌으므로, 그래디언트와 가중치 또한 바뀌게 됩니다.3) 드롭아웃(Dropout)드롭아웃이란, 입력층과 은닉층의 모든 노드에 대해 일정 확률로 노드를 임의로 제거하는 것입니다. 해당되는 노드의 들어오고 나가는 엣지들을 모두 제거합니다.(0을 출력합니다) 보통 드롭아웃될 확률의 0.1~0.5로 합니다.여기서 이렇게 하는 것이 과연 어떤 의미가 있는지 궁금해 하시는 분들이 있을 것 같아 예를 한 가지 들어보도록 하겠습니다. 어떤 회사에서 직원들이 아침마다 일정 확률로 회사를 쉰다면 어떻게 될까요? 회사가 이런 식으로 운영된다면 어떠한 업무도 한 사람에게 전적으로 의지할 수 없게 되고, 전문성이 여러 사람에게 나뉘어져 있어야 합니다. 그렇기에 이 회사는 유연성이 훨씬 더 높아질 것입니다. 한 직원이 직장을 떠나도 크게 달라지는 것이 없을 것입니다.신경망 또한 마찬가지입니다. 노드들은 몇 개의 노드에만 지나치게 의존할 수 없습니다. 모든 노드에 주의를 기울여야 합니다. 그러므로 입력값의 작은 변화에 덜 민감해집니다. 결국 더 안정적인 네트워크가 되어 일반화 성능이 좋아집니다.테스트시에는 드롭아웃을 사용하지 않는 보통 신경망처럼 전방 계산을 수행하기 때문에 출력이 학습할 떄에 비해 1/p배 더 큽니다. 따라서 W에 p를 곱하여 이를 상쇄시켜 줘야합니다.일반적으로 (출력층을 제외한) 맨 위의 층부터 세 번째 층까지 있는 노드에만 드롭아웃을 적용합니다. 또한 많은 최신 신경망 구조는 마지막 은닉층 뒤에만 드롭아웃을 사용합니다.4) 조기 멈춤(Early stopping)보통 학습을 오래 시킬수록 더 최적점에 접근합니다. 하지만 어떤 시점을 넘어서면 모델이 훈련 데이터에만 너무 최적화가 되어 검증집합에 대해서는 오히려 성능이 떨어지기 시작합니다. 다시 말해, 일반화 능력이 하락하기 시작하는 것입니다. 따라서 일반화 능력이 최고인 지점, 즉 검증집합의 오류가 최저인 지점에서 학습을 멈추는 전략을 조기 멈춤이라고 합니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/regulation'> <img src='/images/regulation_1.png' alt='Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/regulation'>Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/performance",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝 성능 향상을 위한 요령          1. 딥러닝 성능 향상의 방향성      2. 데이터 전처리      3. 가중치 초기화(Initialization)      4. 배치 정규화(Batch Normalization)      5. 그래디언트 모멘텀(Gradient Momentum)      6. 적응적 학습률(Adaptive Learning-rate)      딥러닝 성능 향상을 위한 요령1. 딥러닝 성능 향상의 방향성딥러닝의 성능을 향상시킨다고 할 때는 보통 다음과 같은 두 가지를 말합니다.과잉 적합 방지를 통한 일반화 능력 극대화 (실전에 배치되었을 때의 성능을 극대화)학습 알고리즘의 수렴 속도 향상 (더 빠른 학습은 결국 더 좋은 성능을 가져다 준다)2. 데이터 전처리데이터 전처리는 데이터 정규화를 의미합니다.데이터가 양수, 음수 값을 골고루 갖도록 한다 =&amp;gt; 평균: 0특성 scale이 같도록 한다 =&amp;gt; 표준편차: 13. 가중치 초기화(Initialization)역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전파하면서 진행됩니다. 그런데 알고리즘이 하위층으로 진행될수록 그래디언트가 작아지는 경우가 많습니다. 이 문제를 그래디언트 소실이라고 합니다. 어떤 경우엔 반대로 그래디언트가 점점 커져 여러 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산합니다. 이 문제를 그래디언트 폭주라고 하며 순환 신경망에서 주로 나타납니다. 일반적으로 불안정한 그래디언트는 심층 신경망 훈련을 어렵게 만듭니다. 층마다 학습 속도가 달라질 수 있기 때문입니다.기계 학습 초기에는 가중치 초기화를 정규분포 형태(평균:0, 분산:1)를 갖도록 초기화하였습니다. 하지만 입력층의 노드 수가 많다면 출력층의 분포는 밑에 그림과 같이 값들이 대부분 0이나 1로 수렴하게 됩니다. 문제는 0과 1근처에서 활성화 함수의 그래디언트가 거의 0에 가깝다는 것 입니다. 그렇기 때문에 아래층까지 역전파가 진행되기도 전에 이미 그래디언트가 거의 소실됩니다.그렇다면 출력층의 분포가 어떤게 좋을까요? 출력층의 값이 고르게 분포해 시그모이드 함수의 비선형성과 적당한 그래디언트를 갖도록 하는 것이 좋을 것입니다.그렇다면 왜 이런 분포가 안되는 걸까요? 아마 그 이유는 입력층(각 층은 다음 층의 입력층이므로 결국 모든 층)의 노드 수가 많으면 가중치와 데이터가 정규분포를 갖는다고 하더라도 모두 더하게 되면 출력층에 sum(wx)의 값이 치우치게 되고 그러면 sigmoid(sum(wx))는 0또는 1로 주로 분포하게 될 것 입니다. 따라서 이를 완화시켜주기 위해서는 입력층의 노드 수가 많다면 그만큼 가중치의 분산을 작게 하여 최대한 작은 값을 갖도록 하면 sum(wx)의 값이 치우치게 되지 않도록 해줄 것입니다. 이와 관련한 몇 가지 초기화 방법을 살펴보겠습니다.🔔 가중치 초기화는 Gradient vanishing문제를 완화시켜줍니다.평균이 0인 정규 분포를 갖도록 한다표준편차 크면 그리고 노드의 갯수도 많으면 값이 특정 부분에 몰리게 된다.4. 배치 정규화(Batch Normalization)배치 정규화는 각 층에서 활성화 함수를 통과하기 전이나 후에 입력을 정규화한 다음, 두 개의 새로운 파라미터(𝛾, 𝛽)로 결과값의 스케일을 조정하고 이동시킵니다. 정규화 하기 위해서는 평균과 표준편차를 구해야 합니다. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가합니다. 테스트 시에는 어떻게 할까요? 간단한 문제는 아닙니다. 아마 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 합니다. 이 경우 입력의 평균과 표준편차를 계산할 방법이 없습니다. 샘플의 배치를 사용한다 하더라도 매우 작거나 독립 동일 분포(IID)조건을 만족하지 못할 수도 있습니다.케라스에서는 이를 층의 입력 평균과 표준편차의 이동 평균(moving average)을 사용해 훈련하는 동안 최종 통계를 추정함으로써 해결합니다. 케라스의 BatchNormalization층은 이를 자동으로 수행합니다.정리하면 배치 정규화 층마다 네 개의 파라미터 벡터가 학습됩니다.  𝛾(출력 스케일 벡터)와 𝛽(출력 이동 벡터)는 일반적인 역전파를 통해 학습됩니다. 𝜇(최종 입력 평균 벡터)와 𝜎(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정됩니다. 𝜇와 𝜎는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용됩니다.(배치 입력 평균과 표준편차를 대체하기 위해)다음과 같은 배치 정규화는Gradient vanishing문제를 완화시켜준다.Learning rate 높여도 학습이 잘된다.일반화 능력이 좋아진다.5. 그래디언트 모멘텀(Gradient Momentum)모멘텀은 학습을 좀 더 안정감 있게 하도록 해줍니다. 데이터에 의해 Gradient를 계산할 때 만약 Noisy한 데이터인 경우, Gradient가 잘못된 방향으로 갈 가능성이 큽니다. 그렇기 때문에 그 동안 누적된 Gradient를 감안하여 Gradient가 Noisy한 데이터에 의한 안 좋은 영향을 줄여준다. 영어로 잘 설명된 부분이 있어 가져와 보았습니다.  Momentum method can accelelerate gradient descent by taking accounts of previous gradients in the update rule equation in every iteration6. 적응적 학습률(Adaptive Learning-rate)가중치 업데이트의 척도가 되는 학습률을 각 가중치의 학습 진행 정도에 따라 다르게 바꿔주는 것을 적응적 학습률이라고 한다.적응적 학습률은가중치의 업데이트가 많이 이루어질수록 점점 학습률을 줄여나간다.특성마다 업데이트가 많이 된 특성은 학습률을 줄이고, 적게된 특성은 학습률을 늘린다.※ 적응적 학습률과 학습률 스케줄링의 차이가 뭘까? 둘 중 하나만 사용하면 되는걸까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/performance'> <img src='/images/performance_2.png' alt='Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/performance'>Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part5]: 다층 신경망 이론",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/multi_layer_perceptron",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  다층 신경망 이론          1. 인공신경망                  퍼셉트론                    2. 비선형 분류 문제                  다층 퍼셉트론(MLP)                    3. 딥러닝의 등장      4. 다층 신경망에서의 경사 하강법      다층 신경망 이론1. 인공신경망새가 비행기의 발명에 영감이 되었다면, 사람의 뇌는 인공신경망의 영감이 되었습니다. 인공 신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델입니다. 그러나 비행기가 새처럼 날개를 펄럭거릴 필요가 없듯이, 인공 신경망이 사람의 뇌와 똑같이 동작해야 할 이유는 없습니다. 최근 연구자들은 인공 신경망 연구의 창의성을 위해 이러한 비교 자체를 모두 버려야 한다고 주장합니다.인공 신경망은 딥러닝의 핵심입니다. 인공 신경망은 다재다능하고 강력하며 확장성 또한 좋아서 이미지 분류, 음성 인식, 비디오 추천 등 아주 복잡한 문제를 다루는 데 적합합니다.퍼셉트론퍼셉트론은 가장 간단한 인공 신경망 구조 중 하나로 1957년 프랑크 로젠블라트가 제안했습니다. 퍼셉트론은 TLU(Threshhold Logic Unit)이라고 불리는 인공 뉴런을 기반으로 합니다.그렇다면 퍼셉트론은 어떻게 훈련 될까요? 여기서 실제 생물학적 뉴런에 약간의 영감을 받았다고 할 수 있습니다. 도널드 헤브는 1949년에 ‘서로 활성화되는 세포가 서로 연결된다.’라는 아이디어를 제안합니다. 즉 두 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가한다는 것입니다. 여기서 퍼셉트론은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련됩니다. 퍼셉트론 학습규칙은 오차가 감소되도록 연결을 강화시킵니다.그러나 이러한 퍼셉트론은 XOR과 같은 비선형 분류 문제를 해결하지 못하는 약점이 지적되었습니다.2. 비선형 분류 문제XOR문제는 대표적인 비선형 분류 문제입니다. 이 문제는 1970년대 민스키의 『Perceptrons』에서 지적되었고, 이 후 한동안 신경망 연구가 정체기를 겪었습니다. 이 후 이를 해결하기 위한 방법이 몇 가지 발표되며 신경망 연구가 부활하는 계기가 되었습니다. 새로 도입한 기법을 요약하면,은닉층을 둔다.시그모이드 활성함수를 도입한다.오류 역전파 알고리즘을 사용한다.시그모이드 함수와 역전파 알고리즘은 살펴봤기 때문에 여기서는 은닉층의 필요성에 대해 알아보겠습니다. 다시 XOR문제로 돌아와 보면, XOR문제는 주어진 x1, x2 공간에서는 데이터를 분류하는 모델을 만들 수 없습니다. 따라서 분류가 가능하도록 해주는 특징공간으로 옮겨야 하는데, 이를 가능하게 해주는 것이 바로 은닉층의 역할입니다. 밑에 그림과 같이 두 개의 퍼셉트론을 이용해 새로운 특징공간 z1, z2로 옮기면 우리의 데이터를 분류할 수 있게 됩니다.다층 퍼셉트론(MLP)퍼셉트론을 여러 개 쌓아올린 인공 신경망을 다층 퍼셉트론이라 합니다. 다층 퍼셉트론은 하나의 입력층, 하나 이상의 은닉층 그리고 출력층으로 구성됩니다.3. 딥러닝의 등장은닉층을 여러 개 쌓아 올린 인공 신경망을 심층 신경망이라고 합니다. 딥러닝은 이러한 심층 신경망을 연구하는 분야입니다. 깊은 층을 통해 비선형 분류 문제를 해결하게 되며 이를 계기로 다양한 문제에 층을 깊이 쌓은 신경망 구조가 주목을 받기 시작했습니다. 우리의 생각으로는 뚜렷한 구별 방법이 떠오르지 않지만, 신경망을 깊게 쌓음으로써 기계가 여러 가지 특징 공간에서 데이터를 볼 수 있게 되었고 이 방법은 실제로 비정형 데이터(음성, 사진 등)를 다루는 데에 굉장한 성능을 보여주었습니다. 또한 깊은 층의 의미가 있기 위해 각 층마다 활성화 함수를 사용했는데, 미분 연산이 간단한 ReLU함수가 등장하게 되면서 층을 더 깊이 쌓는 것이 실제로 가능해지게 되었습니다. 이 때 부터 본격적으로 비약적인 발전을 하게 되었습니다.4. 다층 신경망에서의 경사 하강법다음과 같이 구한 W1, W2에 대한 각각의 그래디언트를 이용해 각각의 가중치를 업데이트 한다. 행렬로 표기된 이유는 배치 경사 하강법을 가정했기 때문이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/multi_layer_perceptron'> <img src='/images/multi_layer_3.png' alt='Deep Learning Series [Part5]: 다층 신경망 이론'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/multi_layer_perceptron'>Deep Learning Series [Part5]: 다층 신경망 이론</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part4]: 배치 경사 하강법",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/batch_gradient_descent",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  배치 경사 하강법          1. 확률적 경사 하강법(Stochastic Gradient Descent)      2. 배치 경사 하강법(Batch Gradient Descent)                  배치 경사 하강법 수식 과정                          (1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다              (2) Error를 구한다              (3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다              (4) 가중치를 업데이트 한다                                          배치 경사 하강법1. 확률적 경사 하강법(Stochastic Gradient Descent)확률적 경사 하강법은 데이터 세트에서 무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산합니다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용합니다. 그렇기 때문에 굉장히 빠른 속도로 가중치를 업데이트 할 수 있게 됩니다. 하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌이 납니다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것입니다. 그래서 느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법입니다.2. 배치 경사 하강법(Batch Gradient Descent)배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 64, 128개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용합니다. 다시 말해 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 64개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 겁니다.또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정을 겪지 않았습니다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능합니다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어입니다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 됩니다.확률적 경사 하강법과 배치 경사 하강법  배치 경사 하강법 수식 과정(1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다Forward propagation은 앞에서 했던 데이터와 가중치를 곱하고 합하는 과정들을 일컫는 말입니다.(2) Error를 구한다(3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다(4) 가중치를 업데이트 한다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/batch_gradient_descent'> <img src='/images/batch_1.png' alt='Deep Learning Series [Part4]: 배치 경사 하강법'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/batch_gradient_descent'>Deep Learning Series [Part4]: 배치 경사 하강법</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-itertools",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/python-component-itertools'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-itertools'>Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part3]: 분류를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/classification",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents  분류(Logistic regression)          1. 로지스틱 회귀(Logistic regression)      2. 활성화 함수: 시그모이드(Sigmoid)      3. 손실 함수: 크로스 엔트로피(Cross entropy)      4. 가중치 업데이트      분류(Logistic regression)1. 로지스틱 회귀(Logistic regression)앞에서 연속적인 값을 예측하는 모델에 대해 공부했습니다. 이번 포스트에서는 고양이, 개를 분류하는 이진 분류, 2가지 이상을 분류하는 다중 분류에 대해 공부해 보겠습니다. 그럼 회귀가 아니라 분류라고 해야 하지 않는가? 왜 로지스틱 회귀라고 할까? 여기서부터는 저의 생각이니 편하게 보고 그냥 넘기셔도 됩니다. 우선 로지스틱 회귀는 앞에서 봤던 회귀 모델과 같이 연속적인 값을 예측합니다. 다만 뒤에 시그모이드 함수와 합성함수를 취해주게 됩니다. 그럼 결과는 어떨까요? 시그모이드 함수의 결과는 0과 1사이의 실수값입니다. 그렇기 때문에 여전히 연속적인 값을 예측하는 것이라는 점에서 회귀라고 할 수 있는 것이지요. 다만 0과 1 사이의 값을 리턴하니까 예를 들면, 다음 사진이 고양이인지 아닌지에 대한 확률로 이용할 수 있을 것 같다는 생각이 듭니다. 그렇기 때문에 로지스틱 회귀라는 회귀 모델이지만 분류에 사용되는 것 같습니다.로지스틱 회귀는 0과 1사이의 실수값을 리턴해주므로 회귀다0과 1사이의 값을 분류를 위한 확률로 사용될 수 있기에 분류 모델에 사용된다.고양이, 개 사진 분류하기2. 활성화 함수: 시그모이드(Sigmoid)여기서부터 인공지능에서 중요한 개념 중 하나인 활성화 함수에 대해 알아보겠습니다. 활성화 함수의 종류는 다양합니다. 그 중 여기서는 Sigmoid 함수에 대해 알아보겠습니다. 그 밖에도 대표적으로 ReLU(Rectified Linear Unit) 함수가 있으면 ReLU는 최근 딥러닝에 많이 사용되는 대표적인 활성화 함수입니다.Sigmoid 함수는 0과 1사이의 연속적인 값을 리턴하기 때문에 확률로 사용하기 적합합니다. 그래서 이진 분류의 출력층에 활성화 함수로 많이 사용됩니다. ReLU 함수는 값이 0또는 입력값(x)이기 때문에 미분 계산 다른 활성화 함수보다 훨씬 간단합니다. 그러한 이유로 은닉층에 많이 이용되고 있습니다.보통 각 층에 있는 노드는 서로 같은 활성화 함수를 사용합니다. 밑에 그림은 입력층과 은닉층에는 ReLU함수를 사용했고, 출력층에는 이진 분류를 위해 Sigmoid함수를 사용하였습니다.은닉층에 활성화 함수가 없다면 층을 깊게 쌓아도 결국 가중치와 데이터의 곱과 합의 형태를 갖는 하나의 층에 불과하기 때문에 각 은닉층에는 ReLU와 같은 활성화 함수를 사용해야 층을 깊게 쌓는 의미가 있게 됩니다.활성화 함수는 각 층마다 활성화 함수를 가지고 있다.만약 각 층 간에 활성화 함수가 없다면 층이 깊어져도 선형 함수이기 때문에 층이 깊어져도 의미가 없다.보통 각 은닉층에는 ReLU함수가 사용되고, 이진 분류를 위해 출력층에는 Sigmoid 함수가 사용된다.다중 분류에는 출력층으로 Softmax 함수가 사용된다.3. 손실 함수: 크로스 엔트로피(Cross entropy)앞에서 최종적으로 모델의 예측 값을 얻었습니다. 이제 우리가 얻은 값과 실제 값 사이를 비교해 최적화를 하기 위해 손실 함수를 정의 해야 합니다.이 전 회귀 모델에서는 MSE를 사용했습니다. 분류를 위한 손실 함수로는 어떤 것을 선택하는 것이 좋을까요? 그대로 MSE를 쓴다면 어떨지 먼저 생각해봅시다.선형 회귀는 정답과 예상값의 오차 제곱이 최소가 되는 가중치를 찾는 것이 목표였습니다. 그렇다면 분류의 목표는 무엇일까요? 올바르게 분류된 데이터의 비율을 높이는 것이 분류의 목표입니다. 하지만 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용할 수가 없습니다. 대신 비슷한 역할을 하는 함수가 있습니다. 바로 그 함수가 로지스틱 손실 함수입니다.이진 분류를 위한 크로스 엔트로피 함수다중 분류를 위한 크로스 엔트로피 함수정답이 0인 경우 우리의 출력값이 1에 가까워지면 손실함수가 증가하게 됩니다. 반면에 정답이 1인 경우에는 출력값이 0에 가까워 질수록 손실함수가 증가합니다.4. 가중치 업데이트선형 회귀 모델에서는 가중치를 업데이트 할 때 손실함수를 바로 가중치에 대해 미분할 수 있었다. 하지만 실제로 은닉층이 생기고 활성화 함수가 추가되면 더 이상 바로 미분할 수가 없다. 그래디언트를 구하기 위해서는 우선 활성화 함수에 대해 편미분을 해야한다. 이렇게 합성함수를 순서대로 편미분해 곱한 것을 Chain rule이라고 한다.과정은 선형회귀보다 조금 복잡하지만 결과는 같게 나왔다.크로스 엔트로피 손실함수를 활성화 함수를 고려해 그래디언트를 구한 결과 여전히 err*x 이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/classification'> <img src='/images/dog_cat.png' alt='Deep Learning Series [Part3]: 분류를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/classification'>Deep Learning Series [Part3]: 분류를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-collections",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/python-component-collections'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-collections'>Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/linear_regression",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  선형 회귀          1. 선형 회귀(Linear regression)      2. 손실함수: MSE(Mean Squared Error)      3. 가중치 업데이트      선형 회귀1. 선형 회귀(Linear regression)딥러닝을 배울 때 출발점으로 좋은 모델이 바로 선형 회귀입니다. 왜냐하면 뒤에서 배우게 될 모델들은 보통 이러한 선형 회귀에서 약간 변형하고, 추가하는 방법을 통해 만들어 지기 때문입니다.선형 회귀에서선형: 모델이 가중치와 데이터의 곱과 합으로 구성되어 있다. 이러한 경우를 데이터의 선형 조합(linear combination)이라고 한다.회귀: 어떤 연속적인 값을 예측하는 것을 회귀 라고 한다.선형 회귀 모델2. 손실함수: MSE(Mean Squared Error)우리의 모델의 예측 값(y’)이 정답(y)에 가깝도록 하는 것이 목표일 때 손실함수를 어떻게 정의하면 좋을까? 바로 머릿 속에 떠오르는 방법은 둘 간의 오차로 정의하는 것입니다. 이것을 우리는 Error라고 한다. 근데 Error 값은 양수일 수도 있고, 음수일 수도 있습니다. 그 상태에서 Error들을 합하면 실제 우리가 생각하는 것보다 작을 것입니다.그렇기 때문에 항상 각각의 Error가 양수가 되도록 제곱을 취하도록 하겠습니다. 이것을 우리는 MSE라고 합니다. 아마 더 깊이 공부하다 보면 이런 MSE 손실함수로는 부족할 수 도 있다고 생각이 듭니다. 하지만 MSE는 처음에 인공지능을 시작할 때 사용하기 좋은 손실함수이기 때문에 여기서는 회귀 모델에서는 MSE를 손실함수로 사용한다 라고 까지만 하고 마치도록 하겠습니다.MSE를 통해 정의한 손실함수 L3. 가중치 업데이트가중치(w)를 업데이트해 우리의 모델을 최적화시켜보도록 하겠습니다. 앞에서 우리는 가중치를 업데이트 하는 방법으로 경사하강법(Gradient descent)를 사용한다고 했습니다. 그렇기 때문에 우선 각 가중치의 Gradient를 구해야합니다. 그리고 Gradient는 함수값을 가장 가파르게 증가하는 방향이므로 (-)를 취해 손실함수 값을 가장 빠르게 감소시키는 방향으로 가중치를 업데이트 하겠습니다.손실함수를 각각의 가중치에 대해 편미분한다.각각의 편미분에 (-)를 취해 해당 가중치 지점에서 가장 빨리 손실함수를 감소시키는 방향으로 가중치를 업데이트한다.(알파는 학습률이라는 파라미터인데 얼마나 크게/작게 가중치를 업데이트할 것인지 정하는 값이다.)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/linear_regression'> <img src='/images/mse.png' alt='Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linear_regression'>Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part1]: 딥러닝이 처음이라면",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/intro",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  딥러닝이 처음이라면          1. 미래를 바꿀 핵심 기술      2. 딥러닝이란 무엇인가?      3. 인공지능, 머신러닝, 딥러닝                  인공지능          머신러닝          딥러닝                    4. 딥러닝을 해보자                  학습          검증          결론                    딥러닝이 처음이라면1. 미래를 바꿀 핵심 기술그 동안 컴퓨터는 단순한 계산에서 복잡한 계산, 정보 전달을 가능하게 했고, 또 인간이 하던 단순한 작업 이를 테면 스팸 메일 분류와 같은 것들을 해왔습니다.그러나 최근 컴퓨터 하드웨어의 발달과, 축적된 데이터를 통해 사람들은 더 많은 시도를 해왔습니다. 그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다. 지금부터 이것들을 가능하게 한 딥러닝이 어떤 건지 하나씩 살펴 보도록 하겠습니다.2. 딥러닝이란 무엇인가?딥러닝이란 무엇일까요? 위키 백과에서는 다음과 같이 정의합니다.  딥 러닝은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계 학습 알고리즘의 집합으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다.좀 더 자세하고 전문적인 지식은 위키백과에서 찾아 보도록 하고 저는 이해하기 쉽도록 간단하게 정의해 보겠습니다.딥러닝은 특별한 규칙이나 문제를 푸는 방법을 컴퓨터에게 알려주지 않습니다.우리는 그저 복잡하고 설명하기 힘든 특성을 컴퓨터가 잘 잡아낼 수 있도록 신경망을 만듭니다. 그리고 그 신경망에 데이터를 제공함으로써 기계가 숨겨진 특성을 잘 찾아낼 수 있도록 잘 학습시킵니다. 이러한 과정이 바로 딥러닝입니다.3. 인공지능, 머신러닝, 딥러닝(출처: Samstory)인공지능인공지능은 인간이 뇌를 통해 하는 행동들을, 컴퓨터도 마치 생각하는 것처럼 유사하게 동작하는 것을 인공지능이라고 합니다. 머신러닝, 딥러닝도 결국 인공지능을 위한 하나의 방법인 겁니다.머신러닝머신러닝은 정해진 명령보다 데이터를 기반으로 예측이나 결정을 이끌어 내기 위해 특정한 알고리즘을 수행하는 모델을 구축하는 방식으로 모델을 구축함으로써 입력하지 않은 정보에 대해서다 판단이나 결정을 할 수 있게 됩니다.머신러닝 기법은 주로 정형 데이터를 다룹니다. 관계형 데이터베이스(Relational Database)나 엑셀 표로 정리되는 테이블 데이터를 생각하시면 되겠습니다. 의사결정에 필요한 데이터를 사람이 정리해 기계에 알려주면 기계는 이 정보를 토대로 판단이나 예측을 하는 경우입니다.딥러닝딥러닝은 머신러닝의 한 파트이지만 보다 조금 더 추상적인 모델입니다. 머신러닝과 같이 특정한 알고리즘을 수행하도록 모델을 만들기보다는, 사람이 잡아내기 힘든 추상적인 특성을 데이터에서 잘 추출하도록 단일 신경망을 그저 깊게 쌓는 방식으로 모델을 만듭니다.이러한 이유로 딥러닝은 주로 비정형 데이터를 다룹니다. 비정형 데이터란 지정된 방식으로 정리되지 않은 정보를 말합니다. 간단히 말하자면 이미지, 비디오, 텍스트 문장이나 문서, 음성 데이터 등을 말합니다. 비정형 데이터는 인간이 그 특성을 잡아내기 매우 힘든 데이터이기 때문에 딥러닝이 큰 힘을 발휘하게 됩니다.4. 딥러닝을 해보자이번 챕터에서 딥러닝의 전체적인 과정을 정말 간략히만 훑어보도록 하겠습니다. 딥러닝을 포함해 인공지능의 목표는 어떻게 보면 새로운 데이터에 대한 예측이라고 생각하면 될 것 같습니다. 예를 들어 부동산 가격 예측, 주가 예측, 승패 예측, 날씨 예측과 같은 것들이 있겠죠. 앞의 두 가지는 연속적인 값을 예측하므로 회귀(regression)라고 하고, 뒤의 두 가지는 분류(classification)라고 합니다. 그럼 인공지능 모델은 크게 다음과 같이 두 가지 목적에 따라 분류된다고 할 수 있습니다.  회귀(regression)모델: 데이터의 일반적인 경향을 가장 잘 나타내는 모델을 만드는 것이 목표  분류(classification)모델: 데이터를 일반적으로 가장 잘 분류하는 decision boundary를 찾는 것이 목표학습그럼 이제 우리의 목표는 데이터를 이용해 다음과 같은 모델을 만드는 것입니다. 어떻게 만들 수 있을까요? 이것은 마치 어렸을 때 수학 시험을 위해 문제가 가득한 문제집을 무수히 많이 푸는 과정과 비슷하다고 할 수 있습니다. 컴퓨터는 우리의 데이터를 기반으로 최소한의 오차를 내기 위해 계속 학습하게 됩니다. 드디어 우리의 모델이 무엇을 해야할지 목표가 생겼습니다.목표: 오차 함수를 최솟값으로 만드는 것입니다.목표가 생겼으니 이제 목표를 향해 어떻게 나아갈 지를 생각해봐야 합니다. 어떻게 나아가야 할까요? 우리의 컴퓨터가 목표(최솟값)가 어디 있는지 한 번에 알면 좋겠지만 그렇지는 않습니다. 마치 다음과 같습니다.  앞이 보이지 않습니다. (어디가 최솟값인지 알 수 없습니다)      힌트는 최솟값이 우리의 목표라는 것입니다.      깜깜한 상태에서 가장 밑으로 내려가기 위해서는 발을 더듬으며 내리막길 중 어디가 가장 가파른지를 찾을 것입니다. 그쪽으로 가야 가장 빨리 내려갈 수 있겠죠. 우리의 모델도 이와 비슷한 방법으로 학습을 시킬 수 있습니다. 이 방법을 경사 하강법(Gradient descent)이라고 합니다.방법: 경사하강법을 통해 학습할 것입니다.결론: 학습이란 경사하강법을 통해 오차가 최소가 되도록 하는 것입니다.💡 경사하강법이란 함수의 특정 지점에서의 그래디언트(가장 가파르게 증가하는 방향) 반대 방향으로 우리의 모델을 조금씩 수정해 나가는 것으로, 그래디언트 반대 방향인 이유는 그래디언트 가장 가파르게 증가하는 방향이기 때문에 부호(-)를 취해줌으로써 정확히 반대 방향으로 가면 오차함수를 가장 빨리 감소시키는 방향으로 모델을 수정할 수 있다.검증우리는 데이터를 이용해 경사하강법을 사용함으로써 오차함수를 최소로 하도록 모델을 수정하면 된다고 배웠습니다. 하지만 우리는 수능을 앞두고 있는 상황에서 항상 문제집만 풀지는 않습니다. 자칫 잘못하면 문제집에 나오는 문제들은 너무 완벽하게 공부했지만 수능에 출제될 문제와는 전혀 다를 수도 있습니다. 우리는 언제까지 문제집을 푸는게 도움이 되는지 판단을 할 수 있어야 합니다. 매번 문제집을 풀며 중간 중간 모의고사를 통해 모의고사 성적을 확인합니다. 그러다가 어느 순간 모의고사 성적이 오히려 나빠진다면 문제집을 더이상 풀면 안됩니다. (이를 과대적합(overfitting)이라고 합니다.)훈련마다 검증 데이터를 통해 언제 훈련을 멈출 지 정한다.검증 데이터에서 가장 좋은 성능을 갖는 모델을 우리의 모델로 선택한다.결론지금까지 살펴본 과정들이 딥러닝의 간략한 모습이라고 할 수 있습니다.1. 목적에 맞게 회귀 또는 분류 모델을 선택한다. 2. 그에 맞는 목적함수를 설정한다.  3. 경사하강법을 통해 학습한다.  4. 검증 데이터를 통해 언제 학습을 종료할지 결정한다.  5. 검증 데이터에서 가장 좋은 성능을 낸 모델을 선택한다.다음 포스트에서 부터 하나씩 자세하게 다뤄보도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/intro'> <img src='/images/deep_learning.png' alt='Deep Learning Series [Part1]: 딥러닝이 처음이라면'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/intro'>Deep Learning Series [Part1]: 딥러닝이 처음이라면</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part6] 데이터베이스 모델링",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series6",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  데이터베이스 모델링  ERM: Entity Relationship Model  데이터베이스 모델링 단계          비즈니스 룰      Entity, Attribute, Relationship 후보 파악      Entity간 관계 파악      정규화      물리적 모델링(네이밍, 데이터 타입, 제약조건)      ERM으로 표현      데이터베이스 모델링데이터베이스에 데이터를 어떤 식으로 저장할지 계획하는 것은 굉장히 중요합니다. 데이터베이스를 제대로 설계하지 않으면 데이터 삽입, 업데이트, 삭제시 이상 문제가 생길 수 있습니다.삽입: 새로운 데이터를 자연스럽게 저장할 수 없는 문제 (ex. 유저테이블의 주소지 추가)업데이트: 업데이트할 때 정확성을 지키기 어려워지는 문제삭제: 원하는 데이터를 자연스럽게 삭제할 수 없는 문제데이터베이스에 이런 이상 문제가 생기게 되면 새로운 데이터베이스로 데이터를 옮겨야 하는 상황이 발생하게 되고, 이런 문제는 비용적인 부담이 발생하는 일이기 때문에 최대한 사전에 방지하는 것이 좋습니다.데이터베이스 모델링은 크게 세 가지 단계로 구분할 수 있습니다.비즈니스 모델링: 비즈니스 룰을 정의한다 (ex. 유저는 하나의 주문에 하나의 리뷰만 달 수 있다)논리적 모델링: 어떤 것을 테이블 하고, 컬럼으로 하고 테이블끼리 어떻게 관계를 지을지 정의한다물리적 모델링: 테이블명, 컬럼명, 데이터 타입, 제약조건을 정의한다ERM: Entity Relationship Model데이터베이스를 모델링할 때에는 기존의 익숙한 모델 구조인 관계형 모델(Relational Model) 보다는 개체 관계 모델(ERM)을 주로 사용합니다. ERM의 예시는 아래와 같습니다.ERM은 기존 관계형 모델의 로우(Row)를 엔티티(Entity), 컬럼을 어트리뷰트(Attribute)로 표현하고 테이블간의 관계를 릴레이션쉽(Relationship)으로 나타냅니다.두 엔티티간의 관계를 정의하는 방법에는 다음과 같은 경우가 있습니다.예시는 다음과 같습니다.데이터베이스 모델링 단계데이터베이스를 모델링 하는 단계는 다음과 같습니다.1. 비즈니스 룰을 정한다2. Entity, Attribute, Relationship이 될 수 있는 후보를 정한다3. 비즈니스 룰을 통해 Entity간 관계를 파악한다4. 정규화한다5. 네이밍, 데이터 타입, 제약조건을 정한다6. ERM으로 나타낸다예시를 가지고 데이터베이스를 처음부터 간단하게 모델링 해보도록 하겠습니다.비즈니스 룰Entity, Attribute, Relationship 후보 파악Entity간 관계 파악정규화데이터베이스에서 데이터를 삽입/업데이트/삭제할 때 생길 수 있는 문제를 사전에 방지하기 위해 실시하는 작업1NF: 모든 컬럼 값은 나눌 수 없는 단일값이 되어야 한다2NF: 1NF + 모든 non-prime attribute는 candidate key 전체에 함수 종속성이 있어야 한다    (Non-prime attrbute중 2NF를 만족하지 않는 속성은 테이블에서 분리한다)3NF: 2NF + 모든 attribute는 오직 primary key에 대해서만 함수 종속성을 가져야 한다    (모든 attribute는 직접적으로 테이블 엔티티에 대한 내용이어야 한다)    (이행적 함수종속성을 없애야 한다)  1NF          어떤 채용 공고글에서 요구하는 스킬이 리스트 형태([MySQL, Python, Pytorch]로 되어 있으면 skiils를 새로운 테이블로 만들자        2NF          함수 종족성: x, y 속성이 있을 때, y = f(x)라는 관계가 성립하는 경우      Candidate Key: 하나의 로우를 특정 지을 수 있는 속성(attribute)들의 최소 집합      Prime Attribute: Candidate Key에 포함되는 모든 속성        3NF          모든 attribute는 직접적으로 테이블 엔티티에 대한 내용이어야 한다      물리적 모델링(네이밍, 데이터 타입, 제약조건)ERM으로 표현",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series6'> <img src='/images/sql_13.png' alt='MySQL Series [Part6] 데이터베이스 모델링'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series6'>MySQL Series [Part6] 데이터베이스 모델링</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part5] 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series5",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  테이블 다루기          테이블의 컬럼 구조 확인하기      컬럼 추가, 이름 변경      컬럼 삭제      컬럼 타입 변경      컬럼 속성 변경      테이블에 제약 사항 걸기      테이블의 제약 사항 삭제      컬럼 순서 앞으로 당기기      컬럼 순서 정하기      컬럼명 속성 동시에 바꾸기      테이블 복제하기      테이블 뼈대만 복제하기        외래키 설정하기          외래키 설정      외래키 정책      외래키 삭제      외래키 파악      이번 포스트에서는 테이블을 처음 구축할 때 필요한 설정을 하기 위한 SQL문에 대해 배워보겠습니다.테이블 다루기테이블의 컬럼 구조 확인하기DESCRIBE [테이블 이름]컬럼 추가, 이름 변경ALTER TABLE [테이블 이름] ADD [추가할 컬럼] CHAR(10) NULL;ALTER TABLE [테이블 이름]RENAME COLUMN [원래 컬럼명] TO [바꿀 컬럼명];컬럼 삭제ALTER TABLE [테이블 이름]DROP COLUMN [삭제할 컬럼명];컬럼 타입 변경ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT;컬럼 속성 변경-- NOT NULL 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL;-- DEFAULT 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL DEFAULT [주고 싶은 default값];-- DATETIME, TIMESTAMP 타입에 줄 수 있는 특별한 속성-- DEFAULT CURRENT_TIMESTAMP: 값 입력 안되면 default로 현재 시간 입력ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP;-- 처음 default로 현재 시간 넣어주고, 데이터 갱신될 때 마다 갱신된 시간 넣어줌  ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;-- UNIQUE 속성-- UNIQUE는 PRIMARY KEY와 다르게 NULL 허용ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT UNIQUE;테이블에 제약 사항 걸기ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍] CHECK [제약 사항(ex. age &amp;lt; 100)];테이블의 제약 사항 삭제ALTER TABLE [테이블 이름]DROP CONSTRAINT [제약 사항 이름];컬럼 순서 앞으로 당기기ALTER TABLE [테이블 이름]MODIFY [컬럼명] INT FIRST;컬럼 순서 정하기ALTER TABLE [테이블 이름]MODIFY [뒤에 올 컬럼명] INT AFTER [앞에 있는 컬럼명];컬럼명 속성 동시에 바꾸기ALTER TALBE [테이블 이름]CHANGE [원래 컬럼명] [바꿀 컬럼명] VARCHAR(10) NOT NULL;테이블 복제하기CREATE TABLE [복제한 테이블의 이름]AS SELECT * FROM [원본 테이블의 이름]테이블 뼈대만 복제하기CREATE TABLE [복제한 테이블의 이름]LIKE [원본 테이블의 이름]외래키 설정하기외래키(Foreign Key)란 한 테이블의 컬럼 중에서 다른 테이블의 특정 컬럼을 식별할 수 있는 컬럼을 말합니다. 그리고 외래키에 의해 참조당하는 테이블을 부모 테이블(parent table), 참조당하는 테이블(referenced table)이라고 합니다. 외래키를 이용하면 테이블간의 참조 무결성을 지킬 수 있습니다. 참조 무결성이란 아래 그림과 같이 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미합니다.예를 들어, 강의 평가인 review 테이블에는 ‘컴퓨터 개론’에 관한 평가 데이터가 남아있지만, 강의 목록을 나타내는 course 테이블에는 ‘컴퓨터 개론’ 과목이 삭제된다면 이상한 상황이 벌어질 것입니다. 이 때 외래키를 통해 지정해 놓으면 이런 상황을 해결할 수 있습니다.이렇게 외래키는 두 개 이상의 테이블에서 중요한 역할을 하기 때문에 외래키 속성을 어떻게 설정하는지는 굉장히 중요한 문제입니다.외래키 설정ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍]    FOREIGN KEY (자식테이블의 컬럼)    REFERENCES 부모테이블 (부모테이블의 컬럼)    ON DELETE [DELETE정책]    ON UPDATE [UPDATE정책];외래키 정책  RESTRICT: 자식 테이블에서 삭제/갱신해야만 부모 테이블에서도 삭제/갱신 가능  CASCADE: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터도 같이 삭제/갱신  SET NULL: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터의 컬럼에 NULL 지정외래키 삭제ALTER TABLE [테이블 이름]DROP FOREIGN KEY [제약 사항이 걸린 테이블];외래키 파악SELECT    i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME,    k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAMEFROM information_schema.TABLE_CONSTRAINTS iLEFT JOIN information_schema.KEY_COLUMN_USAGE kUSING(CONSTRAINT_NAME)WHERE i.CONSTRAINT_TYPE = &#39;FOREIGN KEY&#39;;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series5'> <img src='/images/sql_2.png' alt='MySQL Series [Part5] 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series5'>MySQL Series [Part5] 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 파이썬의 네임스페이스",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-namespace",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents파이썬 식별자, 스코프, 네임스페이스",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-namespace'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 파이썬의 네임스페이스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-namespace'>Python Basic Series [Part6]: 파이썬의 네임스페이스</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-file_directory",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-component-file_directory'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-file_directory'>Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part4] 데이터 관리(1): CRUD를 이용해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series4",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents  테이블 생성 및 삭제          SQL문 데이터 타입        데이터 CRUD          데이터 추가      데이터 갱신      데이터 삭제      앞에서 저희가 배웠던 SQL문들은 모두 이미 테이블이 주어졌고 그 테이블에 데이터가 쌓여있는 상태에서 원하는 데이터를 조회하는 방법에 관한 것들이었습니다.하지만 저희가 직접 테이블을 생성하고 데이터를 쌓아야 하는 순간도 있을 것입니다. 이번 포스트에서는 이러한 순간에 필요한 SQL문에 대해 배워 보겠습니다.테이블 생성 및 삭제-- 데이터베이스 생성CREATE DATABASE [생성할 데이터베이스 이름]CREATE DATABASE IF NOT EXISTS [생성할 데이터베이스 이름]-- 데이터베이스 지정USE [생성한 데이터베이스 이름]-- 테이블 생성CREATE TABLE [데이터베이스 이름].[생성할 테이블 이름] (    [컬럼1] INT NOT NULL AUTO_INCREMENT PRIMARY KEY,    [컬럼2] VARCHAR(20) NULL,    [컬럼3] VARCHAR(15) NULL,    또는 PRIMARY KEY ([&#39;컬럼1&#39;]));-- 테이블 삭제DROP TABLE [테이블 이름]SQL문 데이터 타입            종류      타입              정수형      TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT              실수형      DECIMAL, FLOAT, DOUBLE              날짜 및 시간      DATE, TIME, DATETIME, TIMESTAMP              문자열      CHAR, VARCHAR, TEXT            TINYINTsigned: -128 ~ 127unsigned: 0 ~ 255        INTsigned: -2147483648 ~ 2147483647unsigned: 0 ~ 4294967295        DECIMALDECIMAL(M, D): M은 전체 숫자의 최대 자리수, D는 소수점 자리 숫자의 최대 자리수DECIMAL(5, 2): -999.99 ~ 999.99M은 최대 65까지 가능, D는 최대 30까지 가능        FLOAT-3.4 * 10^38 ~ 3.4 * 10^38        DOUBLE-1.7 * 10^308 ~ 1.7 * 10^308FLOAT와 비교해 범위도 더 넓고, 정밀도 또한 더 높음(더 많은 소수점 자리 수 지원)        DATE날짜를 저장하는 데이터 타입’2021-03-21’ 이런 형식의 연, 월, 일 순        TIME시간을 저장하는 데이터 타입’09:27:31’ 이런 형식의 시, 분, 초        DATETIME날짜와 시간을 저장하는 데이터 타입’2021-03-21 09:30:27’ 이런 식으로 연, 월, 일, 시, 분, 초        TIMESTAMPDATETIME과 같다차이점은 TIMESTAMP는 타임 존 정보도 포함        CHARCHAR(30): 최대 30자의 문자열을 저장 (0~255까지 가능)차지하는 용량이 항상 숫자값에 고정됨데이터의 길이가 크게 변하지 않는 상황에 적합        VARCHARVARCHAR(30): 최대 30자의 문자열을 저장 (0~65536까지 가능)차지하는 용량이 가변적. 30이어도 그 이하의 길이면 용량도 적게 차지함해당 값의 사이즈를 나타내는 부분(1byte 또는 2byte)이 저장 용량에 추가데이터 길이가 크게 들쑥날쑥해지는 경우에 적합        TEXT문자열이 아주 긴 상황에 적합  데이터 CRUD데이터 추가-- 데이터 추가INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼2, 컬럼3, ...)VALUES (컬럼1의 데이터, 컬럼2의 데이터, 컬럼3의 데이터, ...);-- 특정 컬럼에만 데이터 넣을 수도 있다INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼3)VALUES (컬럼1의 데이터, 컬럼3의 데이터);데이터 갱신-- 데이터 갱신UPDATE [사용할 테이블 이름]    SET 컬럼1 = [갱신 데이터] WHERE [조건]; -- 기존 값을 기준으로 갱신UPDATE [사용할 테이블 이름]SET 컬럼1 = [컬럼1 + 3] WHERE [조건]; 데이터 삭제DELETE FROM [사용할 테이블 이름]WHERE [조건]",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/mysql-series4'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part4] 데이터 관리(1): CRUD를 이용해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series4'>MySQL Series [Part4] 데이터 관리(1): CRUD를 이용해 데이터 관리하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-dictionary",
      "date"     : "Mar 16, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-16T21:01:35+09:00'>16 Mar 2021</time><a class='article__image' href='/python-data-type-dictionary'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-dictionary'>Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part3]: 파이썬 리스트 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-list",
      "date"     : "Mar 15, 2021",
      "content"  : "Table of Contents  리스트          1. 리스트 자료형의 특징      2. 리스트의 장점      3. 리스트 생성      4. 인덱싱, 슬라이싱      5. 리스트 메소드                  5-1 .append(), .extend(), .insert(), .copy()          5-2 .pop(), .remove(), .clear()          5-3 .sort(), .reverse()          5-4 .count(), .index()                    6. 리스트에서 주목할 만한 것들                  6-1 List &amp;amp; Range          6-2 리스트 표현식 (List comprehension)          6-3 리스트와 문자열 넘나들기          6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)                    7. 리스트 주요 연산들의 시간 복잡도      리스트파이썬 언어는 개발자의 편의성, 생산성, 가독성에 가장 초점을 맞춘 언어입니다. 그래서 파이썬에서는 기존 언어에서 동적 배열이라는 기초 자료형의 불편함을 개선해 리스트라는 파이썬만의 강력한 자료형을 제공합니다.1. 리스트 자료형의 특징  파이썬에서 가장 자주 사용하는 자료형  원소들의 순서가 있는 시퀀스  원소들의 변경이 가능 (Mutable)  다양한 타입의 원소 저장 가능  동적배열로 구현됨2. 리스트의 장점  임의의 원소에 O(1) 접근 가능: 이것은 기존 동적배열이 제공해주는 기능입니다  다양한 타입의 원소 저장 가능: 리스트가 값이 아닌 값을 가진 객체의 주소를 동적배열로 저장하고 있기 때문입니다.  왠만한 추상 자료형은 리스트로 구현 가능: 리스트 자료형이 가지고 있는 많은 메소드로 스택, 큐, 트리, 그래프 등 거의 모든 추상 자료형을 구현할 수 있습니다.3. 리스트 생성리스트는 여러 가지 자료형을 가질 수 있는 시퀀스형 자료형입니다.또한 값을 변경할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = [3.1, 2.5, 7]&amp;gt;&amp;gt;&amp;gt; c = [&quot;Hello&quot;, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; d = [1, 4.5, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; a[0] = 100&amp;gt;&amp;gt;&amp;gt;a[100, 2, 3, 4]🔔 리스트를 곱하거나 더하면 값이 반복되거나 추가됩니다&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a + [5][1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a + 5 -&amp;gt; Error&amp;gt;&amp;gt;&amp;gt; a * 2[1, 2, 3, 4, 1, 2, 3, 4]4. 인덱싱, 슬라이싱이번에는 위에서 만들어진 리스트 데이터를 가지고 원하는 부분만 가져올 수 있도록 해주는 인덱싱, 슬라이싱에 대해 알아보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&quot;&quot;&quot;인덱싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0]1&amp;gt;&amp;gt;&amp;gt; a[2]3&amp;gt;&amp;gt;&amp;gt; a[4] = 10 -&amp;gt; Error  (a[50] = 10 이런식으로 하면 그 사이의 인덱스에 값을 표시할 수 없어서 무조건 차례대로 값을 채워넣어야 함 -&amp;gt; 더하기 또는 append 메소드)&quot;&quot;&quot;슬라이싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0:3] # 0에서 부터 3앞까지 -&amp;gt; 인덱스 0~2[1, 2, 3]&amp;gt;&amp;gt;&amp;gt; a[:][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::-1] #처음부터 끝까지 거꾸로 슬라이싱 (중요)[4, 3, 2, 1]5. 리스트 메소드리스트 데이터는 프로그래밍을 하다보면 정말 자주 만나게 되는 자료형 중에 하나입니다.그렇기 때문에 문자열 객체의 메소드를 잘 활용할 줄 아는 것이 굉장히 중요합니다.먼저 어떤 메소드가 있는지 확인해 보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__add__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;,  &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__imul__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;,   &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;,    &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__rmul__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;,    &#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;]실제로 코딩을 하실 때는 기억이 안나면 그 때마다 dir() 함수를 사용해 어떤게 있는지 살펴보면 됩니다.5-1 .append(), .extend(), .insert(), .copy().append()리스트 맨 끝에 인자로 넣어준 값 하나를 추가해준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.append(100)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100]여러 개를 추가하고 싶어서 인자로 값을 여러 개 준다면? -&amp;gt; 에러가 난다그래서 [100, 101, 102] 이런 식으로 추가하면? -&amp;gt; 에러는 안나지만 리스트가 추가되어 원하는 모습과는 다르다..extend()iterable한 객체를 인자로 넣어주면 그 안의 원소들이 모두 차례대로 리스트에 추가된다.&amp;gt;&amp;gt;&amp;gt; a.extend([101, 102, 103]) # 리스트와 같은 iterable한 객체를 인자로 주어야 한다.&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100, 101, 102, 103]이제는 맨 뒤가 아니라 원하는 인덱스에 값을 추가(교체)하고 싶다..insert()인자로 인덱스와 값을 넣어주면 인덱스에 값을 넣어준다.인덱스에 이미 값이 있으면 바꿔주고 리스트 길이보다 인덱스 값이 크거나 같으면 리스트 맨 뒤에 값을 넣어준다.-&amp;gt; 길이 신경쓰지 않고 해줘도 오류는 안난다. (내가 원하는 인덱스에 값이 들어가지 않을 수도 있지만)&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1, 10)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1000, 7)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4, 7].copy()객체와 똑같은 값을 가지는 리스트를 복사한다. 변수를 지정해주면 새로운 메모리에 저장된다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = a.copy()&amp;gt;&amp;gt;&amp;gt; b.append(5)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b[1, 2, 3, 4, 5]5-2 .pop(), .remove(), .clear().pop()리스트의 가장 끝에 있는 원소를 뽑아 리턴해준다.&amp;gt;&amp;gt;&amp;gt; a = [&#39;banana&#39;, &#39;lemon&#39;, &#39;apple&#39;]&amp;gt;&amp;gt;&amp;gt; a.pop()&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;banana&#39;, &#39;lemon&#39;].remove()인자로 받은 값은 값을 제거해준다.&amp;gt;&amp;gt;&amp;gt; a.remove(2)&amp;gt;&amp;gt;&amp;gt; a[1, 3, 4].clear()리스트를 싹 비운다.&amp;gt;&amp;gt;&amp;gt; a.clear()&amp;gt;&amp;gt;&amp;gt; a[]5-3 .sort(), .reverse().sort()  리스트를 작은 값부터 순서대로 정렬해준다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a = [&#39;안녕&#39;, &#39;Hello&#39;, &#39;Hi&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a.sort(key=len)&amp;gt;&amp;gt;&amp;gt; a[&#39;안녕&#39;, &#39;Hi&#39;, &#39;Hello&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: x**2)&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]# 같은 제곱값에 대해 양수가 먼저 나오게 하려면 양수가 논리연산 시 False가 되면 되므로 기준을 0보다 작은지로 하면 된다 &amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: (x**2, x&amp;lt;=0))&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]&amp;gt;&amp;gt;&amp;gt; a =[False, True, False, True, True, False]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[False, False, False, True, True, True]🔔 sorted() 함수  sorted() 함수는 정렬된 값을 리턴해줄 뿐 인자로 받은 리스트를 정렬하지는 않는다.  또 한가지 중요한 특징은 sorted()함수는 리스트 뿐 아니라 모든 iterable한 값들을 정렬시켜 준다는 것입니다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted(a)[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a[3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted({1: &#39;D&#39;, 2: &#39;B&#39;, 3: &#39;B&#39;, 4: &#39;E&#39;, 5: &#39;A&#39;})[1, 2, 3, 4, 5]🔔 .sort()와 sorted() 모두 key, reverse 인자를 갖는다  key: 정렬을 목적으로 하는 함수를 값으로 넣는다. lambda를 이용할 수 있다. key 매개 변수의 값은 단일 인자를 취하고 정렬 목적으로 사용할 키를 반환하는 함수(또는 다른 콜러블)여야 합니다.  reverse: bool값을 넣는다. 기본값은 reverse=False(오름차순)이다..reverse()리스트의 원소의 순서를 뒤집어준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.reverse()&amp;gt;&amp;gt;&amp;gt; a[4, 3, 2, 1]🔔 reversed() 함수뒤집은 값을 리턴해줄 뿐 인자로 받은 리스트는 그대로다.🔔 문자열을 뒤집는 방법a.reverse()a = list(reversed(a))a = a[::-1]5-4 .count(), .index().count()인자로 받은 값이 등장하는 횟수를 리턴해준다.a = [1, 1, 1, 2, 3, 4]a.count(1)---------------------3.index()인자로 받은 값의 인덱스를 리턴해준다.a = [1, 3, 5, 7]a.index(7)--------------------36. 리스트에서 주목할 만한 것들6-1 List &amp;amp; Range1부터 1000까지 값을 하나씩 출력하는 코드를 짠다고 할 때for i in [1, 2, 3, 4, 5, 6, 7, 8, ..., 1000]:  print(i)로 하게 되면 위의 코드를 실행하기 위해 1000개의 요소를 적어서 리스트를 만드는 것은 너무 비효율적입니다.이를 개선시키는 방법으로for i in range(1000):  print(i)이렇게 해주면 훨씬 짧고 간결한 코드를 작성할 수 있습니다.range(start, end, step)range(1000) =&amp;gt; 0, 1, 2, 3, ..., 999range(1, 1000) =&amp;gt; 1, 2, 3, ..., 999range(1, 1000, 2) =&amp;gt; 1, 3, 5, 7, ..., 9996-2 리스트 표현식 (List comprehension)&amp;gt;&amp;gt;&amp;gt; a = []&amp;gt;&amp;gt;&amp;gt; for i in range(100):        if i % 3 == 0 and i % 5 == 0:          a.append(i)&amp;gt;&amp;gt;&amp;gt; [i for i in range(100) if i % 3 == 0 and i % 5 == 0]6-3 리스트와 문자열 넘나들기문자열을 리스트로 바꿔야 하는 경우문자열은 값을 바꿀 수가 없기 때문에 예를 들어 스펠링을 고치기 위해서는리스트로 바꿔서 고친 후 다시 문자열로 변환해줘야 한다.&amp;gt;&amp;gt;&amp;gt; name = &#39;kinziont&#39;&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39; -&amp;gt; 에러&amp;gt;&amp;gt;&amp;gt; name = list(name)&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39;&amp;gt;&amp;gt;&amp;gt; name[&#39;k&#39;, &#39;i&#39;, &#39;m&#39;, &#39;z&#39;, &#39;i&#39;, &#39;o&#39;, &#39;n&#39;, &#39;t&#39;]&amp;gt;&amp;gt;&amp;gt; name = str(name)&amp;gt;&amp;gt;&amp;gt; name&#39;kimziont&#39;문자열 데이터를 단어 단위 또는 문장 단위로 토크나이징하기 위해 문자열 메소드인 .split()을 쓰면자동으로 리스트로 변환된다.6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)a = [1, 2, 3, 4] # 1*4 vectorb = [[1, 2], [3, 4]] # 2*2 matrixc = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] # 2*2*2 tensora[0] -&amp;gt; 1b[0] -&amp;gt; [1, 2]c[0] -&amp;gt; [[1, 2], [3, 4]]c[0][1] -&amp;gt; [3, 4]c[0][1][0] -&amp;gt; 37. 리스트 주요 연산들의 시간 복잡도            연산      시간 복잡도      설명              len(a)      O(1)      전체 요소의 개수를 리턴              a[i]      O(1)      인덱스 i의 요소를 가져온다              a[i:j]      O(k)      객체 k개에 대한 조회가 필요하므로 O(k)이다              x in a      O(n)      정렬되어 있지 않은 a 이므로 순차 탐색              a.append(x)      O(1)      동적배열의 특징              a.pop(x)      O(1)      동적배열의 특징              a.pop(0)      O(n)      배열의 특성상 앞의 원소가 추가/삭제 되면 그 뒤의 모든 원소들의 이동이 발생              del a[i]      O(n)      i에 따라 다르다. 최악의 경우 O(n)이다              a.sort()      O(nlogn)      파이썬에서는 팀소트(Timsort)를 사용              min(a), max(a)      O(n)      최소, 최대값 찾기 위해서는 선형 탐색 해야함              a.reverse()      O(n)      선형 이동하면서 처음과 끝 원소 바꾼다      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-15T21:01:35+09:00'>15 Mar 2021</time><a class='article__image' href='/python-data-type-list'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part3]: 파이썬 리스트 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-list'>Python Basic Series [Part3]: 파이썬 리스트 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part3] 데이터 조회 및 분석(3): 윈도우(Window) 함수",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series3",
      "date"     : "Mar 15, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-15T21:01:35+09:00'>15 Mar 2021</time><a class='article__image' href='/mysql-series3'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part3] 데이터 조회 및 분석(3): 윈도우(Window) 함수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series3'>MySQL Series [Part3] 데이터 조회 및 분석(3): 윈도우(Window) 함수</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part2] 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series2",
      "date"     : "Mar 14, 2021",
      "content"  : "Table of Contents  조인          서로 구조가 다른 테이블간의 조인                  LEFT OUTER JOIN          RIGHT OUTER JOIN          INNER JOIN                    구조가 같은 테이블간의 조인                  UNION          UNION ALL          INTERSECT          MINUS                      서브쿼리  마치며조인여러 테이블을 합쳐서 하나의 테이블인 것처럼 보는 행위를 ‘조인(join)’이라고 합니다. 실무에서는 이 조인을 잘해야 제대로된 데이터 분석을 할 수 있습니다. 조인은 SQL을 얼마나 잘 쓰는지 판단하는 척도 중 하나일만큼 정말 중요한 개념입니다.서로 구조가 다른 테이블간의 조인  서로 구조가 다른 테이블을 특정 컬럼을 기준으로 조인SELECT     p.name    p.team    r.team    r.regionFROM player AS p LEFT OUTER JOIN region AS rON p.team = r.team -- ON 대신 USING(team) 이렇게 할 수도 있음LEFT OUTER JOINRIGHT OUTER JOININNER JOIN구조가 같은 테이블간의 조인UNION  중복을 허용하지 않는 합집합SELECT * FROM old_playerUNIONSELECT * FROM new_player;UNION ALL  중복을 허용하는 합집합INTERSECT  교집합  MySQL에서는 지원 하지 않음  INNER JOIN으로 해결MINUS  차집합  MySQL에서는 지원 하지 않음  LEFT/RIGHT OUTER JOIN으로 해결서브쿼리  SELECT문의 결과로 나온 값/열/테이블을 적재적소에 맞게 다른 SELECT문의 입력으로 사용할 수 있습니다마치며지쳐서 너무 대충해버렸다…생각날 때마다 조금씩 보충해야겠다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-14T21:01:35+09:00'>14 Mar 2021</time><a class='article__image' href='/mysql-series2'> <img src='/images/sql_5.png' alt='MySQL Series [Part2] 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series2'>MySQL Series [Part2] 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part2]: 파이썬 숫자 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-number",
      "date"     : "Mar 13, 2021",
      "content"  : "Table of Contents  1. 숫자 자료형의 종류  2. 파이썬의 특별한 점  3. 2진법, 8진법, 16진법  4. 부동 소수점 연산 오류  5. 숫자 자료형 관련 메소드1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-13T21:01:35+09:00'>13 Mar 2021</time><a class='article__image' href='/python-data-type-number'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part2]: 파이썬 숫자 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-number'>Python Basic Series [Part2]: 파이썬 숫자 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part1]: 파이썬에서 데이터의 특성",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-intro",
      "date"     : "Mar 12, 2021",
      "content"  : "Table of Contents  파이썬 데이터는 객체다          원시타입과 객체        타입  가변성  참조  복사          Alias      Shallow Copy      Deep Copy        참고파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.원시타입과 객체C나 자바 같은 언어는 기본적으로 원시 타입을 제공합니다. 원시 타입은 메모리에 정확하게 타입 크기만큼의 공간을 할당하고 그 공간을 오로지 값으로 채워넣습니다. 배열이라면 요소들이 연속된 순서로 메모리에 배치될 것입니다.객체는 단순히 값 뿐만 아니라 여러 가지 정보를 함께 저장하고, 이를 이용해 여러 가지 작업(비트조작, 시프팅 등)을 수행할 수 있게됩니다. 하지만 이로 인해 메모리 점유율이 늘어나게 되고 계산 속도 또한 감소하게 되는 단점이 있습니다.타입            이름      타입      가변              불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)a = &quot;5&quot;print(type(a))print(id(a))a = int(a)print(type(a))print(id(a))------------------&amp;lt;class &#39;str&#39;&amp;gt;139861785283696&amp;lt;class &#39;int&#39;&amp;gt;139861784516640참조a = 5변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 a라는 변수에 5라는 값을 담는 것이 아니라 a라는 이름이 Int 객체 5를 참조하는 것 입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다. 차이는 객체가 불변이냐 가변이냐의 차이일 뿐입니다.a라는 이름과 객체의 메모리 주소의 매핑 관계는 네임스페이스에 키-밸류 형태로 저장되는데 이 때의 네임스페이스는 메모리 상에서 코드 영역 또는 데이터 영역에 저장된다고 합니다. (스택이나 힙 영역은 아니라고 함, 참고)예를 들어, 왼쪽 그림에서 a가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 a는 새로운 값을 참조합니다.반면 오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사복사와 관련해서 진짜 복사(copy) 기능을 하는 것이 있고 흉내만 내는 것도 있습니다. 대표적으로 3가지 케이스가 있는데 하나씩 살펴보도록 하겠습니다.복사란 기존의 값과 같은 값을 가지는 변수를 하나 더 생성하며 각각의 변수는 독립적이어야 한다Aliasa = [1, 2, 3]b = a대입 연산자(=)를 이용한 경우를 alias(별칭)라고 합니다. 말 그대로 ‘[1, 2, 3]이라는 리스트 객체가 a라는 이름을 가지고 있었는데 b라는 이름을 하나 더 가지게 되었다.’ 정도로 이해할 수 있습니다. 이렇게 되면 a가 가르키는 [1, 2, 3]이 바뀌게 되면 b도 따라서 바뀌게 됩니다.a[2] = 100a -&amp;gt; [1, 2, 100]b -&amp;gt; [1, 2, 100](예시: 가수 ‘비’가 있습니다. ‘비’의 본명은 ‘정지훈’입니다. 만약 ‘비’가 머리를 잘랐다면 ‘정지훈’의 머리도 잘립니다.)Shallow Copy리스트의 슬라이싱 기능인 :를 이용하면 alias보다 더 복사같이 느껴집니다. 이를 얕은 복사(Shallow Copy)라고 합니다. 얕은 복사는 어떤 경우에는 정말 복사의 기능을 합니다.a = [1, 2, 3]c = a[:]a[2] = 100a -&amp;gt; [1, 2, 100]c -&amp;gt; [1, 2, 3]이렇게 봤을 때는 충분히 복사의 기능을 하고 있습니다. 하지만 변경하고자 하는 요소가 가변 객체이면 진짜 복사가 아니었다는 것이 드러나게 됩니다.a = [1, 2, [3, 4, 5]]c = a[:]a[2][2] = 100a -&amp;gt; [1, 2, [3, 4, 100]]c -&amp;gt; [1, 2, [3, 4, 100]]새로운 리스트 객체를 생성하긴 하지만 원래의 리스트 객체와 같은 ob_item(요소들의 포인터목록)을 가지고 생성되기 때문에, 요소가 가변 객체일 경우 따라서 변하게 됩니다.Deep Copy파이썬의 내장 모듈인 copy를 사용하면 어떤 상황에서도 복사를 제공합니다. 이를 깊은 복사(Deep Copy)라고 합니다. 깊은 복사는 아예 요소 자체를 새로 생성하기 때문에 ob_item도 다른 값을 가지는, 다시 말해서 완전히 같은 값을 새로운 메모리에 할당한 복사가 일어나게 됩니다.깊은 복사는 어떠한 상황에서도 복사를 보장하기 때문에 안정된 코드를 제공하지만, 메모리 낭비가 발생할 수 있습니다.import copya = [1, 2, 3]d = copy.deepcopy(a)참고  파이썬 알고리즘 인터뷰 책  YABOONG: 자바 메모리 관리 - 스택 &amp;amp; 힙  nina, memory-management-in-python-the-basics",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-12T21:01:35+09:00'>12 Mar 2021</time><a class='article__image' href='/python-data-type-intro'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part1]: 파이썬에서 데이터의 특성'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-intro'>Python Basic Series [Part1]: 파이썬에서 데이터의 특성</a> </h2><p class='article__excerpt'>이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part1] 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series1",
      "date"     : "Mar 7, 2021",
      "content"  : "Table of Contents  DBMS          Database      DBMS      SQL      MySQL      DBMS의 종류      DBMS의 구조        SQL의 분류          DCL      DDL      DML      TCL        SELECT문          SELECT      FROM      WHERE      ORDER BY      LIMIT      GROUP BY      HAVING      SELECT문의 작성순서와 실행순서      SQL에서 제공하는 함수      NULL 데이터 다루는 방법        마치며  참고DBMS빅 데이터 시대에서 데이터 저장은 가장 중요한 부분 중 하나입니다. 힘들게 얻은 데이터를 저장하지 않는다면 큰 자원 낭비겠죠. 하지만 중요한 것은 단순히 저장에 그치는 것이 아니라, 어떤 식으로 저장해 그 후 데이터를 추가, 갱신, 삭제 할 때 문제(NULL, 중복 등)가 생기지 않도록 할 것인지에 대한 고민도 이루어져야 한다는 것 입니다. 이번 MySQL 시리즈에서 이런 문제들을 어떻게 해결할 것인지에 대해 공부해보도록 하겠습니다.Database데이터베이스는 데이터의 집합 또는 데이터 저장소라고 정의할 수 있습니다.DBMS데이터베이스를 보통 직접적으로 접근하지는 않습니다. 사용자들이 데이터베이스를 그냥 접근한다면 데이터의 일관성도 떨어질 것이고, 관리도 쉽지 않을 것 입니다. 이러한 이유로 데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS(DataBase Management System)이라고 합니다.DBMS는 데이터베이스를 구축하는 틀을 제공하고, 효율적으로 데이터를 검색하고 저장하는 기능을 제공합니다. 또한 응용 프로그램들이 데이터베이스에 접근할 수 있는 인터페이스를 제공하고, 장애에 대한 복구 기능, 사용자 권한에 따른 보안성 유지 기능 등을 제공합니다.SQLDBMS를 이용해 데이터베이스를 사용하게 됩니다. 그렇다면 저희는 DBMS와 소통하는 방법을 알아야 합니다. 여기서 DBMS와 소통하기 위한 언어를 SQL(Structured Query Language)라고 합니다. SQL을 이용하면 데이터베이스 조작에 필요한 모든 명령어를 DBMS에 전달함으로써 수행할 수 있습니다.MySQL처음 SQL이라는 언어는 IBM이라고 하는 회사에서 System/R이라는 DBMS와, 이것을 사용하기 위해 필요한 언어인 SEQUEL을 만들면서 처음 등장했습니다. 그런데 SEQUEL(Structured English Query Language)은 그 단어가 이미 다른 곳에서 사용되고 있다는 문제(상표권 문제) 때문에 그 이름이 SQL(Structured Query Language)로 변경되었습니다. 그러다가 1987년, 국제 표준화 기구(ISO)에서 SQL에 관한 국제 표준(ISO 9075:1987)이 제정되었습니다.하지만 우리가 실제로 사용하는 SQL은 이 국제 표준에 완벽히 부합하지는 않습니다. Oracle, Microsoft SQL Server, MySQL 등의 DBMS에서 지원되는 SQL이 표준을 완벽히 준수하지는 않는다는 뜻입니다. 그 이유는 다양하지만 일단 많은 DBMS 회사들이 성능 향상과 더 다양한 기능 제공을 위해서, 차라리 표준을 일부 벗어나는 것을 택하는 경우가 많기 때문입니다.MySQL은 가장 처음 MySQL AB라고 하는 스웨덴 회사에서 개발되었습니다. 현재는 인수 과정을 거쳐 Oracle의 소유입니다. 이로 인해 지금 Oracle에는 Oracle(회사명과 같은 DBMS)과 MySQL이라는 서비스를 함께 제공하고 있습니다.두 DBMS의 시장에서의 쓰임새를 보면 약간의 차이가 있습니다. 은행, 거래소 등과 같이 데이터 처리의 정확성, 운영의 안정성 등이 엄격하게 요구되는 분야에서는 오라클이 주로 사용되고 있고, 우리가 흔히 쓰는 앱, 웹 사이트 같은 서비스를 만들 때는 MySQL을 쓰는 경우가 많습니다.DBMS의 종류위와 같이 많은 회사에서 성능 향상과 목적에 맞게 SQL이라는 언어를 조금씩 변형, 개선하여 새로운 DBMS로 개발해왔습니다. 이러한 이유로 MySQL과 같이 ~SQL이라는 용어도 사실상은 그 언어를 지원하는 DBMS 자체를 의미하게 되었습니다. 그래서 약간 헷갈리지만 관계형 데이터를 위한 DBMS의 경우 RDBMS, 비 관계형 데이터를 위한 DBMS의 경우 NoSQL이라고 하게 되었습니다.RDBMS: MySQL, Oracle, MariaDB(MySQL 개발자들이 만든 오픈소스), PostgreSQL 등NoSQL: MongoDB, ElasticSearch, Cassandra 등DBMS의 구조  client(클라이언트 프로그램): 유저의 데이터베이스 관련 작업을 위해, SQL을 입력할 수 있는 화면 등을 제공하는 프로그램  server(서버 프로그램): client로부터 SQL 문 등을 전달받아 데이터베이스 관련 작업을 직접 처리하는 프로그램MySQL에서 서버 프로그램의 이름은 mysqld, 클라이언트 프로그램 이름은 mysql입니다. mysql은 보통 CLI 환경에서 사용하는 프로그램입니다. CLI 환경이 아니라 GUI 환경에서 mysql을 사용하려면 mysql을 GUI 환경에서 사용할 수 있도록 해주는 프로그램을 사용하면 됩니다. 대표적으로 Oracle이 공식적으로 제공하는 MySQL Workbench라는 프로그램이 있습니다.SQL의 분류DCL  데이터베이스 접근 권한과 관련한 명령어  GRANT, REVOKE, DENYDDL  테이블과 같은 데이터 구조를 정의하는데 사용되는 명령어  CREATE, ALTER, RENAME, DROP, TRUNCATEDML  데이터 조회/삽입/수정/삭제와 관련한 명령어  SELECT, INSERT, UPDATE, DELETETCL  데이터를 트랜잭션 단위로 처리하는데 필요한 명령어  COMMIT, ROLLBACK, SAVEPOINTSELECT문MySQL에서 데이터를 조회하거나 분석할 때 필요한 SELECT문에 대해서 간단히 정리해 보겠습니다.SELECT  특정 컬럼이나 컬럼의 연산 결과를 지정SELECT *SELECT addressSELECT height / weightSELECT MAX(age)SELECT MAX(age) AS max_ageSELECT     (CASE         WHEN age IS NOT NULL THEN age         ELSE &quot;N/A&quot;     END) AS ageFROM  기준이 되는 테이블 지정FROM customersFROM orders-- 예시SELECT name FROM customers;WHERE  컬럼에 조건을 지정WHERE age = 20WHERE gender != &#39;m&#39;WHERE age &amp;gt;= 27WHERE age NOT BETWEEN 20 AND 30 -- 20~30WHERE age IN (20, 30) -- 20 or 30WHERE address LIKE &#39;서울%&#39;WHERE address LIKE &#39;%고양시%&#39;WHERE address LIKE BINARY &#39;%Kim%&#39; -- Kim 매칭, kim 매칭 xWHERE email LIKE &#39;__@%&#39; -- _는 임의의 문자 1개-- 예시SELECT * FROM customers WHERE age &amp;gt; 25;ORDER BY  정렬 기준을 지정ORDER BY height ASCORDER BY height DESC-- 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASC;LIMIT  보고자 하는 결과의 개수를 지정LIMIT 5 -- 5개LIMIT 10, 5 -- 10번째부터 5개-- 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASCLIMIT 3;GROUP BY  특정 컬럼의 값을 기준으로 그루핑  그루핑 하고나면 모든 함수연산 또한 그룹 단위로 실행GROUP BY genderGROUP BY countryGROUP BY country, genderGROUP BY SUBSTRING(address, 1, 2)-- 예시SELECT genderFROM customersGROUP BY gender;SELECT gender, MAX(age)FROM customersGROUP BY gender;GROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUP-- 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;;SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*)FROM memberGROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUPHAVING region IS NOT NULLORDER BY region ASC, gender DESC;HAVING  그루핑된 결과에 조건을 지정HAVING region = &#39;서울&#39;-- 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;🦊 WHERE과 HAVING의 차이점WHERE: 주어진 테이블의 전체 row에서 필터링을 하는 용도HAVING: GROUP BY 되고 난 후 row에서 필터링 하는 용도SELECT문의 작성순서와 실행순서작성 순서SELECT FROMWHEREGROUP BYHAVING ORDER BYLIMIT 실행 순서FROMWHERE GROUP BYHAVING SELECTORDER BYLIMIT SQL에서 제공하는 함수-- 모든 데이터 타입COUNT(*)DISTINCT(gender)-- 문자열 데이터 타입SUBSTRING(address, 1, 2) -- address의 첫번째 문자에서 2개LENGTH(address)UPPER(address)LOWER(address)LPAD(address)RPAD(address)-- 숫자 데이터 타입-- 집계(aggregation) 함수MAX(height)MIN(weight)AVG(weight)-- 산술(mathematical) 함수ABS(balance)CEIL(height)FLOOR(height)ROUND(height)-- 날짜 및 시간 데이터 타입YEAR(birthday)MONTH(birthday)DAYOFMONTH(birthday)DATEDIFF(birthday, &#39;2002-01-01&#39;)-- 예시SELECT * FROM customers WHERE MONTH(birthday) IN (4, 5, 6);NULL 데이터 다루는 방법WHERE address IS NULLWHERE address IS NOT NULL-- COALESCE(a, b, c) 함수는 a, b, c 중 가장 먼저 NULL아닌 값 리턴COALESCE(height, &quot;키 정보 없음&quot;)COALESCE(height, weight * 2.5, &quot;키 정보 없음&quot;)-- IFNULL(a, b) 함수는 a가 NULL 아니면 a, NULL이면 b 리턴IFNULL(height, &quot;키 정보 없음&quot;)-- IF(condition, a, b) 함수는 condition이 True이면 a, False이면 b리턴IF(address IS NOT NULL, address, &quot;N/A&quot;)-- CASE 함수CASE    WHEN address IS NOT NULL THEN address    ELSE N/AEND-- 예시SELECT addressFROM customersWHERE address IS NOT NULL;SELECT COALESCE(height, &quot;키 정보 없음&quot;), COALESCE(gender, &quot;성별 정보 없음&quot;)FROM customers;SELECT IF(address IS NOT NULL, address, &quot;N/A&quot;)FROM customers;SELECT    CASE        WHEN address IS NOT NULL THEN address        ELSE N/A    ENDFROM customers;마치며여기까지 테이블 한 개에 대해서 데이터를 조회하고 분석하는 방법에 대해 살펴보았습니다. 다음 포스트에서는 테이블이 여러 개인 경우에 대해 데이터를 조회하고 분석할 때 필요한 문법에 대해 알아보겠습니다.참고      MySQL 실습 제공 사이트        MySQL공식문서: Functions and Operators  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-07T21:01:35+09:00'>07 Mar 2021</time><a class='article__image' href='/mysql-series1'> <img src='/images/mysql_1.png' alt='MySQL Series [Part1] 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series1'>MySQL Series [Part1] 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리</a> </h2><p class='article__excerpt'>데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS라고 한다</p></div></div></div>"
    } 
  
]


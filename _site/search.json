[
  
    {
      "title"    : "Apache Spark Series [Part1]: Apache Spark Introduction",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series1",
      "date"     : "Jan 15, 2022",
      "content"  : "Table of Contents  Spark Introduction  RDD          파티션(Partition)      불변성(Immutable)      게으른 연산(Lazy Operation)        Cluster Mode          드라이버 프로그램      클러스터 매니저      워커 노드        알아두면 좋은 것들          Shuffling      Passing Functions to Spark        질문Spark Introduction스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다. 쉽게 말해 대용량 데이터를 여러 컴퓨터에 나누어서 동시에 처리한다고 할 수 있습니다. 이런 방법이 스파크 이전에 없었던 것은 아닙니다. 스파크 이전에 하둡(Hadoop)이 이와 유사한 기능을 제공했었습니다. 참고로 하둡은 더그 커팅(Doug Cutting)이라는 사람이 구글이 발표했던 두 개의 논문(The Google File System_2003, MapReduce: simplified data processing on large clusters_2008)을 직접 구현해 만든 프레임워크입니다. 이처럼 구글에서는 예전부터 대용량의 데이터를 고속 분산처리 하기 위해 노력했었고, 현재 스파크는 대부분의 기업들이 사용하고 있는 소프트웨어입니다.지금부터는 스파크와 하둡을 비교하며 스파크의 특징에 어떤 것이 있는지 알아보겠습니다.            차이점      하둡      스파크              기반      디스크 기반      메모리 기반              처리방식      Map-Reduce      RDD              프로그래밍언어      자바      스칼라, 자바, 파이썬, R              라이브러리      -      다양한 라이브러리(Spark streaming, MLlib, GraphX 등) 제공      처리방식에 대해 조금 더 이야기 해보겠습니다. 맵리듀스(MapReduce)는 2004년 구글에서 대용량 데이터 처리를 분산환경에서 처리하기 위한 목적의 소프트웨어 프레임워크입니다. 맵리듀스는 함수형 프로그래밍에서 일반적으로 사용되는 Map과 Reduce라는 함수를 기반으로 만들어졌습니다. Map은 각각의 분산된 환경에서 독립적으로 실행되는 함수의 일종, Reduce는 분산 환경에서의 데이터를 하나로 모으는 함수라고 생각할 수 있습니다.맵리듀스는 분산된 환경에서 데이터가 처리되는데 필요한 많은 함수들을 제공해주지만, 현업에서 필요한 기능들을 모두 커버하기에는 무리가 있었습니다. 그래서 이러한 단점을 보완하기 위해 2009년 UC Berkeley 대학에서 연구를 시작해 2012년 미국 NSDI 학회에서 스파크의 핵심 개념인 RDD(Resilient Distributed Dataset) 에 대한 논문을 발표하였습니다.RDD  RDD is a fault-tolerant collection of elements that can be operated on in parallel.(아파치 스파크 공식문서 참고)다시 말하면 RDD란 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소 로 구성되며 데이터를 처리하는 과정에서 장애가 발생하더라도 스스로 복구할 수 있는 능력 을 가진 데이터 모델 이라는 뜻입니다. RDD는 분산 데이터에 대한 모델로서 단순히 값으로 표현되는 데이터만 가리키는 것이 아니고, 분산된 데이터를 다루는 방법까지 포함 하는 일종의 클래스와 같은 개념입니다.RDD에서 중요한 특징은 다음과 같습니다.  파티션(Partition)  불변성  게으른 연산(Lazy operation)파티션(Partition)RDD는 분산 데이터 요소로 구성된 데이터 집합입니다. 여기서 분산 데이터 요소를 파티션이라고 합니다. 스파크는 작업을 수행할 때 바로 이 파티션 단위로 나눠서 병렬로 처리합니다. 여기서 제가 헷갈렸던 것은 파티션이 분산처리와 병렬처리 중 어떤 것을 기준으로 나뉘어진 단위인가 라는 것 이었습니다. 공식문서(아파치 스파크 공식문서 참고)를 살펴본 결과 파티션은 병렬 처리가 되는 기준이었습니다. 여러 서버에 분산할 때 보통 하나의 서버 당 2~4개 정도의 파티션을 설정합니다. 이 기준은 개인의 클러스터 환경에 따라 기본 설정 값이 다르며 이 값은 원하는 값으로 바꿀 수 있습니다. 구글에서 이미지를 살펴보았을 때는 다들 task당 한개의 파티션이라고 합니다.불변성(Immutable)한 개의 RDD가 여러 개의 파티션으로 나뉘고 다수의 서버에서 처리되다 보니 작업 도중 일부 파티션 처리에 장애가 발생해 파티션 처리 결과가 유실될 수 있습니다. 하지만 스파크에서 RDD는 불변성이기 때문에 생성 과정에 사용되었던 연산들을 다시 실행하여 장애를 해결할 수 있습니다. 여기서 불변성이라는 말은 RDD에서 어떤 연산을 적용해 다른 RDD가 될 때 무조건 새로 RDD를 생성합니다(RDD는 불변이다). 이러한 방식 덕분에 장애가 발생해도 기존의 RDD 데이터에 다시 연산을 적용해 장애를 해결할 수 있는 것입니다(RDD는 회복 탄력성이 좋다(resilient)).게으른 연산(Lazy Operation)RDD의 연산은 크게 트랜스포메이션 과 액션 이라는 두 종류로 나눌 수 있습니다.  트랜스포메이션: RDD1 -&amp;gt; RDD2 이런식으로 새로운 RDD를 만들어내는 연산, 대표적으로 map 함수  액션: RDD -&amp;gt; 다른 형태의 데이터를 만들어내는 연산, 대표적으로 reduce 함수(아파치 공식문서 참고)트랜스포메이션 연산은 보통 분산된 서버 각각에서 독립적으로 수행할 수 있는 연산입니다. 그리고 액션은 분산된 서버에 있는 데이터가 서로를 참조해야 하는 연산입니다. 그래서 액션은 서버 네트워크간의 이동이 발생하게 됩니다. 이런 현상을 셔플링(Shuffling)이라고 하고, 보통 네트워크에서 읽어오는 연산은 메모리에 비해 100만배 정도 느립니다.그렇기 때문에 셔플링이 발생하는 연산을 할 때에는 그 전에 최대한 데이터를 간추리는 것이 중요한데 스파크의 중요한 특징 중 하나가 바로 게으른 연산을 한다는 것입니다. 게으른 연산이라는 말은 RDD가 액션연산을 수행할 때에 비로소 모든 연산이 한꺼번에 실행된다는 것입니다. 이러한 방식의 장점은 데이터를 본격적으로 처리하기 전에 어떤 연산들이 사용되었는지 알 수 있고, 이를 통해 최종적으로 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그중에서 가장 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다. 이렇게 되면 셔플링이 최대한 작은 사이즈로 발생할 수 있도록 합니다.스파크는 RDD를 사용함으로써 처리 속도도 높이고, 장애 복구도 가능해졌다Cluster Mode이번에는 스파크를 구동시키는 환경에 대해서 알아보겠습니다. 스파크는 단일 서버로 동작시키는 로컬 모드와, 클러스터 환경에서 동작시키는 클러스터 모드가 있습니다.로컬 모드는 위의 클러스터 환경에 있는 구성 요소들을 모두 하나의 서버에 놓는 것(Executor는 1개)과 같기 때문에 여기서는 분산 처리를 가능하게 해주는 클러스터 모드에 대해서 조금 더 자세히 알아보겠습니다. 구성 요소는 크게 다음과 같습니다.  드라이버 프로그램  클러스터 매니저  워커 노드여기서 드라이버 프로그램과 워커 노드를 보통 애플리케이션이라고 하고, 클러스터 매니저는 외부 서비스로 애플리케이션과 연동합니다.드라이버 프로그램드라이버 프로그램의 역할은 다음과 같습니다.  클러스터 매니저와의 connection을 위한 스파크 컨텍스트 객체를 생성  스파크 컨텍스트를 이용해 RDD 생성  스파크 컨텍스트를 이용해 연산 정의  정의된 연산은 DAG 스케줄러에게 전달되고 스케줄러는 연산 실행 계획 수립 후 클러스터 매니저에 전달클러스터 매니저클러스터 매니저에는 다음과 같은 것들이 있습니다.  Standalone: a simple cluster manager included with Spark that makes it easy to set up a cluster  YARN: the resource manager in Hadoop 2  Kubernetes: an open-source system for automating deployment, scaling, and management of containerized applications클러스터 매니저의 종류마다 지원하는 범위가 세부적으로 다르지만 대략적인 역할은 다음과 같습니다.  스파크 컨텍스트 생성시 설정된 Executer의 개수, Executer의 메모리를 바탕으로 자원을 할당  이용 가능한 워커에 태스크를 할당하기 위해 노드를 모니터링워커 노드스파크 컨텍스트는 워커 노드에 Executer를 생성하도록 클러스터 매니저에 요청을 하고 클러스터는 그에 맞춰 Executer를 생성합니다. Executer가 생성되면 드라이버 프로그램은 정의된 연산을 수행합니다. 이 때 작업을 실제로 수행하는 것은 아니고 액션 연산의 수만큼 잡(Job)을 생성하고 잡은 셔플링이 최대한 적게 일어나는 방향으로 스테이지(Stage)를 나눕니다. 나누어진 스테이지는 다시 여러 개의 태스크(Task)로 나누어진 후 워커 노드에 생성된 Executer에 할당됩니다.워커 노드는 Executer를 이용해 태스크를 처리하고, 데이터를 나중에 재사용 할 수 있도록 메모리에 저장도 합니다.알아두면 좋은 것들ShufflingPassing Functions to Spark질문  RDD가 파티션으로 나뉘어지는 시점은 RDD가 생성되는 순간일까 아니면 연산이 실행되는 순간일까?  어떤 기준으로 RDD를 파티셔닝할까?  셔플링은 액션 연산에서만 발생할까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-15T21:01:35+09:00'>15 Jan 2022</time><a class='article__image' href='/spark-series1'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part1]: Apache Spark Introduction'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series1'>Apache Spark Series [Part1]: Apache Spark Introduction</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Github Actions Series [Part1]: Understanding GitHub Actions",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/github_action_series1",
      "date"     : "Jan 13, 2022",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-13T21:01:35+09:00'>13 Jan 2022</time><a class='article__image' href='/github_action_series1'> <img src='/images/github-actions_logo.png' alt='Github Actions Series [Part1]: Understanding GitHub Actions'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action_series1'>Github Actions Series [Part1]: Understanding GitHub Actions</a> </h2><p class='article__excerpt'>GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part2]: Kubernetes Object",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series2",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  쿠버네티스의 오브젝트          Workloads관련 오브젝트                  Pod          ReplicaSet          Deployment                    Service관련 오브젝트                  Service          Ingress                    Config and Storage관련 오브젝트                  ConfigMap          Volume                      참고자료쿠버네티스의 오브젝트Workloads관련 오브젝트  Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.(Rancher문서 참고)PodPod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다. Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다. 컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.Pod가 생성되는 과정은 다음과 같습니다.Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 할당합니다. 그러면 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.🐨 오브젝트 생성을 위한 YAML파일Pod를 포함해 쿠버네티스의 오브젝트를 만들기 위해서는 YAML파일이 필요합니다. YAML파일에 오브젝트를 위한 설정들을 작성할 수 있는데, 이 때 필수적으로 사용되는 key값들이 있습니다.            Key      설명      예              apiVersion      오브젝트 버전      v1, app/v1, ..              kind      오브젝트 종류      Pod, ReplicaSet, Deployment, ..              metadata      메타데이터      name, label, ..              spec      오브젝트 별 상세 설정      오브젝트마다 다름      apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1Pod의 spec에는 containers, volumes, restartPolicy, hostname, hostNetwork 등이 있습니다.(Pod공식문서 참고)ReplicaSetReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트입니다. Pod을 생성하고 개수를 유지하려면 반드시 ReplicaSet을 사용해야 합니다. 보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많습니다.ReplicaSet은 다음과 같이 동작합니다.ReplicaSet controller가 desired state에 맞춰 Pod를 생성합니다. 그러면 Scheduler는 생성된 Pod를 노드에 할당해줍니다.apiVersion: apps/v1kind: ReplicaSetmetadata:  name: echo-rsspec:  replicas: 3  selector:    matchLabels: # app: echo이고 tier: app인 label을 가지는 파드를 관리      app: echo      tier: app  template: # replicaset이 만드는 pod의 템플릿    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v1ReplicaSet의 spec에는 replicas, selector, template, minReadySeconds가 있습니다.(ReplicaSet 공식문서 참고)DeploymentDeployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트입니다. ReplicaSet을 이용하여 Pod을 업데이트하고 이력을 관리하여 롤백Rollback하거나 특정 버전revision으로 돌아갈 수 있습니다.Deployment 오브젝트가 Pod의 버전을 관리하는 과정은 다음과 같습니다.Deployment Controller가 Deploy 조건을 체크하면서 원하는 버전에 맞게 Pod의 버전을 맞춥니다. 이 때 ReplicaSet에 있는 Pod들을 보통 한 번에 바꾸지 않고 조건에 맞게(예를 들어, 25%씩) 바꿔나감으로써 버전을 바꾸더라도 중간에 서비스가 중단되지 않도록 합니다. (무중단배포)apiVersion: apps/v1kind: Deploymentmetadata:  name: echo-deployspec:  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 0  replicas: 4  selector:    matchLabels:      app: echo      tier: app  template:    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v2spec에는 replicas, selector, template, strategy  등이 있습니다.(Deployment 공식문서 참고)Service관련 오브젝트  In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.ServiceService는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod과 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod과 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.  ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할  NodePort: 외부에서 접근가능하도록 하는 포트 역할  LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.# ClusterIP# redis라는 Deployment 오브젝트에 IP할당apiVersion: v1kind: Servicemetadata:  name: redisspec:  ports:    - port: 6379 # clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)      protocol: TCP  selector: # 어떤pod로 트래픽을 전달할지 결정    app: counter    tier: db# NodePortapiVersion: v1kind: Servicemetadata:  name: counter-npspec:  type: NodePort  ports:    - port: 3000 # ClusterIP, Pod IP의 포트      protocol: TCP      nodePort: 31000 # Node IP의 포트  selector:    app: counter    tier: app# LoadBalancerapiVersion: v1kind: Servicemetadata:  name: counter-lbspec:  type: LoadBalancer  ports:    - port: 30000      targetPort: 3000      protocol: TCP  selector:    app: counter    tier: appIngressIngress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 nginx, haproxy, traefik, alb등이 있습니다.minikube를 이용할 경우 다음 명령어로 설치할 수 있습니다.# nginx ingress controllerminikube addons enable ingressapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: echo-v1spec:  rules:    - host: v1.echo.192.168.64.5.sslip.io      http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: echo-v1                port:                  number: 3000# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라spec에는 rules, defaultBackend(어느 rule에도 속하지 않을 경우) 등이 있습니다.(Ingress 공식문서 참고)Config and Storage관련 오브젝트ConfigMapConfigMap은 설정, 환경 변수들을 담는 오브젝트입니다. 예를 들어 개발/운영에 따라 환경 변수값이 다른 경우, ConfigMap 을 활용해 Pod 생성시 넣어줄 수 있습니다.ConfigMap을 다양한 방법으로 만들 수 있습니다.  ConfigMap yaml 파일로 오브젝트 생성  환경 변수 설정을 담고 있는 yaml파일을 ConfigMap 오브젝트로 생성  그냥 환경 변수를 담고 있는 임의의 파일을 ConfigMap 오브젝트로 생성# ConfigMap yaml파일apiVersion: v1 # 참고로 v1이면 core API groupkind: ConfigMapmetadata:  name: my-configdata:  hello: world  kuber: neteskubectl apply -f config-map.yml# 환경 변수 설정을 담고 있는 yaml파일global:  scrape_interval: 15sscrape_configs:  - job_name: prometheus    metrics_path: /prometheus/metrics    static_configs:      - targets:          - localhost:9090# yaml 파일로 ConfigMap 파일 생성kubectl create cm my-config --from-file=config-file.yml# ConfigMap 적용kubectl apply -f my-config.yml# config-env.yml파일 (yml파일 아니지만 그냥 확장자 yml로 해놓아도됨)hello=worldhaha=hoho# 임의의 파일로 ConfigMap 파일 생성kubectl create cm env-config --from-env-file=config-env.yml# ConfigMap 적용kubectl apply -f env-config.yml여러 가지 방법으로 ConfigMap을 Pod에 적용할 수 있습니다.  디스크 볼륨 마운트  환경변수로 사용# ConfigMap yaml파일이 있는 볼륨 마운트apiVersion: v1kind: Podmetadata:  name: alpinespec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      volumeMounts:        - name: config-vol          mountPath: /etc/config  volumes:    - name: config-vol      configMap:        name: my-config# ConfigMap yaml파일 직접 환경변수로 설정apiVersion: v1kind: Podmetadata:  name: alpine-envspec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      env:        - name: hello          valueFrom:            configMapKeyRef:              name: my-config              key: helloVolumeVolume은 저장소와 관련된 오브젝트입니다. 지금까지 만들었던 컨테이너는 Pod을 제거하면 컨테이너 내부에 저장했던 데이터도 모두 사라집니다. MySQL과 같은 데이터베이스는 데이터가 유실되지 않도록 반드시 별도의 저장소에 데이터를 저장하고 컨테이너를 새로 만들 때 이전 데이터를 가져와야 합니다.저장소를 호스트 디렉토리를 사용할 수도 있고 EBS 같은 스토리지를 동적으로 생성하여 사용할 수도 있습니다. 사실상 인기 있는 대부분의 저장 방식을 지원합니다.저장소의 종류에는 다음과 같은 것들이 있습니다.  임시 디스크          emptyDir                  Pod 이 생성되고 삭제될 때, 같이 생성되고 삭제되는 임시 디스크          생성 당시에는 아무 것도 없는 빈 상태          물리 디스크(노드), 메모리에 저장                      로컬 디스크          hostpath                  노드가 생성될 때 이미 존재하고 있는 디렉토리                      네트워크 디스크          awsElasticBlockStore, azureDisk 등      # emptydirapiVersion: v1kind: Podmetadata:  name: shared-volumes spec:  containers:  - name: redis    image: redis    volumeMounts:    - name: shared-storage      mountPath: /data/shared  - name: nginx    image: nginx    volumeMounts:    - name: shared-storage      mountPath: /data/shared  volumes:  - name : shared-storage    emptyDir: {}# hostpathapiVersion: v1kind: Podmetadata:  name: host-logspec:  containers:    - name: log      image: busybox      args: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]      volumeMounts:        - name: varlog          mountPath: /host/var/log  volumes:    - name: varlog      hostPath:        path: /var/log참고자료  subicura님의 kubenetes안내서  하나씩 점을 찍어나가며 블로그  Kubernetes 공식문서  Rancher 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/kubernetes-series2'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part2]: Kubernetes Object'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series2'>Kubernetes Series [Part2]: Kubernetes Object</a> </h2><p class='article__excerpt'>Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part1]: Kubernetes Intro",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-intro",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  들어가기 전에          도커의 등장        Kubernetes          쿠버네티스 소개      쿠버네티스 아키텍쳐      Desired State        마치며  참고자료들어가기 전에도커의 등장2013년 도커가 등장하기 전까지 서버 관리는 굉장히 어렵고 컨트롤하기 어려운 것으로 여겨졌습니다. 하나의 서비스를 제공하기 위해서는 보통 수십에서 수백개의 애플리케이션이 서로 연결되어 동작하는데, 이 때 오류가 나게 되면 어디서 문제가 생긴건지 파악하기가 쉽지 않았습니다.이러한 문제를 해결하기 위해 사람들은 가상화 기술 을 이용해 서버를 애플리케이션별로 격리시키고자 하였습니다. 이 때 크게 두가지 방법으로 접근할 수 있는데, 하나는 가상머신 을 이용해 컴퓨팅 리소스를 따로 분리하여 사용하도록 하는 것이었습니다. 하지만 이 방법은 컴퓨팅 성능을 떨어트립니다.두 번째 방법은 LXC(LinuX Containers)라는 리눅스 커널 기술 로 기존의 하드웨어 레벨에서 하던 방식을 운영체제 레벨에서 해결하도록 했습니다. 이렇게 하면 컴퓨팅 성능도 떨어트리지 않으면서, 파일시스템, 리소스(CPU, 메모리, 네트워크)를 분리할 수 있습니다. 하지만 이 방법은 사용하기에는 운영체제에 대한 깊은 이해를 필요로 해서 많은 개발자들이 쉽게 쓰기는 힘들었습니다.이 때 등장한 것이 바로 도커입니다. 도커가 등장하게 되면서 컨테이너 기술 에 대한 접근성이 훨씬 좋아지게 되자, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 수많은 애플리케이션이 컨테이너로 배포되고 도커파일을 만들어 이미지를 빌드하고 컨테이너를 배포하는 게 흔한 개발 프로세스가 되었습니다.(도커에 관한 더 자세한 내용은 여기를 참고하시기 바랍니다)이제 모든 것들을 컨테이너화하기 시작하면서 우리의 서비스는 다음과 같은 모습을 가지게 되었습니다.이렇게 서비스 하나를 배포하기 위해 수많은 컨테이너를 띄우고, 연결하고, 버전업을 해야하는 상황이 생긴겁니다. 그래서 개발자들은 이제 컨테이너들을 동시에 띄우고 관리까지 해주는 컨테이너 오케스트레이션 기술이 필요해지게 되었습니다.Kubernetes쿠버네티스 소개쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 합니다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작합니다.쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화합니다.쿠버네티스 아키텍쳐  마스터: 전체 클러스터를 관리하는 서버  노드: 컨테이너가 배포되는 서버쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.  컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 상태를 유지, API서버 요청 처리  스케줄러: 새로 생성되는 Pod(컨테이너와 비슷)가 있는지 계속 체크, 생성되면 가장 절절한 노드 선택컨트롤러는 자신이 맡고 있는 오브젝트의 상태를 계속 체크하고 상태를 유지합니다. 또한 API 서버에서 어떤 새로운 상태를 요구할 경우, 맞춰서 또 상태를 바꿔서 유지하고 이 때 새롭게 Pod가 생성되거나 삭제되면 스케줄러가 그에 맞춰서 노드에서 삭제, 할당합니다.Desired State쿠버네티스에서 가장 중요한 것은 desired state(원하는 상태) 라는 개념입니다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말합니다.쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있습니다.이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청합니다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 됩니다.마치며쿠버네티스는 여러 컨테이너를 자동으로 배포해주고 관리해준다는 점에서 정말 좋은 기술입니다. 그리고 마이크로서비스, 클라우드 환경과도 정말 잘 어울리기 때문에 배워두면 정말 쓸모가 많을 것 같습니다. 하지만 쿠버네티스는 많은 영역을 커버하다보니 배워야할 것들이 굉장히 많습니다. 그리고 컨테이너들을 띄우는 서버를 관리하기 위한 서버를 더 사용하게 되는 것이기 때문에, 컴퓨팅 자원이 충분하지 않다면 사용하는 것이 적절하지 않을 수도 있습니다. (쿠버네티스를 운영환경에 설치하기 위해선 최소 3대의 마스터 서버와 컨테이너 배포를 위한 n개의 노드 서버가 필요)다음 포스트에서는 서버가 넉넉하지 않은 상황에서 사용할 수 있는 minikube를 설치, 그리고 쿠버네티스에 명령어를 전달할 때 사용하는 kubectl 설치해보겠습니다.그리고 도커에서는 컨테이너를 띄우지만 쿠버네티스에서는 컨테이너를 관리할 수 있도록 조금 더 패키징한 다양한 오브젝트를 띄우게 되는데 이 때 어떠한 오브젝트들이 있는지도 배워보도록 하겠습니다.참고자료  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/kubernetes-intro'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part1]: Kubernetes Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-intro'>Kubernetes Series [Part1]: Kubernetes Intro</a> </h2><p class='article__excerpt'>쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Series [Part1]: Docker Intro",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-intro",
      "date"     : "Jan 8, 2022",
      "content"  : "파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.타입            이름      타입      가변                  불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)참조변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사변수의 대입을 통한 복사immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.얕은 복사또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.깊은 복사따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/docker-intro'> <img src='/images/docker_logo.png' alt='Docker Series [Part1]: Docker Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-intro'>Docker Series [Part1]: Docker Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part4]: ElasticSearch 검색",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-search",
      "date"     : "Jan 7, 2022",
      "content"  : "검색 APIQuery DSL",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/elasticsearch-search'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part4]: ElasticSearch 검색'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-search'>ElasticSearch Series [Part4]: ElasticSearch 검색</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part1]: AWS Intro",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-intro",
      "date"     : "Jan 7, 2022",
      "content"  : "Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64/v8 OS architecture만 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/aws-intro'> <img src='/images/aws_logo.png' alt='AWS Series [Part1]: AWS Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-intro'>AWS Series [Part1]: AWS Intro</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part3]: ElasticSearch Modeling",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-modeling",
      "date"     : "Jan 6, 2022",
      "content"  : "Table of Contents  Elasticsearch 모델링          데이터 타입                  Keyword 데이터 타입          Text 데이터 타입                    매핑 파라미터      Elasticsearch 모델링엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.  필드에 지정할 데이터 타입  필드별 매핑 파라미터예를 들어, 영화 정보를 엘라스틱서치를 이용해 저장하고 싶다면,            필드명      필드 타입              movieTitle      text              jenre      keyword              nation      keyword              produceYear      integer              releaseDate      date              actor      keyword      이렇게 필드별로 text, keyword, integer, date 등의 데이터 타입을 설정할 수 있습니다.데이터 타입# 대표적인 데이터 타입- 문자열 관련한 데이터 타입: keyword, text- 일반적인 데이터 타입: integer, long, double, boolean- 특수한 데이터 타입: date, ip, geo_point, geo_shapeKeyword 데이터 타입Keyword 데이터 타입은 문자열 데이터를 색인할 때 자주 사용하는 타입 중 하나로, 별도의 분석기 없이 원문 그대로가 저장된다는 것이 특징입니다. 예를 들어 ‘elastic search’라는 문자열을 keyword 타입으로 저장한다면, ‘elastic’이나 ‘search’로는 검색이 되지 않고 정확히 ‘elastic search’라고해야만 검색됩니다. 이러한 데이터 타입은 주로 카테고리형 데이터의 필드 타입으로 적절하며, 문자열을 필터링, 정렬, 집계할 때는 keyword타입을 이용해야 합니다.Text 데이터 타입반지의 제왕 영화 시리즈에는 ‘반지의 제왕: 반지 원정대’, ‘반지의 제왕: 두 개의 탑’, ‘반지의 제왕: 왕의 귀환’이 있습니다. 근데 저는 부제목까지는 기억이 안나고 ‘반지의 제왕’만 기억이 납니다. 그래서 저는 ‘반지의 제왕’이라고만 검색해도 위의 영화들이 나왔으면 좋겠습니다. 이럴 때는 text데이터 타입을 이용합니다. Text타입은 전문 검색이 가a능하다는 점이 가장 큰 특징입니다. Text타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 역색인(inverted index)됩니다.더 자세한 내용은 공식문서를 참고해주시면 좋을 것 같습니다. (엘라스틱서치 공식문서 참고)매핑 파라미터문서를 색인하는 과정은 당연 엘라스틱서치에서 가장 중요한 부분입니다. 그렇기 때문에 엘라스틱서치에서는 매핑 파라미터를 통해 색인 과정을 커스텀하도록 도와줍니다. 예를 들어 어떤 영화 데이터는 장르가 없다고 하면 색인할 때 필드를 생성하지 않습니다. 이럴 때 null_value를 ‘데이터 없음’이라고 설정했다면, 필드가 생성 되고 값에 ‘데이터 없음’이라는 값이 들어갑니다. 또 다른 예시는 특정 필드를 색인에 포함할지 말지를 결정하는 enabled, index 파라미터도 있습니다. (둘의 차이는 stack overflow 참고)# 매핑 파라미터- 문자열에 자주 사용: analyzer, search_analyzer, similarity, term_vector, normalizer- 저장 관련: enabled, index, store, copy_to, doc_values- 색인 방식과 관련: ignore_above, ignore_malformed, coerce, dynamic, null_value- 필드 안의 필드를 정의할 때 사용: properties, fields- 그 밖: position_increment_gap, format(엘라스틱서치 공식문서 참고)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-06T21:01:35+09:00'>06 Jan 2022</time><a class='article__image' href='/elasticsearch-modeling'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part3]: ElasticSearch Modeling'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-modeling'>ElasticSearch Series [Part3]: ElasticSearch Modeling</a> </h2><p class='article__excerpt'>엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-element",
      "date"     : "Jan 5, 2022",
      "content"  : "Elasticsearch를 구성하는 개념“Elasticsearch에서 데이터는 인덱스 안의 특정 타입 에 문서 로 저장되는데 이 때 문서는 필드 를 가지고 있으며, 이러한 필드는 매핑 프로세스로 정의된 이름과 속성을 보통 따른다. 그리고 이 때 모든 문서들은 정의된 샤드 의 개수에 각각 골고루 분배되어 분산처리된다. 또한 장애를 대비해 레플리카 의 개수만큼 복제해 놓기도 한다.”위의 내용은 Elasticsearch의 데이터 저장방식에 관한 글입니다. 하지만 Elasticsearch가 익숙하지 않은 사람에게는 낯선 용어들도 있고 익숙하지만 이해하기 힘든 용어들도 있습니다. 그래도 Elasticsearch에서 사용하는 용어와 RDBMS에서 사용하는 용어를 비교하며 살펴보고 다시 한번 읽어보면 조금 더 이해가 갈 것입니다.            Elasticsearch      RDBMS              인덱스      데이터베이스              타입      테이블              문서      행              필드      열              매핑      스키마              샤드      파티션      인덱스인덱스는 논리적 데이터 저장 공간을 뜻하며, 하나의 물리적인 노드에 여러 개의 인덱스를 생성할 수도 있습니다. 이는 곧 멀티테넌시를 지원한다는 뜻이기도 합니다. 만약 Elasticsearch를 분산 환경으로 구성했다면 하나의 인덱스는 여러 노드에 분산 저장되며 검색 시 더 빠른 속도를 제공합니다.샤드분산 환경으로 저장되면 인덱스가 여러 노드에 분산 저장된다고 했는데, 이렇게 물리적으로 여러 공간에 나뉠 때의 단위를 샤드라고 합니다. 이 때 샤드는 레플리카의 단위가 되기도 합니다.타입타입은 보통 카테고리와 비슷한 의미로 노래를 K-pop, Classic, Rock처럼 장르별로 나누는 것과 같습니다. 하지만 6.1 버전 이후 인덱스 당 한 개의 타입만 지원하고 있습니다.문서한 개의 데이터를 뜻하며, 기본적으로 JSON 형태로 저장됩니다.필드필드는 문서의 속성을 나타내며 데이터베이스의 컬럼과 비슷한 의미입니다. 다만 컬럼의 데이터 타입은 정적이고, 필드의 데이터 타입은 좀 더 동적이라고 할 수 있습니다.매핑매핑은 필드와, 필드의 타입을 정의하고 그에 따른 색인 방법을 정의하는 프로세스입니다.Elasticsearch에서 제공하는 주요 APIfrom elasticsearch import ElasticsearchES_URL = &#39;localhost:9200&#39;ES_INDEX = &#39;first_index&#39;DOC_TYPE = &#39;_doc&#39;es = Elasticsearch(ES_URL)인덱스 관련 API# 인덱스 메타데이터, 매핑 정의index_settings = {    &#39;settings&#39;: {        &#39;number_of_shards&#39;: 2,        &#39;number_of_replicas&#39;: 1    },    &#39;mappings&#39;: {         &#39;properties&#39;: {            &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},            &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}        }            }}# 인덱스 생성es.indices.create(index=ES_INDEX, **index_settings)--------------------------------------------------------{&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;first_index&#39;}# 인덱스 메타 데이터 확인es.indices.get_settings()--------------------------------------------------------{&#39;first_index&#39;: {&#39;settings&#39;: {&#39;index&#39;: {&#39;routing&#39;: {&#39;allocation&#39;: {&#39;include&#39;: {&#39;_tier_preference&#39;: &#39;data_content&#39;}}},    &#39;number_of_shards&#39;: &#39;2&#39;,    &#39;provided_name&#39;: &#39;first_index&#39;,    &#39;creation_date&#39;: &#39;1641728644368&#39;,    &#39;number_of_replicas&#39;: &#39;1&#39;,    &#39;uuid&#39;: &#39;3QtIZXthRcGtCdV40WmUCg&#39;,    &#39;version&#39;: {&#39;created&#39;: &#39;7160299&#39;}}}}}# 인덱스 매핑 확인es.indices.get_mapping(index=ES_INDEX)--------------------------------------------------------{&#39;first_index&#39;: {&#39;mappings&#39;: {&#39;properties&#39;: {&#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},    &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}}}}}# 인덱스 삭제es.indices.delete(ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}# 인덱스 존재 유무es.indices.exists(ES_INDEX)--------------------------------------------------------True# 매핑 업데이트new_field = {  &quot;properties&quot;: {    &quot;school&quot; : {      &quot;type&quot;: &quot;text&quot;    }  }}es.indices.put_mapping(new_field, index=ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}문서 관련 API# 문서 삽입unit_document = {    &#39;name&#39;: &#39;Jay Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,    &#39;phone&#39;: &#39;010-1234-5678&#39;}es.index(index=ES_INDEX, doc_type=DOC_TYPE, id=1, document=unit_document)--------------------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 4, &#39;_primary_term&#39;: 1}# 문서 정보 확인es.get(index=ES_INDEX, doc_type=DOC_TYPE, id=1)-----------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;_seq_no&#39;: 0, &#39;_primary_term&#39;: 1, &#39;found&#39;: True, &#39;_source&#39;: {&#39;name&#39;: &#39;Jay Kim&#39;,  &#39;age&#39;: 28,  &#39;gender&#39;: &#39;male&#39;,  &#39;email&#39;: &#39;abc@gmail.com&#39;,  &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,  &#39;phone&#39;: &#39;010-1234-5678&#39;}}# 문서를 가지는 인덱스 생성 (이미 있으면 삽입)unit_document = {    &#39;name&#39;: &#39;Jae yeong Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;경북 구미 형곡동&#39;,    &#39;phone&#39;: &#39;010-3321-5668&#39;}es.create(index=ES_INDEX, id=2, body=unit_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 1, &#39;_primary_term&#39;: 1}# 문서 삭제es.delete(index=ES_INDEX, id=2)---------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;deleted&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 2, &#39;_primary_term&#39;: 1}# query로 삭제body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.delete_by_query(index=ES_INDEX, body=body)----------------------------------------------{&#39;took&#39;: 23, &#39;timed_out&#39;: False, &#39;total&#39;: 1, &#39;deleted&#39;: 1, &#39;batches&#39;: 1, &#39;version_conflicts&#39;: 0, &#39;noops&#39;: 0, &#39;retries&#39;: {&#39;bulk&#39;: 0, &#39;search&#39;: 0}, &#39;throttled_millis&#39;: 0, &#39;requests_per_second&#39;: -1.0, &#39;throttled_until_millis&#39;: 0, &#39;failures&#39;: []}# 문서 수정# &quot;&quot;doc&quot;&quot; is essentially Elasticsearch&#39;s &quot;&quot;_source&quot;&quot; fieldupdate_document = {&#39;doc&#39;: {         &#39;address&#39;: &#39;경북 구미 송정동&#39;,         &#39;age&#39;: 28,         &#39;email&#39;: &#39;ziont0510@gmail.com&#39;,    }}es.update(index=ES_INDEX, id=1, body=update_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;updated&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 5, &#39;_primary_term&#39;: 1}검색 API# 매칭되는 문서 개수body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.count(body=body, index=ES_INDEX)---------------------------------------------------------------{&#39;count&#39;: 2, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}}# 문서 검색body = {    &#39;size&#39;:10,    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}    es.search(body=body, index=ES_INDEX)-------------------------------------------------------{&#39;took&#39;: 6, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 0, &#39;relation&#39;: &#39;eq&#39;},  &#39;max_score&#39;: None,  &#39;hits&#39;: []}}집계 API",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-05T21:01:35+09:00'>05 Jan 2022</time><a class='article__image' href='/elasticsearch-element'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-element'>ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드</a> </h2><p class='article__excerpt'>Elasticsearch에서 데이터는 인덱스 안의 특정 타입에 문서로 저장되는데..</p></div></div></div>"
    } ,
  
    {
      "title"    : "MongoDB Series [Part1]: MongoDB Intro",
      "category" : "",
      "tags"     : "MongoDB",
      "url"      : "/mongodb-intro",
      "date"     : "Jan 4, 2022",
      "content"  : "파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.타입            이름      타입      가변                  불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)참조변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사변수의 대입을 통한 복사immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.얕은 복사또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.깊은 복사따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-04T21:01:35+09:00'>04 Jan 2022</time><a class='article__image' href='/mongodb-intro'> <img src='/images/mongodb_logo.png' alt='MongoDB Series [Part1]: MongoDB Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-intro'>MongoDB Series [Part1]: MongoDB Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part1]: Network Intro",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-intro",
      "date"     : "Jan 3, 2022",
      "content"  : "Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64/v8 OS architecture만 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-03T21:01:35+09:00'>03 Jan 2022</time><a class='article__image' href='/network-intro'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part1]: Network Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-intro'>Network Series [Part1]: Network Intro</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part1]: ElasticSearch Installation",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-intro",
      "date"     : "Jan 3, 2022",
      "content"  : "Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64 OS architecture을 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-03T21:01:35+09:00'>03 Jan 2022</time><a class='article__image' href='/elasticsearch-intro'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part1]: ElasticSearch Installation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-intro'>ElasticSearch Series [Part1]: ElasticSearch Installation</a> </h2><p class='article__excerpt'>Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진</p></div></div></div>"
    } ,
  
    {
      "title"    : "Pytorch Series [Part1]: torch",
      "category" : "",
      "tags"     : "Pytorch",
      "url"      : "/pytorch-torch",
      "date"     : "Jan 2, 2022",
      "content"  : "1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-02T21:01:35+09:00'>02 Jan 2022</time><a class='article__image' href='/pytorch-torch'> <img src='/images/pytorch_logo.webp' alt='Pytorch Series [Part1]: torch'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/pytorch-torch'>Pytorch Series [Part1]: torch</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Option of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-option",
      "date"     : "Jan 6, 2021",
      "content"  : "Kafka의 주요 Option모든 기업에게 있어 데이터는 중요합니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 더더욱 중요합니다. 이러한 데이터는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동 그 밖에 모든 것들이 데이터가 될 수 있습니다. Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다. 그럼 지금부터 저희는 Event는 무엇을 말하고, Kafka가 다른 platform(RabbitMQ, Pulsar)에 비해 어떤 점에서 더 뛰어난지 한 번 알아보겠습니다.ProducerBrokerConsumerReplicationRebalance",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-01-06T21:01:35+09:00'>06 Jan 2021</time><a class='article__image' href='/kafka-option'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part3]: Option of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-option'>Kafka Series [Part3]: Option of Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Structure of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-structure",
      "date"     : "Jan 4, 2021",
      "content"  : "Kafka의 구조모든 기업에게 있어 데이터는 중요합니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 더더욱 중요합니다. 이러한 데이터는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동 그 밖에 모든 것들이 데이터가 될 수 있습니다. Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다. 그럼 지금부터 저희는 Event는 무엇을 말하고, Kafka가 다른 platform(RabbitMQ, Pulsar)에 비해 어떤 점에서 더 뛰어난지 한 번 알아보겠습니다.Kafka ClientKafka ClusterZookeeper",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-01-04T21:01:35+09:00'>04 Jan 2021</time><a class='article__image' href='/kafka-structure'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part3]: Structure of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-structure'>Kafka Series [Part3]: Structure of Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part2]: Main elements of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-element",
      "date"     : "Jan 4, 2021",
      "content"  : "Kafka의 주요 구성요소Kafka는 크게 3가지로 이루어 있습니다.  Producer: Kafka로 메시지를 보내는 모든 클라이언트  Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버  Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트(참고: cloudkarafka)Topic, Partition, SegmentKafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.  Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)  Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)  Segment: 메시지가 실제로 저장되는 파일(참고: cloudkarafka)위와 같이 하나의 브로커에는 여러 개의 Topic을 가질 수 있으며, Topic은 여러 개의 Partition으로 나누어 저장되어 있습니다. 이렇게 Partition들에 메시지들이 저장되는 방법은 프로듀서에 있는 Partitioner로 메시지를 미리 나눠서 배치 전송할 수도 있고, Broker에서 메시지를 받을 때마다 메시지에 있는 key값으로 Partition에 분배할 수도 있습니다. 이 때 고려할 수 있는 사항은 다음과 같은 것들이 있습니다.  메시지를 프로듀서에서 Partition으로 분배할 것인가 브로커에서 분배할 것인가  몇 개의 Partition으로 나눌 것인가  Segment파일은 언제마다 새로 파일을 만들 것인가(Roll strategy)Producer프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 그렇다면 프로듀서에서 메시지를 전송할 때 고려해야 할 것에는 어떤 것들이 있을까요?  메시지를 어떤 형태로 전송할 것인가  메시지를 파티션에 골고루 분배  메시지 중복 전송을 허용할 것인가  브로커에 제대로 전달이 되었는가  배치 전송Broker브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.Consumer컨슈머는 브로커에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다.컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.  파티션 할당 전략  프로듀서가 브로커에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가  컨슈머의 개수가 파티션보다 많지는 않은가  컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가마치며Kafka를 구성하는 요소에는 크게 Producer, Broker, Consumer가 있었고, 이 밖에도 Broker에 관한 정보를 저장하고 관리하는 Zookeeper가 있습니다. 각 요소에 대해서는 조금 더 공부를 해서 나중에 더 정리를 하도록 하겠습니다.위의 글을 보면 각 요소별로 고려해야할 것들이 많습니다. 보통 이런 것들은 Kafka를 구성할 때 옵션으로 값을 줘서 설정하게 됩니다. 옵션에 대해서는 나중에 따로 포스트를 정리할 계획입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-01-04T21:01:35+09:00'>04 Jan 2021</time><a class='article__image' href='/kafka-element'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part2]: Main elements of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-element'>Kafka Series [Part2]: Main elements of Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part1]: What is Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-intro",
      "date"     : "Jan 3, 2021",
      "content"  : "Table of Contents  Apache Kafka 소개  Event  기업 사례: 잘란도(Zalando)  Kafka의 핵심 기능          - 순서 보장      - 적어도 한 번 전송 방식      - Pull based approach      - 강력한 Partitioning      - 비동기식 방식        정리Apache Kafka 소개모든 기업에게 있어 데이터는 중요한 자산입니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 그 가치가 더욱 큽니다. 이러한 데이터에는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동(배송, 결제, 송금 등) 그 밖에 모든 것들이 데이터가 될 수 있습니다. Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming 플랫폼입니다. 그럼 지금부터 저희는 Event는 무엇을 말하고, Kafka가 다른 플랫폼(RabbitMQ, Pulsar)에 비해 어떤 점에서 더 뛰어난지 한 번 알아보겠습니다.EventKafka에서는 Event, Data, Record, Message를 모두 혼용해서 쓰고 있습니다. Event는 어떠한 행동이나, 사건도 모두 될 수 있습니다. 다음과 같은 것 들이 있습니다.  웹 사이트에서 무언가를 클릭하는 것  센서의 온도/압력 데이터  청구서  배송 물건의 위치 정보이렇게 세상에 있는 모든 정보를 실시간으로 저장하고, 처리하기 위해서는 높은 throughput, 낮은 latency가 요구됩니다. Kafka는 최대 600MB/s의 throughput과 200MB에 대해 5ms의 낮은 latency를 제공하고 있습니다.Benchmarking Kafka vs. Pulsar vs. RabbitMQ: Which is Fastest?지금까지는 Kafka가 높은 throughput과 낮은 latency로 엄청난 양의 데이터를 실시간으로 처리해주는 플랫폼이라고 배웠습니다. 이제 이러한 개념을 가지고 조금 더 앞으로 나가보겠습니다. 다음은 Kafka를 설명하는 좋은 문장이라고 생각되어 가져와 봤습니다. (Apache Kafka Series [Part 1]: Introduction to Apache Kafka)  Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.  Distributed publish-subscribe messaging platform  Handle real-time streaming data  Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.Kafka를 이용하면 특정 Source에서 특정 Destination으로 데이터를 흘려보내는 것이 아니라, Publisher들이 실시간으로 언제든 데이터를 저장할 수 있으며, Subscriber는 언제든 저장된 데이터를 가지고 올 수 있습니다. 이러한 구조를 Pub/Sub 모델이라고 합니다. Pub/sub은 Messaging platform의 architecture를 훨씬 간단하게 만들고, 확장성을 용이하게 해줍니다.기업 사례: 잘란도(Zalando)Kafka는 현재 Fortune 100대 기업 중 80% 이상이 사용하고 있는 데이터 플랫폼의 핵심 기술입니다. 해외의 링크드인, 트위터, 아마존, 넷플릭스, 우버를 포함해 국내에서는 대표적으로 카카오와 라인 등이 Kafka를 이용하고 있습니다. 제가 여기서 소개드릴 사례는 유럽의 대표 온라인 쇼핑몰 잘란도(Zalando)입니다. (참고: Event First Development - Moving Towards Kafka Pipeline Applications)잘란도는 회사의 규모가 점점 커지고 사업이 다각화되면서 내부적으로 데이터에 대한 문제가 점점 대두되었습니다. 처리해야 할 데이터 양의 증가, 복잡해져가는 데이터 파이프라인(데이터를 Produce하는 곳과 Consume하는 곳의 다양화), 데이터 수집 장애로 인한 신뢰도 하락과 같은 문제로 잘란도에서는 이벤트 드리븐 시스템을 도입하기로 결정하였습니다.  The aim here was the democratization of data for all potential users on the new platform.(참고: https://realtimeapi.io/hub/event-driven-apis/)결과적으로 잘란도는 Kafka를 도입함으로써 내부의 데이터 처리 파이프라인을 간소화하고, 확장을 용이하게 했으며, 스트림 데이터 처리량을 높일 수 있었습니다. 이러한 결과를 얻을 수 있었던 것은 Kafka에서 제공하는 몇 가지 핵심 기능 덕분이었습니다.Kafka의 핵심 기능- 순서 보장이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.- 적어도 한 번 전송 방식분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(itempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다.- Pull based approach카프카에서 데이터를 소비하는 클라이언트는 풀 방식으로 동작합니다. 풀 방식의 장점은 자기 자신의 속도로 데이터를 처리할 수 있다는 점입니다. 푸시 방식은 브로커가 보내주는 속도에 의존해야 한다는 한계가 있습니다.- 강력한 Partitioning파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.- 비동기식 방식데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)정리  Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.  데이터를 분산처리하여 높은 throughput과 낮은 latency를 제공한다.  심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-01-03T21:01:35+09:00'>03 Jan 2021</time><a class='article__image' href='/kafka-intro'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part1]: What is Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-intro'>Kafka Series [Part1]: What is Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python data type dictionary",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-dictionary",
      "date"     : "Apr 23, 2020",
      "content"  : "1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-dictionary'> <img src='/images/python_logo.jpg' alt='Python data type dictionary'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-dictionary'>Python data type dictionary</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python data type list",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-list",
      "date"     : "Apr 23, 2020",
      "content"  : "1. 리스트 자료형의 특징문자열 자료형은 순서가 있는 시퀀스형 자료형입니다. 그렇기 때문에 인덱싱, 슬라이싱을 통해 데이터의 일부분을 추출할 수 있습니다.데이터 관련 분야에서 일을 하다 보면 리스트 자료형 데이터를 목적에 맞게 다듬거나 또는 다른 자료형으로 변환해 사용하는 일이 많기 때문에 자료형 중에서도 비중이 높은 편입니다.2. 리스트 생성리스트는 여러 가지 자료형을 가질 수 있는 시퀀스형 자료형입니다.또한 값을 변경할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = [3.1, 2.5, 7]&amp;gt;&amp;gt;&amp;gt; c = [&quot;Hello&quot;, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; d = [1, 4.5, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; a[0] = 100&amp;gt;&amp;gt;&amp;gt;a[100, 2, 3, 4]🔔 리스트를 곱하거나 더하면 값이 반복되거나 추가됩니다&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a + [5][1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a + 5 -&amp;gt; Error&amp;gt;&amp;gt;&amp;gt; a * 2[1, 2, 3, 4, 1, 2, 3, 4]3. 인덱싱, 슬라이싱이번에는 위에서 만들어진 리스트 데이터를 가지고 원하는 부분만 가져올 수 있도록 해주는 인덱싱, 슬라이싱에 대해 알아보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&quot;&quot;&quot;인덱싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0]1&amp;gt;&amp;gt;&amp;gt; a[2]3&amp;gt;&amp;gt;&amp;gt; a[4] = 10 -&amp;gt; Error  (a[50] = 10 이런식으로 하면 그 사이의 인덱스에 값을 표시할 수 없어서 무조건 차례대로 값을 채워넣어야 함 -&amp;gt; 더하기 또는 append 메소드)&quot;&quot;&quot;슬라이싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0:3] # 0에서 부터 5앞까지 -&amp;gt; 인덱스 0~4[1, 2, 3]&amp;gt;&amp;gt;&amp;gt; a[:][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::-1] #처음부터 끝까지 거꾸로 슬라이싱 (중요)[4, 3, 2, 1]4. 리스트 메소드리스트 데이터는 프로그래밍을 하다보면 정말 자주 만나게 되는 자료형 중에 하나입니다.그렇기 때문에 문자열 객체의 메소드를 잘 활용할 줄 아는 것이 굉장히 중요합니다.먼저 어떤 메소드가 있는지 확인해 보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__add__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;,  &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__imul__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;,   &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;,    &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__rmul__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;,    &#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;]실제로 코딩을 하실 때는 기억이 안나면 그 때마다 dir() 함수를 사용해 어떤게 있는지 살펴보면 됩니다.4-1 .append(), .extend(), .insert(), .copy().append()리스트 맨 끝에 인자로 넣어준 값 하나를 추가해준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.append(100)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100]여러 개를 추가하고 싶어서 인자로 값을 여러 개 준다면? -&amp;gt; 에러가 난다그래서 [100, 101, 102] 이런 식으로 추가하면? -&amp;gt; 에러는 안나지만 리스트가 추가되어 원하는 모습과는 다르다..extend()iterable한 객체를 인자로 넣어주면 그 안의 원소들이 모두 차례대로 리스트에 추가된다.&amp;gt;&amp;gt;&amp;gt; a.extend([101, 102, 103]) # 리스트와 같은 iterable한 객체를 인자로 주어야 한다.&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100, 101, 102, 103]이제는 맨 뒤가 아니라 원하는 인덱스에 값을 추가(교체)하고 싶다..insert()인자로 인덱스와 값을 넣어주면 인덱스에 값을 넣어준다.인덱스에 이미 값이 있으면 바꿔주고 리스트 길이보다 인덱스 값이 크거나 같으면 리스트 맨 뒤에 값을 넣어준다.-&amp;gt; 길이 신경쓰지 않고 해줘도 오류는 안난다. (내가 원하는 인덱스에 값이 들어가지 않을 수도 있지만)&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1, 10)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1000, 7)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4, 7].copy()객체와 똑같은 값을 가지는 리스트를 복사한다. 변수를 지정해주면 새로운 메모리에 저장된다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = a.copy()&amp;gt;&amp;gt;&amp;gt; b.append(5)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b[1, 2, 3, 4, 5]4-2 .pop(), .remove(), .clear().pop()리스트의 가장 끝에 있는 원소를 뽑아 리턴해준다.&amp;gt;&amp;gt;&amp;gt; a = [&#39;banana&#39;, &#39;lemon&#39;, &#39;apple&#39;]&amp;gt;&amp;gt;&amp;gt; a.pop()&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;banana&#39;, &#39;lemon&#39;].remove()인자로 받은 값은 값을 제거해준다.&amp;gt;&amp;gt;&amp;gt; a.remove(2)&amp;gt;&amp;gt;&amp;gt; a[1, 3, 4].clear()리스트를 싹 비운다.&amp;gt;&amp;gt;&amp;gt; a.clear()&amp;gt;&amp;gt;&amp;gt; a[]4-3 .sort(), .reverse().sort()  리스트를 작은 값부터 순서대로 정렬해준다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a = [&#39;안녕&#39;, &#39;Hello&#39;, &#39;Hi&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a.sort(key=len)&amp;gt;&amp;gt;&amp;gt; a[&#39;안녕&#39;, &#39;Hi&#39;, &#39;Hello&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: x**2)&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]# 같은 제곱값에 대해 양수가 먼저 나오게 하려면 양수가 논리연산 시 False가 되면 되므로 기준을 0보다 작은지로 하면 된다 &amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: (x**2, x&amp;lt;=0))&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]&amp;gt;&amp;gt;&amp;gt; a =[False, True, False, True, True, False]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[False, False, False, True, True, True]🔔 sorted() 함수sorted() 함수는 정렬된 값을 리턴해줄 뿐 인자로 받은 리스트를 정렬하지는 않는다.또 한가지 중요한 특징은 sorted()함수는 리스트 뿐 아니라 모든 iterable한 값들을 정렬시켜 준다는 것입니다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted(a)[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a[3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted({1: &#39;D&#39;, 2: &#39;B&#39;, 3: &#39;B&#39;, 4: &#39;E&#39;, 5: &#39;A&#39;})[1, 2, 3, 4, 5]  🔔 .sort()와 sorted() 모두 key, reverse 인자를 갖는다  key: 정렬을 목적으로 하는 함수를 값으로 넣는다. lambda를 이용할 수 있다. key 매개 변수의 값은 단일 인자를 취하고 정렬 목적으로 사용할 키를 반환하는 함수(또는 다른 콜러블)여야 합니다.  reverse: bool값을 넣는다. 기본값은 reverse=False(오름차순)이다..reverse()리스트의 원소의 순서를 뒤집어준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.reverse()&amp;gt;&amp;gt;&amp;gt; a[4, 3, 2, 1]🔔 reversed() 함수뒤집은 값을 리턴해줄 뿐 인자로 받은 리스트는 그대로다.🔔 문자열을 뒤집는 방법a.reverse()a = list(reversed(a))a = a[::-1]4-4 .count(), .index().count()인자로 받은 값이 등장하는 횟수를 리턴해준다. (코드 생략).index()인자로 받은 값의 인덱스를 리턴해준다. (코드 생략)5. 리스트에서 주목할 만한 것들5-1 List &amp;amp; Range1부터 1000까지 값을 하나씩 출력하는 코드를 짠다고 할 때for i in [1, 2, 3, 4, 5, 6, 7, 8, ..., 1000]:  print(i)로 하게 되면 위의 코드를 실행하기 위해 1000개의 요소를 적어서 리스트를 만드는 것은 너무 비효율적입니다.이를 개선시키는 방법으로for i in range(1000):  print(i)이렇게 해주면 훨씬 짧고 간결한 코드를 작성할 수 있습니다.range(start, end, step)range(1000) =&amp;gt; 0, 1, 2, 3, ..., 999range(1, 1000) =&amp;gt; 1, 2, 3, ..., 999range(1, 1000, 2) =&amp;gt; 1, 3, 5, 7, ..., 9995-2 리스트 표현식 (List comprehension)&amp;gt;&amp;gt;&amp;gt; a = []&amp;gt;&amp;gt;&amp;gt; for i in range(100):        if i % 3 == 0 and i % 5 == 0:          a.append(i)&amp;gt;&amp;gt;&amp;gt; [i for i in range(100) if i % 3 == 0 and i % 5 == 0]5-3 리스트와 문자열 넘나들기문자열을 리스트로 바꿔야 하는 경우문자열은 값을 바꿀 수가 없기 때문에 예를 들어 스펠링을 고치기 위해서는리스트로 바꿔서 고친 후 다시 문자열로 변환해줘야 한다.&amp;gt;&amp;gt;&amp;gt; name = &#39;kinziont&#39;&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39; -&amp;gt; 에러&amp;gt;&amp;gt;&amp;gt; name = list(name)&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39;&amp;gt;&amp;gt;&amp;gt; name[&#39;k&#39;, &#39;i&#39;, &#39;m&#39;, &#39;z&#39;, &#39;i&#39;, &#39;o&#39;, &#39;n&#39;, &#39;t&#39;]&amp;gt;&amp;gt;&amp;gt; name = str(name)&amp;gt;&amp;gt;&amp;gt; name&#39;kimziont&#39;문자열 데이터를 단어 단위 또는 문장 단위로 토크나이징하기 위해 문자열 메소드인 .split()을 쓰면자동으로 리스트로 변환된다.5-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)a = [1, 2, 3, 4] # 1*4 vectorb = [[1, 2], [3, 4]] # 2*2 matrixc = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] # 2*2*2 tensora[0] -&amp;gt; 1b[0] -&amp;gt; [1, 2]c[0] -&amp;gt; [[1, 2], [3, 4]]c[0][1] -&amp;gt; [3, 4]c[0][1][0] -&amp;gt; 3",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-list'> <img src='/images/python_logo.jpg' alt='Python data type list'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-list'>Python data type list</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python data type number",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-number",
      "date"     : "Apr 23, 2020",
      "content"  : "1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-number'> <img src='/images/python_logo.jpg' alt='Python data type number'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-number'>Python data type number</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python data type Intro",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-intro",
      "date"     : "Apr 23, 2020",
      "content"  : "파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.타입            이름      타입      가변                  불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)참조변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사변수의 대입을 통한 복사immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.얕은 복사또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.깊은 복사따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/python-data-type-intro'> <img src='/images/python_logo.jpg' alt='Python data type Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-intro'>Python data type Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL Series [Part1]: MySQL Intro",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-intro",
      "date"     : "Apr 23, 2020",
      "content"  : "파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.타입            이름      타입      가변                  불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)참조변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사변수의 대입을 통한 복사immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.얕은 복사또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.깊은 복사따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2020-04-23T21:01:35+09:00'>23 Apr 2020</time><a class='article__image' href='/mysql-intro'> <img src='/images/mysql_logo.webp' alt='MySQL Series [Part1]: MySQL Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-intro'>MySQL Series [Part1]: MySQL Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Korean-to-French Translation",
      "category" : "",
      "tags"     : "",
      "url"      : "/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say",
      "date"     : "Nov 7, 2018",
      "content"  : "Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Sam Bark diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Sam Bark on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Sam Bark potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Sam Bark on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-11-07T00:00:00+09:00'>07 Nov 2018</time><a class='article__image' href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'> <img src='/images/08.jpg' alt='Korean-to-French Translation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'>Korean-to-French Translation</a> </h2><p class='article__excerpt'>한국어를 프랑스어로 번역해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Real Dubbing using Tacotron",
      "category" : "",
      "tags"     : "",
      "url"      : "/the-way-to-get-started-is-to-quit-talking-and-begin-doing",
      "date"     : "Apr 23, 2018",
      "content"  : "Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Jeroen Bendeler diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Jeroen Bendeler on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Jairph potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Jairph on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-04-23T00:00:00+09:00'>23 Apr 2018</time><a class='article__image' href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'> <img src='/images/15.jpg' alt='Real Dubbing using Tacotron'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'>Real Dubbing using Tacotron</a> </h2><p class='article__excerpt'>실제 배역 주인공의 목소리를 이용해 더빙을 합니다.</p></div></div></div>"
    } 
  
]

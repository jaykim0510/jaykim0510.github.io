[
  
    {
      "title"    : "Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series6",
      "date"     : "Apr 21, 2022",
      "content"  : "Table of Contents  데이터 파이프라인 설계데이터 파이프라인 설계",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-21T21:01:35+09:00'>21 Apr 2022</time><a class='article__image' href='/data-engineering-series6'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series6'>Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part5]: 데이터 멱등성과 ACID Transaction",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series5",
      "date"     : "Apr 20, 2022",
      "content"  : "Table of Contents  참고참고  Avoiding Double Payments in a Distributed Payments System  Naver D2: DBMS는 어떻게 트랜잭션을 관리할까?  Baeldung: Introduction to Transactions  Yuchen Z., A Deep Dive Into Idempotence  Youtube: ACID 2.0: Designing Better API’s and Messages - Improving Talks Series",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-20T21:01:35+09:00'>20 Apr 2022</time><a class='article__image' href='/data-engineering-series5'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part5]: 데이터 멱등성과 ACID Transaction'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series5'>Data Engineering Series [Part5]: 데이터 멱등성과 ACID Transaction</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series6",
      "date"     : "Apr 17, 2022",
      "content"  : "Table of Contents  도커 컴포즈로 컨테이너 띄우기  spark-client 컨테이너에서 pyspark 셸을 실행  pyspark 셸에서 몽고DB와 연결  참고도커 컴포즈로 컨테이너 띄우기version: &#39;3.2&#39;services:    spark-client:      image: kimziont/spark:1.0      hostname: spark-client      depends_on:        - spark-master      command:         - bash        - -c        - |          apt -y install python-is-python3          sleep infinity    spark-master:      image: kimziont/spark-master:1.0      hostname: spark-master      ports:        - &quot;4041:8080&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-master.sh &amp;amp;          sleep infinity    spark-worker1:      image: kimziont/spark-worker:1.0      hostname: worker1      depends_on:        - spark-master      ports:        - &quot;4042:8081&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;          sleep infinity    spark-worker2:      image: kimziont/spark-worker:1.0      hostname: worker2      depends_on:        - spark-master      ports:        - &quot;4043:8081&quot;      command:         - bash        - -c        - |          ./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;          sleep infinity        mongodb:      image: mongo:latest      hostname: mongodb      ports:        - &quot;27017:27017&quot;      environment:        MONGO_INITDB_ROOT_USERNAME: root        MONGO_INITDB_ROOT_PASSWORD: root      tty: true  마스터의 UI는 디폴트로 8080포트로 보여준다, 워커는 8081포트이다  워커들은 마스터의 7077포트로 연결될 수 있다docker compose upspark-client 컨테이너에서 pyspark 셸을 실행./bin/pyspark --master spark://spark-master:7077 --packages org.mongodb.spark:mongo-spark-connector_2.12:3.0.1pyspark 셸에서 몽고DB와 연결from pyspark.sql import SparkSessionspark = SparkSession.builder.master(&#39;spark://spark-master:7077&#39;).config(&#39;spark.mongodb.input.uri&#39;, &#39;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&#39;).getOrCreate()df = spark.read.format(&quot;mongo&quot;).option(&quot;uri&quot;, &quot;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&quot;).load()df.show()참고  MongoDB 공식문서: Connection String URI Format  MongoDB 공식문서: Connection String URI Format: authSource  Error connecting from pyspark to mongodb with password",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-17T21:01:35+09:00'>17 Apr 2022</time><a class='article__image' href='/spark-series6'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series6'>Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL 옵티마이저를 이용한 실행 최적화",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series9",
      "date"     : "Apr 13, 2022",
      "content"  : "Table of Contents  참고참고  Real MySQL 8.0 (1권) 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-13T21:01:35+09:00'>13 Apr 2022</time><a class='article__image' href='/mysql-series9'> <img src='/images/mysql_logo.webp' alt='MySQL 옵티마이저를 이용한 실행 최적화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series9'>MySQL 옵티마이저를 이용한 실행 최적화</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL 트랜잭션과 잠금",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series8",
      "date"     : "Apr 12, 2022",
      "content"  : "Table of Contents  트랜잭션          ACID      트랜잭션 관련 실행 명령어        페이지 캐시          캐싱      캐시 만료      페이지 동기화      페이지 고정      페이지 교체 알고리즘        복구          WAL      언두(Undo) 로그와 리두(Redo) 로그      스틸(Steal)과 포스(Force) 정책        동시성 제어          잠금      격리 수준        참고트랜잭션DBMS에서 트랜잭션이란 하나의 논리적 작업 단위를 의미하며, 여러 작업(DB의 읽기, 쓰기)을 한 단계로 표현하는 방법입니다.ACID이러한 트랜잭션을 정의하기 위해서는 다음의 4가지 속성이 보장되어야 합니다.원자성(Atomicity)  트랜잭션은 더 작은 단계로 나눌 수 없습니다. 트랜잭션과 관련된 작업은 모두 실행되거나 모두 실패해야 합니다.  All or Nothing  ex) A에서 B로 계좌이체를 할 때, A가 출금이 되면 B도 반드시 입금이 되어야 한다.일관성(Consistency)  트랜잭션이 일어나더라도 데이터베이스의 제약이나 규칙은 그대로 지켜져야 합니다.  사용자가 제어할 수 있는 유일한 속성입니다.  ex) 고객 정보 DB에서 이름을 반드시 입력하도록 제약을 두었다면 트랜잭션 또한 이러한 제약을 가져야 한다.격리성(Isolation)  하나의 트랜잭션은 다른 트랜잭션으로부터 간섭없이 독립적으로 수행되어야 합니다.  동시에 여러 개의 트랜잭션들이 수행될 때, 각 트랜잭션은 연속으로 실행된 것과 동일한 결과를 나타내야 합니다.  ex) A가 만원이 있는 계좌에서 B에게 3천원을 송금하던 도중 자신의 잔액을 확인할 때는 여전히 만원이 있어야 한다.  많은 데이터베이스는 성능상의 이유로 정의에 비해 약한 격리 수준을 사용합니다. (동시성 제어의 격리 수준 참고)지속성(Durability)  커밋된 데이터는 장애가 발생 하더라도 데이터베이스에 저장되어야 한다.  ex) A에서 B로 송금이 완료되어 커밋을 했다면 시스템 중단, 정전으로 장애가 발생해도 DB에 데이터가 그대로 유지되어야 한다트랜잭션 관련 실행 명령어  Commit: A commit ends the current transaction and makes permanent all changes performed in the transaction. The transaction is a sequence of SQL statements that the database treats as a single unit. A commit also erases all savepoints in the transaction and releases transaction locks. After your data is committed, it is visible to other users of the system.Commit means save the cache changes to the databaseA SQL statement that ends a transaction, making permanent any changes made by the transaction. It is the opposite of rollback, which undoes any changes made in the transaction.InnoDB uses an optimistic mechanism for commits, so that changes can be written to the data files before the commit actually occurs. This technique makes the commit itself faster, with the tradeoff that more work is required in case of a rollback.By default, MySQL uses the autocommit setting, which automatically issues a commit following each SQL statement.      Save: A save writes your changes to the database, however at this point these changes are only visible to you within your transaction scope. The database has also generated undo information which contains the old values of your transaction which can be used to rollback your modifications.        Flush: flush() will synchronize your database with the current state of object/objects held in the memory but it does not commit the transaction. So, if you get any exception after flush() is called, then the transaction will be rolled back.  To write changes to the database files, that had been buffered in a memory area or a temporary disk storage area. The InnoDB storage structures that are periodically flushed include the redo log, the undo log, and the buffer pool.  Rollback:페이지 캐시대부분의 데이터베이스는 상대적으로 속도가 느린 영구 저장소(디스크)에 접근하는 회수를 줄이기 위해 페이지(읽고 쓰는 가장 작은 단위)를 메모리에 캐시합니다. 이를 페이지 캐시(page cache)라고 하며 이 때의 메모리 영역을 버퍼 풀(buffer pool)이라고 합니다. 메모리에 있는 페이지에 변경사항이 생겼을 때 아직 디스크로 플러시(flush)되지 않은 페이지를 더티(dirty) 페이지라고 합니다.정리하면 페이지 캐시의 주요 기능은 다음과 같습니다.  페이지를 메모리에 캐시함으로써 빠른 읽기를 지원  쓰기 요청이 발생할때마다 디스크로 플러시하지 않고 버퍼링 후 플러시 할 수 있다캐싱스토리지 엔진이 특정 페이지를 요청하면 우선 캐시된 버전이 있는지 확인합니다. 페이지가 있다면 반환하고 없다면 페이지 번호를 물리적 주소로 변환해 해당 페이지를 메모리로 복사하고 반환합니다.이때 해당 페이지가 저장된 버퍼는 참조상태라고 표현합니다. 작업이 끝나면 스토리지 엔진은 참조 해제해야 합니다.캐시 만료일반적으로 버퍼 풀은 데이터셋보다 크기가 작기 때문에 새로운 페이지를 추가하기 위해 기존 페이지를 만료시키는 작업도 필요하게 됩니다. 페이지가 동기화됐고 고정 또는 참조 상태가 아니라면 바로 제거할 수 있습니다. 페이지를 제거할 때에는 페이지와 관련된 로그도 WAL에서 삭제합니다.페이지 동기화위에서 버퍼 풀의 메모리 용량을 관리하기 위해서는 캐시가 만료된 페이지는 제거해야 한다고 했습니다. 그리고 이때 페이지를 제거하기 위해서는 우선 페이지가 동기화되어야 한다고 했습니다. 페이지 동기화는 더티페이지를 디스크에 반영(flush)하는 것입니다.이렇게 플러시하는 것은 언제 얼마나 자주하는 것이 좋을까요? 변경 사항이 생길 때마다 플러시하게 되면 데이터 손실 가능성을 줄일 수 있겠지만 결국 잦은 디스크 접근을 유발하기 때문에 트레이드 오프가 있습니다. 그래서 데이터베이스에서는 이러한 플러시를 주기적으로 하게 되며 이 시점을 체크포인트(checkpoint)라고 합니다.체크포인트 시점에 플러시가 일어나는데 이 때 플러시는 디스크에 있는 데이터베이스에 데이터가 저장되는 것을 의미하지는 않습니다. 플러시는 메모리에 있는 페이지에 요청된 작업 명령들을 디스크의 WAL(Write Ahead Log)에 남겨두고 페이지와 싱크를 맞추는 것입니다. (보통 커밋이 일어나면 플러시도 그 과정에 포함되어 플러시를 디스크에 저장하는 것으로 정의하기도 함. 애매하네)정리하면  캐시가 만료된 페이지를 삭제하려면 먼저 페이지를 동기화 해야 한다.  동기화된 시점을 체크포인트라고 한다.  동기화는 플러시하는 것이며 플러시는 페이지의 변경시 요청된 작업 명령을 디스크의 WAL에 기록하는 것이다.페이지 고정가까운 시간 내에 요청될 확률이 높은 페이지는 캐시에 가둬 두는 것이 좋습니다. 이를 페이지 고정(pinning)이라고 합니다. 예를 들어 이진 트리 탐색에서 트리의 상위 노드는 접근될 확률이 높기 때문에 이러한 상위 노드는 고정해두면 성능 향상에 도움이 됩니다.페이지 교체 알고리즘저장 공간이 부족한 캐시에 새로운 페이지를 추가하려면 일부 페이지를 만료시켜야 한다고 했습니다. 하지만 빈번하게 요청될 수 있는 페이지를 만료시키면 같은 페이지를 여러 차례 페이징하는 상황이 발생할 수 있습니다. 페이지 교체 알고리즘은 다시 요청될 확률이 가장 낮은 페이지를 만료시키고 해당 위치에 새로운 페이지를 페이징합니다.하지만 페이지의 요청 순서는 일반적으로 특정 패턴이 없기 때문에 어떤 페이지가 다시 요청될지 정확하게 예측하는 것은 불가능 합니다. 그래서 보통은 그 기준을 최근에 요청되었는지 여부, 요청된 빈도수 등으로 합니다. 관련 알고리즘에는 FIFO(First In First Out), LRU(Least Recently Used), LFU(Least Frequently Used), CLOCK-sweep 알고리즘이 있습니다.복구데이터베이스 시스템은 각자 다른 안정성과 신뢰성 문제를 내재한 하드웨어와 소프트웨어 계층으로 구성됩니다. 따라서 여러 지점에서 장애가 발생할 수 있고, 데이터베이스 개발자는 이러한 장애 시나리오를 고려해 데이터를 저장해야 합니다.WAL선행 기록 로그(WAL)는 장애 및 트랜잭션 복구를 위해 디스크에 저장하는 추가 자료 구조입니다. WAL은 페이지에 캐시된 데이터가 디스크로 커밋(책에서는 플러시라고 표기)될 때 까지 관련 작업 이력의 유일한 디스크 기반 복사본입니다.WAL에 있는 각각의 로그에는 단조 증가하는 고유 로그 시퀀스 번호(LSN: Log Sequence Number)가 있습니다.WAL의 주요 기능은 다음과 같습니다.  장애 발생 시 WAL을 기반으로 마지막 메모리 상태를 재구성한다. (undo)  WAL의 로그를 재수행해서 트랜잭션을 커밋한다. (redo)언두(Undo) 로그와 리두(Redo) 로그UNDO는 왜 필요할까?오퍼레이션 수행 중에 수정된 페이지들이 버퍼 관리자의 버퍼 교체 알고리즘에 따라서 디스크에 출력될 수 있다. 버퍼 교체는 전적으로 버퍼의 상태에 따라서 결정되며, 일관성 관점에서 봤을 때는 임의의 방식으로 일어나게 된다. 즉 아직 완료되지 않은 트랜잭션이 수정한 페이지들도 디스크에 출력될 수 있으므로, 만약 해당 트랜잭션이 어떤 이유든 정상적으로 종료될 수 없게 되면 트랜잭션이 변경한 페이지들은 원상 복구되어야 한다. 이러한 복구를 UNDO라고 한다. 만약 버퍼 관리자가 트랜잭션 종료 전에는 어떤 경우에도 수정된 페이지들을 디스크에 쓰지 않는다면, UNDO 오퍼레이션은 메모리 버퍼에 대해서만 이루어지면 되는 식으로 매우 간단해질 수 있다. 이 부분은 매력적이지만 이 정책은 매우 큰 크기의 메모리 버퍼가 필요하다는 문제점을 가지고 있다. 수정된 페이지를 디스크에 쓰는 시점을 기준으로 다음과 같은 두 개의 정책으로 나누어 볼 수 있다.  STEAL: 수정된 페이지를 언제든지 디스크에 쓸 수 있는 정책  No-STEAL: 수정된 페이지들을 최소한 트랜잭션 종료 시점(EOT, End of Transaction)까지는 버퍼에 유지하는 정책STEAL 정책은 수정된 페이지가 어떠한 시점에도 디스크에 써질 수 있기 때문에 필연적으로 UNDO 로깅과 복구를 수반하는데, 거의 모든 DBMS가 채택하는 버퍼 관리 정책이다.REDO는 왜 필요할까?이제는 UNDO 복구의 반대 개념인 REDO 복구에 대해서 알아볼 것인데, 앞서 설명한 바와 같이 커밋한 트랜잭션의 수정은 어떤 경우에도 유지(durability)되어야 한다. 이미 커밋한 트랜잭션의 수정을 재반영하는 복구 작업을 REDO 복구라고 하는데, REDO 복구 역시 UNDO 복구와 마찬가지로 버퍼 관리 정책에 영향을 받는다. 트랜잭션이 종료되는 시점에 해당 트랜잭션이 수정한 페이지들을 디스크에도 쓸 것인가 여부로 두 가지 정책이 구분된다.  FORCE: 수정했던 모든 페이지를 트랜잭션 커밋 시점에 디스크에 반영하는 정책  No-FORCE: 수정했던 페이지를 트랜잭션 커밋 시점에 디스크에 반영하지 않는 정책여기서 주의 깊게 봐야 할 부분은 No-FORCE 정책이 수정했던 페이지(데이터)를 디스크에 반영하지 않는다는 점이지 커밋 시점에 어떠한 것도 쓰지 않는다는 것은 아니다. 어떤 일들을 했었다고 하는 로그는 기록하게 되는데 이 부분은 아래에서 자세히 설명한다.FORCE 정책을 따르면 트랜잭션이 커밋되면 수정되었던 페이지들이 이미 디스크 상의 데이터베이스에 반영되었으므로 REDO 복구가 필요 없게 된다. 반면에 No-FORCE 정책을 따른다면 커밋한 트랜잭션의 내용이 디스크 상의 데이터베이스 상에 반영되어 있지 않을 수 있기 때문에 반드시 REDO 복구가 필요하게 된다. 사실 FORCE 정책을 따르더라도 데이터베이스 백업으로부터의 복구, 즉 미디어(media) 복구 시에는 REDO 복구가 요구된다. 거의 모든 DBMS가 채택하는 정책은 No-FORCE 정책이다.정리해보면 DBMS는 버퍼 관리 정책으로 STEAL과 No-FORCE 정책을 채택하고 있어, 이로 인해서 UNDO 복구와 REDO 복구가 모두 필요하게 된다.스틸(Steal)과 포스(Force) 정책DBMS는 스틸/노스틸 정책과 포스/노포스 정책을 기반으로 메모리에 캐시된 변경 사항을 디스크로 플러시하는 시점을 결정합니다. 이러한 정책들은 복구 알고리즘 선택에 큰 영향을 미칩니다.스틸(Steal)트랜잭션이 완료되지 않은 상태에서 데이터를 디스크에 기록할 것인가?  Steal: 기록한다(Undo 필요)  No-Steal: 기록하지 않는다버퍼 관리자가 트랜잭션 종료 전에는 어떤 경우에도 수정된 페이지들을 디스크에 쓰지 않는다면, UNDO 오퍼레이션은 메모리 버퍼에 대해서만 이루어지면 되는 식으로 매우 간단해질 수 있다. 이 부분은 매력적이지만 이 정책은 매우 큰 크기의 메모리 버퍼가 필요하다는 문제점을 가지고 있다. 수정된 페이지를 디스크에 쓰는 시점을 기준으로 다음과 같은 두 개의 정책으로 나누어 볼 수 있다.  STEAL: 수정된 페이지를 언제든지 디스크에 쓸 수 있는 정책  No-STEAL: 수정된 페이지들을 최소한 트랜잭션 종료 시점(EOT, End of Transaction)까지는 버퍼에 유지하는 정책STEAL 정책은 수정된 페이지가 어떠한 시점에도 디스크에 써질 수 있기 때문에 필연적으로 UNDO 로깅과 복구를 수반하는데, 거의 모든 DBMS가 채택하는 버퍼 관리 정책이다.포스(Force)트랜잭션이 완료된 후 바로 데이터를 디스크에 기록할 것인가?  Force: 바로 기록한다  No-Force: 바로 기록하지 않는다(Redo 필요)성능상의 이유로 때로는 트랜잭션이 완료되기도 전에 디스크에 기록하기도 하고 완료되고 나서도 기록하지 않기도 합니다.스틸과 포스 정책은 트랜잭션 언두와 리두 작업과 관련되기 때문에 매우 중요합니다.동시성 제어잠금격리 수준참고  Real MySQL 8.0 (1권) 책  데이터베이스 인터널스 책  Naver D2: DBMS는 어떻게 트랜잭션을 관리할까?  온달의 해피클라우드: ACID 이해하기  노력 이기는 재능 없고 노력 외면하는 결과도 없다: [MySQL Internals] FLUSH  MySQL 공식문서: MySQL Glossary  stackoverflow: SQLAlchemy: What’s the difference between flush() and commit()?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-12T21:01:35+09:00'>12 Apr 2022</time><a class='article__image' href='/mysql-series8'> <img src='/images/mysql_logo.webp' alt='MySQL 트랜잭션과 잠금'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series8'>MySQL 트랜잭션과 잠금</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part8]: SSH(Secure SHell)이란 무엇인가",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series8",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  참고참고  The Secret Security Wiki",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/network-series8'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part8]: SSH(Secure SHell)이란 무엇인가'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series8'>Network Series [Part8]: SSH(Secure SHell)이란 무엇인가</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part7]: 소켓(Socket)이란 무엇인가",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series7",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  참고참고  곰돌이 놀이터: 소켓 통신이란?  아무거나올리는블로그: Socket Programming - Socket  stackoverflow: difference between socket programming and Http programming",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/network-series7'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part7]: 소켓(Socket)이란 무엇인가'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series7'>Network Series [Part7]: 소켓(Socket)이란 무엇인가</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part7]: Kafka Connector",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series7",
      "date"     : "Apr 11, 2022",
      "content"  : "Table of Contents  Why Connector?  Kafka Connect          Kafka Connect 구성요소      Connect? Connector?      Standalone과 Distributed Workers        Debezium  도커 컴포즈 파일  kafka 컨테이너에서 워커 실행 모드 설정  kafka 컨테이너에서 커넥터 워커 실행  connect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행  기타 커넥터 관련 REST API  커넥터와 백엔드(Java Spring)의 관계  참고Why Connector?커넥터 없이도 프로듀서 컨슈머 사용 가능하지만 커넥터를 이용하면 카프카를 사용하면서 발생할 수 있는 장애에 대한 복구를 비롯한 필요한 기능들을 따로 개발할 필요없이 사용가능Kafka Connect  Kafka Connect는 다른 데이터 시스템을 Kafka와 통합하는 과정을 표준화한 프레임워크  통합을 위한 Connector 개발, 배포, 관리를 단순화Kafka Connect 구성요소  Connector: Task를 관리하여 데이터 스트리밍을 조정하는 jar파일  Task: 데이터 시스템간의 전송 방법을 구현한 구현체  Worker: Connector와 Task를 실행하는 프로세스  Converter: 데이터 포맷을 변환하는데 사용하는 구성요소  Trasform: 데이터를 변환하는데 사용하는 구성요소Connect? Connector?커넥트는 커넥터를 실행시키기 위한 환경(프레임워크)을 제공해줌. 커넥트 위에서 커넥터 설치하고 커넥터(jar파일) 실행하면 됨커넥트 이미지로 인스턴스 띄우고 거기서 각종 커넥터 다운로드 받아서 커넥터를 몽고db, mysql, s3같은데 RESTapi로 등록Standalone과 Distributed WorkersWorker 프로세스를 한 개만 띄우는 Standalone 모드와 여러개 실행시키는 Distributed 모드가 있다.보통 확장성과 내결함성을 이유로 Distributed 모드를 많이 사용한다.DebeziumDebezium은 변경 데이터 캡처를 위한 오픈 소스 분산 플랫폼이다.Debezium 에서 변경된 데이터 캡쳐를 위해 mysql의 경우 binlog, postgresql의 경우 replica slot(logical)을 이용하여 데이터베이스에 커밋하는 데이터를 감시하여 Kakfa, DB, ElasticSearch 등 미들웨어에 이벤트를 전달한다도커 컴포즈 파일version: &#39;3.2&#39;services:  mongodb:    image: mongo:latest    hostname: mongodb    ports:      - &quot;27017:27017&quot;    environment:      MONGO_INITDB_ROOT_USERNAME: root      MONGO_INITDB_ROOT_PASSWORD: root    tty: true    zookeeper:    image: zookeeper:3.7    hostname: zookeeper    ports:      - &quot;2181:2181&quot;    environment:      ZOO_MY_ID: 1      ZOO_PORT: 2181    volumes:      - ./data/zookeeper/data:/data      - ./data/zookeeper/datalog:/datalogco  kafka:    image: wurstmeister/kafka    hostname: kafka    ports:      - &quot;9092:9092&quot;    environment:      KAFKA_BROKER_ID: 1      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1    tty: true    volumes:      - ./data/kafka/data:/tmp/kafka-logs    depends_on:      - zookeeper    connect:    image: confluentinc/cp-kafka-connect:latest.arm64    hostname: connect1    depends_on:      - kafka    environment:      CONNECT_BOOTSTRAP_SERVERS: kafka:29092      CONNECT_REST_ADVERTISED_HOST_NAME: connect1      CONNECT_GROUP_ID: connect-cluster      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter      CONNECT_CONFIG_STORAGE_TOPIC: connect-configs      CONNECT_OFFSET_STORAGE_TOPIC: connect-offsets      CONNECT_STATUS_STORAGE_TOPIC: connect-status      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1      CONNECT_PLUGIN_PATH: /usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/      CONNECT_REST_PORT: 8083    ports:      - 18083:8083    volumes:      - ./connectors/1:/usr/share/confluent-hub-components    command:      - bash      - -c      - |        confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:1.7.0        /etc/confluent/docker/run &amp;amp;        sleep infinity  producer:    build:      context: ./      dockerfile: Dockerfile_producer    stdin_open: true    tty: true  consumer:    build:      context: ./      dockerfile: Dockerfile_consumer    stdin_open: true    tty: truevolumes:  mongodb:kafka 컨테이너에서 워커 실행 모드 설정cd opt/kafka/configvi connect-distributed.properties# connect 컨테이너에서 커넥터(jar파일)가 설치되어 있는 경로 설정plugin.path=/usr/share/java/,/usr/share/confluent-hub-components/mongodb-kafka-connect-mongodb/lib/# 컨버터 설정key.converter=org.apache.kafka.connect.json.JsonConvertervalue.converter=org.apache.kafka.connect.json.JsonConverterkey.converter.schemas.enable=falsevalue.converter.schemas.enable=falsekafka 컨테이너에서 커넥터 워커 실행./bin/connect-distributed.sh ./config/connect-distributed.propertiesconnect 제외한 아무 컨테이너(나의 경우 kafka 컨테이너)에서 REST API를 이용해 커넥터 등록/실행curl -X POST -H&#39;Accept:application/json&#39; -H&#39;Content-Type:application/json&#39; http://connect1:8083/connectors \  -w &quot;\n&quot; \  -d &#39;{&quot;name&quot;: &quot;mongo-sink&quot;,      &quot;config&quot;: {         &quot;connector.class&quot;:&quot;com.mongodb.kafka.connect.MongoSinkConnector&quot;,         &quot;connection.user&quot;: &quot;root&quot;,         &quot;connectioin.password&quot;: &quot;root&quot;,         &quot;connection.uri&quot;:&quot;mongodb://root:root@mongodb:27017&quot;,         &quot;database&quot;:&quot;quickstart&quot;,         &quot;collection&quot;:&quot;topicData&quot;,         &quot;topics&quot;:&quot;taxi&quot;,        &quot;key.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;value.converter&quot;: &quot;org.apache.kafka.connect.json.JsonConverter&quot;,        &quot;key.converter.schemas.enable&quot;: &quot;false&quot;,        &quot;value.converter.schemas.enable&quot;: &quot;false&quot;         }     }&#39;기타 커넥터 관련 REST API# 커넥터 상태 확인(커넥터 등록과 태스크 실행이 RUNNING이면 성공)curl -X GET http://connect1:8083/connectors/mongo-sink/status# 커넥터 삭제curl -X DELETE http://connect1:8083/connectors/mongo-sink커넥터와 백엔드(Java Spring)의 관계커넥터가 있으면 알아서 커넥터가 토픽에서 데이터를 가져와 DB로 잘 반영을하는 것 같다.이런거보면 딱히 스프링부트 같은 걸 이용해서 백엔드 프로그램을 개발하지 않아도 되는 것 같아보인다.하지만 만약 내가 스프링부트 같은 거를 엄청 잘 알아서 직접 개발하는데 불편함이 없다면 왠만한 것들은 스프링 부트를 이용하고 부분적으로 특정 프로듀서/컨슈머는 커넥터를 사용하는 것이 아마 가장 좋은 방법이 아닐까 라는 생각이 든다.나는 지금 스프링부트를 모른다. 심지어 자바 언어도 써본 적이 없다. 커넥터는 아예 러닝 커브가 없는 것은 아니지만 훨씬 쉽다.하지만 백엔드의 중요한 철학들을 공부하는 것은 굉장히 중요해보인다.결론은 지금 당장 구현이 필요한 부분들은 커넥터로 구현을 하고, 백엔드 공부는 스프링 부트를 통해서 계속 하자.백엔드 공부를 스프링 부트로 하기로 한 이유는, 내가 사용하고 있는 언어는 파이썬이지만 데이터 엔지니어링 공부에서 자바 언어는 필요해보인다. (데이터 엔지니어링 분야의 관련 오픈 소스들이 자바로 많이 개발됨)파이썬으로 백엔드를 구현하도록 해주는 장고나 플라스크도 있지만, 아직은 스프링 부트를 사용하는 비중이 더 커보이고 뭔가 공부하는 관점에서는 스프링 부트가 더 도움이 많이 될 것 같다.참고  Confluent 공식문서: Kafka Connect Tutorial on Docker  Connect 도커 이미지  Confluent 공식문서: MongoDB 커넥터  MongoDB 공식문서: MongoDB 커넥터를 위한 Configuration  kudl: CDC - debezium 설정  Confluent 공식문서: 커넥터 관련 강의  Confluent 공식문서: Connect 관련 configuration  sup2is: Kafka Connect로 데이터 허브 구축하기  깃허브: mongodb-university/kafka-edu  Kafka Connect Deep Dive – Converters and Serialization Explained  정몽실이: 카프카 커넥트 실행  Stackoverflow: Connector and Spring Kafka",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-11T21:01:35+09:00'>11 Apr 2022</time><a class='article__image' href='/kafka-series7'> <img src='/images/kafka_logo.png' alt='Kafka Series [Part7]: Kafka Connector'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series7'>Kafka Series [Part7]: Kafka Connector</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part3]: 펀더멘털 분석",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series2",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  참고참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series2'> <img src='/images/finance_logo.jpeg' alt='Finance Series [Part3]: 펀더멘털 분석'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series2'>Finance Series [Part3]: 펀더멘털 분석</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part2]: 연방 준비 시스템의 이해",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series1",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  참고참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series1'> <img src='/images/finance_logo.jpeg' alt='Finance Series [Part2]: 연방 준비 시스템의 이해'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series1'>Finance Series [Part2]: 연방 준비 시스템의 이해</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Finance Series [Part1]: 금융시장의 메커니즘",
      "category" : "",
      "tags"     : "Finance",
      "url"      : "/finance-series0",
      "date"     : "Apr 10, 2022",
      "content"  : "Table of Contents  금융시장 메커니즘 이해  이자율 관련 기본 개념          금리(이자율)      단리/복리      연준 기준금리      우대금리      LIBOR 단기 이자율      수익률      물가 상승      실질이자율      순 현재 가치        금융시장 내 자산시장들 간의 관계          채권 시장      주식 시장      외환 시장      커머더티 시장      ETF 시장      선물 시장      옵션 시장        참고금융시장 메커니즘 이해이자율 관련 기본 개념금리(이자율)단리/복리연준 기준금리우대금리LIBOR 단기 이자율수익률물가 상승실질이자율순 현재 가치금융시장 내 자산시장들 간의 관계채권 시장주식 시장외환 시장커머더티 시장ETF 시장선물 시장옵션 시장참고  미국 주식 투자 바이블 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-10T21:01:35+09:00'>10 Apr 2022</time><a class='article__image' href='/finance-series0'> <img src='/images/finance_1.png' alt='Finance Series [Part1]: 금융시장의 메커니즘'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/finance-series0'>Finance Series [Part1]: 금융시장의 메커니즘</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL 아키텍처",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series7",
      "date"     : "Apr 9, 2022",
      "content"  : "Table of Contents  MySQL의 전체 구조  MySQL 엔진 아키텍처  InnoDB 스토리지 엔진 아키텍처          InnoDB 버퍼 풀      로그 버퍼      리두 로그                  버퍼 풀과 리두 로그의 관계          버퍼 풀 플러시                    언두 로그      어댑티브 해시 인덱스        참고MySQL의 전체 구조  MySQL 엔진: 요청된 SQL문장을 분석하거나 최적화하는 등 DBMS의 두뇌에 해당하는 처리를 수행  스토리지 엔진: 데이터를 디스크에 저장하거나 디스크로부터 읽어오는 역할MySQL 엔진 아키텍처  커넥션 핸들러: 크랄이언트의 접속, 쿼리 요청을 처리  SQL 파서: 실행 이전에 문법성 오류 체크  SQL 옵티마이저: 쿼리의 최적화InnoDB 스토리지 엔진 아키텍처InnoDB 버퍼 풀  스토리지 엔진에서 가장 핵심적인 부분  데이터 캐시: 디스크의 데이터 파일이나 인덱스 정보를 메모리에 캐시해 두는 공간  쓰기 버퍼링: 쓰기 작업을 지연시켜 일괄 작업으로 처리할 수 있게 해주는 버퍼 역할  페이지 크기의 조각으로 쪼개어 디스크로부터 읽어온 페이지를 저장  메모리 공간을 관리하기 위해 LRU리스트, 플러시 리스트, 프리 리스트라는 자료구조를 관리로그 버퍼  디스크의 로그 파일에 쓸 데이터를 버퍼링하는 메모리 영역  기본 크기는 16MB  로그 버퍼의 내용은 주기적으로 디스크로 플러시리두 로그  데이터의 변경 내용을 기록하는 디스크 기반 자료구조  ACID의 D(Durable)에 해당하는 영속성과 가장 밀접하게 연관  장애로 데이터 파일에 기록되지 못한 데이터를 잃지 않게 해주는 역할  상대적으로 비용이 높은 쓰기 작업의 성능 향상을 위해 로그 버퍼에 리두 로그를 버퍼링한 후 디스크 영역에 저장  데이터가 데이터 파일에 저장되는 시점보다 리두 로그가 로그 파일에 먼저 저장 -&amp;gt; 리두 로그를 WAL 로그라고도 함버퍼 풀과 리두 로그의 관계  버퍼 풀과 리두 로그의 관계를 이해하면 버퍼 풀 성능 향상에 도움이 되는 요소를 알 수 있음  데이터 변경이 발생하면 버퍼 풀에는 더티 페이지가 생기고, 로그 버퍼에는 리두 로그 레코드가 버퍼링  이 두가지 요소는 체크포인트마다 디스크로 동기화되어야 함  체크포인트는 장애 발생시 리두 로그의 어느 부분부터 복구를 실행해야 할지 판단하는 기준점버퍼 풀 플러시  버퍼 풀을 플러시하면 버퍼 풀 메모리 공간과 리두 로그 공간을 모두 얻을 수 있음  버퍼 풀을 플러시(더티 페이지들을 디스크에 동기화)하면 오래된 리두 로그 공간을 지울 수 있음  이를 위해 InnoDB 스토리지 엔진은 주기적으로 플러시 리스트 플러시 함수를 호출  플러시 리스트에서 오래 전에 변경된 데이터 페이지 순서대로 디스크에 동기화  또한 사용 빈도가 낮은 데이터 페이지를 제거하기 위해 LRU 리스트 플러시 함수를 호출  이 때 InnoDB 스토리지 엔진은 버퍼 풀을 스캔하며 더티 페이지는 동기화 클린 페이지는 프리 리스트로 옮김언두 로그  DML(INSERT, UPDATE, DELETE)로 변경되기 이전 버전의 백업된 데이터를 기록해두는 디스크 기반 자료구조  트랜잭션 보장: 트랜잭션이 롤백될 경우 원래 데이터로 복구하기 위해 언두 로그에 백업해둔 데이터를 이용  격리수준 보장: 특정 커넥션이 변경 중인 레코드에 다른 커넥션이 접근할 경우 격리수준에 맞게 언두 로그의 이전 데이터 제공  대용량 데이터를 변경하거나, 오랜 시간 유지되는 트랜잭션이 증가할 경우 언두 로그의 크기 급격히 증가  언두 로그의 사이즈가 커지면 쿼리 실행시 스토리지 엔진은 언두 로그를 필요한 만큼 스캔해야해서 쿼리의 성능이 감소어댑티브 해시 인덱스참고  Real MySQL 8.0 (1권) 책  MySQL 공식문서: The InnoDB Storage Engine        ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-09T21:01:35+09:00'>09 Apr 2022</time><a class='article__image' href='/mysql-series7'> <img src='/images/mysql_logo.webp' alt='MySQL 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series7'>MySQL 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part3]: 해시테이블(Hash Table)",
      "category" : "",
      "tags"     : "Data_Structure",
      "url"      : "/hash_table",
      "date"     : "Apr 9, 2022",
      "content"  : "Table of Contents  해시테이블  딕셔너리해시테이블해시 테이블은 키-값을 쌍으로 저장해둔 자료구조로 값(value)에 해당하는 키(key)가 무엇인지만 알면 어디에 위치한 값(value)이든 시간 복잡도 O(1)으로 값을 찾을 수 있습니다.딕셔너리파이썬에서 해시 테이블로 구현된 자료형은 딕셔너리입니다.딕셔너리는 대부분의 연산을 O(1)으로 처리할 수 있다는 점에서 성능이 매우 우수합니다.            연산      시간복잡도              len(a)      O(1)              a[key]      O(1)              key in a      O(1)      파이썬에서는 이외에도 딕셔너리에서 자주 사용되는 성질들을 편하게 사용하는 기능을 제공해주는 다양한 모듈들을 가지고 있습니다. 대표적으로 collections.defaultdict(), collections.Counter()가 있습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-09T21:01:35+09:00'>09 Apr 2022</time><a class='article__image' href='/hash_table'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part3]: 해시테이블(Hash Table)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/hash_table'>Coding Test Series [Part3]: 해시테이블(Hash Table)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)",
      "category" : "",
      "tags"     : "Data_Structure",
      "url"      : "/stack_queue",
      "date"     : "Apr 8, 2022",
      "content"  : "Table of Contents  스택  큐스택스택은 가장 나중에 들어간 요소가 가장 먼저 나온다는 LIFO(Last In First Out)의 성질을 가지는 자료구조입니다. 이런 특징은 함수 호출과 같은 프로그래밍 세계에서 굉장히 자주 사용되기 때문에 중요합니다.스택에서 가장 중요한 것은 append와 pop을 얼마나 빨리 처리할 수 있냐입니다. 그렇기 때문에 이러한 기능을 O(1)의 시간복잡도로 제공해주는 동적배열(파이썬에서는 리스트)을 통해 스택을 구현할 수 있습니다.큐큐는 가장 먼저 들어간 요소가 가장 먼저 처리되는 FIFO(First In First Out)의 성질을 가지는 자료구조입니다.큐에서는 append와 pop(0)을 얼마나 빨리 처리할 수 있냐가 중요합니다. 동적배열에서 가장 앞 원소를 제거하는 pop(0) 연산의 시간 복잡도가 O(n)이기 때문에 파이썬에서는 리스트가 아닌 deque(데크)라는 별도의 자료형을 사용합니다.🦊 데크(deque)데크는 더블 엔디드 큐(Double-Ended Queue)의 줄임말로, 글자 그대로 양쪽 끝을 모두 추출할 수 있는 추상자료형으로 큐와 스택의 역할을 모두할 수 있습니다. 데크는 이중 연결 리스트로 구현되어 있습니다.큐는 큐 그자체로 보다는 우선순위 큐라는 변형된 방법으로 조금 더 많이 사용됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-08T21:01:35+09:00'>08 Apr 2022</time><a class='article__image' href='/stack_queue'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/stack_queue'>Coding Test Series [Part2]: 스택(Stack)과 큐(Queue)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Coding Test Series [Part1]: 배열(Array)",
      "category" : "",
      "tags"     : "Data_Structure",
      "url"      : "/array",
      "date"     : "Apr 7, 2022",
      "content"  : "Table of Contents  배열  itertools배열자료구조는 크게 메모리 공간 기반의 연속 방식과 포인터 기반의 연결 방식으로 나뉩니다. 배열은 이 중에서 연속 방식의 가장 기본이 되는 자료형입니다. 연결 방식의 경우는 연결 리스트입니다.추상자료형(스택, 큐, 트리, 그래프 등)은 대부분 배열과 연결리스트를 기반으로 구현되었습니다.파이썬에서는 정확히 배열, 연결 리스트라는 자료형은 따로 없습니다. 하지만 더욱 편리한 리스트라는 자료형이 있습니다. 리스트 자료형은 배열과 연결 리스트의 장점을 모두 합쳐놓은 파이썬만의 자료형입니다.그렇기 때문에 저는 지금부터는 코딩테스트에서 리스트를 사용할 때 도움이 될만한 지식들을 여기에 기록해두도록 하겠습니다.파이썬 리스트는 그 자체로 정말 유용한 메소드들을 많이 보유하고 있습니다.a = list()print(dir(a))--------------------------------------------------------------------------------------------------&#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;itertools배열은 대표적인 시퀀스 자료형입니다. 그렇기 때문에 배열안의 원소들을 순회하는 경우가 많습니다. 이 때 파이썬의 itertools 모듈을 적절히 사용하면 코드를 훨씬 더 간결하고 가독성 높게 작성할 수 있습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-07T21:01:35+09:00'>07 Apr 2022</time><a class='article__image' href='/array'> <img src='/images/algorithm_logo.webp' alt='Coding Test Series [Part1]: 배열(Array)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/array'>Coding Test Series [Part1]: 배열(Array)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part4]: 여러가지 파일 포맷(JSON, BSON, Avro, Parquet)",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series4",
      "date"     : "Apr 7, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-07T21:01:35+09:00'>07 Apr 2022</time><a class='article__image' href='/data-engineering-series4'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part4]: 여러가지 파일 포맷(JSON, BSON, Avro, Parquet)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series4'>Data Engineering Series [Part4]: 여러가지 파일 포맷(JSON, BSON, Avro, Parquet)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part3]: 데이터 저장 서비스, 분석 엔진 서비스 선정하기",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series3",
      "date"     : "Apr 5, 2022",
      "content"  : "Table of Contents  참고참고  DB Engine 비교",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-05T21:01:35+09:00'>05 Apr 2022</time><a class='article__image' href='/data-engineering-series3'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part3]: 데이터 저장 서비스, 분석 엔진 서비스 선정하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series3'>Data Engineering Series [Part3]: 데이터 저장 서비스, 분석 엔진 서비스 선정하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part3]: 비트코인에 관한 기술적 접근",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series2",
      "date"     : "Apr 4, 2022",
      "content"  : "Table of Contents  해시 함수(비트코인 주소)  비대칭 암호화 기법(전자서명)  작업 증명  하드포크와 소프트포크  참고해시 함수(비트코인 주소)비대칭 암호화 기법(전자서명)작업 증명하드포크와 소프트포크참고  블록체인 해설서 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-04T21:01:35+09:00'>04 Apr 2022</time><a class='article__image' href='/blockchain-series2'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part3]: 비트코인에 관한 기술적 접근'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series2'>BlockChain Series [Part3]: 비트코인에 관한 기술적 접근</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part7]: Javascript 객체",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series7",
      "date"     : "Apr 3, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-03T21:01:35+09:00'>03 Apr 2022</time><a class='article__image' href='/javascript-series7'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part7]: Javascript 객체'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series7'>Javascript Series [Part7]: Javascript 객체</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series1",
      "date"     : "Apr 3, 2022",
      "content"  : "Table of Contents  블록체인 구조          노드      블록        블록체인 동작원리          트랜잭션 요청      블록 생성      검증        비트코인 생태계          블록체인      비트코인 시스템      비트코인 지갑(UTXO)      중개소        참고블록체인 구조노드노드는 일반적으로 어떤 네트워크에 속해있는 기기(컴퓨터, 스마트폰 등) 또는 기기를 사용하는 유저를 일컫습니다. 블록체인 네트워크에서 추구하는 목표는 신뢰할 수 있는 기관(정부, 은행, 핀테크 기업 등) 없이도 사용자(노드)들이 주체가 되어 경제 시스템을 동작하도록 하는 것입니다. 그렇기 때문에 블록체인 기반의 네트워크 구조에서는 노드들의 역할이 굉장히 중요합니다.아래 그림은 기존의 거래를 위한 네트워크 구조와 블록체인 기반의 구조를 비교한 것입니다.기존에 존재하던 네트워크의 경우 A가 B와 거래를 하기 위해서는 자기 자신의 거래내역만 가지고 있으면 됐었습니다. 이게 가능한 이유는 중앙의 신뢰할 수 있는 기관이 거래에서 발생할 수 있는 문제들을 중재하기 때문이었습니다. 하지만 블록체인 기반 네트워크는 이러한 기관을 없애는 것이 목표입니다. 그래서 블록체인에서는 거래내역의 위,변조를 막기 위해 네트워크내 모든 노드들에게 이러한 거래내역을 저장하도록 하고 데이터에 변화가 일어났는지 서로를 감시하도록 합니다.아래 그림은 모든 노드가 블록체인 데이터를 함께 저장(분산 저장이 아니라 중복 저장)하고 블록체인 데이터에 변화가 일어나는지 감시합니다. 이 부분에 대해서는 나중에 블록체인의 동작원리 파트에서 더 자세히 설명하도록 하겠습니다.그러면 지금까지 이렇게 저장된 블록의 용량은 얼마나 될까요? 블록체인 기반 서비스의 조상인 비트코인의 경우 2021년 9월 그동안 생성된 블록의 개수는 70만개를 넘어섰다고 합니다. 블록 하나의 최대 크기는 1MB이므로 평균 0.5MB라고 했을 때 대략 340GB입니다. 저희는 블록체인 기반의 거래 서비스(중 비트코인)을 이용하기 위해 최소한 340GB의 용량이 컴퓨터에 필요하다는 뜻입니다. 이러한 문제 때문에 비트코인의 경우 노드를 설계할 때 두가지의 다른 형태를 두었습니다. 하나는 완전 노드고, 다른 하나는 단순 지급 검증 노드입니다.  완전 노드          블록체인 데이터 전체 다운 (300GB 이상 디스크 공간 요구)      블록 검증 (검증에 대해서는 보상 없음) -&amp;gt; 사실상 인센티브 공학의 결함      블록 생성 (비트코인 보상) -&amp;gt; 채굴자        단순 지급 검증 노드          블록의 헤더 정도만 다운 (100MB 정도의 작은 공간)      트랜잭션을 요청하는 단순 서비스 이용자      블록블록은 마치 거래 장부에서 특정 페이지에 비유할 수 있습니다. 블록 하나에는 2000~3000개 정도의 트랜잭션(거래내역)을 담을 수 있는데 그 이유는 블록 하나의 크기는 1MB로 제한되는데 트랜잭션 하나의 크기가 보통 0.3kB정도이기 때문입니다.비트코인 블록의 구조는 크게 블록의 크기, 블록 헤더, 블록 내 거래내역 개수, 모든 거래내역 이렇게 4가지로 구성됩니다.  블록의 크기          4바이트      블록 하나의 크기 (최대 1메가바이트)        블록 헤더          80바이트      버전 정보, 이전 블록 헤더 해시, 머클 트리 루트, 타임 스탬프, 타깃 난이도 비트, 난스로 구성        블록 내 전체 거래내역 수          1~9바이트        블록 내 모든 트랜잭션          가변 크기      2000~3000개 정도 되는 트랜잭션 저장      블록체인 동작원리여기서는 트랜잭션을 요청한 후 어떻게 블록이 생성되고 그 보상으로 채굴자가 비트코인을 얻게 되는지에 관한 과정을 알아보도록 하겠습니다.트랜잭션 요청블록체인에서는 중앙 서버가 존재하지 않기 때문에 누가 트랜잭션을 처리할 것인지 정해져 있지 않습니다. 그렇기 때문에 블록체인에서는 기본적으로 트랜잭션을 모든 노드에게 브로드캐스팅합니다.이렇게 전달된 트랜잭션들은 각 노드의 대기실에 쌓인 채로 처리되기를 기다립니다. 또한 이 때 처리되는 트랜잭션의 목록과 순서는 노드마다 다릅니다. 노드 A는 a, b, c를 묶어서 처리하고 노드 B는 b, a, d를 처리하는 등 네트워크 상태와 트랜잭션에 책정된 수수료에 따라 달라집니다. 통상 노드는 수수료가 더 높은 트랜잭션을 먼저 처리합니다.블록 생성완전 노드 역할을 하는 노드들은 자신들이 가지고 있는 트랜잭션을 가지고 서로 비동기적으로 블록을 생성하기 위해 경쟁합니다. 노드들이 블록을 만들기 위해 경쟁을 한다고 했는데 어떤 것을 가지고 경쟁하고 있을까요? 그것은 바로 해시 퍼즐입니다. 해시 퍼즐은 공식이 없이 단순 반복을 통해서만 정답을 찾을 수 있습니다. 이러한 단순 반복을 통해 가장 먼저 정답을 찾은 하나의 노드만이 리더로 선출되고 블록을 만들 자격을 얻는 것입니다. 이렇게 블록을 만들기 위해 해시 퍼즐을 푸는 과정을 채굴한다고 합니다. 또한 이렇게 해시 퍼즐을 통해 리더를 선출하는 방법을 작업 증명 방식이라고 합니다. 이외에도 지분 증명이라는 것이 있지만 여기서는 다루지 않도록 하겠습니다.검증위의 그림에서 살펴봤듯이 노드들은 리더가 만든 블록을 자신의 블록체인 데이터에 추가하기 전에 두 가지를 점검합니다. 먼저 리더가 해시 퍼즐의 정답을 제대로 찾았는지 확인하고, 두 번째로 리더가 추가한 트랜잭션에 문제(위조, 변조, 이중 사용 등)가 없는지 검증하는 절차를 가집니다. 이 두 검증 과정은 해시 함수와 전자 서명, 비대칭 암호화 기법을 활용해 순식간에 이뤄집니다.비트코인 생태계블록체인비트코인 시스템비트코인 지갑(UTXO)중개소참고  블록체인 해설서 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-03T21:01:35+09:00'>03 Apr 2022</time><a class='article__image' href='/blockchain-series1'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series1'>BlockChain Series [Part2]: 블록체인의 동작 원리(feat.비트코인)</a> </h2><p class='article__excerpt'>블록체인 네트워크에서 추구하는 목표는 신뢰할 수 있는 기관 없이도 사용자들이 주체가 되어 경제 시스템을 동작하도록 하는 것입니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part6]: Javascript 함수와 스코프",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series6",
      "date"     : "Apr 2, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-02T21:01:35+09:00'>02 Apr 2022</time><a class='article__image' href='/javascript-series6'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part6]: Javascript 함수와 스코프'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series6'>Javascript Series [Part6]: Javascript 함수와 스코프</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part2]: 하둡 생태계(Hadoop Ecosystem) 살펴보기",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series2",
      "date"     : "Apr 2, 2022",
      "content"  : "Table of Contents  Hadoop  HDFS  Hive  HBase  Cassandra  참고HadoopHDFSHiveHBaseCassandra참고  Hoing: 하둡 프로그래밍(2) – 빅데이터 - 하둡 에코시스템",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-02T21:01:35+09:00'>02 Apr 2022</time><a class='article__image' href='/data-engineering-series2'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part2]: 하둡 생태계(Hadoop Ecosystem) 살펴보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series2'>Data Engineering Series [Part2]: 하둡 생태계(Hadoop Ecosystem) 살펴보기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part5]: Javascript 제어문과 반복문",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series5",
      "date"     : "Apr 1, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-01T21:01:35+09:00'>01 Apr 2022</time><a class='article__image' href='/javascript-series5'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part5]: Javascript 제어문과 반복문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series5'>Javascript Series [Part5]: Javascript 제어문과 반복문</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse",
      "category" : "",
      "tags"     : "Data_Engineering",
      "url"      : "/data-engineering-series1",
      "date"     : "Apr 1, 2022",
      "content"  : "Table of Contents  DataBase  Data Warehouse  Data Lake  Data Lakehouse  Summary  참고DataBaseA database is a collection of data or information. Databases are typically accessed electronically and are used to support Online Transaction Processing (OLTP). Database Management Systems (DBMS) store data in the database and enable users and applications to interact with the data. The term “database” is commonly used to reference both the database itself as well as the DBMS.A variety of database types have emerged over the last several decades. All databases store information, but each database will have its own characteristics. Relational databases store data in tables with fixed rows and columns. Non-relational databases (also known as NoSQL databases) store data in a variety of models including JSON (JavaScript Object Notation), BSON (Binary JSON), key-value pairs, tables with rows and dynamic columns, and nodes and edges. Databases store structured and/or semi-structured data, depending on the type.You may also find database characteristics like:  Security features to ensure the data can only be accessed by authorized users.  ACID (Atomicity, Consistency, Isolation, Durability) transactions to ensure data integrity.  Query languages and APIs to easily interact with the data in the database.  Indexes to optimize query performance.  Full-text search.  Optimizations for mobile devices.  Flexible deployment topologies to isolate workloads (e.g., analytics workloads) to a specific - set of resources.  On-premises, private cloud, public cloud, hybrid cloud, and/or multi-cloud hosting options.If your application needs to store data (and nearly every interactive application does), your application needs a database. Applications across industries and use cases are built on databases. Many types of data can be stored in databases, including:  Patient medical records  Items in an online store  Financial records  Articles and blog entries  Sports scores and statistics  Online gaming information  Student grades and scores  IoT device readings  Mobile application informationDatabase examples  Relational databases: Oracle, MySQL, Microsoft SQL Server, and PostgreSQL  Document databases: MongoDB and CouchDB  Key-value databases: Redis and DynamoDB  Wide-column stores: Cassandra and HBase  Graph databases: Neo4j and Amazon NeptuneData Warehouse데이터 웨어하우스는 정보에 입각한 의사 결정을 내릴 수 있도록 분석 가능한 정보의 중앙 repository다트랜잭션 시스템, RDB 및 기타 소스의 데이터들이 데이터웨어하우스에 들어간다(insert)이렇게 데이터웨어하우스에 있는 데이터들을 Business Analyst나 Data Scientist 와 같은 사람들이 BI(Business Inteligence)나 SQL 등을 이용해서 데이터에 액세스 한다하단 티어 즉 최하위에 있는 부분은 Data 부분인 위에서 말했던 트랜잭션 시스템, RDB 등을 비롯한 데이터들이고 이것들이 ETL(Extract Transform Load) 과정을 거쳐서 Data Warehouse에 적재가 된다중간 티어 OLAP Server는 데이터에 액세스하고 분석하는데 사용되는 분석엔진 들이다, 여기서 OLAP란 OnLine Analytical Processing을 말한다.상위 티어는 사용자가 실제로 데이터를 분석하고 마이닝을 하고 또 보고할 때 사용하게 되는 frontend가 존재하는 티어다.이렇게 이루어진 데이터 웨어하우스는 데이터를 정수, 데이터 필드 또는 문자열과 같은 레이아웃 및 유형들을 설명하는 스키마로 구성함으로써 동작하게 된다. 즉 ETL, 데이터를 추출해서 변환해서 스키마에 적재해두는 것이다A data warehouse is a unified data repository for storing large amounts of information from multiple sources within an organization. A data warehouse represents a single source of “data truth” in an organization and serves as a core reporting and business analytics component.Typically, data warehouses store historical data by combining relational data sets from multiple sources, including application, business, and transactional data. Data warehouses extract data from multiple sources and transform and clean the data before loading it into the warehousing system to serve as a single source of data truth. Organizations invest in data warehouses because of their ability to quickly deliver business insights from across the organization.Data warehouses enable business analysts, data engineers, and decision-makers to access data via BI tools, SQL clients, and other less advanced (i.e., non-data science) analytics applications.Note that data warehouses are not intended to satisfy the transaction and concurrency needs of an application. If an organization determines they will benefit from a data warehouse, they will need a separate database or databases to power their daily operations.  Amazon Redshift.  Google BigQuery.  IBM Db2 Warehouse.  Microsoft Azure Synapse.  Oracle Autonomous Data Warehouse.  Snowflake. (YOUR DATA WAREHOUSE AND DATA LAKE)  Teradata Vantage.Data Lake데이터 레이크는 데이터 웨어하우스와는 달리 별도로 정형화나 정규화 등을 하지 않고 데이터를 있는 그대로 원시데이터 상태를 저장한다는 것이다.데이터 레이크로 유입되는 데이터들은 자신의 출처와 시간 같은 메타데이터가 존재하여야 한다.데이터 레이크는 그 크기가 매우 커질것이고 대부분의 저장소는 스키마가 없는 큰 규모의 구조를 지향하기 때문에 일반적으로 데이터 레이크를 구현을 할 때 Hadoop과 HDFS를 비롯한 에코시스템을 사용하는 것이다. (?)A data lake is a centralized, highly flexible storage repository that stores large amounts of structured and unstructured data in its raw, original, and unformatted form. In contrast to data warehouses, which store already “cleaned” relational data, a data lake stores data using a flat architecture and object storage in its raw form. Data lakes are flexible, durable, and cost-effective and enable organizations to gain advanced insight from unstructured data, unlike data warehouses that struggle with data in this format.In data lakes, the schema or data is not defined when data is captured; instead, data is extracted, loaded, and transformed (ELT) for analysis purposes. Data lakes allow for machine learning and predictive analytics using tools for various data types from IoT devices, social media, and streaming data.A data lake is a repository of data from disparate sources that is stored in its original, raw format. Like data warehouses, data lakes store large amounts of current and historical data. What sets data lakes apart is their ability to store data in a variety of formats including JSON, BSON, CSV, TSV, Avro, ORC, and Parquet.Typically, the primary purpose of a data lake is to analyze the data to gain insights. However, organizations sometimes use data lakes simply for their cheap storage with the idea that the data may be used for analytics in the future.You might be wondering, “Is a data lake a database?” A data lake is a repository for data stored in a variety of ways including databases. With modern tools and technologies, a data lake can also form the storage layer of a database. Tools like Starburst, Presto, Dremio, and Atlas Data Lake can give a database-like view into the data stored in your data lake. In many cases, these tools can power the same analytical workloads as a data warehouse.(데이터 레이크는 데이터 분석 단계에서 사용할 목적으로 수집한 저장소는 아니다. 하지만 Presto 같은 걸 사용하면 데이터 레이크에서도 데이터 웨어하우스나 데이터베이스에서 처럼 데이터 분석을 할 수 있다)Data lakes store large amounts of structured, semi-structured, and unstructured data. They can contain everything from relational data to JSON documents to PDFs to audio files.Data does not need to be transformed in order to be added to the data lake, which means data can be added (or “ingested”) incredibly efficiently without upfront planning.The primary users of a data lake can vary based on the structure of the data. Business analysts will be able to gain insights when the data is more structured. When the data is more unstructured, data analysis will likely require the expertise of developers, data scientists, or data engineers.The flexible nature of data lakes enables business analysts and data scientists to look for unexpected patterns and insights. The raw nature of the data combined with its volume allows users to solve problems they may not have been aware of when they initially configured the data lake.Data in data lakes can be processed with a variety of OLAP systems and visualized with BI tools.Data lakes are a cost-effective way to store huge amounts of data. Use a data lake when you want to gain insights into your current and historical data in its raw form without having to transform and move it. Data lakes also support machine learning and predictive analytics.Like data warehouses, data lakes are not intended to satisfy the transaction and concurrency needs of an application.  AWS S3  Azure Data Lake Storage Gen2  Google Cloud StorageOther technologies enable organizing and querying data in data lakes, including:  MongoDB Atlas Data Lake.  AWS Athena.  Presto.  Starburst.  Databricks SQL Analytics.Data LakehouseA data lakehouse is a new, big-data storage architecture that combines the best features of both data warehouses and data lakes. A data lakehouse enables a single repository for all your data (structured, semi-structured, and unstructured) while enabling best-in-class machine learning, business intelligence, and streaming capabilities.Data lakehouses usually start as data lakes containing all data types; the data is then converted to Delta Lake format (an open-source storage layer that brings reliability to data lakes). Delta lakes enable ACID transactional processes from traditional data warehouses on data lakes.SummaryDatabases, data warehouses, and data lakes are all used to store data. So what’s the difference?The key differences between a database, a data warehouse, and a data lake are that:  A database stores the current data required to power an application.  A data warehouse stores current and historical data from one or more systems in a predefined and fixed schema, which allows business analysts and data scientists to easily analyze the data.  A data lake stores current and historical data from one or more systems in its raw form, which allows business analysts and data scientists to easily analyze the data.                   Database      Data Warehouse      Data Lake              Workloads      Operational and transactional      Analytical      Analytical              Data Type      Structured or semi-structured      Structured and/or semi-structured      Structured, semi-structured, and/or unstructured              Schema Flexibility      Rigid or flexible schema depending on database type      Pre-defined and fixed schema definition for ingest (schema on write and read)      No schema definition required for ingest (schema on read)              Data Freshness      Real time      May not be up-to-date based on frequency of ETL processes      May not be up-to-date based on frequency of ETL processes              Users      Application developers      Business analysts and data scientists      Business analysts, application developers, and data scientists              Pros      Fast queries for storing and updating data      The fixed schema makes working with the data easy for business analysts      Easy data storage simplifies ingesting raw data. A schema is applied afterwards to make working with the data easy for business analysts. Separate storage and compute              Cons      May have limited analytics capabilities      Difficult to design and evolve schema. Scaling compute may require unnecessary scaling of storage, because they are tightly coupled      Requires effort to organize and prepare data for use      참고  MongoDB 공식문서: Databases vs. Data Warehouses vs. Data Lakes (추천)  striim: Data Warehouse vs. Data Lake vs. Data Lakehouse  EDUCBA: Redis vs MongoDB  EDUCBA: Data Warehouse vs Data Lake  비투엔: Data Warehouse vs Data Lake",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-04-01T21:01:35+09:00'>01 Apr 2022</time><a class='article__image' href='/data-engineering-series1'> <img src='/images/data_engineering_logo.png' alt='Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/data-engineering-series1'>Data Engineering Series [Part1]: Database, Warehouse, Lake, Lakehouse</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series3",
      "date"     : "Mar 11, 2022",
      "content"  : "Table of Contents  참고참고  nanjangpan.log: Terraform 생존기(1) - 테라폼이란?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-11T21:01:35+09:00'>11 Mar 2022</time><a class='article__image' href='/terraform_series3'> <img src='/images/terraform_logo.jpg' alt='Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series3'>Terraform Series [Part3]: 테라폼 명령어를 통한 간단한 실습</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series2",
      "date"     : "Mar 10, 2022",
      "content"  : "Table of Contents  HCL 코드  Block          Provider      Resource      Data      Variables      Output      Locals      Module        참고HCL 코드이번 포스트에서는 인프라 관리를 위한 테라폼 코드를 작성하기 위해 알아두면 좋은 몇 가지 구성 요소에 대해 알아보도록 하겠습니다.테라폼 언어는 선언적입니다. 또한 각각의 코드가 목표에 도달하기 위한 단계를 의미하는 것이 아니기 때문에 순서는 크게 중요하지 않습니다. 테라폼은 오직 resource들의 관계만을 고려해 실행 순서를 정합니다.테라폼 코드는 큰 틀에서 몇 가지 Block으로 이루어져 있습니다.&amp;lt;BLOCK TYPE&amp;gt; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; {  # Block body  &amp;lt;IDENTIFIER&amp;gt; = &amp;lt;EXPRESSION&amp;gt; # Argument}resource &quot;aws_vpc&quot; &quot;main&quot; {  cidr_block = var.base_cidr_block}Block블록은 resource와 같은 오브젝트를 나타내기 위해 필요한 설정값을 담고 있습니다. 블록의 종류에는 크게 provider,  resource, data, variables, output, locals, module이 있습니다. 테라폼 코드는 이러한 블록들로 구성되어 있다고 생각해도 무방합니다.Provider  인프라를 제공해주는 주체가 누구인지 설정하는 블럭  예: Local, AWS, Azure, GCP 등  provider를 설정하고 나면 해당 provider 플러그인을 설치하고 필요한 API를 사용할 수 있음  https://registry.terraform.io/browse/providers 참고provider &quot;aws&quot; {  region     = &quot;us-west-2&quot;  access_key = &quot;my-access-key&quot;  secret_key = &quot;my-secret-key&quot;}Resource  테라폼 코드에서 가장 중요한 블럭  각각의 resource 블럭은 인프라스트럭처 오브젝트를 나타냄(컴퓨팅, 스토리지, 네트워크 등)  각각의 resource 종류마다 설정하는 인자값 다르므로 공식 문서 참고          예: aws_network_interface의 경우 subnet_id, private_ips 등이 있고 aws_instance에는 ami, instance_type 등이 있음        resource의 속성값을 다른 resource에서 참조할 수도 있음          예: network_interface_id = aws_network_interface.foo.id      resource &quot;aws_network_interface&quot; &quot;foo&quot; {  subnet_id   = aws_subnet.my_subnet.id  private_ips = [&quot;172.16.10.100&quot;]  tags = {    Name = &quot;primary_network_interface&quot;  }}resource &quot;aws_instance&quot; &quot;foo&quot; {  ami           = &quot;ami-005e54dee72cc1d00&quot; # us-west-2  instance_type = &quot;t2.micro&quot;  network_interface {    network_interface_id = aws_network_interface.foo.id    device_index         = 0  }  credit_specification {    cpu_credits = &quot;unlimited&quot;  }}Data  테라폼 코드 외부에서 설정된 값을 코드로 가져오고 싶은 경우  아래의 예시와 같이 filter, most_recent 와 같은 인자값은 data source마다 달라서 문서 참조해야함                예를 들어 아래와 같이 aws_ami source를 사용하는 경우 다음의 공식문서 참고        resource 블럭과 같은 곳에서 사용하고자 할 때는 data.\&amp;lt;data source&amp;gt;.\&amp;lt;name&amp;gt;.\&amp;lt;attribute&amp;gt;          예: data.aws_ami.web.id      data source별로 속성도 당연히 다르다 공식문서 참고      # AWS AMI 중 state가 available 이고 Component 태그가 web인 것 중 가장 최근 AMIdata &quot;aws_ami&quot; &quot;web&quot; {  # filter에 어떤 설정할 수 있는지는 https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-images.html 참고 (테라폼 공식문서에 링크 다 명시해놨음)  filter {    name   = &quot;state&quot;    values = [&quot;available&quot;]  }  filter {    name   = &quot;tag:Component&quot;    values = [&quot;web&quot;]  }  most_recent = true}# 필터링 거쳐서 얻은 AMI의 id를 사용한다resource &quot;aws_instance&quot; &quot;web&quot; {  ami           = data.aws_ami.web.id  instance_type = &quot;t1.micro&quot;}Variables  Variables는 테라폼 코드를 실행할 때에 파라미터로 값을 동적으로 넘겨줄 수 있도록 함  인자값에는 default, type, description, validation, sensitive, nullable가 있음 (공식문서 참고)  값을 넘겨주는 방법은 다음과 같음          테라폼 명령어 사용시 -var 옵션 사용하기                  terraform apply -var=”image_id=ami-abc123”          terraform apply -var=’image_id_list=[“ami-abc123”,”ami-def456”]’ -var=”instance_type=t2.micro”          apply -var=’image_id_map={“us-east-1”:”ami-abc123”,”us-east-2”:”ami-def456”}’                    .tfvars 파일에 정의하기        # testing.tfvarsimage_id = &quot;ami-abc123&quot;availability_zone_names = [&quot;us-east-1a&quot;,&quot;us-west-1c&quot;,]                # CLIterraform apply -var-file=&quot;testing.tfvars&quot;                    환경변수로 설정하기        export TF_VAR_image_id=ami-abc123                    variable &quot;user_information&quot; {  type = object({    name    = string    address = string  })  sensitive = true}resource &quot;some_resource&quot; &quot;a&quot; {  name    = var.user_information.name  address = var.user_information.address}Output  프로그래밍 언어에서 return과 비슷한 역할을 하는 블럭  그냥 print하는 정도의 역할인가?  필수 인자값에는 value가 있고, 옵셔널 인자값에는 description, sensitive, depends_on이 있다output &quot;instance_ip_addr&quot; {  value = aws_instance.server.private_ip}Locals  반복적인 표현을 줄이고자 할 때 유용한 블럭locals {  service_name = &quot;forum&quot;  owner        = &quot;Community Team&quot;}locals {  # Common tags to be assigned to all resources  common_tags = {    Service = local.service_name    Owner   = local.owner  }}resource &quot;aws_instance&quot; &quot;example&quot; {  # ...  tags = local.common_tags}Module  여러 개의 resource의 묶음  반복적으로 함께 사용되는 resource들을 묶어서 사용할 수 있음  사용자들이 미리 만들어 공유해놓은 Module들이 있다 (공식문서 참고)# AWS VPC modulemodule &quot;vpc&quot; {  # &amp;lt;NAMESPACE&amp;gt;/&amp;lt;NAME&amp;gt;/&amp;lt;PROVIDER&amp;gt;  source = &quot;terraform-aws-modules/vpc/aws&quot;  name = &quot;my-vpc&quot;  cidr = &quot;10.0.0.0/16&quot;  azs             = [&quot;eu-west-1a&quot;, &quot;eu-west-1b&quot;, &quot;eu-west-1c&quot;]  private_subnets = [&quot;10.0.1.0/24&quot;, &quot;10.0.2.0/24&quot;, &quot;10.0.3.0/24&quot;]  public_subnets  = [&quot;10.0.101.0/24&quot;, &quot;10.0.102.0/24&quot;, &quot;10.0.103.0/24&quot;]  enable_nat_gateway = true  enable_vpn_gateway = true  tags = {    Terraform = &quot;true&quot;    Environment = &quot;dev&quot;  }}참고  Terraform 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-10T21:01:35+09:00'>10 Mar 2022</time><a class='article__image' href='/terraform_series2'> <img src='/images/tf_3.png' alt='Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series2'>Terraform Series [Part2]: HCL 코드를 이용한 인프라 관리</a> </h2><p class='article__excerpt'>이번 포스트에서는 인프라 관리를 위한 테라폼 코드를 작성하기 위해 알아두면 좋은 몇 가지 구성 요소에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part4]: Javascript 데이터 타입과 연산자",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series4",
      "date"     : "Mar 5, 2022",
      "content"  : "Table of Contents  데이터 타입          숫자 타입      문자열 타입      템플릿 리터럴      undefined 타입      심벌 타입        연산자          산술 연산자      비교 연산자      삼항 조건 연산자        논리 연산자데이터 타입ES6 기준으로 자바스크립트에서는 7개의 데이터 타입을 제공한다.            구분      데이터 타입      설명              원시 타입      숫자      숫자, 정수와 실수의 구분 없음                     문자열      문자열                     불리언      논리적 참과 거짓                     undefined      var 키워드로 선언된 변수에 암묵적으로 할당되는 값                     null      값이 없다는 것을 의도적으로 명시할 때 사용되는 값                     symbol      ES6에서 추가된 타입              객체 타입             객체, 함수, 배열 등      숫자 타입자바스크립트는 독특하게 하나의 숫자 타입만 존재한다.ECMAScript 사양에 따르면 숫자 타입의 값은 배정밀도(double precision) 64비트 부동소수점 형식을 따른다.정수, 실수, 2진수, 8진수, 16진수 리터럴은 모두 메모리에 배정밀도 64비트 부동소수점 형식의 2진수로 저장된다. 자바스크립트는 2진수, 8진수, 16진수를 표현하기 위한 데이터 타입을 제공하지 않기 때문에 이들 값을 참조하면 모둗 10진수로 해석된다.var binary = 0b01000001;var octal = 0o101;var hex = 0x41;console.log(binary); // 65console.log(octal); // 65console.log(hex); // 65문자열 타입문자열 타입은 텍스트 데이터를 나타내는 데 사용한다. 문자열은 0개 이상의 16비트 유니코드 문자(UTF-16)의 집합으로 전 세계 대부분의 문자를 표현할 수 있다. 자바스크립트의 문자열은 원시타입이며, 변경 불가능한 값이다.문자열은 작은따옴표(‘’), 큰따옴표(“”) 또는 백틱(``)으로 텍스트를 감싼다. 가장 일반적인 표기법은 작은따옴표를 사용하는 것이다. 따옴표로 감싸는 이유는 키워드나 식별자 같은 토큰과 구분하기 위해서다.var string;string = &#39;apple&#39;;string = &quot;apple&quot;;string = `apple`;템플릿 리터럴ES6부터 템플릿 리터럴이라고 하는 새로운 문자열 표기법이 도입되었다. 템플릿 리터럴은 멀티라인 문자열, 표현식 삽입 등 편리한 문자열 처리 기능을 제공한다. 템플릿 리터럴은 런타임에 일반 문자열로 변환되어 처리된다.템플릿 리터럴은 백틱(``)을 사용해 표현한다.// 멀티라인 문자열var html_code = `&amp;lt;ul&amp;gt;    &amp;lt;li&amp;gt;&amp;lt;a href=&quot;#&quot;&amp;gt;Home&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&amp;lt;/ul&amp;gt;`;// 표현식 삽입var first = &#39;Ung-mo&#39;;var last = &#39;Lee&#39;;console.log(&#39;My name is &#39; + first + &#39; &#39; + last + &#39;.&#39;); // ES5console.log(`My name is ${first} ${last}.`); // ES6undefined 타입var 키워드로 선언한 변수는 암묵적으로 undefined로 초기화된다. 다시 말해, 변수 선언에 의해 확보된 메모리 공간을 처음 할당이 이뤄질 때까지 자바스크립트 엔진이 undeefined로 초기화한다. 변수를 참조했을 때 undefined가 반환된다면 참조한 변수가 선언 이후 값이 할당된 적이 없는 변수라는 것을 간파할 수 있다.만약 undefined를 개발자가 의도적으로 변수에 할당한다면 undefined의 본래 취지와 어긋날뿐더러 혼란을 줄 수 있으므로 권장하지 않는다.변수에 값이 없다는 것을 명시하고 싶을 때는 null을 할당하는 것이 좋다.var foo;console.log(foo); // undefined심벌 타입심벌은 ES6에서 추가된 7번째 타입으로, 변경 불가능한 원시 타입의 값이다. 심벌 값은 다른 값과 중복되지 않는 유일무이한 값이다.다른 원시 값은 리터럴을 통해 생성하지만 심벌은 Symbol 함수를 호출해 생성한다. 이때 생성된 심벌 값은 외부에 노출되지 않으며, 다른 값과 절대 중복되지 않는다.var key = Symbol(&#39;key&#39;); // 심벌 생성var obj = {}; // 객체 생성obj[key] = &#39;value&#39;;// 유일한 값을 가지는 심벌을 프로퍼티 키로 사용한다console.log(obj[key]) // value연산자산술 연산자            단항 산술 연산자      ++, –, +, -              이항 산술 연산자      +, -, *, /, %      ++, -- 연산자는 위치에 의미가 있다.var x = 5, result;result = x++; // 선할당 후증가console.log(result, x); // 5 6var x = 5, result;result = ++x; // 선증가 후할당console.log(result, x); // 6 6비교 연산자            비교 연산자      의미      설명              ==      동등 비교      x와 y의 값이 같음              ===      일치 비교      x와 y의 값과 타입이 같음              !=      부동등 비교      x와 y의 값이 다름              !==      불일치 비교      x와 y의 값과 타입이 다름      동등 비교 연산자(==)는 좌항과 우항의 피연산자를 비교할 때 먼저 암묵적으로 타입 변환을 통해 타입을 일치시킨 후 같은 값인지 비교한다.5 == &#39;5&#39;; // true5 === &#39;5&#39;; // false그래서 동등 비교 연산자는 예측하기 어려운 결과를 만들어낸다. 따라서 동등 비교 연산자보다는 일치 비교 연산자를 권장한다.삼항 조건 연산자조건식 ? 조건식이 true일 때 반환할 값 : false일 때 반환할 값var x = 2;var result = x % 2 ? &#39;홀수&#39; : &#39;짝수&#39;;console.log(result); // 짝수삼항 조건 연산자 표현식은 값처럼 사용할 수 있다.논리 연산자            논리 연산자      의미              ||      논리합(OR)              &amp;amp;&amp;amp;      논리곱(AND)              !      부정(NOT)      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-05T21:01:35+09:00'>05 Mar 2022</time><a class='article__image' href='/javascript-series4'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part4]: Javascript 데이터 타입과 연산자'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series4'>Javascript Series [Part4]: Javascript 데이터 타입과 연산자</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part5]: AWS Storage Service",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series5",
      "date"     : "Mar 5, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-05T21:01:35+09:00'>05 Mar 2022</time><a class='article__image' href='/aws-series5'> <img src='/images/aws_logo.png' alt='AWS Series [Part5]: AWS Storage Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series5'>AWS Series [Part5]: AWS Storage Service</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series4",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/jenkins_series4'> <img src='/images/jenkins_logo.png' alt='Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series4'>Jenkins Series [Part4]: Jenkins를 이용한 간단한 실습</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part4]: AWS Network Service CloudFront",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series4",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/aws-series4'> <img src='/images/aws_logo.png' alt='AWS Series [Part4]: AWS Network Service CloudFront'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series4'>AWS Series [Part4]: AWS Network Service CloudFront</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series3",
      "date"     : "Mar 4, 2022",
      "content"  : "Table of Contents  1. VPC란          Why VPC?      VPC의 특징      VPC의 구성 요소                  1) IP          2) 서브넷          3) 라우팅 테이블                    VPC의 주요 서비스                  1) Security Group          2) NACL(Network Access Control List)          예시                          Security Group              NACL                                3) IGW (Internet Gateway)          4) NAT (Newtork Address Translation)          5) Bastion host          6) Endpoint                      2. VPC 실습          VPC 생성      서브넷 생성      IGW 생성      라우팅 테이블 설정      NACL 설정      Bastion host 생성      NAT gateway 생성      Endpoint 생성      1. VPC란Amazon VPC는 AWS 클라우드에서 논리적으로 격리된 네트워크 공간을 할당하여 줌으로써 가상 네트워크 내에서 AWS 리소스를 이용할 수 있도록 하는 서비스입니다.Why VPC?  네트워킹 환경을 완벽하게 제어할 수 있다  보안성 높은 클라우드 환경을 구현할 수 있다VPC의 특징  VPC 자체 IP주소 범위, 서브넷, 라우팅 테이블, 게이트웨이 등 네트워킹 환경 제어 가능  VPC는 하나의 Region에만 속할 수 있음(다른 Region으로 확장 불가능)  기존 데이터 센터와의 연결을 통해 하이브리드 환경 구성 가능VPC의 구성 요소1) IP  Public IP: 인터넷을 통해 연결할 수 있는 IP주소  Private IP: 인터넷과 연결되지 않은, VPC 내부에서만 사용할 수 있는 IP주소  Elastic IP: 동적 컴퓨팅을 위해 고안된 고정 Public IP주소2) 서브넷  서비스 목적에 따라 VPC 내부의 네트워크를 IP Block으로 나누기도 하는데 이 때의 IP Block의 모음을 서브넷(Subnet)이라고 합니다  리전(Region)내의 가용 영역(Availability Zone)들은 여러 개의 Subnet을 가질 수 있다  하지만 Subnet이 여러 가용 영역에 확장될 수는 없다.  Public 서브넷: 인터넷 게이트웨이로 라우팅이 되는 서브넷, Public IP나 Elastic IP 주소가 필요  Private 서브넷: 인터넷 게이트웨이로 라우팅 되지 않는 서브넷, 높은 보안성을 필요로 하는 DB(DataBase) 서버 같은 경우 Private 서브넷에 주로 생성  서브넷은 CIDR 블록을 통해 분리/분배할 수 있다          1번째 서브넷 : 211.11.124.0/26 (211.11.124.0 ~ 211.11.124.63)      2번째 서브넷 : 211.11.124.64/26 (211.11.124.64 ~ 211.11.124.127)      3번째 서브넷 : 211.11.124.128/26 (211.11.124.128 ~ 211.11.124.191)      4번째 서브넷 : 211.11.124.192/26 (211.11.124.192 ~ 211.11.124.255)      (26은 IP주소 32자리 중 26자리는 고정, 뒤의 6자리로만 IP주소 구성 -&amp;gt; 2의 6승 -&amp;gt; 하나의 서브넷에 64개의 IP주소가 존재)3) 라우팅 테이블  서브넷 외부로 나가는 아웃바운드 트래픽에 대해 허용된 경로를 알려주는 나침반  서브넷 간의 원활한 통신을 위해 라우팅 테이블을 이용VPC의 주요 서비스1) Security Group  IP와 Port를 기준으로 특정 트래픽만을 허용  Stateful 검문소2) NACL(Network Access Control List)  IP와 Port를 기준으로 특정 트래픽을 허용하거나 거부  Stateless 검문소예시Security Group(Security Group은 Stateful해서 Outbound를 none으로 해도 1025포트로는 내보낼 수 있다)NACL(NACL은 Stateless해서 Outbound를 none으로 하면 1025포트로 내보낼 수 없다)3) IGW (Internet Gateway)  인터넷으로 나가는 통로4) NAT (Newtork Address Translation)  IP Address를 Translation해주는 서비스  나의 Private IP는 알려주고 싶지 않고, 외부 인터넷과는 통신을 하고 싶을 때  Private subnet 안에 있는 private instance가 외부의 인터넷과 통신하기 위한 방법  Private instance가 외부 인터넷과 통신을 하고 싶을 때, Private subnet의 Route Table이 IGW로 라우팅하지 않고, Public subnet에 있는 NAT gateway로 라우팅, NAT gateway에서 IP주소 변환해 IGW로 이동해 외부 인터넷과 통신5) Bastion host  관리자가 private subnet에 접근하고 싶을 때 가능하도록 해주는 서비스  NAT와 반대의 느낌  Public subnet에 있는 EC2를 Bastion host로 사용한다6) Endpoint  AWS의 여러 서비스(S3,dynamodb, ..)들과 VPC를 연결시켜주는 중간 매개체  AWS에서 VPC 바깥으로 트래픽이 나가지 않고 AWS의 여러 서비스를 사용하게끔 만들어주는 서비스  Interface Endpoint : Private ip를 만들어 서비스로 연결해줌(SQS, SNS, Kinesis, Sagemaker 등 지원)  Gateway Endpoint : 라우팅 테이블에서 경로의 대상으로 지정하여 사용(S3, Dynamodb 지원)2. VPC 실습VPC 생성(VPC를 생성하면 기본적인 라우팅 테이블과 NACL을 만들어준다)서브넷 생성IGW 생성라우팅 테이블 설정(VPC가 생성될 때 만들어진 라우팅 테이블이 있지만, 서브넷과 연결된 라우팅 테이블은 없다)(각 서브넷에 라우팅 테이블이 연결되었다)(10.0.0.0/16 이외의 IP주소는 모두 IGW로 보낸다, 순서유효)NACL 설정(인바운드, 아웃바운드의 규칙은 순서가 유효하다)(Public 서브넷에 EC2를 만들어 보았다)Bastion host 생성NAT gateway 생성Endpoint 생성",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-04T21:01:35+09:00'>04 Mar 2022</time><a class='article__image' href='/aws-series3'> <img src='/images/aws_logo.png' alt='AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series3'>AWS Series [Part3]: AWS Network Service VPC(Virtual Private Cloud)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series3",
      "date"     : "Mar 3, 2022",
      "content"  : "Table of Contents  Jenkinsfile  Jenkinsfile의 구조          Sections                  agent          stages          stage          steps          post                    Directives                  environment          options          parameters          triggers          tools          when                      참고JenkinsfileJenkinsfile은 파이프라인을 만들기 위해 작성하는 코드입니다. 이 때 Jenkinsfile을 작성하는 방법에는 두 가지가 있습니다.  Scripted Pipeline2016년까지 Jenkins 코드는 전체적인 틀을 위해서만 Jenkins DSL(Domain-Specific Language)가 약간 사용되었을 뿐 실질적인 프로그램의 흐름은 그루비(Groovy) 언어에 의해 작성되었습니다. 그래서 Jenkinsfile을 작성하기 위해 Jenkins DSL뿐만 아니라 그루비 언어까지 배워야 했습니다. 이렇게 작성된 코드가 바로 Scripted Pipeline입니다.이 때까지는 파이프라인에서 빌드후처리, 파이프라인 에러 상태확인, 알림기능 등 Jenkins에 특화된 내용은 없고 대부분 그루비의 try-catch-finally 구조로 구현해야 했습니다.  Declarative Pipeline그러다 2017년부터 클라우드 비스(CloudBees)에서 선언적 파이프라인을 개발하였습니다. 이 문법을 통하여 파이프라인을 심플하고 독자적인 방법으로 작성할 수 있게 되었습니다.Jenkinsfile의 구조모든 선언적 파이프라인은 반드시 pipeline 블록으로 감싸야 합니다. 그리고 그 안에 Sections과 Directives(지침)로 구성되어 있습니다.SectionsSections에는 agent, stages, steps, post가 있습니다. (stage는 Jenkins DSL에서 Directives로 분류하고 있습니다. 하지만 저는 Sections로 분류하는 것이 편해 여기서 stage도 함께 소개하겠습니다. 공식문서 참고)pipeline {    agent {}    stages {        stage {            steps {}        }    }    post {}}agentagent는 파이프라인 혹은 스테이지를 어디서 실행할지를(파일 경로, 컨테이너 등) 의미합니다.            필수      Yes              값      any, none, label, node, docker, dockerfile, kubernetes              위치      pipeline block, stage block      (공식문서 참고)stagesstage 섹션을 묶는 블럭입니다.            필수      Yes              값      stage 블럭              위치      pipeline block      stagestage는 공식문서에서 Directives(지침)으로 분류하는 블럭이며 파이프라인 내에서 구분하고 싶은 각각의 단계(예를 들어 Build, Test, Deploy)를 나타냅니다.            필수      Yes              파라미터      필수 파라미터: 각 stage의 이름              위치      stages block      stepssteps는 각각의 stage 안에서 실행될 것들을 묶는 블럭입니다.            필수      Yes              값      Basic step: echo, sleep, timeout, AWS step: s3Download, ecrDeleteImage 등              위치      stage block      postpost는 pipeline 또는 각각의 stage가 실행된 후 조건에 따라 실행되는 블록입니다.            필수      No              조건      always, changed, fixed, regression, aborted, failure, success, unstable, unsuccessful, cleanup              위치      pipeline block, stage block      (공식문서 참고)Directivesenvironment환경변수를 지정하기 위한 키-밸류 쌍입니다.            필수      None              값      key-value pairs              위치      pipeline block, stage block      options파이프라인 또는 각 스테이지별로 옵션을 제공합니다. 다양한 종류의 옵션이 있습니다.  newContainerPerStage: docker, dockerfile agent 사용시 가능한 옵션. 각 스테이지마다 새로운 컨테이너 생성  retry: 파이프라인이 실패할 때 다시 실행  timeout: 제한된 시간 동안만 실행되도록 제한이 밖에 많은 옵션들이 있습니다.[공식문서 참고]            필수      No              값      newContainerPerStage, retry, timeout, timestamps, skipDefaultCheckout 등              위치      pipeline block, stage block      parameters파이프라인을 실행할 때 유저가 파라미터를 줄 수 있도록 합니다.            필수      No              값      string, text, booleanParam, choice, password              위치      pipeline block      pipeline {    agent any    parameters {        string(name: &#39;PERSON&#39;, defaultValue: &#39;Mr Jenkins&#39;, description: &#39;Who should I say hello to?&#39;)        text(name: &#39;BIOGRAPHY&#39;, defaultValue: &#39;&#39;, description: &#39;Enter some information about the person&#39;)        booleanParam(name: &#39;TOGGLE&#39;, defaultValue: true, description: &#39;Toggle this value&#39;)        choice(name: &#39;CHOICE&#39;, choices: [&#39;One&#39;, &#39;Two&#39;, &#39;Three&#39;], description: &#39;Pick something&#39;)        password(name: &#39;PASSWORD&#39;, defaultValue: &#39;SECRET&#39;, description: &#39;Enter a password&#39;)    }    stages {        stage(&#39;Example&#39;) {            steps {                echo &quot;Hello ${params.PERSON}&quot;                echo &quot;Biography: ${params.BIOGRAPHY}&quot;                echo &quot;Toggle: ${params.TOGGLE}&quot;                echo &quot;Choice: ${params.CHOICE}&quot;                echo &quot;Password: ${params.PASSWORD}&quot;            }        }    }}triggers파이프라인을 어떤 기준으로 자동화할지 정하는 옵션입니다. 소스코드가 Github 또는 BitBucket이라면  triggers가 필요하지 않을 수 있습니다. 현재 Jenkins에서 제공하는 triggers는 cron, pollSCM, upstream이 있습니다.            필수      No              값      cron, pollSCM, upstream              위치      pipeline block      tools파이프라인 내에서 빌드할 때 필요한 도구들을 참조합니다.            필수      No              값      maven, jdk, gradle              위치      pipeline bloc, stage block      when각각의 스테이지를 실행할지를 설정하는 조건입니다.            필수      No              값      branch, buildingTag, environment, equals, expression, tag, not, allOf, anyOf 등              위치      stage block      pipeline {    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                branch &#39;production&#39;            }            steps {                echo &#39;Deploying&#39;            }        }    }}pipeline {    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                allOf {                    branch &#39;production&#39;                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39;                }            }            steps {                echo &#39;Deploying&#39;            }        }    }}    agent any    stages {        stage(&#39;Example Build&#39;) {            steps {                echo &#39;Hello World&#39;            }        }        stage(&#39;Example Deploy&#39;) {            when {                expression { BRANCH_NAME ==~ /(production|staging)/ }                anyOf {                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;production&#39;                    environment name: &#39;DEPLOY_TO&#39;, value: &#39;staging&#39;                }            }            steps {                echo &#39;Deploying&#39;            }        }    }}참고  Jenkins 공식문서: Pipeline Syntax  SeungHyeon, [Jenkins] # 선언적(Declarative) 파이프라인",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-03T21:01:35+09:00'>03 Mar 2022</time><a class='article__image' href='/jenkins_series3'> <img src='/images/jenkins_17.png' alt='Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series3'>Jenkins Series [Part3]: 선언적 파이프라인(Feat.Jenkinsfile)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part2]: AWS Computing Service",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series2",
      "date"     : "Mar 3, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-03T21:01:35+09:00'>03 Mar 2022</time><a class='article__image' href='/aws-series2'> <img src='/images/aws_logo.png' alt='AWS Series [Part2]: AWS Computing Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series2'>AWS Series [Part2]: AWS Computing Service</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series2",
      "date"     : "Mar 2, 2022",
      "content"  : "Table of Contents  Jenkins 설치하기  도커 컨테이너로 Jenkins 띄우기          도커 이미지 Pull      Jenkins 컨테이너 실행        Jenkins 컨테이너에서 도커 CLI가 필요한 순간이 온다  참고Jenkins 설치하기  Jenkins can be installed through native system packages, Docker, or even run standalone by any machine with a Java Runtime Environment (JRE) installed.도커 컨테이너로 Jenkins 띄우기도커 이미지를 이용해서 Jenkins를 띄우는 방법은 간단합니다. 사용하다보면 커스터마이징할 필요가 생겨 이미지를 직접 빌드해야 하는 상황이 오겠지만, 아직 저는 Jenkins를 사용해 본 경험이 없기 때문에 단순히 띄우는 데 의의를 두고 이번 포스트를 작성했습니다.도커 이미지 Pull우선 도커 허브에서 이미지를 다운받습니다. 저는 arm64 아키텍처를 필요로 하기 때문에 그에 맞는 이미지를 다운 받았습니다.docker pull jenkins/jenkins:2.332.1-lts-jdk11Jenkins 컨테이너 실행# p: jenkins 웹 UI를 localhost로 접속하기 위해# u: 내가 사용한 jenkins 이미지 사용자가 root로 안되어 있어 컨테이너 내에서 apt-get update 이런게 안되서 root로 바꿔줌# v: 나중에 뒤에서 사용할 DooD 패턴을 위해 jenkins 컨테이너와 호스트의 소켓을 마운트해야함  docker run -p 8080:8080 -u root -v /var/run/docker.sock:/var/run/docker.sock -it jenkins/jenkins:2.332.1-lts-jdk11 /bin/bash# 웹 브라우저에서 로컬호스트 8080포트로 Jenkins Web UI에 접근localhost:8080그러면 뭔가 비밀번호 같은거를 입력하라고 뜨는데 이는 컨테이너 로그에 남아있어서 복붙하면 됩니다.docker logs &amp;lt;컨테이너명&amp;gt;그러면 플러그인 설치를 어떤 방법으로 할 것인지 묻는데 저는 Jenkins에서 선택해준 것들로 우선 설치하겠습니다.플러그인들을 열심히 설치하고 있습니다. Git이 자주 사용되는지 기본적으로 설치가 되는 모습입니다.설치가 끝나면 계정 설정을 하라고 나오고 간단히 입력하고 나면 Jenkins web UI가 잘 보입니다.Jenkins 컨테이너에서 도커 CLI가 필요한 순간이 온다Jenkins는 CI/CD를 위한 기본적인 툴로써 요즘같은 마이크로서비스 패턴이 트렌드인 시대에서 도커는 반드시 필요해 보입니다. 여기서 문제가 있습니다. 저는 방금 Jenkins 서버를 도커 컨테이너로 띄웠는데 컨테이너에서 도커를 또 설치해도 괜찮을까요?이렇게 도커 컨테이너 안에 도커를 또 설치하는 패턴을 Docker in Docker(DinD)라고 하는데 많은 시니어 개발자들은 이 방식을 권장하지 않는다고 합니다. 다음은 DinD 방식에 대한 장단점을 정리해둔 포스트이니 참고해봐도 좋을 것 같습니다. (~jpetazzo/Using Docker-in-Docker for your CI or testing environment? Think twice. 참고)DinD의 단점을 해결하고자 나온 방식이 Docker out of Docker(DooD)라고 합니다. 이 방법은 컨테이너에 도커 엔진(도커 클라이언트와 도커 호스트)을 설치하지 않고 도커 클라이언트만 설치하는 방식입니다.# 도커 Client 설치  # https://docs.docker.com/engine/install/debian/ 데비안 위에서 도커 클라이언트 설치apt-get updateapt-get install \    ca-certificates \    curl \    gnupg \    lsb-releasecurl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpgecho \  &quot;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \  $(lsb_release -cs) stable&quot; | tee /etc/apt/sources.list.d/docker.list &amp;gt; /dev/nullapt-get updateapt-get install docker-ce-cli도커 클라이언트가 잘 설치되었는지 확인하기 위해 docker ps 명령어를 실행해봤습니다. 아래의 에러는 위에서 docker run 명령어를 실행할 때 v 옵션으로 마운트하지 않은 경우 발생하는 에러입니다.root@bdab333aab12:/# docker psCannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?위에 설명한 대로 잘 따라오셨다면 아마 다음과 같은 결과가 잘 보일겁니다.여기서 핵심은      컨테이너 안의 도커 소켓과 호스트의 도커 소켓을 마운트 한다.     docker run \ -v /var/run/docker.sock:/var/run/docker.sock \ ...            컨테이너 안의 jenkins 유저에게 호스트의 도커 소켓 실행 권한을 준다.    # 아직 되는지 확인해보진 않았다usermod -u &amp;lt;호스트의사용자아이디&amp;gt; jenkins &amp;amp;&amp;amp; \ groupmod -g &amp;lt;호스트의도커그룹아이디&amp;gt; docker &amp;amp;&amp;amp; \ usermod -aG docker jenkins      참고  기억 저장소: DooD (docker-outside-of-docker) 를 통해 Jenkins 컨테이너에서 docker 사용하기  postlude: Jenkins를 docker 컨테이너로 구축하기(Docker in Docker)  아이단은 어디 갔을까: DinD(docker in docker)와 DooD(docker out of docker)  도커 컨테이너에서 permission denied 해결하는 방법: docker run -u root …  도커 공식문서: 데비안 위에 도커 설치하는 방법  Do we need java for jenkins?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-02T21:01:35+09:00'>02 Mar 2022</time><a class='article__image' href='/jenkins_series2'> <img src='/images/jenkins_1.png' alt='Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series2'>Jenkins Series [Part2]: Docker에서 Jenkins 설치하기(Feat.DooD)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Terraform Series [Part1]: What is Terraform?",
      "category" : "",
      "tags"     : "Terraform",
      "url"      : "/terraform_series1",
      "date"     : "Mar 1, 2022",
      "content"  : "Table of Contents  Terraform  인프라를 코드로 관리  테라폼 코드  State Driven Workflow Engine  좋은 글귀  참고Terraform  HashiCorp Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share.테라폼은 하시코프에서 오픈소스로 개발 중인 IaC 툴입니다. 테라폼을 통해 인프라 구성에 필요한 파일, 리소스 등을 코드로 정의하여 일관된 워크플로우를 사용해 전체 라이프사이클에 걸쳐 인프라를 프로비저닝하고 관리할 수 있습니다. 쉽게 말해 원하는 인프라를 코드로 작성할 수 있다는 말입니다. 테라폼은 컴퓨팅, 스토리지, 네트워킹 리소스와 같은 저수준의 구성 요소뿐만 아니라 DNS 및 SaaS 기능과 같은 고수준의 구성 요소를 관리할 수도 있습니다.🦊 프로비저닝어떤 프로세스나 서비스를 실행하기 위한 준비 단계를 프로비저닝이라고 이야기합니다. 프로비저닝에는 크게 네트워크나 컴퓨팅 자원을 준비하는 작업과 준비된 컴퓨팅 자원에 사이트 패키지나 애플리케이션 의존성을 준비하는 단계로 나뉘어집니다. 명확한 경계는 불분명하지만 테라폼은 전자를 주로 다루는 도구입니다. (44bits: 테라폼(Terraform) 기초 튜토리얼 참고)인프라를 코드로 관리인프라(컴퓨팅, 스토리지, 네트워킹)를 구성하기 위해 그저 테라폼 코드만 작성하면 된다는 것은 굉장히 편하게 느껴집니다. 제가 AWS를 맛보기로 사용해 봤을 때의 기억으로는 인프라를 구성하기 위해 EC2, S3, VPC 등 AWS 내에서 많은 서비스들을 왔다갔다 하면서 클릭을 했어야 했는데 이제 이러한 작업들을 할 필요 없어진다는 점이 굉장히 편할 것 같습니다. 인프라를 코드로 관리하게 되면 얻게 되는 이점에 대해 한 번 정리해보면 다음과 같은 것들이 있을 것 같습니다.  인프라를 언제 어디서든 일관되게 구성할 수 있다.  인프라 관리를 위한 협업을 코드로 할 수 있다.  인프라를 버전 컨트롤 할 수 있다.  인프라도 지속적 통합/지속적 배포가 가능해진다.테라폼 코드테라폼 코드는 어떻게 생겼길래 인프라를 관리할 수 있는 걸까요? 테라폼 코드 생김새를 한 번 구경하도록 하겠습니다.# main.tfprovider &quot;aws&quot; {  region = &quot;ap-northeast-2&quot;}resource &quot;aws_vpc&quot; &quot;myvpc&quot; {  cidr_block = &quot;10.0.0.0/16&quot;}provider를 통해 인프라를 제공해주는 주체를 정할 수 있습니다. provider는 나의 컴퓨터인 local이 될 수도 있고, aws, gcp, azure와 같은 클라우드 서비스가 될 수도 있습니다.resource를 이용해 내가 생성하고자 하는 인프라를 설정할 수 있습니다. 위에서는 aws_vpc를 생성했습니다.위의 코드를 일반적으로 나타내면 다음과 같습니다.&amp;lt;BLOCK TYPE&amp;gt; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; &quot;&amp;lt;BLOCK LABEL&amp;gt;&quot; {  &amp;lt;IDENTIFIER&amp;gt; = &amp;lt;EXPRESSION&amp;gt;}State Driven Workflow Engine  It will parse our HCL configuration/code files.  Using the information in our HCL, Terraform will build up a graph of all the resources we want to provision (desired state) and figure out any dependencies between them to try and decide a logical order they need to be created in.  Terraform will next inspect its State to better understand what it has and hasn’t deployed (if it is our first deployment, the State will be empty). This is known as perceived state. It is perceived state because there is a disconnect between what Terraform “thinks” exists and what “actually” exists.  Terraform next performs a logical delta between our desired state, and what it knows to be our perceived state. It then decides which CRUD actions it needs to perform, and the order to perform them in, in order to bring our perceived state in-line with our desired state.  Terraform next performs all necessary operations to achieve the desired state. The result of this operation will be that resources will likely start to appear in our Azure subscription and this then becomes known as actual state.  Terraform updates the state to reflect what it has done.좋은 글귀Terraform in a nutshellIn its most basic form, Terraform is an application that converts configuration files known as HCL (Hashicorp Configuration Language) into real world infrastructure, usually in Cloud providers such as AWS, Azure or Google Cloud Platform.This concept of taking configuration files and converting them into real resources is known as IaC (Infrastructure as Code) and is the new hotness in the world of Software Engineering. And the reason it is becoming so hot right now, is because this code can live alongside your app code in repos, be version controlled and easily integrated into your CI/CD pipelines.간단히 말해 Terraform은 상태 기반 클라우드 플랫폼 프로비저닝 엔진입니다. 또한 추상화 툴링(provider and backend)을 활용하여 일관성 있고 결정론적인 클라우드 프로바이더별 CRUD API 호출로 해석 및 번역할 수 있는 코드를 작성할 수 있으므로 많은 업무와 스트레스가 제거됩니다.AWS의 CloudFormation참고  44bits: 테라폼(Terraform) 기초 튜토리얼  C:\Dave\Storey, Terraform For Beginners  Jayendra’s Cloud Certification Blog: Terraform Cheat Sheet",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-01T21:01:35+09:00'>01 Mar 2022</time><a class='article__image' href='/terraform_series1'> <img src='/images/tf_1.png' alt='Terraform Series [Part1]: What is Terraform?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/terraform_series1'>Terraform Series [Part1]: What is Terraform?</a> </h2><p class='article__excerpt'>테라폼을 통해 인프라 구성에 필요한 파일, 리소스 등을 코드로 정의하여 인프라를 프로비저닝하고 관리할 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Jenkins Series [Part1]: What is Jenkins?",
      "category" : "",
      "tags"     : "Jenkins",
      "url"      : "/jenkins_series1",
      "date"     : "Mar 1, 2022",
      "content"  : "Table of Contents  CI/CD  Jenkins  참고CI/CD  CI: 지속적 통합 (Continuous Integration)보통 하나의 서비스를 출시하기 위해 다수의 개발자들이 하나의 저장소에서 코드를 작성하는데 이 때 서로의 작업을 병합하고 에러를 수정하고 테스트하는 일련의 과정을 반복하게 됩니다. 이러한 과정은 보통 비슷한 작업이 반복되는 형태로 진행되기 때문에, 개발자들은 이 과정에서 발생하는 시간 낭비를 줄이고 싶어했습니다.Jenkins는 이러한 요구에 맞춰 개발된 CI 서비스를 제공하는 툴입니다.대표적인 CI 툴은 다음과 같습니다.  CD: 지속적 배포 (Continuous Deploy)CD는 CI 과정을 통해 코드에 문제가 없는 것을 확인하고 서버에 배포하는 과정을 의미합니다.대표적인 CD 툴은 다음과 같습니다.Jenkins  Jenkins is a self-contained, open source automation server which can be used to automate all sorts of tasks related to building, testing, and delivering or deploying software.Jenkins는 파이프라인(Pipeline)을 사용해 거의 모든 언어의 조합과 소스코드 리포지토리에 대한 CI/CD 환경을 구축하기 위한 간단한 방법을 제공합니다.Jenkins가 각각의 단계에 대한 스크립트 작성의 필요성을 없애주지는 않지만, 일반 사용자가 구축할 수 있는 것보다 더 빠르고 더 강력하게 빌드(Build), 테스트, 그리고 배포(deployment) 도구 등 체인 전체를 통합할 수 있는 방법을 제공해 줍니다.Jenkins는 자바 언어로 개발된 툴로써 Jenkins를 실행하기 위해서는 별도의 서버가 존재해야 하며 서버에는 Java가 설치되어 있어야 합니다.Jenkins의 CI 관련 프로세스는 Jenkins의 파이프라인으로 정의됩니다.이러한 파이프라인은 다음의 3 가지 방법으로 정의할 수 있습니다.  Jenkinsfile     pipeline { agent {} environment {} stages {     stage(&quot;Build&quot;) {         environment {}         steps {}     }     stage(&quot;Test&quot;) {         environment {}         steps {}     } } }            Jenkins UI    Blue Ocean (Jenkins 공식문서 참고: Getting started with Pipeline) 참고  IT World: Jenkins란 무엇인가, CI(Continuous Integration) 서버의 이해  DevOps Story, CD를 위한 Jenkins, Argo CD 연계  메쉬코리아 플랫폼실 최제필, CI/CD 도구 및 방법론 도입기  javatpoint: Gradle vs. Jenkins  GitLab vs Jenkins  Katalon: Best 14 CI/CD Tools You Must Know | Updated for 2022",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-03-01T21:01:35+09:00'>01 Mar 2022</time><a class='article__image' href='/jenkins_series1'> <img src='/images/jenkins_10.png' alt='Jenkins Series [Part1]: What is Jenkins?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/jenkins_series1'>Jenkins Series [Part1]: What is Jenkins?</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part5]: 스파크 SQL",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series5",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents  RDD, Dataframe, Dataset          RDD      Dataframe      Dataset        Dataframe          Dataframe 생성      기본 연산      액션 연산      비타입 트랜스포메이션 연산      함수가 아닌 SQL문        참고RDD, Dataframe, DatasetRDDSpark Core에 RDD가 있다면 Spark SQL에는 Dataframe과 Dataset이 있습니다. 기존의 RDD를 이용해 스파크 애플리케이션 코드를 작성할 때에는 RDD가 가지고 있는 메서드나 특성을 알아야지만 코드를 작성할 수 있었습니다. 그래서 RDD에 대한 이해도가 높아야 분산 환경에서 높은 처리 성능을 이끌어 낼 수 있었습니다.Dataframe그러던 중 Spark 1.3버전에서 Dataframe이라는 새로운 데이터 모델이 공개되었습니다. Dataframe은 개발자들에게 친숙한 SQL과 비슷한 방식으로 작성할 수 있도록하는 API를 제공해 진입 장벽을 낮췄으며 코드의 가독성 또한 높여주었습니다. Dataframe도 마찬가지로 low-level에서는 RDD로 코드가 동작하는데 Spark SQL은 내부적으로 Catalyst Optimizer를 통해 최적의 RDD 코드로 변환됩니다. 따라서 쉬운 코드 작성과 높은 성능을 모두 얻게되었습니다.그러나 Dataframe에도 아쉬운 점이 있었는데, 바로 RDD에서 가능했던 컴파일 타임 오류 체크 기능을 사용할 수 없다는 점이었습니다.DatasetSpark 1.6버전에서 RDD의 장점과 Dataframe의 장점을 합친 새로운 데이터 모델인 Dataset이 등장했습니다.그리고 Spark 2.0 이후부터는 Dataframe이 Dataset 안에 포함되었습니다.# 데이터셋은 데이터를 처리할 때 데이터의 타입을 있는 그대로 활용할 수 있습니다.데이터셋: Dataset[String], Dataset[Int]# 데이터프레임은 데이터를 처리할 때 데이터 타입을 무조건 org.apache.spark.sql.Row로 감싸줘야 합니다.데이터프레임: Dataset[Row]이렇게 Dataframe은 원래 데이터가 가지고 있던 타입의 특성은 사용하지 않기 때문에 Dataframe API은 비타입 트랜스포메이션 연산(untyped operations)으로 분류됩니다.            데이터 모델      사용 가능한 연산              Dataframe      기본 연산, 액션 연산, 비타입 트랜스포메이션 연산              Dataset      기본 연산, 액션 연산, 타입 트랜스포메이션 연산      저는 파이썬을 주언어로 사용하고 있으며 파이썬 언어는 Dataframe API만 제공하기 때문에 이번 포스트에서는 액션 연산과 비타입 트랜스포메이션 연산에 대해서만 다루도록 하겠습니다.DataframeDataframe 생성Dataframe은 SparkSession을 이용해 생성합니다. 생성 방법은 파일이나 데이터베이스와 같은 스파크 외부에 저장된 데이터를 이용할 수도 있고, 스파크 내에서의 RDD나 Dataframe을 이용해 새로운 Dataframe을 생성할 수도 있습니다.외부 데이터 소스파일이나 데이터베이스같은 외부 저장소의 데이터를 읽어와서 Dataframe을 생성할 때는 SparkSession의 read()메소드를 이용하면 됩니다. read()메소드는 DataFrameReader 인스턴스를 생성하고 이를 이용해 다양한 유형의 데이터를 읽고 Dataframe을 생성할 수 있습니다.from pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;sample&quot;).master(&quot;local[*]&quot;).getOrCreate()df = spark.read.format(&quot;json&quot;).option(&quot;allowComments&quot;, &quot;true&quot;).load(&quot;&amp;lt;spark_home_dir&amp;gt;/test.json&quot;)  전체적인 생성 과정은 크게 다음과 같습니다.1. Spark Session의 read() 메소드를 호출해 DataFrameReader 인스턴스 생성2. format() 메소드로 데이터소스의 유형을 지정3. option() 메소드로 데이터소스 처리에 필요한 옵션을 지정4. load() 메소드로 대상 파일을 읽고 데이터프레임을 생성다음은 DataFrameReader가 제공하는 주요 메소드입니다.- format()    읽어들이고자 하는 데이터 소스의 유형을 문자열로 지정(&quot;kafka&quot;, &quot;csv&quot;, &quot;json&quot;, &quot;parquet&quot;, &quot;text&quot; 등)    이 밖에도 지원하지 않는 데이터소스는 라이브러리를 클래스패스에 추가해서 사용할 수 있습니다- option/options()    데이터소스에 사용할 설정 정보를 지정    데이터소스에  따라 다름- load()    데이터소스로부터 실제 데이터를 읽어서 Dataframe을 생성- json()    JSON 형식을 따르는 문자열로 구성된 파일이나 RDD로부터 Dataframe 생성- parquet()    파케이 형식응로 작성된 파일을 읽어서 Dataframe 생성- text()    일반 텍스트 형식으로 작성된 파일을 읽어서 Dataframe 생성- csv()    CSV 파일을 읽어 Dataframe 생성 RDD, Dataframefrom pyspark.sql import SparkSessionspark = SparkSession.builder.appName(&quot;sample&quot;).master(&quot;local[*]&quot;).getOrCreate()row1 = Row(name=&quot;kim&quot;, age=20, job=&quot;student&quot;)row2 = Row(name=&quot;mike&quot;, age=17, job=&quot;student&quot;)data = [row1, row2]df = spark.createDataFrame(data)기본 연산  persist()  printSchema()  columns  dtypes  createOrReplaceTempView()  explain()액션 연산  show()  head()  take()  count()  describe()비타입 트랜스포메이션 연산Dataframe에서 제공하는 비타입 트랜스포메이션 연산  select()  filter()  agg()  orderBy()  groupBy()  withColumn()org.apache.spark.Column에서 제공하는 비타입 트랜스포메이션 연산  !==, ===  alias()  isin()  when()  like()org.apache.spark.sql.functions에서 제공하는 비타입 트랜스포메이션 연산(왜 트랜스포메이션이지?)  max(), mean(), sum()  count(), countDistince()  explode()  when()  col()  lit()함수가 아닌 SQL문  createOrReplaceTempView()참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  loustler, [Apache Spark] Spark RDD, Dataframe and DataSet  Apache Spark 공식문서: Spark SQL on PySpark",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series5'> <img src='/images/spark_16.jpg' alt='Apache Spark Series [Part5]: 스파크 SQL'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series5'>Apache Spark Series [Part5]: 스파크 SQL</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series4",
      "date"     : "Feb 22, 2022",
      "content"  : "Table of Contents아직 작성 전입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-22T21:01:35+09:00'>22 Feb 2022</time><a class='article__image' href='/spark-series4'> <img src='/images/spark_logo.png' alt='Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series4'>Apache Spark Series [Part4]: 스파크 RDD(Resilient Distributed Dataset)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part3]: 스파크의 클러스터 환경",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series3",
      "date"     : "Feb 21, 2022",
      "content"  : "Table of Contents  클러스터 환경  스파크의 분산처리 아키텍처  스파크 애플리케이션 동작 순서  참고클러스터 환경스파크는 본질적으로 분산처리 프레임워크입니다. 그래서 단순히 테스트를 위한 용도로는 단일 로컬 서버만으로도 가능하지만, 실제 배포 단계에서 스파크를 제대로 활용하기 위해서는 여러 대의 서버를 이용한 클러스터 환경을 구축할 필요가 있습니다.클러스터란 여러 대의 서버가 네트워크를 통해 연결되어 마치 하나의 서버인 것처럼 동작하는 방식을 의미합니다. 하지만 여러 서버들을 이 같은 방식으로 동작시키는 것은 쉬운 일이 아닙니다. 그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다.스파크에서는 자체 구현한 클러스터 매니저도 제공하고 외부 클러스터 매니저를 임포트해서 사용할 수도 있습니다. 이렇게 여러 종류의 클러스터 매니저를 지원하게 되면 선택의 폭이 넓어진다는 장점도 있긴 하지만 클러스터 매니저마다 동작 방식이나 용어가 다르면 혼동이 될 수 있습니다. 스파크에서는 이런 혼란을 없애고자 추상화된 클러스터 모델을 제공함으로써 사용하는 클러스터의 종류에 관계없이 일관된 방법으로 프로그램을 작성하고 클러스터를 관리할 수 있게 해줍니다.내용에 들어가기 전에 한 가지 알아둘 것은 클러스터 환경이라고 해서 로컬 환경에서 사용하던 스파크 애플리케이션 코드를 새로 작성해야 할 필요는 없습니다. 다만 클러스터 환경에서는 여러 서버를 마치 하나의 서버인 것처럼 다뤄야 하기 때문에 하나의 작업을 여러 서버에 분산해서 실행하고 그 결과를 취합할 수 있는 분산 작업 관리 기능이 추가되어야 할 것입니다.따라서 이번 포스트의 목적은 분산처리를 위한 시스템 아키텍처를 이해하고, 이를 구현하기 위해 필요한 설정과 매개변수를 이해하는 것입니다.스파크의 분산처리 아키텍처아래 그림은 분산처리를 위한 스파크의 전형적인 아키텍처입니다.보시다시피 클러스터 매니저는 가운데에서 분산처리를 위한 매니저 역할을 하고 있습니다. 각각의 컴포넌트에 대한 설명은 앞의 포스트에서 다룬 적이 있음으로 여기서는 간단하게만 요약하도록 하겠습니다.  드라이버 프로그램: 스파크 컨텍스트를 생성하고 클러스터 매니저와 연결시켜주는 프로그램  스파크 컨텍스트: 클러스터와 연결되는 객체로 스파크 애플리케이션 코드를 작성하는데 필요한 거의 모든 기능을 제공  클러스터 매니저: 워커 노드를 모니터링하며 최적의 자원(CPU, 메모리) 할당  워커 노드: 분산된 데이터를 할당받고 요청된 작업을 처리하는 서버  익스큐터: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위  잡(Job): 액션 연산의 수  태스크: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위스파크 클러스터는 이와 같이 드라이버, 클러스터 매니저, 워커 노드의 조합으로 구성됩니다. 여기서 실행 모드, 클러스터 매니저의 종류에 따라 약간의 다른 점이 있지만 큰 맥락에서는 같습니다.실행 모드의 경우 두 가지가 있습니다. 클러스터 모드, 클라이언트 모드입니다.두 모드 가운데 어떤 것을 선택하든 수행 결과는 동일합니다. 하지만 클러스터 모드의 경우 드라이버 프로그램과 익스큐터 간의 네트워크 비용이 상대적으로 낮아져서 성능 향상을 기대할 수 있습니다. 하지만 스파크 셸과 같은 인터랙티브 환경을 이용한 디버깅이 어려워서 정형화된 작업에만 주로 사용하고, 클라이언트 모드의 경우 사용성이 편리하지만 드라이버 프로그램과 워커 노드가 네트워크 상에서 너무 많이 떨어져 있으면 전체적인 성능에 영향을 줄 수 있으므로 가급적 동일 네트워크 상에 존재하는 서버로 선택하는 것이 좋습니다.스파크 애플리케이션 동작 순서지금까지 스파크의 클러스터 환경에서 갖게되는 아키텍처와 컴포넌트에 대해 살펴봤습니다. 지금부터는 아키텍처에서 실제로 스파크 애플리케이션이 구동되는 과정을 살펴보도록 하겠습니다.  가장 먼저 스파크 애플리케이션 코드를 작성합니다. 이 때 코드에는 스파크컨텍스트를 생성하는 드라이버 프로그램이 포함돼 있어야 합니다.  작성한 코드를 빌드하고 관련 라이브러리와 함께 jar나 zip 파일 등으로 패키징합니다.  패키지 파일을 스파크에서 제공하는 spark-submit 셸 스크립트를 이용해 클러스터에 배포하고 실행합니다.  코드에 있는 드라이버 프로그램이 실행되고 스파크컨텍스트가 클러스터 매니저와 연동되어 워커 노드에 익스큐터를 생성합니다.  드라이버 프로그램은 작성된 코드에서 액션 연산의 수만큼 잡(Job)을 생성합니다.  잡(Job)을 셔플링이 가장 적게 일어나는 방법으로 스테이지를 나누고 각 스테이지 단계를 여러 개의 태스크로 나눕니다.  태스크를 익스큐터에 적절히 분배하여 분산 처리합니다.참고  빅데이터 분석을 위한 스파크2 프로그래밍 책  What is SparkContext? Explained",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-21T21:01:35+09:00'>21 Feb 2022</time><a class='article__image' href='/spark-series3'> <img src='/images/spark_13.png' alt='Apache Spark Series [Part3]: 스파크의 클러스터 환경'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series3'>Apache Spark Series [Part3]: 스파크의 클러스터 환경</a> </h2><p class='article__excerpt'>그래서 스파크에서는 전체 서버의 자원과 동작을 세밀하고 효율적으로 제어할 수 있는 별도 모듈이 필요한데, 이를 클러스터 매니저라고 합니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part2]: 스파크 개발환경 구축하기",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series2",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  스파크 설치          pyspark                  자바, 파이썬 설치          pyspark 설치                    Spark        로컬 개발 환경  클러스터 환경  참고스파크 설치스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다. 자바가 필요한 이유는 스파크가 JVM 위에서 실행되기 때문입니다.하지만 실무에서는 대부분의 빅데이터 소프트웨어들이 클러스터 환경에서 동작하기 때문에 제대로 활용하기 위해서는 여러 가지 준비할 것도 많고 설정해야 할 것들도 많습니다. 그래서 스파크는 개발/테스트를 위한 용도로 간단하게 사용할 때에는 단독 서버에서 동작하는 로컬 모드를, 배포를 위한 용도로 클라이언트, 클러스터 모드를 지원합니다.스파크 애플리케이션 코드는 자바, 스칼라, 파이썬, R언어로 작성할 수 있습니다.pyspark우선 저는 파이썬을 주언어로 사용하기 때문에 pyspark를 이용해 파이썬으로 스파크 애플리케이션 코드를 작성할 예정입니다. pyspark의 장점은 만약 개발/테스트를 위한 목적으로만 스파크를 사용할 예정이라면 스파크를 설치할 필요가 없다는 것입니다. 스파크를 사용하는데 스파크를 설치할 필요가 없다? 무슨 뜻이냐면 pyspark를 설치하기만 해도 스파크를 실행하기 위해 필요한 최소한의 파일을 함께 설치해줍니다.하지만 여전히 자바는 설치해주어야 합니다.  To run Spark, you only require a Java runtime environment (JRE) but you may also download the Java development kit (JDK) which includes the JRE.저는 파이썬이 설치되어 있는 도커 이미지를 이용해 컨테이너 안에서 실습을 진행해 보았습니다.자바, 파이썬 설치# 파이썬이 설치된 컨테이너 생성docker run -it python:3.8-buster# JDK 설치apt-get updateapt-get install openjdk-11-jdk# JAVA_HOME 변수 설정, 경로 추가export JAVA_HOME=/etc/openjdk-11-jdk     # 본인의 자바 설치 경로export PATH=$JAVA_HOME/bin:$PATH. /etc/profile # bash쉘이면 source /etc/profilepyspark 설치# pyspark 설치pip install pyspark# 잘 설치되었는지 확인import pysparksc = pyspark.SparkContext(appName=&quot;SparkContext&quot;)sc--------------------------------SparkContextVersionv3.2.1Masterlocal[*]AppNameSparkContextSpark이번에는 파이썬에 국한되지 않는 조금 더 일반적인 방법으로 스파크를 설치해보겠습니다. 이번에는 리눅스 운영체제만 가지는 컨테이너 위에서 실습을 진행하도록 하겠습니다.처음에는 우분투 이미지를 바로 컨테이너로 띄우고 그 위에서 자바를 설치하려 했지만, 오라클에서 다운받는 방법을 제한하고 있어서 아래의 방법으로 진행했습니다. (우분투 이미지에 로컬에서 다운받은 자바를 하나의 이미지로 새로 빌드)그래서 사실 위에서 진행한 pyspark만 설치하는 방법에서도 python이미지에 로컬 자바로 한 번 이미지를 빌드한 후 사용하는 것이 좋을 것 같습니다. 저도 아직 본격적으로 사용해보지는 않아서 에러가 있는지는 확인해보지 않았지만 로컬에서 자바를 다운 받고 빌드하는 방법은 확실히 안전합니다.🦊 자바 설치자바 라이센스를 소유하고 있는 오라클에서 2019년 4월부터 자바를 외부의 허용하지 않은 방법으로 다운받는 것을 금지시켰습니다. 그래서 wget과 같은 방식으로 자바8 버전을 더이상 다운받을 수 없게 되고 무조건 오라클에 로그인을 한 후 로컬에 먼저 다운을 받아야합니다. 자바 17은 가능한데 스파크에서 자바 17로 설치하니까 오류가 난다. 구글링에서는 자바를 다운그레이드 하라고 나와있다. 자바8로 해보니까 된다. 그래서 자바8을 지금 다운 받으려고 하는 것이다.그래서 저같은 경우에는 왠만한 작업들은 무조건 도커 컨테이너에서 진행하는 편이라 처음에는 도커허브에서 자바8이 설치되어 있는 이미지를 찾아봤지만 뭔가 세부설정들이 마음에 들지 않게 되어 있어서 이미지를 직접 빌드하기로 결정했습니다. 제가 사용한 방법의 과정은 다음과 같습니다.# 자바를 로컬에 다운로드# 다운로드 페이지 접속https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html# 저는 M1 칩을 사용하고 있어서 ARM64 전용 파일을 다운로드 받았습니다.jdk-8u311-linux-aarch64.tar.gz# 다운로드 받은 폴더에서 압축해제tar -xzvf jdk-8u311-linux-aarch64.tar.gz# Dockerfile 작성# 로컬에 설치한 자바를 컨테이너로 옮기고 스파크까지 설치해주었습니다FROM ubuntu:latestCOPY jdk1.8.0_321 ./jdk1.8.0_321RUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim \&amp;amp;&amp;amp; apt-get -y install wget \&amp;amp;&amp;amp; wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz \&amp;amp;&amp;amp; tar -xzvf spark-3.2.1-bin-hadoop3.2.tgzvim /etc/profile# 환경 변수설정해줍니다. 이부분은 Dockerfile에서 ENV로 설정해 줄 수도 있습니다export JAVA_HOME=/jdk1.8.0_321export PATH=$JAVA_HOME/bin:$PATH. /etc/profilecd spark-3.2.1-bin-hadoop3.2ls--------------------------------------------------------------------------------------------------------------LICENSE  NOTICE  R  README.md  RELEASE  bin  conf  data  examples  jars  kubernetes  licenses  python  sbin  yarn# 스파크에서 제공하는 실행 파일cd binls# 스파크 셸 실행./bin/spark-shell# 셸 종료:q위의 과정은 이미지에서 매번 스파크를 다운받는 방식이기 때문에, 스파크를 다운받은 컨테이너를 다시 한 번 이미지로 만들면 그 다음부터는 새로 만든 이미지를 이용하면 컨테이너를 띄우는 속도가 더 빨라지게 됩니다. 그래서 docker commit 명령어를 이용해 한번 더 이미지를 빌드하는 것을 권장드립니다.# 로컬 터미널에서 docker commit 명령어로 이미지 생성# dockere commit &amp;lt;원하는 컨테이너 이름&amp;gt; &amp;lt;생성할 이미지 이름&amp;gt;docker commit thirsty_galois spark_container로컬 개발 환경위의 설치과정을 완료한 후 스파크의 설정 정보를 확인해 보겠습니다../bin/spark-shell --verbose다른 부분은 일단 신경쓰지 말고 master 부분만 보도록 하겠습니다. 현재 master가 local[*]로 설정되어 있습니다. 이는 현재 드라이버 프로그램을 실행하는 서버를 포함해 워커 노드까지 모두 로컬 서버를 이용하고 있다는 뜻입니다. *는 로컬 서버의 모든 스레드를 사용하겠다는 뜻입니다.따라서 여기까지만 설정하게 되면 로컬에서 테스트 목적으로 사용하기 위한 최소한의 준비는 끝난 것입니다. 이 외에도 여러 가지 설정들을 직접하고 싶을 때에는 ./conf에 설정을 위한 여러가지 파일의 템플릿을 이용할 수 있습니다.클러스터 환경참고  Pyspark 코드는 어디서 실행되는가?  Pyspark만으로 스파크 애플리케이션 실행할 수 있나?  Pyspark의 한계  bin/sh: 1: source: not found  [Linux] 우분투에 자바 설치  Unable to download Oracle JDK 8 using Wget command  자바(JDK, JRE) 모든 버전 다운로드( 6,7,8,9,10,11,12,13,14,15, 16, 17..)  How to Set Up a Multi Node Apache Spark Cluster with Quobyte  Updating the Apache Spark configuration files  [spark] Spark 3 클러스터 설치",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/spark-series2'> <img src='/images/spark_10.png' alt='Apache Spark Series [Part2]: 스파크 개발환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series2'>Apache Spark Series [Part2]: 스파크 개발환경 구축하기</a> </h2><p class='article__excerpt'>스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. 자바와 스파크만 설치하면 스파크를 사용할 수 있습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(4): In Memory",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/dev_know-series4",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contentshttps://www.samsungsds.com/kr/insights/In-Memory-Data-Grid.html",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series4'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(4): In Memory'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series4'>Development Knowledge Series(4): In Memory</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(3): Networks and Distributed Systems",
      "category" : "",
      "tags"     : "Network and OS",
      "url"      : "/dev_know-series3",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contentshttps://www.pling.org.uk/cs/nds.html",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series3'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(3): Networks and Distributed Systems'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series3'>Development Knowledge Series(3): Networks and Distributed Systems</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(2): Java 관련 용어",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/dev_know-series2",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents아직 글을 작성 중입니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series2'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(2): Java 관련 용어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series2'>Development Knowledge Series(2): Java 관련 용어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Development Knowledge Series(1): Scripting Language vs Compile Language",
      "category" : "",
      "tags"     : "Javascript, Python, and Java",
      "url"      : "/dev_know-series1",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contentshttps://well-made-codestory.tistory.com/30?category=978350",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/dev_know-series1'> <img src='/images/development_knowledge_logo.png' alt='Development Knowledge Series(1): Scripting Language vs Compile Language'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/dev_know-series1'>Development Knowledge Series(1): Scripting Language vs Compile Language</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "BlockChain Series [Part1]: 블록체인을 공부하기 전에",
      "category" : "",
      "tags"     : "Blockchain_basic",
      "url"      : "/blockchain-series0",
      "date"     : "Feb 20, 2022",
      "content"  : "Table of Contents  블록체인이란  블록체인에 대한 오해  블록체인 용어 바로 알기          분산 vs 탈중앙화      디지털화 vs 블록체인      가상 화폐 vs 암호 화폐      거래소 vs 중개소      트랜잭션 vs 거래내역      디지털 자산        블록체인의 역사          사이퍼펑크      비트코인의 탄생        블록? 블록체인? 비트코인?          블록      블록체인      비트코인        마치며  참고블록체인이란블록체인(blockchain)이란 다수의 거래내역을 묶어 블록을 구성하고, 해시를 이용하여 여러 블록들을 체인처럼 연결한 뒤, 다수의 사람들이 복사하여 분산 저장하는 알고리즘이다.블록체인은 비트코인과 이더리움 등 암호화폐에 사용된 핵심 기술이다. 은행 등 제3의 중개기관이 없더라도 블록체인 기술을 이용하면 누구나 신뢰할 수 있는 안전한 거래를 할 수 있다. 블록체인은 암호화폐뿐 아니라, 온라인 거래내역이 있고 이력관리가 필요한 모든 데이터 처리에 활용할 수 있다. 블록체인 기반의 스마트 계약, 물류관리 시스템, 문서관리 시스템, 의료정보관리 시스템, 저작권관리 시스템 등 다양한 활용이 가능하다.블록체인은 간략히 ‘분산원장’(分散元帳, distributed ledger) 기술이라고 한다. 즉, 거래내역을 기록한 원장을 다수의 사람들에게 분산하여 저장·관리하는 기술이다. 자세히 설명하면, 블록체인이란 다수의 온라인 거래 기록을 묶어 하나의 데이터 블록(block)을 구성하고, 해시(hash) 값을 이용하여 이전 블록과 이후 블록을 마치 체인(chain)처럼 연결한 뒤, 이 정보의 전부 또는 일부를 피투피(P2P) 방식으로 전 세계 여러 컴퓨터에 복사하여 분산 저장·관리하는 기술이다.블록체인에 대한 오해위의 내용은 (해시넷: 블록체인)에서 작성한 글의 일부를 가져온 것입니다.‘제 3자가 필요 없어지고, 이에 따라 수수료도 사라지는 이상적인 플랫폼’. 많은 사람들은 블록체인을 이용하면 그동안의 거래 시스템 전반에 많은 혁신을 가져다 줄 것으로 기대했습니다. 하지만 현실은 그렇지 않았습니다. 블록체인의 등장은 그동안 불필요했던 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다. 또한 독립적인 화폐를 내세웠던 암호 화폐는 선채굴을 악용한 채굴업자들이 암호화폐 대부분을 장악한 후 전 세계 중개소를 통해 일반인들을 선동, 호도하며 내다팔아 막대한 수익을 얻었습니다.저는 이번 BlockChain Series를 준비하며 앞으로 블록체인의 미래가 어떻게 될지 한 번 알아보려고 합니다.블록체인 용어 바로 알기분산 vs 탈중앙화블록체인을 공부하다 보면 분산, 탈중앙화라는 단어가 많이 사용되는 것을 보게 됩니다. 하지만 블록체인과 일반적인 분산 시스템과는 약간 다른 점이 있습니다. 분산 시스템은 데이터를 여러 노드에 분산해 나누어 처리하기 때문에 효율적입니다. 하지만 블록체인은 같은 데이터(거래 내역 등)를 모든 노드가 중복 처리합니다. 이 방법은 데이터의 신뢰도를 높일 수는 있지만 비효율적입니다.            특징      분산      탈중앙화              저장      여러 노드에 분산 저장      여러 노드에 중복 저장              처리      여러 노드가 분산 처리      여러 노드가 중복 처리              장점      높은 효율성      높은 신뢰성      디지털화 vs 블록체인블록체인을 공부하면서 블록체인의 장점으로 시스템의 효율성과 안전한 데이터 저장이라는 글을 본 적이 있습니다. 하지만 이는 디지털화(digitalization)의 장점이며 블록체인은 정확히 이것과 반대입니다. 블록체인은 효율성을 포기하고 데이터의 신뢰성을 높인 것이며, 데이터는 보안되지 않고 모든 사용자에게 공개되어 저장됩니다. 따라서 블록체인은 보안과 효율을 위한 도구가 아니고 신뢰받는 제 3자가 없더라도 거래가 가능한 플랫폼을 만들기 위한 실험적 과정에 있는 개념이라고 생각하면 됩니다.가상 화폐 vs 암호 화폐가상 화폐는 어떤 실물의 가치를 디지털화한 것을 의미하며, 암호 화폐는 가상 화폐의 일부로 블록 체인을 기반으로 만들어진 화폐를 의미합니다. 암호화 되었다는 뜻은 암호 화폐를 보내는 송신인과 수신인이 암호화된 공개키를 통해서만 거래를 하기 때문으로 이는 금융 시스템의 투명성을 해치고 악용될 여지가 많습니다.거래소 vs 중개소거래소는 금융 거래소에서 가져온 단어로 금융 거래소는 거래 시스템에 필요한 환경과 제도가 잘 갖춰져 있습니다. 하지만 암호 화폐 거래소의 경우 암호 화폐의 가격이 단일화 되어 있지도 않고, 거래소 안에 엄격한 규정이 적용되어 있지도 않습니다. 그래서 정확한 명칭은 암호 화폐 중개소 정도가 적합합니다. 그리고 온라인 상에 있는 많은 중개소들은 블록체인과는 별 관련이 없습니다. 중개소는 그저 암호 화폐를 이용하는 지갑이라는 소프트웨어와 온라인 주식 매매에 이용되는 HTS 기능 중 일부를 사용해 거래를 중개하는 브로커일 뿐입니다.트랜잭션 vs 거래내역IT 분야에서는 트랜잭션을 보통 업무 처리의 단위로 얘기 하고 특히 데이터베이스 분야에서는 더 이상 쪼갤 수 없는(또는 더 쪼개면 심각한 오류가 발생할 수 있는)최소한의 업무 처리 단위를 의미합니다. 블록체인에 있어서 트랜잭션은 ‘정의된 이벤트가 발생하는 것’을 의미하며 거래 내역을 포함한 더 포괄적인 의미입니다.디지털 자산암호 화폐는 디지털 자산이 아닙니다. 디지털 자산은 저작권, 소유권 등의 권리를 디지털화 한 것으로 이는 실질적인 가치를 내재하고 있습니다. 하지만 암호 화폐는 내재 가치가 0인 디지털 숫자에 불과합니다. 하지만 내재 가치가 0인 경우에도 가치를 가질 수 있습니다. 그러기 위해 필요한 것이 바로 ‘신뢰’입니다. 따라서 암호 화폐가 실제로 가치를 가지기 위해서는 사람들로부터의 신뢰가 필요합니다. 예를 들어 지폐가 있습니다. 지폐는 종이에 불과하지만 그 뒤에 그 지폐를 발행한 국가의 법령과 신뢰가 뒷받침하기 때문에 실질적인 가치를 가질 수 있는 것입니다. 주식도 온라인 증명서에 불과하지만 주식을 발행한 기관의 신뢰 덕분에 가치를 가지게 됩니다.블록체인의 역사사이퍼펑크블록체인은 사이퍼펑크(cypherpunk) 운동에 뿌리를 두고 있습니다. 사이퍼펑크란 중앙집권화된 국가와 거대 기업들에 대항하여 개인의 프라이버시를 보호하기 위해 암호기술을 이용하여 익명성을 보장하는 탈중앙화 시스템을 만드려는 행동주의자들을 말합니다.비트코인의 탄생2008년 사토시 나카모토란 가명으로 암호화 커뮤니티에 논문이 하나 올라왔습니다. 제목은 비트코인: P2P 전자 캐시 시스템으로 사토시는 이 논문에서 코인을 그 누구의 간섭도 받지 않는 결제 수단이라고 설명했습니다. 그리고 2009년 1월 비트코인의 개념을 실제로 소프트웨어로 구현되어 최초의 블록(제네시스 블록 또는 0번 블록)이 생성되었습니다. 비트코인은 현재까지 10분에 하나꼴로 블록이 만들어지고 있으며 2019년 10월 기준 70만개의 블록이 만들어졌습니다. (참고로 사토시의 정체는 아직까지 밝혀지지 않았다고 합니다)블록? 블록체인? 비트코인?블록체인을 공부하다 보면 블록의 정체가 무엇인지 또 나는 블록체인을 공부하는데 왜 자꾸 나도 모르는 사이에 비트코인이라는 단어가 등장하는 건지 의아했었습니다. (참고로 사토시가 공개한 비트코인에 관한 논문에는 ‘블록’과 ‘체인’이라는 명사가 독립적으로 사용되기는 했지만 ‘블록체인’이라는 단어는 한 번도 등장하지 않았습니다.)블록블록은 전산학에서 보통 한꺼번에 처리되는 논리적 데이터 단위를 일컫습니다. 비트코인에서는 트랜잭션들을 1메가바이트를 넘지 않는 선에서 계속 묶었다가 1메가바이트 직전이 되면 블록으로 만듭니다. 보통 이렇게 하나의 블록이 만들어지는데 10분 정도가 걸립니다.            블록      설명              정의      한꺼번에 처리되는 데이터 단위              크기      최대 1MB              트랜잭션 수      2000~3000개              생성 시간      평균 10분      블록체인트랜잭션 데이터를 포함하는 블록을 앞의 블록의 해시값을 이용해 연결한 것을 블록체인이라고 합니다. 블록은 각각의 노드에서 자신이 가지고 있는 트랜잭션들을 묶다가 블록을 만들 수 있는 시점이 되면 비동기적으로 블록을 만들기 위해 경쟁합니다. 이 때의 경쟁을 채굴(mining)이라고 하며 가장 먼저 블록을 만든 노드의 블록만을 모든 노드의 블록체인에 추가합니다.비트코인비트코인 생태계(네트워크)에서 채굴로 얻게되는 보상을 ‘비트코인’이라고 합니다. 이 보상을 얻기 위해 채굴자들이 그렇게 앞다투어 채굴을 했던 것입니다. 여기서 채굴은 블록을 만들 때 블록에 필요한 해시값을 찾는 과정을 일컫습니다. 채굴에 대한 보상으로 얻게되는 비트코인의 수량은 어떻게 책정될까요? 보상금은 보조금과 수수료의 합으로 이루어지는데, 보조금은 채굴로 얻게되는 고정 수익을 말하고, 수수료는 비트코인 거래 시 송신자로부터 얻게 되는 것으로 수수료는 송신자가 스스로 정하게 됩니다. 그래서 보통 더 높은 수수료를 지불한 트랜잭션이 먼저 블록에 포함되게 됩니다. 보조금의 경우 제네시스 블록이 생성됐을 때에는 50BTC 였으나, 보조금은 블록이 21만개 생성될 때마다 반감되도록 설정되어 현재는 블록을 하나 생성할 때마다 6.25BTC를 얻게 됩니다.블록은 평균 10분 마다 한 개씩 생성되고 보조금은 21만개 마다 계속 반감되기 때문에 시간이 지나면 블록을 생성해도 더 이상 비트코인이 (거의) 발행되지 않게 될 것입니다. 이론적으로 2033년에 거의 포화되는 시점에 다다를 것으로 보며 그 때까지 누적된 비트코인의 양은 2100만BTC라고 합니다. 이렇게 채굴에 대한 보상이 극도로 낮아질 경우, 채굴자가 급격히 줄어들게 될 것이고, 이는 비트코인 시스템의 안전성을 급격하게 저하시키게 됩니다.마치며지금까지 블록체인에 대해 공부하기 전에 알고가면 좋은 지식들에 대해 살펴보았습니다. 다음 포스트에서는 블록체인의 동작원리에 대해 알아보겠습니다.참고  블록체인 해설서 책  해시넷: 블록체인",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-20T21:01:35+09:00'>20 Feb 2022</time><a class='article__image' href='/blockchain-series0'> <img src='/images/blockchain_logo.webp' alt='BlockChain Series [Part1]: 블록체인을 공부하기 전에'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/blockchain-series0'>BlockChain Series [Part1]: 블록체인을 공부하기 전에</a> </h2><p class='article__excerpt'>하지만 현실은 블록체인의 등장으로 불필요한 새로운 형태의 중개인을 양산했고, 암호화폐 재단, 채굴업자, 중개소에 종속되며 빠르게 중앙화되어갔습니다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-memory_allocation",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  메모리 관리          Python Memory Manager      Garbage Collection        메모리 할당          Stack 할당      Heap 할당        참고요즘에는 컴퓨터, 스마트폰을 사용할 때 한가지 프로그램/어플리케이션만 실행하는 사람은 없을 것입니다. 그렇기 때문에 내가 만든 프로그램/어플리케이션이 메모리를 효율적으로 사용하도록 개발하는 것은 중요합니다.메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다. 메모리 할당은 내가 사용하는 프로그래밍 언어, 운영체제, 컴퓨터 아키텍처에 따라 조금씩 다르지만 전체적인 과정은 비슷합니다.파이썬의 메모리 관리는 대부분 Python Memory Manager에 의해 수행되지만, Python Memory Manager를 공부하면 프로그래밍 전반에 대한 이해와 코드 최적화, 디버깅을 더욱 잘 할 수 있게 될 것입니다.메모리 관리파이썬에서의 메모리 관리는 크게 두 가지 레벨로 나누어서 생각할 수 있습니다. 첫 번째로 운영체제 레벨에서는 각 프로세스에 메모리를 얼마나 할당할지를 정하고, 각각의 프로세스가 다른 프로세스에 접근하지 못하도록 관리합니다.두 번째로 파이썬 내에서의 메모리 관리입니다.파이썬에서는 컴파일 단계에서 스택 영역에 메모리를 정적으로 크기를 정하고, 실행 단계에서는 Python memory manager를 이용해 힙 영역에 동적으로 메모리를 할당하고 그 외의 역할(공유, 할당, 제거 등)들을 수행함으로써 메모리를 관리합니다.Python Memory ManagerPython memory manager는 모든 Python objects와 data structures를 포함하는 private 힙을 포함합니다. Python memory manager는 공유(sharing), 분할(segmentation), 사전 할당(preallocation) 또는 캐싱(caching)과 같은 다양한 동적 스토리지 관리 측면을 다루는 다양한 구성 요소를 가지고 있습니다.가장 낮은 수준에서 raw memory allocator는 운영 체제의 메모리 관리자와 상호 작용하여 모든 파이썬 관련 데이터를 저장할 수 있는 충분한 공간을 private 힙에 확보합니다. raw memory allocator 외에도 여러 object-specific allocators가 동일한 힙에서 object의 특성에 맞는 고유한 메모리 관리 정책을 구현합니다.파이썬 힙의 관리는 인터프리터 자체에 의해 수행되며 힙 내부의 메모리 블록에 대한 객체 포인터를 사용자가 직접 제어할 수 없다는 것을 의미합니다. Python objects를 위한 힙 공간 할당은 파이썬/C API 함수를 통해 파이썬 메모리 관리자에 의해 수행됩니다.Garbage Collection가비지 컬렉션은 인터프리터가 프로그램을 사용하지 않을 때 프로그램의 메모리를 비우는 것입니다. 파이썬이 이렇게 할 수 있는 것은 파이썬 개발자들이 백엔드에서 우리를 위해 가비지 컬렉터를 구현했기 때문입니다. 파이썬 가비지 컬렉터는 reference counting 방법으로 객체에 더 이상 참조가 없을 때는 객체가 차지하고 있던 메모리의 할당을 취소하고 메모리를 비우게 됩니다.메모리 할당메모리 할당(memory allocation)은 프로그램이 컴퓨터 메모리의 특정 빈 블록에 할당되거나 할당되는 과정을 의미합니다. 파이썬에서 이 모든 것은 Python memory manager에 의해 수행됩니다.Stack 할당스택 할당은 정적 메모리를 저장하는데, 정적 메모리는 특정 함수나 메서드 호출 내에서만 필요한 메모리입니다. 함수가 호출되면 프로그램의 호출 스택에 추가됩니다. 변수 초기화 같은 특정 함수 내부의 모든 로컬 메모리 할당은 함수 호출 스택에 임시로 저장되며, 함수가 돌아오면 삭제되고 호출 스택이 다음 작업으로 이동합니다. 연속적인 메모리 블록에 대한 이 할당은 미리 정의된 루틴을 사용하여 컴파일러에 의해 처리되기 때문에 개발자들은 이것에 대해 걱정할 필요가 없습니다.(그러면 이 부분은 함수, 메서드, 그리고 함수나 메서드 안에서 사용되는 지역변수들을 컴파일 단계에서 확인하고 이에 알맞은 메모리 크기를 예측해서 정적으로 할당하는 건가?)(그리고 코드 실행 단계에서 함수가 호출될 때마다 스택에 Call stack이 쌓인다?)Heap 할당힙 할당은 프로그램에서 전역 범위로 사용되는 메모리인 동적 메모리를 저장합니다. 이러한 변수들은 특정 메서드나 함수 호출 외부에 필요하거나 전역적으로 여러 함수 내에서 공유됩니다. 스택 할당과 달리 힙 할당은 힙 데이터 구조와 관련이 없습니다. 힙 영역은 단순히 할당하고 어느 정도 임의의 주소에서 자유롭게 사용할 수 있는 큰 메모리 공간이며, 저장되는 객체에 필요한 공간을 기반으로 합니다.(힙 할당은 코드 실행 단계에서 객체의 타입에 맞게 동적으로 메모리 할당된다?)(Python memory manager의 가비지 컬렉션 기능에 의해 더이상 참조되지 않는 객체는 제거되고 메모리 비운다?)참고  How does Memory Allocation work in Python (and other languages)?  Python 공식문서: Memory Management  What’s the difference between a stack and a heap?  RealPython: Memory Management in Python  muchogusto.log: 파이썬 런타임과 메모리 관리  python의 메모리 할당과 관리 (Stack &amp;amp; Heap Memory)  파이썬 메모리 영역",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-memory_allocation'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-memory_allocation'>Python Advanced Series [Part2]: 파이썬의 메모리 관리(Feat.Garbage Collection)</a> </h2><p class='article__excerpt'>메모리를 효율적으로 사용하는 프로그램/어플리케이션을 만들기 위해서는 메모리 할당에 대해 이해해야합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-cpython_pypy",
      "date"     : "Feb 16, 2022",
      "content"  : "Table of Contents  파이썬 언어의 특징  파이썬은 인터프린터 언어?  컴파일 언어  인터프리터 언어  파이썬 인터프리터의 종류          CPython      Jython      PyPy        참고파이썬 언어의 특징파이썬(Python)은 1980년대 후반 귀도 반 로섬이 개발하고 1991년에 출시한 high-level 범용 프로그래밍 언어입니다. 동적 타입 언어이므로 변수의 타입을 선언할 필요가 없으며, 코드가 실행되어 메모리 관리가 자동으로 수행됩니다.이번 포스트에서는 파이썬 코드를 실행할 때 어떤 동작이 내부적으로 일어나는지 알아보도록 하겠습니다.저희가 CLI 환경에서 파이썬 코드를 실행하는 경우를 생각해봅시다.python my_code.py여기서 python은 바로 파이썬 인터프리터 프로그램을 의미합니다. 따라서 파이썬 코드는 파이썬 인터프리터를 통해 실행하는 것입니다.  (Python Interpreter - Stanford Computer Science 참고)파이썬은 인터프린터 언어?위에서 파이썬 인터프리터를 통해 파이썬 코드를 실행한다고 했습니다. 그러면 파이썬은 인터프리터 언어일까요? 과거에 C는 컴파일 언어이고, 쉘 프로그래밍 언어는 인터프리터 언어였기 때문에 이 경계가 비교적 명확했지만, 비교적 최근에 나온 언어들은 그 경계가 모호합니다.파이썬 또한 어느 정도의 컴파일 언어적 특성과, 인터프리터 언어의 특성을 모두 가지고 있어서 이분법적으로 나누기가 힘들지만 프로그래밍 언어의 대표인 C언어가 완전한 컴파일 언어이기 때문에 이와 구분하기 위해 그냥 편하게 인터프리터 언어라고 하는 것 같습니다. 정리하면 파이썬은 컴파일 언어이기도 하면서 인터프리터 언어이기도 합니다.컴파일 언어먼저 간단하게 컴파일 언어의 뜻과 컴파일 언어가 코드를 실행하는 과정에 대해 살펴보겠습니다.  컴파일은 소스코드를 다른 타겟 언어(기계어, 자바, C, 파이썬)로 변환하는 과정을 의미  코드를 실행할 때 코드 전체를 인풋으로 사용  코드는 컴파일 단계에서 한 번 기계어로 변환되어 저장되고 나면 언제든 바로 실행가능  컴파일러는 실행하는 역할이 아니고 기계어로 변환하는 역할인터프리터 언어  인터프리터 언어는 소스 코드를 바로바로 실행하게 됩니다.  인터프리터는 코드를 한 줄씩 입력으로 사용해 실행합니다.  인터프리터에는 종류마다 다른 실행 방법이 있습니다.  A. 소스 코드를 파싱해서 설정한 방법에 따라 실행  B. 소스 코드를 먼저 중간 단계의 바이트 언어로 변환하고 A의 과정을 수행  C. 컴파일러에 의해 먼저 변환된 코드를 이용해 1의 과정을 수행파이썬 인터프리터는 이 중 B에 해당합니다.1. 소스 코드를 컴파일러를 이용해 중간 단계의 바이트 언어의 파일(.pyc)로 변환2. Python Virual Machine 위에서 바이트 언어 파일 한 줄씩 실행  파이썬 인터프리터의 종류CPythonJythonPyPyPyPy는 파이썬 언어(RPython: 파이썬의 일부)로 작성된 인터프리터입니다. 디폴트인 CPython과의 호환성을 유지하면서도 CPython 보다 속도가 빠르다고 알려져있습니다.참고  Shalini Ravi, How Does Python Code Run: CPython And Python Difference  elhay efrat, Python » CPython  geeksforgeeks, Difference between various Implementations of Python  파이썬 언어의 특징  파이썬 인터프리터 고르기  Wireframe: 파이썬은 인터프리터언어입니까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-16T21:01:35+09:00'>16 Feb 2022</time><a class='article__image' href='/python-cpython_pypy'> <img src='/images/python_advanced_logo.png' alt='Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-cpython_pypy'>Python Advanced Series [Part1]: 파이썬 코드 동작 원리(Feat.CPython, PyPy)</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part3]: Javascript 표현식과 문",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series3",
      "date"     : "Feb 6, 2022",
      "content"  : "Table of Contents  값  리터럴  표현식  문  세미콜론 자동 삽입  표현식인 문과 표현식이 아닌 문값값은 표현식이 평가되어 생성된 결과를 말한다. 평가란 식을 해석해서 값을 생성하거나 참조하는 것을 의미한다.10 + 20;x - 1;리터럴리터럴은 사람이 이해할 수 있는 문자 또는 약속된 기호를 사용해 값을 생성하는 표기법을 말한다.            리터럴      예시              정수      100              부동소수점      0.4              2진수      0b010001              문자열      ‘Hello’              불리언      true, false              null      null              undefined      undefined              객체      { name: ‘Lee’, address: ‘Seoul’ }              배열      [1, 2, 3]              함수      function() {}              정규 표현식      /[A-Z]+/g      표현식표현식(expression)은 값으로 평가될 수 있는 문이다. 위에서 살펴본 리터럴은 값으로 평가될 수 있기 때문에 리터럴도 표현식이라 할 수 있다.10&#39;Hello&#39;person.namearr[1]10 + 20sum = 10sum !== 10square()person.getName()x + 3문문(statement)은 프로그램을 구성하는 기본 단위이자 최소 실행 단위다. 문은 여러 토큰으로 구성된다. 토큰이란 문법적인 의미를 가지며, 문법적으로 더 이상 나눌 수 없는 코드의 기본 요소를 의미한다. 예를 들어, 키워드, 식별자, 연산자, 리터럴, 세미콜론, 마침표 등의 특수기호는 모두 토큰이다.문은 선언문, 할당문, 조건문, 반복문 등으로 구분할 수 있다.// 선언문var x;// 할당문x = 5;// 함수 선언문funciton foo() {}// 제어문if (x &amp;gt; 1) { console.log(x); }// 반복문for (var i = 0; i &amp;lt; 2; i++) { console.log(i); }세미콜론 자동 삽입세미콜론(;)은 문의 종료를 나타낸다. 즉 자바슼립트 엔진은 세미콜론으로 문이 종료한 위치를 파악하고 순차적으로 하나씩 문을 실행한다. 단 괄호로 묶은 코드 블록 뒤에는 세미콜론을 붙이지 않는다. 블록은 문의 종료를 의미하는 자체 종결성을 갖기 때문이다.세미콜론은 생략 가능하다. 이는 자바스크립트 엔진이 소스코드를 해석할 때 문의 끝이라고 예측되는 지점에 세미콜론을 자동으로 붙여주는 세미콜론 자동 삽입 기능(ASI:Automatic Semicolon Insertion)이 암묵적으로 수행되기 때문이다.하지만 개발자의 의도와 일치하지 않는 경우도 생기기 때문에 자바스크립트 커뮤니티에서는 세미콜론의 사용을 권장한다.표현식인 문과 표현식이 아닌 문값으로 평가될 수 있으면 표현식인 문이고, 그렇지 않으면 표현식이 아닌 문이다.// 표현식이 아닌 문var x;// 표현식인 문x = 1 + 2;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-06T21:01:35+09:00'>06 Feb 2022</time><a class='article__image' href='/javascript-series3'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part3]: Javascript 표현식과 문'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series3'>Javascript Series [Part3]: Javascript 표현식과 문</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part4]: 플링크의 아키텍처",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series4",
      "date"     : "Feb 6, 2022",
      "content"  : "Table of Contents  Flink 생태계 구성  Flink 런타임 아키텍처          Job Manager      Task Manager      Tasks and Operator Chains      Task Slots and Resources        Flink API  참고Flink 생태계 구성Flink 런타임 아키텍처플링크 런타임은 두 종류의 프로세스로 구성됩니다. 하나는 Job Manager이고 다른 하나는 Task Manger로 Task Manager는 한 개 이상으로 구성할 수 있습니다.플링크 클라이언트는 런타임에 포함되지는 않지만 데이터플로우를 Job Manager로 보내는 역할을 합니다. 이 후 클라이언트는 연결을 끊을 수도 있고 또는 attached mode를 통해 진행 경과를 보고 받을 수도 있습니다. 클라이언트는 자바 위에서 클라이언트 명령어로 실행됩니다.Job Manager잡 매니저는 어플리케이션의 실행을 제어하는 마스터 프로세스입니다. 클라이언트는 잡 매니저로 어플리케이션을 제출합니다. 어플리케이션은 데이터플로우 그래프(또는 잡그래프)와 필요한 클래스, 라이브러리 등을 포함하는 JAR파일로 구성되어 있습니다.잡 매니저는 잡그래프를 실행그래프(ExecutionGraph)라고 불리는 물리적인 그래프로 변환합니다. 잡 매니저는 태스크 매니저내에 사용 가능한 태스크 슬롯에 잡그래프를 태스크 형태로 배포합니다.또한 잡 매니저는 완료된 태스크, 실행 실패, 장애 복구, 체크포인트 조율 등 중앙에서 제어해야 하는 모든 동작에 책임을 집니다.Task Manager태스크 매니저(워커)는 데이터플로우의 태스크들을 실행합니다. 태스크 매니저에 할당하는 가장 작은 작업 단위를 태스크 슬롯이라고 합니다. 태스크 매니저 안에 있는 슬롯의 개수는 동시에 처리되는 작업의 개수를 나타냅니다. 하나의 슬롯안에서 여러개의 연산자가 실행될 수도 있습니다.Tasks and Operator Chains플링크의 태스크는 연산자를 체이닝한 서브태스크의 집합으로 이루어져 있습니다. 각각의 서브태스크는 하나의 스레드에서 실행됩니다. 이렇게 연산자들을 체이닝 함으로써 플링크는 스레드간 핸드오버와 버퍼링으로 인한 오버헤드를 줄입니다. 이는 전체적인 처리량 증가와 지연율 감소를 가능하게 합니다.Task Slots and Resources태스크 매니저는 각각 하나의 JVM 프로세스입니다. 그리고 태스크 매니저는 하나 이상의 서브태스크를 스레드로 분리할 수 있습니다.각각의 태스크 슬롯은 각각의 분리된 자원으로 가장 작은 작업 단위 입니다. 태스크 슬롯으로 분리된 작업들은 자원을 위해 서로 경쟁할 일이 없습니다. 태스크 슬롯에서 분리하는 자원은 메모리 뿐입니다. CPU는 분리되지 않습니다.태스크 매니저는 각각 하나의 JVM 프로세스이기 때문에 태스크 매니저가 여러개의 태스크 슬롯을 가진다면 여러개의 태스크가 하나의 JVM 프로세스 위에서 실행된다는 의미입니다. 하나의 JVM 위에서 실행되는 태스크들은 TCP 연결을 통해 서로 하트비트 메세지를 주고 받습니다. 또한 태스크간 데이터셋을 공유함으로써 태스크별로 발생하는 오버헤드를 줄여줍니다.플링크는 디폴트로 작업의 전체 파이프라인을 하나의 슬롯에 할당합니다.Flink API      The lowest level abstraction simply offers stateful and timely stream processing. It is embedded into the DataStream API via the Process Function. It allows users to freely process events from one or more streams, and provides consistent, fault tolerant state. In addition, users can register event time and processing time callbacks, allowing programs to realize sophisticated computations.        In practice, many applications do not need the low-level abstractions described above, and can instead program against the Core APIs: the DataStream API (bounded/unbounded streams) and the DataSet API (bounded data sets). These fluent APIs offer the common building blocks for data processing, like various forms of user-specified transformations, joins, aggregations, windows, state, etc. Data types processed in these APIs are represented as classes in the respective programming languages.    The low level Process Function integrates with the DataStream API, making it possible to use the lower-level abstraction on an as-needed basis. The DataSet API offers additional primitives on bounded data sets, like loops/iterations.        The Table API is a declarative DSL centered around tables, which may be dynamically changing tables (when representing streams). The Table API follows the (extended) relational model: Tables have a schema attached (similar to tables in relational databases) and the API offers comparable operations, such as select, project, join, group-by, aggregate, etc. Table API programs declaratively define what logical operation should be done rather than specifying exactly how the code for the operation looks. Though the Table API is extensible by various types of user-defined functions, it is less expressive than the Core APIs, and more concise to use (less code to write). In addition, Table API programs also go through an optimizer that applies optimization rules before execution.    One can seamlessly convert between tables and DataStream/DataSet, allowing programs to mix the Table API with the DataStream and DataSet APIs.        The highest level abstraction offered by Flink is SQL. This abstraction is similar to the Table API both in semantics and expressiveness, but represents programs as SQL query expressions. The SQL abstraction closely interacts with the Table API, and SQL queries can be executed over tables defined in the Table API.  참고  Flink and Kafka Streams: a Comparison and Guideline for Users  Kartik Khare, Here’s How Apache Flink Stores Your State data  stackoverflow: Storage in Apache Flink",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-06T21:01:35+09:00'>06 Feb 2022</time><a class='article__image' href='/flink-series4'> <img src='/images/flink_logo.png' alt='Flink Series [Part4]: 플링크의 아키텍처'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series4'>Flink Series [Part4]: 플링크의 아키텍처</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part2]: Javascript 변수",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  변수란 무엇인가? 왜 필요한가?  변수 선언  변수 호이스팅  값의 할당  변수 네이밍  참고변수란 무엇인가? 왜 필요한가?사람은 계산과 기억을 모두 두뇌에서 하지만, 컴퓨터는 연산과 기억을 수행하는 부품이 나눠져 있다. 컴퓨터는 CPU를 사용해 연산하고, 메모리를 사용해 데이터를 기억한다.메모리는 데이터를 저장할 수 있는 메모리 셀의 집합체다. 메모리 셀 하나의 크기는 1바이트이다. 각 셀은 고유의 메모리 주소를 갖는다. 모든 값은 메모리 상의 임의의 위치에 저장되고 CPU는 이 값을 읽어들여 연산을 수행한다. 연산 결과로 생성된 값도 메모리 상의 임의의 위치에 저장된다. 하지만 문제는 이 값을 재사용하기 위해서는 어디 저장되어야 하는지 알아야 하는데 모른다는 것이다. 설령 안다고 하더라도 개발자가 직접적으로 메모리에 접근하는 것은 위험하다. 그래서 자바스크립트는 직접적인 메모리 제어를 허용하지 않는다.프로그래밍 언어는 기억하고 싶은 값을 메모리에 저장하고, 저장된 값을 읽어 들여 재사용하기 위해 변수라는 메커니즘을 제공한다.변수는 하나의 값을 저장하기 위해 확보한 메모리 공간을 식별하기 위해 붙인 이름을 말한다.변수는 컴파일러 또는 인터프리터에 의해 메모리 공간의 주소로 치환되어 실행된다. 따라서 개발자가 직접 메모리 주소를 통해 값을 저장하고 참조할 필요가 없고 변수를 통해 안전하게 값에 접근할 수 있다.변수 선언변수 선언이란 변수를 생성하는 것을 말한다. 좀 더 자세히 말하면 메모리에는 값을 저장하기 위한 공간을 확보하고, 변수 이름을 메모리 공간의 주소로 연결(name binding)하는 것을 말한다.변수를 사용하려면 반드시 선언이 필요하다. 변수를 선언할 때는 var, let, const 키워드를 사용한다. ES6에서 let, const 키워드가 도입되기 전까지는 var 키워드가 변수 선언을 위한 유일한 키워드였다.🦄 **var 키워드의 단점**  var 키워드의 가장 대표적인 단점은 블록 레벨 스코프를 지원하지 않고, 함수 레벨 스코프만 지원한다는 것이다.  자바스크립트 엔진은 변수 선언을 다음과 같은 2단계에 걸쳐 수행한다.  선언 단계: 변수 이름을 등록해서 자바스크립트 엔진에 변수의 존재를 알린다.  초기화 단계: 값을 저장하기 위한 메모리 공간을 확보하고 암묵적으로 undefined를 할당해 초기화한다.이렇게 초기화 단계를 거침으로써 이전에 다른 애플리케이션이 사용했던 값(garbage value)이 재사용되는 것을 방지해줄 수 있다.변수 호이스팅console.log(score);var score;위의 코드를 보면 변수 선언문보다 변수를 참조하는 코드가 앞에 있다. 그럼에도 결과는 에러가 아닌 undefined다.자바스크립트에서는 변수 선언이 런타임(소스코드가 실행되는 시점)이 아니라 그 이전 단계에서 먼저 실행된다.자바스크립트 엔진은 소스코드를 실행하기에 앞서 먼저 소스코드의 평가 과정을 거치면서 소스코드를 실행하기 위한 준비를 한다. 이 과정에서 엔진은 변수 선언을 포함한 모든 선언문을 소스코드에서 찾아서 먼저 실행한다. 그리고 이 과정이 끝나면 비로소 모든 선언문을 제외한 나머지 코드를 한 줄씩 순차적으로 실행한다.이처럼 모든 선언문(var, let, const, function, function*, class이 코드의 선두로 끌어 올려진 것처럼 동작하는 자바스크립트의 특징을 변수 호이스팅이라 한다.값의 할당변수에 값을 할당할 때는 연산자 =를 사용한다.var score; // 변수 선언score = 80; // 값의 할당var score = 80; // 변수 선언과 값의 할당자바스크립트 엔진은 변수 선언과 값의 할당을 하나의 문으로 단축 표현해도 2개의 문으로 나누어 각각 실행한다. 이 때 주의할 점은 변수 선언과 값의 할당의 실행 시점이 다르다는 것이다. 변수 선언은 값의 할당이 일어나는 런타임 이전에 이루어진다.console.log(score); // undefinedvar score = 80;console.log(score); // 80또한 변수의 선언과 값의 할당을 하나의 문으로 단축 표현해도 자바스크립트 엔진은 나누어 실행하므로, 변수에 먼저 undefined가 할당되어 초기화되는 것은 변함이 없다. 그리고 값이 할당될 때에는 새로운 메모리 공간을 확보하고 그 곳에 할당 값을 저장한다.이렇게 되고나면 undefined는 가비지 컬렉션에 의해 메모리에서 자동 해제된다.변수 네이밍자바스크립트는 변수 네이밍을 할 때에 카멜 케이스와 파스칼 케이스를 권장한다.참고  name-binding",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/javascript-series2'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part2]: Javascript 변수'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series2'>Javascript Series [Part2]: Javascript 변수</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series3",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  State          Operator State      Keyed State        Recovery          Checkpoint      Savepoint      Exactly Once        StateBackend  참고State대부분의 어플리케이션은 상태를 저장하고 활용(stateful)합니다. 만약 각각의 이벤트를 단순히 변환(transformation)하는 용도의 어플리케이션이라면 상태(state)가 필요하지 않을 수도 있습니다. 하지만 단순한 비즈니스 로직조차도 이전의 이벤트나 중간 결과를 사용하는 경우가 대부분이기 때문에 상태를 저장해 활용하는 것은 대부분의 서비스에 중요한 요소입니다.스트림 처리 어플리케이션의 경우 상태(state)의 중요성이 더욱 커지게 됩니다. 스트림 처리 어플리케이션에서 상태가 중요한 이유는 다음과 같습니다.  Low Latency: 스트림 처리는 짧은 지연을 위해 값을 출력 후 갱신을 통해 정확도를 높이는 방식이기에 이전 값을 기억해야함  Recovery: 장애 발생시 저장된 상태를 통해 다시 복구가 가능플링크에서는 상태를 프로그래밍 모델의 일급 시민(first-class citizen)으로 사용하고 있습니다. 플링크에서 상태 핸들링에 관한 주요 특징들에는 다음과 같은 것들이 있습니다.  Multiple State Primitives: Flink provides state primitives for different data structures, such as atomic values, lists, or maps. Developers can choose the state primitive that is most efficient based on the access pattern of the function.  Pluggable State Backends: Application state is managed in and checkpointed by a pluggable state backend. Flink features different state backends that store state in memory or in RocksDB, an efficient embedded on-disk data store. Custom state backends can be plugged in as well.  Exactly-once state consistency: Flink’s checkpointing and recovery algorithms guarantee the consistency of application state in case of a failure. Hence, failures are transparently handled and do not affect the correctness of an application.  Very Large State: Flink is able to maintain application state of several terabytes in size due to its asynchronous and incremental checkpoint algorithm.  Scalable Applications: Flink supports scaling of stateful applications by redistributing the state to more or fewer workers.플링크에서 상태는 단순히 변수에 저장되는 하나의 값일 수도 있고, 파일이나 데이터베이스에 저장되는 데이터일 수도 있습니다. 이런 상태의 다양성을 반영하기 위해 플링크에서는 크게 두 가지 종류의 상태를 제공하며 또 각각의 상태별로 표현할 수 있는 기본 상태(State primitive)도 여러가지 입니다.Operator State연산자 상태(Operator State)는 태스크별로 자신의 상태를 저장하고 있습니다. 카프카에서 컨슈머 그룹 내 컨슈머별로 토픽의 파티션을 공유하지 않고 자신의 파티션을 가지는 것과 비슷한 용도입니다.대부분의 플링크 어플리케이션에서는 연산자 상태가 필요하지 않습니다. 연산자 상태는 대부분 소스나 싱크쪽에서 필요로 하는 상태이기 때문입니다. 이러한 이유로 파이선의 DataStream API에서는 아직 연산자 상태를 지원하지 않습니다.Keyed State키 상태(Keyed State)는 스트림 데이터가 가지는 키별로 접근할 수 있는 상태를 말합니다. 예를 들어 태스크로 들어온 데이터가 ‘Lion’이라는 키를 가지고 있으면 ‘Lion’키와 관련된 상태에만 접근할 수 있습니다. 따라서 스트림의 형태는 기본적으로 KeyedStream이어야 합니다.KeyedState로 사용할 수 있는 기본 상태의 종류는 다음과 같습니다.  ValueState&amp;lt;T&amp;gt; : 키별로 하나의 값만을 저장합니다(ex. 키별 최대값)  ListState&amp;lt;T&amp;gt; : 키별로 리스트를 저장합니다(ex. 키별 회원 이름)  MapState&amp;lt;UK, UV&amp;gt; : 키별로 키-값 쌍을 저장합니다. (ex. 키별 회원 이름과 직업 쌍)Recovery플링크는 중간 결과를 저장하기 위해 상태를 로컬 버퍼에 저장하게 됩니다. 이를 통해 플링크는 짧은 지연으로 결과를 계속 갱신할 수 있습니다. 하지만 로컬 버퍼는 태스크매니저 프로세스가 종료되면 같이 사라지는 휘발성 저장소입니다. 이는 장애 발생시 실행중이던 태스크가 종료될 뿐 아니라 저장해두었던 상태도 함께 사라진다는 뜻입니다. 이를 위해 플링크에서는 상태를 주기적으로 체크포인팅해 원격의 영구저장소로 저장합니다.Checkpoint스트림 처리 어플리케이션은 장비, 네트워크 등 예상치 못한 장애로 상태가 유실되거나 일관성을 잃게될 수 있습니다. 이를 위해 플링크는 주기적으로 원격 영구 저장소로 상태를 체크포인팅해야 합니다.체크포인트의 naive한 알고리즘은 다음과 같습니다.1. 입력 데이터의 인입을 멈춘다2. 어플리케이션에 남아있는 데이터를 처리한다3. 각 태스크의 상태를 복사해 원격 영구저장소로 체크포인팅한다4. 입력 데이터를 다시 받는다플링크에서는 위와같은 naive한 알고리즘을 사용하지는 않습니다. 왜냐하면 위와 같이 체크포인팅을 할 경우 일명 ‘Stop the world’와 같이 체크포인팅을 하는 동안 어플리케이션 전체가 멈춰버리는 현상이 발생하기 때문에 짧은 지연을 요구하는 스트림 처리 어플리케이션에는 적합하지 않습니다.따라서 플링크에서는 전체를 정지하지 않고 체크포인트와 처리간의 결합을 분리하였습니다. 다시 말해 일부 태스크는 상태를 저장하고, 일부 태스크는 데이터를 계속 처리하게 됩니다. 이를 챈디-램포트 알고리즘 기반의 분산 스냅샷 체크포인팅이라고 합니다.플링크의 분산 스냅샷의 핵심 요소는 체크포인트 배리어(barrier)입니다. 배리어는 이전 포스트에서 봤던 워터마크처럼 특별한 용도의 레코드입니다. 레코드이기 때문에 데이터 스트림에 주입되고 데이터 스트림의 일부로 레코드와 함께 흐릅니다.배리어는 데이터 스트림에 주입되어 스트림에 흐르고 있는 레코드들이 어떤 체크포인트에 속하는지 식별하도록 해주는 식별자 역할을합니다. 따라서 배리어 이전에 처리된 레코드가 만든 상태 변경은 배리어의 현재 체크포인트에 포함됩니다.1. 잡 매니저가 모든 소스 태스크에 체크포인트 배리어를 주입한다2. 태스크가 배리어를 수신하면 모든 입력 스트림에서 배리어가 도착할 때 까지 기다린다3. 기다리는 동안 배리어를 전송한 스트림에서 들어온 레코드는 버퍼링 해놓는다4. 기다리는 동안 배리어를 전송하지 않은 스트림에서 들어온 데이터를 계속 처리한다5. 모든 입력 스트림으로부터 배리어가 도착하면 현재 태스크 상태를 백엔드에 체크포인트한다6. 이후 태스크에 연결된 모든 하위 병렬 태스크에 배리어를 브로드캐스팅한다7. 이후 태스크에 버퍼링 되어 있던 레코드를 처리하고 입력 스트림은 체크포인트 시점 위치에서 다시 시작된다8. 이렇게 소스에서부터 시작된 모든 배리어가 싱크 태스크에 도착하면 잡 매니저는 모든 태스크에서 체크포인트가 완료됐음을 기록한다9. 완료된 체크포인트는 장애 복구에 사용된다플링크의 체크포인트 수행은 모두 상태백엔드가 책임지고 있습니다. 상태백엔드의 한종류인 RocksDB는 비동기 체크포인팅을 지원합니다. 따라서 체크포인팅이 시작되면 RocksDB는 상태를 로컬에 복사하고 체크포인팅이 완료되면 이를 태스크 매니저에 알리고 태스크매니저는 이를 잡매니저에 알리며 하던 작업을 별도로 계속 수행하게 됩니다.Savepoint플링크의 세이브 포인트는 기존 데이터베이스 시스템의 복구 로그와 유사한 방식으로 체크 포인트와 다릅니다.체크포인트의 주요 목적은 예기치 않은 작업 실패 시 복구 메커니즘을 제공하는 것입니다. 체크포인트의 라이프사이클은 플링크에 의해 관리됩니다. 즉, 체크포인트는 사용자의 개입 없이 플링크에 의해 생성, 소유 및 해제됩니다.세이브포인트는 체크포인트와 동일한 메커니즘으로 내부적으로 생성되지만 개념적으로 다르기 때문에 생성 및 복원 비용이 다소 비쌀 수 있습니다. 세이브포인트는 주로 Flink 버전 업데이트, 작업 그래프 변경 등에 사용됩니다.세이브포인트는 사용자만 생성, 소유 및 삭제합니다. 즉, Flink는 작업 종료 후나 복원 후에도 저장 지점을 삭제하지 않습니다아래 표는 세이브포인트와 체크포인트를 비교해 놓은 것입니다.Exactly Once플링크에서 정확히 한 번 보장은 스트림 처리에서 상태의 일관성을 의미합니다. 결과를 보장하는 방법 중 가장 엄격한 방식으로 자주 비교되는 보장 방식에는 At Least Once 방식이 있습니다. 정확히 한 번 보장은 스트리밍 시스템이 배치 시스템을 뛰어넘기 위해 반드시 요구되는 조건 중에 하나입니다.At Least Once  데이터 유실 방지에 최우선  결과의 완결만 중요하고 중복은 수용 가능한 조건인 경우(ex. 최대값 구하는 연산)  소스나 어떤 버퍼에서 이벤트를 재생(re-play) 기능을 가지고 있어야함Exactly Once  가장 엄격한 보장 방식  유실 방지 뿐 아니라, 이벤트마다 정확히 한 번씩만 상태를 갱신해야함  At Least Once의 데이터 재생기능은 필수적으로 내포해야함  내부 상태 일관성을 보장하기 위해 트랜잭션 또는 스냅샷 메커니즘을 사용플링크에서는 위에서 설명했듯이 챈디-램포트 기반의 분산 스냅샷 메커니즘을 사용해 내부 상태를 체크포인팅함으로써 일관성을 보장합니다. 여기서 한가지 추가되어야 할 점은 체크포인트 수행 당시 마지막으로 소비했던 위치로 재설정 하여 이벤트를 재생(re-play)할 수 있어야 한다는 것입니다.데이터 소스의 입력 스트림 재생기능은 소스의 구현과 인터페이스에 달려있습니다. 따라서 정확히 한 번 보장을 위해서는 아파치 카프카와 같은 이벤트 로그를 입력 소스로 사용해야 합니다. 카프카는 스트림을 이전 오프셋으로 설정해 과거 레코드를 다시 재생할 수 있도록 구현되어 있습니다.StateBackend상태 백엔드의 역할은 크게 다음과 같습니다.  로컬 상태 관리  원격 저장소에 상태를 체크포인팅플링크에서 제공하는 대표적인 상태 백엔드 두가지는 다음과 같습니다.HashMap StateBackend  Java Heap에 상태를 객체로 저장  해시테이블에 변수와 트리거를 저장  메모리 사용으로 빠른 처리Embedded RocksDB StateBackend  직렬화한 상태 데이터를 로컬 하드 디스크에 저장  디스크와 serialize 사용으로 성능과 처리량간의 트레이드-오프참고  아파치 플링크로 하는 스트림 데이터 처리 책  Flink 공식문서: Stateful Stream Processing  Flink 공식문서: State Backends  Flink 공식문서: What is Apache Flink? — Applications  Flink 공식문서: Working with State  Ververica: 3 important performance factors for stateful functions and operators in Flink  mehmetozanguven: Apache Flink Series 8-State Backend &amp;amp; State Example",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/flink-series3'> <img src='/images/flink_logo.png' alt='Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series3'>Flink Series [Part3]: 스트림 처리의 핵심(2) 상태관리</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Container Series [Part2]: 진화하는 컨테이너 표준",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/container-series2",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents  참고참고  Docker 공식문서: Docker Engine overview  kubernetes 공식문서: 컨테이너 런타임  tutorialworks: The differences between Docker, containerd, CRI-O and runc  이곤아이: docker 와 cri-o 비교  LinkedIn: containerd는 무엇이고 왜 중요할까?  cloud native wiki: 3 Types of Container Runtime and the Kubernetes Connection  pageseo: Docker Engine, 제대로 이해하기 (1)  Devin Jeon, Kubernetes의 Docker container runtime 지원 중단에 대하여",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/container-series2'> <img src='/images/container_7.png' alt='Container Series [Part2]: 진화하는 컨테이너 표준'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series2'>Container Series [Part2]: 진화하는 컨테이너 표준</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Airflow Series [Part1]: What is Airflow",
      "category" : "",
      "tags"     : "Airflow",
      "url"      : "/airflow-series1",
      "date"     : "Feb 5, 2022",
      "content"  : "Table of Contents아직 작성 전 입니다…",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-05T21:01:35+09:00'>05 Feb 2022</time><a class='article__image' href='/airflow-series1'> <img src='/images/airflow_logo.png' alt='Airflow Series [Part1]: What is Airflow'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/airflow-series1'>Airflow Series [Part1]: What is Airflow</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part2]: 스트림 처리의 핵심(1) 시간",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series2",
      "date"     : "Feb 4, 2022",
      "content"  : "Table of Contents  시간의 중요성  처리 시간과 이벤트 시간  Timely Stream Processing          트리거                  반복 업데이트 트리거          완료 트리거                    워터마크                  워터마크 전파                      참고시간의 중요성스트림 처리에서 핵심적인 개념은 ‘시간’과 ‘상태 관리’입니다. 이번 포스트에서는 그 중 첫 번째인 ‘시간’에 대해 알아보도록 하겠습니다. 네트워크와 통신 채널 같은 현실 세계 시스템은 완벽하지 않기 때문에 데이터는 지연되거나 그로 인해 순서가 바뀌어 도착할 수도 있습니다. 이런 상황에서 스트림 처리가 정확하고 일관된 결과를 생성하는 것은 매우 중요한 문제입니다.처리 시간과 이벤트 시간  이벤트 시간: 이벤트가 발생한 시간  처리 시간: 이벤트가 스트림 처리 연산을 실행 중인 장비에서 처리된 시간만약 이벤트가 발생한 즉시 처리 될 수 있다면 두 시간을 굳이 구분할 필요가 없을 것입니다. 하지만 이러한 두 가지 시간은 일치하지 않을 뿐 아니라, 두 시간의 차이 또한 일정하지 않습니다. 현실 상황에서 일어날 수 있는 예시를 통해 각각의 방법으로 시간을 정의했을 때 어떤 차이가 생기는지 알아보겠습니다.베를린에 살고 있는 앨리스는 출근마다 지하철 안에서 모바일 게임을 합니다. 게임에서는 500개의 풍선을 1분 안에 터트리면 보상을 받는 식입니다. 그런데 베를린 지하철의 네트워크는 자주 끊긴다는 문제가 있습니다. 앨리스가 게임을 하게 되면 발생된 이벤트를 스트림 처리 어플리케이션으로 보내진다고 할 때, 만약 게임 도중 네트워크가 끊기면 어떻게 될까요?위의 그림을 보면, 앨리스의 핸드폰에서는 1분 동안 총 6개의 데이터가 발생했습니다. 하지만 뒤에 2개의 데이터는 지하철이 터널을 지나고 있던 시점이라 아직 스트림 처리 어플리케이션으로 전송이 지연되었습니다. 이러한 상황에서 처리 시간을 기준으로 데이터를 처리한다면 4개의 데이터만을 고려해 결과를 낼 것입니다.하지만 이벤트 시간은 이벤트 내용 안에 포함된 타임스탬프를 기반으로 하기 때문에, 지연되어 도착한 2개의 데이터도 1분 안에 보내진 데이터로 간주됩니다. 그렇기 때문에 스트림 분석 어플리케이션은 지연된 2개의 데이터를 기다릴 것이고 모두 도착한 이후 결과를 계산할 겁니다. 따라서 이벤트 시간은 이벤트의 일부가 지연되더라도 발생했던 일을 제대로 반영할 수 있습니다.위의 예를 통해 알 수 있듯이, 이벤트 시간을 기반으로 하는 연산들은 예측할 수 있고 결과는 항상 결정적(deterministic)입니다. 지연된 이벤트의 처리 뿐만 아니라 데이터의 순서가 바뀌는 문제 또한 이벤트 시간을 사용함으로써 결과의 정확성을 보장할 수 있습니다.만약 시간이나 순서가 중요한 데이터가 아니라면 처리 시간을 기준으로 연산을 해도 됩니다. 하지만 시간에 따른 사용자 행동 분석, 결제 관련 서비스, 이상 탐지 등 대부분의 스트림 처리 어플리케이션은 지연 고려, 순서와 같은 요소들이 중요하기 때문에 이벤트 시간을 기준으로 연산을 제공할 수 있어야 합니다.이벤트 시간을 기준으로 사용하기 위해서는 워터마크라는 추가적인 설정을 해주어야 합니다.Timely Stream Processing실시간 스트림 처리가 어려운 이유 중 하나는 데이터가 Unbounded(무한) Unordered(비정렬) 할 수 있다는 점입니다. Unordered한 특성은 이벤트 시간을 기준으로 타임스탬프를 적용해 해결할 수 있습니다. 그러면 Unbounded는 어떻게 해결할 수 있을까요? Unbounded한 특성은 윈도우를 이용해 해결할 수 있습니다.아래 애니메이션은 Bounded(유한)한 데이터인 경우입니다. 이 경우에는 일정 시간이 지난 후에는 데이터가 더 들어오지 않기 때문에 데이터가 들어온 시간 범위 내에서 전체 데이터를 배치로 처리할 수 있습니다.하지만 Unbounded한 경우에는 데이터가 끝이 없습니다. 일정 시간이 지난 후에 데이터가 들어올지 안들어올지 모르는 상황이기 때문에, 이러한 경우에는 윈도우를 사용해 데이터를 처리해야 합니다.여기까지 오면 이제 실시간 스트림 처리에 조금 더 자신감이 생긴 것 같습니다. 이벤트 시간을 기준으로 타임스탬프를 정의하고, 윈도우를 사용해 무한한 데이터를 유한한 데이터로 나누어 봄으로써 Unbounded Unordered한 데이터가 발생하더라도 스트림 처리가 가능해졌습니다.하지만 한가지 문제점이 남아있습니다. 현실 시스템에서 데이터 지연은 필연적으로 발생하게 되는데, 각 윈도우들에서 지연된 데이터를 포함한 모든 데이터를 받았다는 사실을 어떻게 알며 언제쯤 윈도우에서 결과를 출력할 수 있을까요? 여기서 등장하는 중요한 개념이 바로 ‘트리거’와 ‘워터마크’입니다.트리거  윈도우가 출력되는 시점을 선언하는 방법  결과가 생성되는 시기 제어  윈도우별로 생성되는 각 출력을 윈도우의 패널이라고 함반복 업데이트 트리거  주기적으로 결과를 출력 (새로운 데이터를 만날 때마다, 1분 마다 등)  간단하고 쉽게 구현  언제 결과의 정확성이 신뢰할만한 수준에 다다랐는지 모른다는 한계점 (데이터 만날 때마다 결과를 출력하는 반복 업데이트 트리거)완료 트리거  윈도우 내 입력이 일정 기준 완료됐다고 믿는 시점에 결과를 출력  워터마크가 각 윈도우의 입력 완료 시점을 나타냄  각 윈도우의 입력이 완료되었다고 판단하는 순간 워터마크가 윈도우의 끝으로 이동하고 패널 출력  지연된 데이터 고려해 시점을 결정  패널이 업데이트 되지 않음(지연된 데이터를 알고있는 ‘완벽한 워터마크’와 지연된 데이터의 유무를 모르는 현실적인 ‘휴리스틱 워터마크’)노란색 숫자는 윈도우의 출력인 패널의 값을 나타냅니다. 트리거 설정이 중요한 이유는 어떻게 설정하는가에 따라 정확성과 지연 시간간의 트레이드-오프가 생긴다는 점입니다. 이 부분에 대해서는 뒤에서 다시 자세히 얘기하도록 하겠습니다.완료 트리거가 윈도우 내 입력이 일정 기준 완료되었다고 판단하는 기준이 워터마크입니다. 그렇기 때문에 워터마크에 대해 조금 더 알아보고 완료 트리거에 관한 내용을 정리한 뒤, 트리거 설정에 따른 정확성과 지연 시간 사이의 트레이드-오프 얘기를 마무리 하겠습니다.워터마크  어떤 이벤트 시간을 기준으로 입력이 완료됐음을 표시하는 방법  완료 트리거에서의 핵심  종류별로 이상적인 워터마크, 완벽한 워터마크, 휴리스틱 워터마크가 있음          이상적인 워터마크: 이벤트 시간과 처리 시간이 같다고 가정(이벤트 발생과 동시에 처리), 비현실적      완벽한 워터마크: 시스템이 지연되는 데이터 유무를 알 수 있는 경우, 비현실적      휴리스틱 워터마크: CPU, 분산 시스템, 네트워크 상황을 통해 지연된 데이터를 예측, 현실적      이제 다시 위 애니메이션을 보면 왼쪽 초록색 점선의 완벽한 워터마크는 지연 데이터 9를 알고 워터마크가 X축(이벤트 시간)으로 더 증가하지 않고 있습니다. 오른쪽 초록색 실선의 휴리스틱 워터마크는 지연 데이터 9를 놓쳐서 누락되었습니다.휴리스틱 워터마크에서 지연 데이터 9가 누락된 이유에는 2가지 가능성이 있습니다. 첫 번째는 알고 있었지만 지연 데이터 9를 기다렸다가 패널을 출력하기에는 지연율이 높아져서 데이터를 포기한 것일 수도 있고, 두 번째는 휴리스틱 워터마크의 불완전성으로 인해 놓친 경우일 수도 있습니다.첫 번째 이유를 보면 휴리스틱 워터마크를 사용한 완료 트리거가 지연율을 낮추기 위해 정확성을 조금 포기했습니다. 이를 통해 트리거에 지연율과 정확성의 트레이드-오프가 있음을 알 수 있습니다.이러한 트레이드-오프를 조절하는 것도 가능합니다. 이를 위해 조기/정시/지연 트리거들이 있습니다.조기(early) 트리거  지연 데이터로 휴리스틱 워터마크가 찍히는 시점이 너무 늦어지는 경우에 사용하면 좋음  업데이트 트리거 - 완료 트리거 순으로 조합정시(on-time) 트리거  완료 트리거  정시 트리거는 항상 워터마크가 윈도우 끝에 도달한 순간에만 패널 출력지연(late) 트리거  휴리스틱 워터마크의 불완전성으로 계속 지연 데이터를 놓치는 경우에 사용하면 좋음  완료 트리거 - 업데이트 트리거 순으로 조합위 애니메이션에서 이벤트 시간 12:03 부분을 보게 되면 워터마크는 아직 12:03부분에 머물러있는데 3과 4 데이터를 가진 윈도우가 패널을 출력했습니다. 이렇게 워터마크 이전에 패널을 출력하는 경우가 조기 트리거를 사용한 경우입니다.워터마크 전파지금까지 위에서 워터마크의 용도에 대해 알아보았습니다. 워터마크가 윈도우내 입력 완료 시점을 알려주고 완료 트리거를 보조한다는 점에서 굉장히 유용하다는 것을 깨달았습니다. 여기서는 조금 더 실제적인 측면에 대해 보려고 합니다.  워터마크는 어디서 만들어지는 것인가?  파이프라인 내 여러 컴포넌트들로 어떻게 전파되는가?참고  Flink 공식문서: Timely Stream Processing  Flink and Kafka Streams: a Comparison and Guideline for Users  Flink 공식문서: Windows  Streaming Systems 시각 자료 참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-04T21:01:35+09:00'>04 Feb 2022</time><a class='article__image' href='/flink-series2'> <img src='/images/flink_logo.png' alt='Flink Series [Part2]: 스트림 처리의 핵심(1) 시간'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series2'>Flink Series [Part2]: 스트림 처리의 핵심(1) 시간</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 네트워크 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series9",
      "date"     : "Feb 4, 2022",
      "content"  : "Table of Contents  Bridge Network Driver  Overlay Network Driver  도커 네트워크 실습          도커 네트워크의 몇 가지 특징      Bridge 드라이버 사용해보기      Overlay 드라이버 사용해보기        참고도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다.이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.당연히 다음 질문은 어떤 네트워크 드라이버를 사용해야 하는가 하는 것입니다. 각 드라이버는 트레이드오프를 제공하며 사용 사례에 따라 다른 장점이 있습니다. 도커 엔진과 함께 제공되는 내장 네트워크 드라이버가 있으며 네트워킹 벤더와 커뮤니티에서 제공하는 플러그인 네트워크 드라이버도 있습니다. 가장 일반적으로 사용되는 내장 네트워크 드라이버는 bridge, overlay, macvlan입니다. 이번 포스트에서는 비교적 간단한 드라이버인 bridge와 overlay에 대해서만 살펴보겠습니다.Bridge Network Driverbridge 네트워크 드라이버가 우리 목록의 첫 번째 드라이버입니다. 이해하기 쉽고, 사용하기 쉽고, 문제 해결이 간단하기 때문에 개발자와 Docker를 처음 접하는 사람들에게 좋은 네트워킹 선택이 됩니다. bridge 드라이버는 private 네트워크를 호스트 내부에 생성해 컨테이너들이 생성한 네트워크 안에서 통신할 수 있도록 합니다. 컨테이너에 포트를 노출함으로써 외부 액세스가 허용됩니다. 도커는 서로 다른 도커 네트워크 간의 연결을 차단하는 규칙을 관리하여 네트워크를 보호합니다.내부적으로 도커 엔진은 리눅스 브리지, 내부 인터페이스, iptables 규칙 및 호스트 경로를 만들어 컨테이너 간의 연결을 가능하게 합니다. 아래 강조 표시된 예에서는 도커 브리지 네트워크가 생성되고 두 개의 컨테이너가 이 네트워크에 연결됩니다. 도커 엔진은 별도의 설정 없이 필요한 연결을 수행하고 컨테이너에 대한 서비스 디스커버리를 제공하며 다른 네트워크와의 통신을 차단하도록 보안 규칙을 구성합니다.우리의 애플리케이션은 현재 호스트 8000번 포트에서 서비스되고 있습니다. 도커 브리지는 컨테이너 이름으로 web이 db와 통신할 수 있도록 하고 있습니다. 브릿지 드라이버는 같은 네트워크에 있기 때문에 자동으로 우리를 위해 서비스 디스커버리를 합니다.브리지 드라이버는 로컬 범위 드라이버이므로 단일 호스트에서 서비스 디스커버리, IPAM 및 연결만 제공합니다. 다중 호스트 서비스 검색을 수행하려면 컨테이너를 호스트 위치에 매핑할 수 있는 외부 솔루션이 필요합니다. 이 때 필요한 것이 바로 overlay 드라이버입니다.Overlay Network Driveroverlay 네트워크 드라이버는 multi-host 네트워킹의 많은 복잡성을 획기적으로 단순화합니다. Swarm 스코프 드라이버로, 개별 호스트가 아닌 전체 Swarm 또는 UCP 클러스터에서 작동합니다.overlay 드라이버는 컨테이너 네트워크를 물리적 네트워크와 분리해주는 VXLAN data plane을 사용합니다. 덕분에 다양한 클라우드, 온-프레미스 네트워크 환경 속에서 최고의 이식성을 제공해줍니다.도커 네트워크 실습(라우드 엔지니어 Won의 성장 블로그 참고)도커 네트워크의 몇 가지 특징  도커는 컨테이너에 내부 IP(eth0)를 순차적으로 할당  컨테이너 외부에 노출시킬 엔드포인트로 veth(Virtual Ethernet) 생성  컨테이너마다 veth 네트워크 인터페이스 자동 생성  docker0는 기본 생성되는 디폴트 브리지로 각 veth 인터페이스와 호스트의 기본 네트워크인 eth0와 연결Bridge 드라이버 사용해보기Overlay 드라이버 사용해보기참고  도커 공식문서  MARK CHURCH, Understanding Docker Networking Drivers and their use cases  클라우드 엔지니어 Won의 성장 블로그, 06. 도커 네트워크 포스트  DaleSeo: Docker 네트워크 사용법  Julie의 Tech블로그, 도커 - 네트워킹 / bridge와 overlay",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-04T21:01:35+09:00'>04 Feb 2022</time><a class='article__image' href='/docker-series9'> <img src='/images/docker_network_logo.png' alt='Docker의 네트워크 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series9'>Docker의 네트워크 이해하기</a> </h2><p class='article__excerpt'>이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part6]: RESTful API란",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series6",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  참고HTTP API와 REST API는 사실 거의 같은 의미로 사용됩니다.그런데 디테일하게 들어가면 차이가 있습니다.HTTP API는 HTTP를 사용해서 서로 정해둔 스펙으로 데이터를 주고 받으며 통신하는 것으로 이해하시면 됩니다.그래서 상당히 넓은 의미로 사용됩니다.반면에 REST API는 HTTP API에 여러가지 제약 조건이 추가됩니다.REST는 다음 4가지 제약조건을 만족해야 합니다.(https://ko.wikipedia.org/wiki/REST)      자원의 식별        메시지를 통한 리소스 조작        자기서술적 메서지        애플리케이션의 상태에 대한 엔진으로서 하이퍼미디어  여러가지가 있지만 대표적으로 구현하기 어려운 부분이 마지막에 있는 부분인데요. 이것은 HTML처럼 하이퍼링크가 추가되어서 다음에 어떤 API를 호출해야 하는지를 해당 링크를 통해서 받을 수 있어야 합니다.그리고 이런 부분을 완벽하게 지키면서 개발하는 것을 RESTful API라고 하는데요. 실무에서 이런 방법으로 개발하는 것은 현실적으로 어렵고, 또 추가 개발 비용대비 효과가 있는 것도 아닙니다.그런데 이미 많은 사람들이 해당 조건을 지키지 않아도 REST API라고 하기 때문에, HTTP API나 REST API를 거의 같은 의미로 사용하고 있습니다. 하지만 앞서 말씀드린 것 처럼 엄격하게 위의 내용들을 모두 지켜야 REST API라고 할 수 있습니다.(하지만 다들 HTTP API를 REST API라고 이미 하고 있기 때문에, 누군가 REST API라고 하면 그냥 아~ HTTP API를 이야기 하는구나 라고 생각하고 들으시면 됩니다. 물론 엄격하게는 다릅니다.)참고  이영한님의 HTTP API와 REST API의 차이에 관한 질문에 대한 답변  REST 논문을 정리한 자료",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/network-series6'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part6]: RESTful API란'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series6'>Network Series [Part6]: RESTful API란</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part5]: HTTP의 기초",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series5",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  인터넷 프로토콜 스택  URL  HTTP Method  HTTP 메세지          시작 라인                  요청 메세지인 경우          응답 메세지의 경우                    HTTP 헤더      HTTP 바디      예시                  요청 메세지          응답 메세지                      참고인터넷 프로토콜 스택저희가 단말기를 통해 인터넷상에서 데이터를 전송하거나 요청할 때 일련의 과정들이 있습니다. 이러한 과정들을 계층적으로 정의해놓은 것을 OSI 7 계층이라고 합니다. 요즘에는 이러한 계층을 조금 더 간소화해 TCP/IP 4 계층으로 표현하기도 합니다.이러한 OSI 7 계층 또는 TCP/IP 4 계층에는 계층마다 가지고 있는 프로토콜(규칙)들이 있습니다. 위 그림의 가운데 열에 나열된 것들이 각 계층에서 사용되는 프로토콜들이고 이러한 열을 인터넷 프로토콜 스택이라고 합니다.HTTP(HyperText Transfer Protocol)는 응용 계층(Application layer)에서 압도적으로 많이 사용되는 프로토콜입니다.HTTP가 하는 역할은 무엇일까요? 저희는 응용 계층에서 데이터를 주고받기 위해(크롬, 사파리와 같은 웹 브라우저에서 뉴스, 사진, 동영상을 보고 물건을 주문하는 것과 같은 행위) 클라이언트는 요청(request), 서버는 응답(response)하는 방식을 사용합니다.  이 때 응용 계층에 있는 단말기(우리의 핸드폰, 노트북 그리고 구글이 가지고 있는 웹 서버와 같은 것들)들이 서로 일관된 방법으로 데이터를 주고받기 위해 규약이 필요했는데 이때 생긴 규약이 바로 HTTP입니다.이 때 클라이언트는 HTTP 메세지를 작성하기 위해 두 가지를 사용합니다. 바로 URL과 HTTP 메소드입니다.URLURL은 Uniform Resource Locator의 약자입니다. URL은 URI(Uniform Resource Identifier)를 표현하기 위한 방법 중 하나입니다. URL말고도 URN이라는 것이 있지만 지금은 거의 URL만 사용하기 때문에 URN은 생략하도록 하겠습니다.인터넷에서 어떤 자원(회원 정보, 주문서, 사진, 동영상 등)을 유일하게 표현하기 위해 URI라는 개념이 등장했고 이를 위한 방법으로 URL을 사용하는 것입니다. URL은 이러한 자원들에게 부여된 고유한 주소를 말합니다.인터넷에서는 모든 자원에 URL이라는 고유한 주소를 부여해 이들을 식별한다URL의 예시를 보겠습니다.https://google.co.kr/search?q=hello&amp;amp;hl=kohttps://order.kyobobook.co.kr/order/orderStepOneURL 문법은 아래와 같습니다.# URL 문법scheme://[userinfo@]host[:port][/path][?query][#fragment]예: https://www.google.com/search?q=hello&amp;amp;hl=ko# scheme예: https- 주로 프로토콜이 사용됩니다.- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 (https, http, ftp)- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트- https는 http에 보안 추가 (HTTP Secure)# host예: www.google.com- 도메인명 또는 IP주소# port예: 8888- 접속 포트# path예: /search- 리소스 경로 (계층적 구조)- 디렉토리명/파일명# query예: ?q=hello&amp;amp;hl=ko- key=value 형태- ?로 시작, &amp;amp;로 추가 가능- query parameter 또는 query string으로 보통 불림# fragment예: #getting-started-introducing-spring-boot- html 내부 북마크 등에 사용- 서버에 전송하는 정보는 아님URL에서 유의할 점은 URL은 자원을 식별하는 용도로만 써야 한다는 것입니다. 예를 들어 어떤 물건을 주문할 때는 주문(order)만을 URL로 표현해야지 주문 확인(order-check), 주문 취소(order-cancel) 이런 행위까지를 포함시키면 안됩니다.HTTP Method이러한 행위를 나타내기 위해 사용하는 것이 바로 HTTP 메소드입니다.인터넷에서 발생하는 행위는 크게 CRUD(Create-Read-Update-Delete)로 나눌 수 있습니다. CRUD를 HTTP에서 제공하는 메소드로 구현할 수 있습니다.            HTTP Method      설명              GET      읽기(리소스 조회)              POST      쓰기(리소스 등록)              PUT      업데이트(리소스 완전 대체)              PATCH      부분 업데이트(리소스 부분 대체)              DELETE      삭제(리소스 삭제)      HTTP 메세지클라이언트와 서버는 URL과 HTTP 메소드를 이용해서 HTTP 메세지를 만들어 통신한다고 했습니다. HTTP 메세지는 바이너리로 표현할 수 있는 모든 데이터를 전송할 수 있습니다. (HTML, TEXT, JSON, XML, 이미지, 영상 파일 등)서버간에 데이터를 주고받을 때에도 대부분 HTTP를 사용한다고 합니다.HTTP 메세지의 구조는 다음과 같습니다.시작 라인요청 메세지인 경우  HTTP 메소드          종류: GET, POST, PUT, DELETE …      서버가 수행해야 할 동작 지정                  GET: 리소스 조회          POST: 요청 내역 처리                      요청 대상          절대경로[?쿼리]      절대경로=”/”로 시작하는 경로        HTTP 버전응답 메세지의 경우      HTTP 버전        HTTP 상태 코드          200: 성공      400: 클라이언트 요청 오류      500: 서버 내부 오류      HTTP 헤더  용도          HTTP 전송에 필요한 모든 부가정보      메세지 바디의 내용, 메세지 바디의 크기, 압축, 클라이언트 정보, ..      HTTP 바디  실제 전송할 데이터  HTML, 이미지, 영상, JSON 등 byte로 표현 가능한 모든 데이터 전송 가능이렇게 HTTP 메세지를 통해서 두 단말기가 응용계층에서 쉽게 통신할 수 있도록 하는 API(Application Program Interface)를 REST(Representational State Transfer) API라고 합니다.예시요청 메세지응답 메세지참고  인프런에서 제공하는 이영한님의 모든 개발자를 위한 HTTP 웹 기본 지식 강의  REST 논문을 정리한 자료  사바라다는 차곡차곡: [REST API] REST에서의 Resource",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/network-series5'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part5]: HTTP의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series5'>Network Series [Part5]: HTTP의 기초</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part4]: IP주소와 DNS 서버",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series4",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  IP(Internet Protocol) 주소          IP 주소      서브넷 마스크(Subnet Mask)        DNS(Domain Name System) 서버IP(Internet Protocol) 주소IP 주소IP 주소는 인터넷에 연결하고자 하는 디바이스가 가지고 있는 NIC(Network Interface Controller)의 고유한 주소를 뜻합니다. 편지를 주고 받기 위해서는 서로의 주소가 필요한 것처럼 디바이스간 통신을 위해서는 IP주소가 필요합니다. IP주소는 네트워크 번호와 호스트 번호로 이루어진 32비트 숫자입니다.(IPv4 기준)서브넷 마스크(Subnet Mask)DNS(Domain Name System) 서버DNS 서버는 도메인 네임을 IP주소로 매핑하여 보관하고 있는 서버입니다. 하지만 모든 도메인 정보를 저장할 수는 없고 저장한다고 해도 IP주소를 가지고 오는데 많은 시간이 소요됩니다. 이를 해결하기 위해 DNS 서버를 계층적으로 구성해 IP 주소를 가져오도록 했으며 한 번 가져온 정보는 캐시에 저장해둡니다. 하지만 캐시에 저장된 후 정보가 변경될 수 있기 때문에 캐시에 저장된 정보는 유효기간이 지나면 캐시에서 삭제됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/network-series4'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part4]: IP주소와 DNS 서버'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series4'>Network Series [Part4]: IP주소와 DNS 서버</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초",
      "category" : "",
      "tags"     : "Flink",
      "url"      : "/flink-series1",
      "date"     : "Feb 3, 2022",
      "content"  : "Table of Contents  Flink  Use Cases          이벤트 기반 아키텍처      스트림 데이터 처리 및 분석        결론  참고Flink플링크는 분산 스트림 처리 오픈소스 프레임워크입니다 플링크는 상태가 있는 스트림 처리(Stateful Stream Processing) 애플리케이션을 구현하는 데 필요한 다양한 API를 제공합니다. 플링크는 2014년 4월, 아파치 소프트웨어 재단의 인큐베이터 프로젝트로 시작해 2015년 1월, 아파치 최상위 프로젝트가 되었습니다. 플링크는 현재 넷플릭스, 에어비앤비, 우버 등 전 세계 많은 기업의 대규모 스트림 처리 업무에서 핵심적인 역할을 하고 있습니다.플링크가 스트림 처리를 위해 제공하는 스펙은 다음과 같습니다.  Low Latency: ms단위의 지연시간 보장  High Throughput: 초당 100만 단위의 이벤트 처리  In-Memory Computing: 인-메모리 컴퓨팅  Exactly-Once State: 정확히 한 번 상태 보장(데이터의 중복, 유실 방지, 장애 복구 가능)  Out-of-Order Event Handling: 순서가 뒤바뀐 이벤트가 들어오더라도 일관성 있는 결과 제공  Scale-Out Architecture: 수 천대의 클러스터 환경으로 확장 가능(Flink 공식문서 참고)Use Cases플링크는 스트림 처리뿐만 아니라 짧은 지연을 요구하는 다양한 어플리케이션에 사용될 수 있습니다.  이벤트 기반 어플리케이션  스트림 데이터 처리 및 분석 어플리케이션이벤트 기반 아키텍처이벤트 기반 어플리케이션은 이벤트 스트림을 입력으로 받아 특정 비즈니스 로직을 처리합니다. 비즈니스 로직에 따라 알람이나 이메일을 전송할 수도 있고, 다른 어플리케이션이 소비할 수 있도록 외부로 이벤트를 내보낼 수도 있습니다. 일반적으로 실시간 추천, 패턴 감지, 이상 탐지 SNS 웹 어플리케이션 등에 사용됩니다.이벤트 기반 아키텍처는 기존의 트랜잭션 처리 아키텍처와 다음과 같은 차이점이 있습니다.트랜잭션 처리 아키텍처  컴퓨팅 계층과 데이터 스토리지 계층이 분리  원격(외부) 데이터베이스에서 데이터를 읽음  REST 통신 방식으로 서로 연결이벤트 기반 아키텍처  로컬(메모리 내 또는 디스크 내) 데이터 액세스를 통한 빠른 처리량, 낮은 지연율 제공  원격 영구 스토리지에 정기적으로 체크포인트를 기록함으로써 내결함성 달성  이벤트 로그로 서로 연결함으로써 데이터 전송과 수신을 비동기화이벤트 기반 아키텍처로 구현한 어플리케이션을 위해서는 정확히 한 번 수준의 상태 일관성과 수평 확장성을 요구하며 이벤트 시간 지원 여부, 상태 관리의 품질 등이 결정하게 됩니다. 플링크는 이 모든 기능을 포함하고 있기에 이벤트 기반 어플리케이션 구현을 위한 좋은 선택이 될 수 있습니다.스트림 데이터 처리 및 분석스트림 데이터 처리 및 분석 어플리케이션이라고 해서 이벤트 기반 어플리케이션과 크게 다른 것은 아닙니다. 기본적으로 실시간으로 들어오는 스트림 이벤트 또는 데이터를 받으면 그에 맞게 저지연으로 동작하고 상태를 관리하고 결과를 내보내는 것이기 때문에 비슷한 개념입니다.이벤트 기반 어플리케이션이 어떤 비즈니스 로직을 처리한다면, 처리 및 분석 어플리케이션은 처리(Processing)나 분석(Analytics)을 한다는 차이만 있을 뿐입니다.(공식문서에서는 유스케이스를 이벤트 기반 어플리케이션, 데이터 파이프라인, 스트리밍 분석으로 나누어 구분하지만 이것들을 구분하는 것은 내가 느끼기에는 사소하고 괜히 구분지으려고 하는 것이 더 헷갈린다.)결론결론은 플링크는 결국 실시간으로 발생하는 데이터 또는 이벤트를 짧은 지연으로 비즈니스 로직으로 처리, 프로세싱, 분석하기 위한 용도입니다.참고  Flink 공식문서: What is Apache Flink? — Applications  Flink 공식문서: Stateful Stream Processing  High-throughput, low-latency, and exactly-once stream processing with Apache Flink  What makes Flink provides low latnecy, Is Apache Flink the future of Real-time Streaming?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-03T21:01:35+09:00'>03 Feb 2022</time><a class='article__image' href='/flink-series1'> <img src='/images/flink_logo.png' alt='Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/flink-series1'>Flink Series [Part1]: 플링크로 알아보는 스트리밍 처리 기초</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part3]: 웹브라우저의 동작(Application Layer)",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series3",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  어플리케이션 계층(Application Layer)  웹 브라우저의 동작원리          HTTP 리퀘스트 작성                  URL 입력          URL에 관한 문법          HTTP 리퀘스트 작성                    DNS 서버에 웹 서버의 IP주소 조회                  DNS Resolver를 이용해 DNS 서버 조회                    프로토콜 스택에 메시지 송신 요청        참고어플리케이션 계층(Application Layer)어플리케이션 계층은 인터넷으로 연결 가능한 두 디바이스의 OSI layer 가장 끝단에 있는 계층으로, 웹 브라우저, 게임, 메일과 같은 것들이 있습니다. 그 중에서도 웹 브라우저(사파리, 크롬 등)는 웹 서버로의 접근, 파일 업로드/다운로드, 메일 전송과 같은 다양한 클라이언트 기능을 겸비한 복합적인 클라이언트 소프트웨어입니다. 그래서 저는 이번 포스트에서 웹 브라우저의 동작원리에 대해 집중적으로 알아보도록 하겠습니다.디바이스간 연결을 위해서는 계층별로 지켜야 규약들이 있습니다. 만약 이러한 규약들이 없다면 세상에 존재하는 다양한 디바이스들을 연결시키기 어렵습니다. 어플리케이션에서도 이러한 규약들이 있는데 대표적으로 다음과 같은 것들이 있습니다.Application Layer Protocols  HTTP: HyperText Transfer Protocol의 약자. 하이퍼링크로 연결된 html 문서(사실상 거의 모든 데이터)를 전송할 때의 규약  FTP: File Transfer Protocol의 약자. 파일을 업로드/다운로드 할 때 사용되는 규약  SMTP: Simple Mail Transfer Protocol의 약자. 메일을 전송할 때 사용되는 규약웹 브라우저의 동작원리우리가 웹 브라우저(크롬, 사파리 등)에서 뉴스 보기를 클릭하거나 유튜브 비디오를 시청할 때 내부적으로 어떤 일들이 일어나는지 한 번 알아보겠습니다.HTTP 리퀘스트 작성우리는 보통 웹 브라우저에서 URL을 입력하거나 어떤 버튼을 클릭하는 식으로 웹 서버와 상호작용 하게 되는데 이 때 웹 브라우저는 내부에서 HTTP 리퀘스트라는 것을 웹 서버에 전송합니다.URL 입력URL에 관한 문법# URL 문법scheme://[userinfo@]host[:port][/path][?query][#fragment]예: https://www.google.com/search?q=hello&amp;amp;hl=ko# scheme예: https- 주로 프로토콜이 사용됩니다.- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 (https, http, ftp)- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트- https는 http에 보안 추가 (HTTP Secure)# host예: www.google.com- 도메인명 또는 IP주소# port예: 8888- 접속 포트# path예: /search- 리소스 경로 (계층적 구조)- 디렉토리명/파일명# query예: ?q=hello&amp;amp;hl=ko- key=value 형태- ?로 시작, &amp;amp;로 추가 가능- query parameter 또는 query string으로 보통 불림# fragment예: #getting-started-introducing-spring-boot- html 내부 북마크 등에 사용- 서버에 전송하는 정보는 아님HTTP 리퀘스트 작성URL을 입력하고 나면 웹 브라우저는 URL을 바탕으로 HTTP 리퀘스트 메시지를 만듭니다.HTTP 리퀘스트 메시지의 형태는 다음과 같습니다.(joie-kim님 블로그 참고)DNS 서버에 웹 서버의 IP주소 조회HTTP 리퀘스트를 작성하고 나면 이제 OS에게 이것을 웹 서버로 전송해달라고 요청합니다. (웹 브라우저가 직접 전송하지 않는 이유는 메시지를 송신하는 기능은 하나의 애플리케이션에만 종속되는 기능이 아니므로 OS에서 전송 기능을 담당하는 것이 더 좋다고 합니다.)OS에서는 리퀘스트 메시지를 전송하기 전에 먼저 도메인 네임을 IP 주소로 변환하는 과정을 거칩니다. 이를 네임 레졸루션(name resolution)이라고 합니다.DNS Resolver를 이용해 DNS 서버 조회네임 레졸루션을 시행하는 것이 DNS 리졸버(DNS Resolver)입니다. 리졸버는 Socket 라이브러리에 들어있는 부품화된 프로그램입니다. Socket 라이브러리는 네트워크 관련 기능을 하는 프로그램을 모아놓은 라이브러리입니다.프로토콜 스택에 메시지 송신 요청DNS Resolver가 IP주소를 찾아오면 이제 진짜 웹 서버로 보낼 준비가 완료되었습니다. 이렇게 준비된 HTTP Request 메시지는 OS의 내부에 포함된 프로토콜 스택을 호출하여 실행을 요청합니다.참고  성공과 실패를 결정하는 1%의 네트워크 원리 책  imperva 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/network-series3'> <img src='/images/cs_logo.jpeg' alt='Network Series [Part3]: 웹브라우저의 동작(Application Layer)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series3'>Network Series [Part3]: 웹브라우저의 동작(Application Layer)</a> </h2><p class='article__excerpt'>웹 브라우저는 웹 서버로의 접근, 파일 업로드/다운로드, 메일 전송과 같은 다양한 클라이언트 기능을 겸비한 복합적인 클라이언트 소프트웨어입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 볼륨 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series8",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  도커에서 데이터 관리하기  마운트 종류  Volume          Volume을 사용하기 좋은 경우        Bind Mount          Bind mount를 사용하기 좋은 경우        tmpfs Mount          tmpfs mount를 사용하기 좋은 경우        사용 방법          Dockerfile      docker run command      docker compose        참고도커에서 데이터 관리하기기본적으로 컨테이너 안에서 생성된 모든 파일은 컨테이너 레이어에 저장됩니다. 그래서, 해당 컨테이너가 삭제되면 데이터도 함께 사라집니다. 따라서 컨테이너의 생명 주기와 관계없이 데이터를 영구적으로 저장하기 위한 방법이 필요합니다. 또한 여러 컨테이너가 데이터를 공유할 수 있으면 데이터를 컨테이너별로 중복 저장할 필요가 없어 컨테이너를 더욱 효율적으로 관리할 수 있게 될 것입니다.이러한 이유로 다음과 같은 기능을 가진 옵션을 도커에서는 제공해주고 있습니다.  데이터 영구 저장  컨테이너간 데이터 공유도커에서 볼륨을 위해 제공해주는 옵션 중 volume 또는 bind mount를 사용하면 컨테이너가 중지된 후에도 파일을 호스트 머신에 파일을 저장함으로써 유지할 수 있습니다.참고로 도커에서 tmpfs mount를 사용하면 호스트의 시스템 메모리에 인-메모리 형식으로 파일을 저장할 수 있습니다.마운트 종류어떤 종류의 마운트를 사용하더라도 컨테이너 안에서 데이터의 모습은 같습니다. 각 마운트 종류의 차이를 이해하는 쉬운 방법은 데이터가 도커 호스트 내에서 어디 존재하는지 생각해보는 것입니다.      Volume을 사용하면 도커에 의해 관리되는 호스트 파일 시스템(Linux기준 /var/lib/docker/volumes/)에 데이터가 저장됩니다. 비-도커 프로세스들은 여기 파일들을 수정해서는 안됩니다.        Bind mount는 호스트 머신 어디든 저장될 수 있습니다. 비-도커 프로세스들도 여기 파일들을 언제든 수정할 수 있습니다.        tmpfs mount는 호스트 메모리 시스템에만 저장됩니다. 호스트 파일 시스템에는 절대 저장되지 않습니다.  VolumeVolume은 완전히 도커에 의해서만 관리되어 호스트 머신의 디렉토리 구조나 OS에 독립적인, 도커에서 데이터를 유지하기 위한 권장되는 메커니즘입니다.또한 볼륨이 컨테이너를 사용하는 컨테이너의 크기를 늘리지 않고 컨테이너의 라이프사이클 외부에 존재하기 때문에 컨테이너 레이어에서 데이터를 유지하는 것보다 더 나은 선택인 경우가 많습니다.Volume은 docker volume create 명령어를 이용해 명시적으로 볼륨을 생성할 수도 있고, 컨테이너를 생성할 때 같이 볼륨을 생성할 수도 있습니다.볼륨은 도커 호스트 내의 디렉토리에 저장됩니다. 컨테이너에 마운트되는 볼륨이 바로 이것 입니다. volume은 bind mount와 비슷하지만 다른 점은 도커에 의해 관리 되고 호스트 머신으로부터 isolated 되었다는 점입니다.주어진 볼륨은 여러 컨테이너에 동시에 마운트 될 수 있습니다. 볼륨을 사용해 실행 중인 컨테이너가 없더라도 볼륨이 저절로 제거되지 않습니다. 만약 사용하지 않는 볼륨을 제거하고 싶다면 docker volume prune 명령어를 사용하면 됩니다.볼륨 드라이버를 사용해 클라우드 또는 리모트 호스트에 데이터를 저장할 수도 있습니다.Volume을 사용하기 좋은 경우  여러 컨테이너에 마운트하고 싶은 경우. 명시적으로 표현한 볼륨이 없으면 자동으로 생성하고 마운트 해줍니다.  도커 호스트의 파일 구조를 모르는 경우. bind mount와 달리 Volume은 볼륨 명으로 관리(bind mount는 디렉토리 경로)  로컬이 아닌 리모트 호스트 또는 클라우드 서버에 저장하고 싶은 경우  높은 성능의 I/O을 요구하는 경우. 볼륨은 호스트가 아닌 Linux VM에 저장. 따라서 읽고 쓰는데 있어 성능이 훨씬 뛰어납니다.  Docker Desktop에서 완전히 네이티브한 파일 시스템이 필요한 경우  백업, 데이터 통합이 필요한 경우Bind Mountbind mount는 volume에 비해 기능이 제한됩니다. bind mount를 사용하면 호스트 머신의 파일 또는 디렉토리가 컨테이너에 마운트됩니다. 파일 또는 디렉토리는 호스트 시스템의 절대 경로에서 참조됩니다. 반대로 volume을 사용하면 호스트 머신의 Docker 스토리지 디렉토리 내에 새 디렉토리가 생성되고 Docker가 해당 디렉토리의 내용을 관리합니다.파일 또는 디렉토리가 도커 호스트에 아직 존재하지 않아도 됩니다. 아직 존재하지 않는 경우 요청 시 생성됩니다. bind mount는 성능이 매우 뛰어나지만 사용 가능한 특정 디렉토리 구조가 있는 호스트 시스템의 파일 시스템에 의존합니다.bind mount를 사용하면 컨테이너에서 실행되는 프로세스를 통해 호스트 파일 시스템을 변경할 수 있습니다. 이는 호스트 시스템에 Docker가 아닌 프로세스에 영향을 미치는 등 보안에 안좋은 영향을 미칠 수 있는 강력한 기능입니다.Bind mount를 사용하기 좋은 경우  호스트 머신에 있는 설정 파일을 컨테이너와 공유하고 싶은 경우  파일 또는 디렉토리 구조가 항상 일관될 수 있는 경우tmpfs Mounttmpfs 마운트는 도커 호스트 또는 컨테이너 내에서 디스크에 유지되지 않습니다. 컨테이너 수명 동안 컨테이너가 비영구 상태 또는 중요한 정보를 저장하는 데 사용할 수 있습니다. 예를 들어, 내부적으로, swarm 서비스는 tmpfs 마운트를 사용하여 서비스의 컨테이너에 비밀을 탑재한다.tmpfs mount를 사용하기 좋은 경우  보안상의 이유 또는 대용량 데이터로 인한 성능 저하 우려로 데이터가 호스트 머신 또는 컨테이너에 유지되길 원하지 않는 경우.사용 방법Dockerfile  Dockerfile에는 볼륨 기능을 위해 VOLUME을 제공합니다.  이미지가 빌드되는 호스트 머신은 사용자에 따라 달라지므로 source는 지정할 수 없습니다.  VOLUME에서 표기하는 것은 오직 컨테이너안에 있는 볼륨의 destination입니다.VOLUME /myvolume  데이터가 사라지지 않도록 저장해두는 source는 컨테이너 생성/실행 시 표기할 수 있습니다.docker run -it -v $(pwd)/src:/src my-image  사실 Dockerfile에서 VOLUME은 사용하지 않는 것이 좋습니다. (어차피 source를 지정할 수 없으므로)docker run command컨테이너를 생성하거나 실행할 때 -v(--volume) or --mount 옵션을 이용해 볼륨을 마운트할 수 있습니다.  -v(--volume)# first field# volume옵션을 사용할 때는 명명된 볼륨이면 볼륨의 이름, 익명 볼륨이면 생략 가능# bind mount옵션의 경우 호스트 머신의 디렉토리 경로# second field는 컨테이너내의 마운트하고자 하는 경로# third field는 옵셔널, 예를 들어 ro(read only라는 의미)-v &amp;lt;first field&amp;gt;:&amp;lt;second field&amp;gt;:&amp;lt;thrid field&amp;gt;# 예시docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  --mount# type=volume, bind, tmpfs# source(src)# target(destination, dst)# readonly--mount &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, &amp;lt;key&amp;gt;=&amp;lt;value&amp;gt;, ..# 예시--mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app \  nginx:latest  볼륨 생성docker volume create my-voldocker volume ls-------------------------------DRIVER              VOLUME NAMElocal               my-voldocker volume inspect my-vol[    {        &quot;CreatedAt&quot;: &quot;2022-02-02T17:03:46Z&quot;,        &quot;Driver&quot;: &quot;local&quot;,        &quot;Labels&quot;: {},        &quot;Mountpoint&quot;: &quot;/var/lib/docker/volumes/my-vol/_data&quot;,        &quot;Name&quot;: &quot;my-vol&quot;,        &quot;Options&quot;: {},        &quot;Scope&quot;: &quot;local&quot;    }]docker volume rm my-vol  볼륨과 함께 컨테이너 실행# --mount flag 이용docker run -d \  --name devtest \  --mount source=myvol2,target=/app \  nginx:latest# -v flag 이용docker run -d \  --name devtest \  -v myvol2:/app \  nginx:latest  읽기 전용 볼륨# --mount flag 이용docker run -d \  -it \  --name devtest \  --mount type=bind,source=&quot;$(pwd)&quot;/target,target=/app,readonly \  nginx:latest# -v flag 이용docker run -d \  -it \  --name devtest \  -v &quot;$(pwd)&quot;/target:/app:ro \  nginx:latestdocker composedocker compose에 관한 포스트는 여기를 참고해주시면 감사드리겠습니다.version: &quot;3.9&quot;services:  frontend:    image: node:lts    volumes:      - myapp:/home/node/appvolumes:  myapp:참고  도커 공식문서: Manage data in Docker  도커 공식문서: Docker-compose volume configuration  DaleSeo: Docker 컨테이너에 데이터 저장 (볼륨/바인드 마운트)  stack overflow: Understanding “VOLUME” instruction in DockerFile",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series8'> <img src='/images/docker_17.png' alt='Docker의 볼륨 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series8'>Docker의 볼륨 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker의 아키텍처 이해하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series7",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Desktop, Engine, Server, Host, Daemon  Docker Server          Docker Daemon      Docker Driver                  Storage Driver          Network Driver          Execdriver                      참고Desktop, Engine, Server, Host, Daemon도커를 공부하면서 Docker Desktop, Engine, Server, Host, Daemon이라는 용어들의 관계가 조금 헷갈렸었습니다. 지금까지 배웠던 내용을 토대로 다음 용어들을 짧게 정리해볼까 합니다.      Docker DesktopMac, Windows 환경에서의 도커 애플리케이션입니다. 도커는 리눅스 기반의 운영체제에서 동작하는 어플리케이션이지만 Docker Desktop을 통해 Mac, Windows에서도 사용할 수 있도록 해줍니다. 또한 Docker Engine뿐 아니라 Docker Compose도 기본적으로 함께 설치되며 Kubernetes도 클릭 한 번으로 설치가능하도록 해줍니다.        Docker Engine일반적으로 저희가 도커를 생각할 때 가지고 있는 기능들을 곧 Docker Engine이라고 합니다. 다시 말해 도커 컨테이너를 생성하기 위해 요청하는 Client, 실제 컨테이너를 생성하고 관리하는 Server를 포함하는 Client-Server Application을 말합니다.        Docker ServerDocker Client로 부터 REST API 형태로 요청을 받았을 때 그 요청을 토대로 실제로 컨테이너를 생성하고 관리하는 부분을 말합니다.        Docker HostDocker Host는 Docker Engine이 설치된 곳을 말합니다. 제 컴퓨터가 Linux기반의 운영체제였다면 제 컴퓨터 자체가 Host가 되었을 것이고, 만약 클라우드 환경에서 서버를 하나 빌려서 거기에 도커를 설치했다면 빌린 서버가 Host가 될 것입니다.    참고로 제가 지금 사용하고 있는 환경은 Mac입니다. Mac의 운영체제는 Unix 계열의 운영체제로 Linux와는 사용하는 커널이 약간 달라서 결론적으로 도커를 다이렉트로 설치할 수 없습니다. 그래서 macOS 위에 Linux Virtual Machine을 하나 더 띄우고 그 위에서 도커를 설치 사용하게 됩니다. 이렇게 사용하면 기본적으로 Linux VM에 2GB 정도의 메모리가 사용된다고 합니다.  (참고: How much overhead from running Docker on a Mac?)        Docer DaemonDocker Daemon은 Docker Server 안에 있는 핵심 요소 중 하나로 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.  지금까지 도커의 큰 그림에서의 구성요소에 대해 살펴보았습니다. 지금부터는 그 중 Docker Server의 내부에 대해서 조금 더 살펴보려고 합니다. 위의 그림을 보면 Docker Client가 요청을 하면 나머지는 Docker Server에서 실행이 이루어지는데 Docker Server가 요청을 수행하기 위해 내부적으로 어떤 과정을 거치는지 한 번 알아보겠습니다.Docker Server아래 그림은 Docker Server의 아키텍처를 보여주는 좋은 그림입니다. 비록 2014년도에 그려진 그림이어서 최근 버전의 도커와는 차이가 있을 수 있지만 도커의 기본 구성요소를 공부하는 데에는 좋은 자료라고 생각합니다.크게 두 개의 사각형 덩어리가 각각 Docker Daemon과 Docker Driver입니다.(개인적으로 Engine이라고 적힌 부분은 마치 엔진과 같은 역할을 한다는 뜻일 뿐 저희가 위에서 배운 Docker Engine을 뜻하는 건 아니라고 생각합니다.)Docker DaemonDocker daemon 은 docker engine 내에서 주로 client 및 registry, driver 의 중심에서 작업의 분배를 담당하는 중심점이라고 보면 됩니다. client 로부터의 HTTP 요청을 내부 job 단위(가장 기본적인 작업 실행 단위)로 처리할 수 있도록 분배합니다. 즉, HTTP server 의 역할과 함께 client 요청을 분배(route and distribute), scheduling 하고, 요청에 대한 적합한 Handler 를 찾습니다. 요청에 대해 실질적인 처리는 Handler 를 통해 다른 모듈 들에게 전달하여 수행하고 그 결과를 응답으로 작성하여 client 에게 제공합니다.Docker DriverDocker Driver 는 크게 세 가지 범주로 나눌 수 있습니다.  graphdriver : container image 관리  networkdriver : 가상 bridge 등 container 의 network 관리  execdriver : container 생성 관리Storage Drivergraphdriver는 Storage Driver 라고 이해하면 됩니다. /var/lib/docker 내에 저장되어 있는 container, image 관련 정보들을 이용하여 사용자에게 통합된 File System으로 제공하는 드라이버입니다. built-in graphdriver 로는 btrfs, vfs, auts, devmapper, overlay2 등이 있습니다. Storage Driver에 관한 내용은 이 포스트를 참고하시면 됩니다.Network Driver도커의 네트워크의 철학은 CNM(Container Network Model)을 따릅니다. CNM은 컨테이너를 사용하는 환경에서 사용자가 네트워크 설계를 쉽게 하기 위한 것입니다. 다시 말해, 복잡한 물리적인 환경을 고려할 필요없이 사용자는 네트워크를 설계할 때 추상적인 개념만을 이용해 설계할 수 있게 됩니다. 이러한 추상화는 운영체제나 인프라 환경에 구애받지 않는 설계를 가능하도록 해줍니다. CNM을 구성하는 요소는 크게 다음과 같이 3가지가 있습니다.  Sandbox: 컨테이너의 Network의 많은 Endpoint를 설정하는 곳으로 Linux network namespace와 비슷한 개념으로 구현  Endpoint: 컨테이너 내의 eth 와 외부의 vth의 페어  Network: 네트워크는 직접적으로 통신을 할 수 있는 엔드포인트를 연결하는 역할2개의 Sandbox 안에 각각 Endpoint 요소를 하나 씩 만들고, 그 Endpoint 둘을 Network 이라는 요소에 연결해 컨테이너 간의 통신을 위한 네트워크를 구현할 수 있습니다. 이러한 개념(CNM)으로 네트워크를 구현해 놓은 것이 libnetwork이고 사용자가 사용할 수 있도록 기능을 제공하는 드라이버가 Networkdriver 입니다.Libnetwork provides the network control and management plane (native service discovery and load balancing). It accepts different drivers to provide the data plane (connectivity and isolation).Some of the network drivers that we can choose are:  bridge: it creates single-host bridge networks. Containers connect to these bridges. To allow outbound traffic to the container, the Kernel iptables does NAT. For inbound traffic, we would need to port-forward a host port with a container port.🦊 **Note**  Every Docker host has a default bridge network (docker0).  All new container will attach to it unless you override it (using --network flag).     MACVLAN: Multi-host network. Containers will have its own MAC and IP addresses on the existing physical network (or VLAN). Good things: it is easy and does not use port-mapping. Bad side: the host NIC has to be in promiscuous mode (most cloud provider does not allow this).  Overlay: it allows containers in different hosts to communicate using encapsulation. It allows you to create a flat, secure, layer-2 network.Note: Docker creates an Embedded DNS server in user-defined networks. All new containers are registered with the embedded Docker DNS resolver so can resolve names of all other containers in the same network.ExecdriverExecdriver는 컨테이너 생성 및 관리에 관한 역할을 담당합니다. 즉, 커널의 격리 기술을 이용하여 컨테이너를 생성하고 실행하는 역할을 합니다. Execdriver의 하위 드라이버인 Runtime driver로는 예전에는 리눅스의 LXC를 이용했지만 최근버전의 도커는 도커내에서 개발한 Docker native runtime driver인 libcontainer나 runc를 이용합니다.Execdriver에서 선택된 LXC 또는 native driver는 Linux Kernel 에서 제공하는 cgroups, namespace 등의 기능을 이용할 수 있는 interface를 제공하고, 이를 통해 도커는 컨테이너 생성 및 관리에 필요한 실질적인 기능들을 제공합니다.docker run 을 실행하면 이는 결국 execdriver -&amp;gt; runtime driver -&amp;gt; cgroups, namespace 등의 기능을 이용하는 인터페이스에 의해 container 환경이 마련되고 기동되는 것이다.참고  Rain.i님의 도커 컨테이너 까보기(4) – Docker Total Architecture 포스트  Maria Valcam, Docker: All you need to know — Containers Part 2",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series7'> <img src='/images/docker_13.png' alt='Docker의 아키텍처 이해하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series7'>Docker의 아키텍처 이해하기</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker 컨테이너에 저장된 데이터는 어떻게 될까?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series6",
      "date"     : "Feb 2, 2022",
      "content"  : "Table of Contents  Container Layer  UFS  CoW  Storage Driver  정리  참고도커를 공부하면서 궁금했던 것 중에 하나가 컨테이너에서 생성된 파일은 어디에 저장되어 있는걸까? 였습니다. 그동안 저는 도커에 다른 저장소를 마운트하면 컨테이너에서 생성된 데이터를 저장할 수 있고 그렇지 않다면 컨테이너가 삭제되면서 같이 사라진다라고 알고 있었는데 그러면 컨테이너가 사라지기 전까지는 어디에 저장되어 있는지 궁금해졌습니다.그러던 중 좋은 글을 공유해 놓은 블로그를 알게되어 이와 관련해 정리해보았습니다. (참고: Rain.i 블로그)Container Layer도커 컨테이너는 도커 이미지로부터 만들어진 인스턴스입니다. 도커 이미지를 토대로 여러 개의 컨테이너를 만들 수 있습니다. 예를 들어 우분투 운영체제를 제공하는 이미지를 이용해 어떤 컨테이너에는 파이썬을 설치하고, 어떤 곳에는 nginx를 설치해 웹 서버로 사용할 수도 있습니다. 이렇게 새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다. 이걸 보면 도커는 각각의 서비스를 컨테이너화 했을 뿐 아니라 컨테이너도 또 컨테이너화 한 것 같은 느낌이 드네요.도커가 컨테이너를 이런식으로 구현한 이유는 이미지의 상태를 최대한 그대로 보존하여 컨테이너를 계속 생성하더라도 토대가 변하지 않아 예상치 못한 오류를 예방할 수 있고 관리하기도 편합니다. 사용하는 입장에서도 어차피 컨테이너를 삭제하면 원래 기본 이미지 상태로 돌아가니까 걱정없이 컨테이너를 조작할 수 있을 것 입니다.우선 컨테이너를 생성하고 새로운 데이터를 생성하면 도커 상에서는 Container Layer에 저장된다는 것을 알았습니다. 그런데 Container Layer도 결국 도커를 띄운 호스트의 자원을 이용하기 때문에 제 컴퓨터(로컬이라면 데스크탑이나 노트북, 리모트라면 AWS의 EC2 정도) 어딘가에 저장이 되어 있을 것입니다. 이렇게 컨테이너들이 사용하는 이미지나 변경사항들은 모두 호스트 File system 의 /var/lib/docker 디렉토리 내에 저장된다. 이 영역을 Docker area 또는 Backing Filesystem 이라고 부르기도 한다.만약 컨테이너에서 생성된 파일을 버리지 않고 저장하고 싶다면 다음의 두 가지 방법을 사용할 수 있습니다.  Commit: 컨테이너 상에서 변경을 수행한 후 새로운 이미지로 만들어둔다.  Volume: 변경사항을 로컬 또는 외부 볼륨에 저장하도록 한다.UFS위의 내용을 읽다보면 이러한 의문이 생길 수 있습니다. ubuntu 이미지가 가지고 있던 Filesystem이 아닌 별도의 Filesystem에 Container Layer의 데이터가 저장이 되는데 왜 우리는 컨테이너를 사용할 때 이러한 사실을 몰랐을까? 그 이유는 바로 도커에서는 UFS(Union File System)라는 방식을 이용해 Image Layer와 Container Layer의 Filesystem을 하나로 통합해서 저희에게 제공해줍니다.이러한 UFS 방식의 장점은 무엇일까요? 가장 큰 장점은 Image Layer의 데이터를 여러 컨테이너가 공유할 수 있다는 점입니다. 공유한다는 것은 여러 개의 컨테이너를 띄우더라도 Image Layer의 데이터 용량은 단 1개만큼만 저장된다는 말입니다.CoW위의 그림과 같이 Image Layer의 a라는 파일을 a&#39;으로 수정할 때 Image Layer에서 파일이 수정되지 않고 Container Layer 위에서 새로 파일을 복사한 후 수정하는 것을 CoW(Copy on Write)라고 합니다. 이러한 기법을 통해 기존의 이미지에 대한 변경을 막을 수 있습니다. 하지만 Copy-on-Write 기법은 그 동작 구조 상 다음의 단점이 있습니다.  Performance Overhead: data 를 먼저 복제(Copy)한 후 변경을 수행해야함  Capacity Overhead: 원본 데이터 뿐 아니라, 변경된 데이터도 저장해야함따라서 되도록이면 중복 사용되고 수정되지 않을만한 데이터들을 이미지 레이어로 구성하는 것이 좋습니다.Storage Driver위에서 그동안 배운 UFS와 CoW 방식을 도커에서 쉽게 이용할 수 있는 것은 도커의 Storage Driver 덕분입니다. Storage Driver는 컨테이너 내에서의 파일 I/O 처리를 담당하는 드라이버입니다. Storage Driver는 Pluggable한 구조로 되어 있고 특성도 다릅니다. 또한 리눅스 배포판마다 지원하는 드라이버도 다르므로 자신의 workload에 맞는 Storage Driver를 선택해아 합니다.Storage Driver의 종류(참고: 도커 공식문서)리눅스 배포판별 지원하는 Storage Driver(참고: 도커 공식문서)Storage Driver와 Backing File SystemStorage Driver는 Container Layer의 데이터를 Backing filesystem(/var/lib/docker)으로 저장하고 사용자에게 layered filesystem으로 제공해 줍니다.   (참고로 볼륨 마운트는 이러한 Storage Driver의 도움없이 직접 Host의 Filesystem에 접근 가능합니다.)참고로 Storage Driver와 Backing filesystem 간에도 종속성이 있습니다.(참고: 도커 공식문서)Storage Driver와 graphDBStorage Driver는 사용자에게 최적의 통합된 파일 시스템을 제공하기 위해서는 layer 별 관계를 조회하고 key를 통해 특정 image를 검색하는 등, 이러한 일련의 정보 검색 및 관리하는 데이터베이스가 필요합니다. 이런 정보를 저장하고 있는 데이터베이스를 graphDB라고 합니다. (graphDB는 Storage Driver의 뇌와 같은 역할?)정리  UFS: Container Layer와 Image Layer의 파일이 통합되어 보인다  CoW: Image Layer 내의 파일을 원본은 유지하는 방향으로 파일을 수정할 수 있다  Storage Driver: 위의 기능들을 실제로 수행하는 드라이버  graphDB: Storage Driver가 최적의 실행을 하는데 필요한 정보를 저장하고 있는 SQLite기반 DB참고  도커 공식문서 About Storage Driver  Rain.i님의 도커 컨테이너 까보기(2) – Container Size, UFS 포스트  Davaom’s Tech Blog, [Docker] 컨테이너의 구조 포스트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-02T21:01:35+09:00'>02 Feb 2022</time><a class='article__image' href='/docker-series6'> <img src='/images/docker_5.png' alt='Docker 컨테이너에 저장된 데이터는 어떻게 될까?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series6'>Docker 컨테이너에 저장된 데이터는 어떻게 될까?</a> </h2><p class='article__excerpt'>새로운 소프트웨어를 설치하거나 파일을 생성하는 등의 작업은 Container Layer 위에서 이루어집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part2]: 네트워크 장비",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series2",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  랜카드  허브  스위치  라우터랜카드랜카드는 유저의 데이터를 케이블에 실어서 허브나 스위치 혹은 라우터 등으로 전달해주고, 자신에게 온 데이터를 CPU에게 전달해주는 역할을 합니다. 랜카드는 어떤 환경에서 사용하는가에 따라 이더넷용 랜카드, 토큰링용 랜카드, FDDI, ATM용 랜카드 등으로 구분하지만 요즘은 90% 이상 이더넷용 랜카드를 사용합니다. 랜카드는 데스크탑의 메인보드에 기본적으로 붙어있고, 추가적으로 랜카드를 부착할 수도 있습니다.허브허브는 한마디로 정의하면 멀티포트(Multiport) 리피터(Repeater)라고 할 수 있습니다. 멀티포트는 포트가 많이 붙어있다는 뜻이고, 리피터는 들어온 데이터를 그대로 재전송한다는 의미입니다. 따라서 허브는 특정 포트에서 들어온 데이터를 나머지 포트로 데이터를 뿌려주는 역할을 합니다.허브를 이용한 네트워크의 예시를 하나 들어보겠습니다. 허브에도 이더넷 방식의 허브와 토큰링 방식의 허브가 있는데, 보통 이더넷 방식이 더 빠르기 때문에 이더넷 허브를 기준으로 얘기하도록 하겠습니다.허브로 연결된 컴퓨터 5대가 있을 때, 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하려고 합니다. 데이터를 허브로 보내주게 되면 허브는 일단 이 데이터를 허브내의 나머지 컴퓨터에 모두 전송합니다. 그러면 각각의 컴퓨터는 랜카드를 통해 들어온 데이터가 자신의 데이터가 맞는지 확인하고 자기의 것이 아니면 버리게 됩니다.여기서 중요한 점은 위에서 구성한 네트워크가 이더넷 네트워크이기 때문에 1번 컴퓨터가 2번 컴퓨터에 데이터를 전송하는 동안 다른 컴퓨터들은 데이터를 주고 받을 수 없다는 점입니다. 따라서 이렇게 같은 허브에 연결된 컴퓨터들은 모두 같은 콜리전 도메인에 있다고 말합니다. 그래서 만약 허브만으로 약 100대의 컴퓨터를 연결할 수 있는 네트워크를 연결했다면 1대의 컴퓨터가 통신하는 동안 나머지 99대의 컴퓨터들은 모두 기다리고 있어야 합니다. 이는 네트워크의 속도를 저하시키는 치명적인 원인이 됩니다.스위치위에서 살펴본 허브의 단점은 아주 명확합니다. 네트워크의 규모를 확장하기 위해 아무리 허브의 개수를 늘려도 하나의 콜리전 도메인이기 때문에 네트워크의 속도를 느리게 만든다는 점입니다. 이를 해결하기 위해서는 네트워크의 규모를 확장시켜도 콜리전 도메인이 커지지 않아야 하는데 이 때 등장한 것이 바로 스위치입니다. 사실 스위치 이전에 브릿지라는 것이 있었지만 요즘은 브릿지를 스위치가 대체하였기 때문에 스위치에 대해서 알아보도록 하겠습니다.스위치는 콜리전 도메인을 나눠준다고 했습니다. 예시를 보도록 하겠습니다.보시다시피 콜리전 도메인은 스위치에 의해 분리되었습니다. 이렇게 되면 왼쪽 콜리전 도메인 내의 컴퓨터들이 통신하는 동안 오른쪽에서도 이와 상관없이 통신이 가능합니다. (왼쪽 콜리전 도메인 내의 컴퓨터와 오른쪽 콜리전 도메인에 있는 컴퓨터가 통신하는 경우는 제외. 이런 경우에 해당하는지는 스위치에서 저장하고 있는 맥 주소 테이블을 바탕으로 판단합니다)라우터라우터가 필요한 이유는 또 스위치로는 해결하지 못하는 문제가 있기 때문입니다. 바로 브로드캐스트 도메인 분할 문제입니다. 브로드캐스트 도메인은 무엇이고 콜리전 도메인이랑 차이는 무엇인지 보도록 하겠습니다. 콜리전 도메인 영역은 A라는 컴퓨터가 B라는 컴퓨터에 데이터를 보내고 싶은데 어디로 보내야 할지 몰라 콜리전 도메인 영역 내의 모든 컴퓨터에 데이터를 보내는 영역입니다. 이 때는 B 컴퓨터를 제외한 나머지 컴퓨터는 데이터를 받아도 본인 것이 아니기 때문에 랜카드가 자신의 CPU까지 데이터를 보내지 않기 때문에 컴퓨터 성능에 영향을 주지 않습니다.콜리전 도메인은 컴퓨팅 성능에는 영향을 주지 않는다. 다만 네트워크 속도를 저하시킬 뿐이다. 브로드캐스트 도메인은 컴퓨팅 성능에도 영향을 주는 영역입니다. A라는 컴퓨터가 만약 어떤 데이터를 브로드캐스팅하면 브로드캐스트 도메인 내의 모든 컴퓨터는 이 데이터의 목적지가 됩니다. 그렇기 때문에 이 데이터는 브로드캐스트 도메인 내의 모든 컴퓨터에 도착하고 랜카드는 이 데이터를 CPU에 전달해주게 됩니다. 이런 일이 자주 발생하게 되면 모든 컴퓨터의 컴퓨팅 성능에 영향을 미칠 것입니다.라우터는 브로드캐스트 도메인을 분할해줍니다. 이 말은 네트워크를 분리해 준다는 말입니다. 브로드캐스팅은 하나의 네트워크 내에서만 일어납니다. 그래서 라우터를 이용하면 네트워크를 분할할 수 있고 브로드캐스트 도메인 영역을 분할할 수 있습니다.라우터는 네트워크를 분할한다.  🦊🐱 VLAN(Virtual LAN)위에서 라우터를 이용해 네트워크를 분할했습니다. 이런 역할을 스위치가 할 수도 있습니다. 스위치의 VLAN 기능을 이용하면 네트워크 영역을 분할할 수 있습니다. 하지만 딱 그 뿐입니다. 두 네트워크 간의 통신은 스위치로 할 수 없습니다. 두 네트워크가 통신하기 위해서는 라우터가 필요합니다.스위치의 VLAN을 이용하면 네트워크를 분할할 수 있다.라우터는 네트워크를 분할하고 두 네트워크 간의 통신을 가능하게 해준다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series2'> <img src='/images/net_7.png' alt='Network Series [Part2]: 네트워크 장비'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series2'>Network Series [Part2]: 네트워크 장비</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Network Series [Part1]: 네트워크 용어",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/network-series1",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  네트워크  인터넷  LAN  이더넷  참고네트워크네트워크는 서로를 연결시켜 놓은 망이라고 할 수 있습니다. 그렇다고 연결만 해놓고 끝내면 되는 것은 아니고, 연결된 장비들끼리 대화(통신)를 할 수 있어야 합니다. 왜 이런 개념이 등장했을까요? 그 이유는 바로 자원을 공유하고 싶어서 였습니다.지금과 같이 개인마다 컴퓨터가 보급되기 이전에는 보통 터미널이라고 불리는 단말기 여러 대를 호스트 컴퓨터에 붙여서 사용했습니다. 그러다가 프린트기를 공유하기도 하고, 나중에는 하나의 호스트를 공유하는 것이 아니라 여러 대의 호스트를 공유하고자 하는 요구가 생겨나면서 지금의 네트워크로 발전하게 되었습니다.인터넷그럼 인터넷은 무엇일까요? 인터넷의 인터(Inter)는 연결을 의미합니다. 따라서 인터넷이란, 여러 개의 네트워크를 연결한 것이라는 의미를 가지고 있습니다.위의 네트워크에서 설명했듯이, 네트워크는 자원의 공유를 위해 등장하게 되었습니다. 이 당시에는 물론 그 범위가 회사 또는 특정 단체 내에서의 자원 공유였을 겁니다. 그러다가 사람들이 이제 다른 네트워크와도 정보를 공유하고 싶어졌습니다. 이 때 등장하게 된 개념이 바로 인터넷인 것입니다.인터넷의 특징은 하나의 프로토콜만을 사용한다는 것입니다. 프로토콜은 쉽게 말해 무엇인가를 하기 위해 정해놓은 규약을 의미합니다. 인터넷은 네트워크간의 통신을 의미하기 때문에, 인터넷에서 사용하는 프로토콜은 네트워크간의 통신을 위한 규약이라고 할 수 있습니다. 전세계에서 수많은 네트워크가 생성될텐데 이들간의 통신을 위한 규약을 정해놓지 않는다면, 이들을 연결하기란 불가능할 것입니다. 이를 위해 인터넷에서 모든 네트워크가 사용하는 프로토콜이 있는데 이를 TCP/IP라고 합니다.LANLAN은 Local Area Network의 약자로, 즉 한정된 공간에서 구성한 네트워크를 의미합니다.보통 집, 사무실, 학교, 회사 내에서 사용하는 네트워크를 LAN이라고 합니다. 반면 멀리 떨어진 지역과도 통신할 수 있도록 구성한 네트워크를 WAN(Wide Area Network)라고 합니다. 보통 네트워크내에서 인터넷을 사용할 수 있다면 이러한 네트워크를 WAN이라고 할 수 있습니다.이더넷네트워크를 공부하다 보면 이더넷(Ethernet)이라는 용어를 자주 듣게 됩니다. 이더넷은 네트워크에서 통신을 하는 방식 중 하나라고 생각하면 될 것 같습니다. 대표적으로 토큰링 방식과 이더넷 방식이 있는데 요즘에는 이더넷 방식이 성능적으로 훨씬 우수하기 때문에 대부분의 네트워크는 이더넷 방식을 사용하고 있다고 보면 됩니다.  토큰링: 네트워크내의 장비들 중 토큰을 가지는 장비만 데이터 전송, 전송 후 토큰 넘기는 방식  이더넷: 정해진 순서없이 모든 장비들이 데이터 전송, 데이터 간 충돌 발생하면 기다렸다 또 전송위에서 설명했듯이 이더넷은 네트워크 통신 방식의 일종으로 네트워크 내 장비들이 자유롭게 데이터를 전송하고, 충돌이 발생하면 랜덤한 시간을 기다린 후 다시 전송하는 방식으로 이를 CSMA/CD (Carrier Sense Multiple Access / Collision Detection)라고 합니다. 초기에는 이러한 방식보다 토큰링이 더 안정된 방식이라는 인식이 있었지만, 이더넷과 관련한 기술들이 계속 발전을 거듭하게 되면서 이제는 대부분 이더넷 방식으로 네트워크를 구성합니다.참고  성공과 실패를 결정하는 1%의 네트워크 원리 책  양햄찌가 만드는 세상: 맥 어드레스란 무엇인가?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/network-series1'> <img src='/images/net_6.png' alt='Network Series [Part1]: 네트워크 용어'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/network-series1'>Network Series [Part1]: 네트워크 용어</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Linux Series [Part1]: wget, curl",
      "category" : "",
      "tags"     : "Network",
      "url"      : "/linux-series1",
      "date"     : "Feb 1, 2022",
      "content"  : "Table of Contents  참고참고  lesstif: linux command line 에서 HTTP 로 파일 받기 - wget 사용법  lesstif: curl 주요 사용법 요약  lesstif: curl 설치 및 사용법 - HTTP GET/POST, REST API 연계등",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-02-01T21:01:35+09:00'>01 Feb 2022</time><a class='article__image' href='/linux-series1'> <img src='/images/net_6.png' alt='Linux Series [Part1]: wget, curl'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linux-series1'>Linux Series [Part1]: wget, curl</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series6",
      "date"     : "Jan 31, 2022",
      "content"  : "Table of Contents  Connection Client To Broker          Scenario 0: Client and Kafka running on the same local machine      Scenario 1: Client and Kafka running on the different machines      Scenario 2: Kafka and client running in Docker      Scenario 3: Kafka in Docker container with a client running locally                  Adding a new listener to the broker                    Scenario 4: Kafka running locally with a client in Docker container      원문: Confluent블로그Connection Client To Broker클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.  브로커와의 초기 연결. 연결이 되면 브로커는 클라이언트에게 연결 가능(resolvable and accessible from client machine)한 브로커의 엔드포인트 제공(advertised.listeners)  클라이언트와 연결 가능한 브로커와의 연결초기 연결은 producer = KafkaProducer(bootstrap_servers=[&quot;localhost:9092&quot;]) 와 같이 bootstrap_servers 중 하나의 서버와 초기 연결된다. 그러면 연결된 서버는 클라이언트에게 advertised.listeners를 노출해 연결되도록 한다.예시로 클라이언트와 카프카가 서로 다른 머신에 있는 경우를 보자.연결이 성공되는 경우는 다음과 같다.연결이 실패되는 경우는 다음과 같다.이러한 경우에는 advertised.listeners를 localhost:9092로 설정하면 안된다.Scenario 0: Client and Kafka running on the same local machinebootstrap_servers = &#39;localhost:9092&#39;advertised_listeners = &#39;localhost:9092&#39;  잘 동작한다.클라이언트에 전달되는 메타데이터는 192.168.10.83이다. 이 값은 로컬 머신의 IP 주소이다.Scenario 1: Client and Kafka running on the different machines카프카 브로커가 다른 머신에서 동작하는 경우를 살펴보자. 예를 들면 AWS, GCP와 같은 클라우드에서 생성한 머신여기 예제에서 클라이언트는 나의 노트북이고 카프카 브로커가 동작하고 있는 머신의 LAN은 asgard03이라고 해보자.초기 연결은 성공한다. 하지만 메타데이터에서 돌려주는 노출된 리스너는 localhost이다. 하지만 클라이언트의 localhost에는 카프카 브로커가 없으므로 연결은 실패한다.이 문제를 해결하기 위해서는 server.properties에서 advertised.listeners 값을 수정해 클라이언트에서 접근 가능한 올바른 호스트네임과 포트를 제공해주어야 한다.# advertised.listeners 수정 전advertised.listeners=PLAINTEXT://localhost:9092listeners=PLAINTEXT://0.0.0.0:9092# advertised.listeners 수정 후advertised.listeners=PLAINTEXT://asgard03.moffatt.me:9092listeners=PLAINTEXT://0.0.0.0:9092Scenario 2: Kafka and client running in Docker도커를 이용할 때 기억해야할 점은 도커는 컨테이너를 통해 그들만의 작은 세상을 만든다는 것이다. 컨테이너는 자체적인 호스트네임, 네트워크 주소, 파일 시스템을 가지고 있다. 따라서 컨테이너를 기준으로 localhost는 더이상 나의 노트북이 아니다. 도커 컨테이너에서 localhost는 컨테이너 자기 자신이다.여기서는 카프카와 클라이언트를 모두 각각 도커 호스트 위에 컨테이너로 만들어 본다.클라이언트를 컨테이너로 만들어주는 Dockerfile이다.FROM python:3# We&#39;ll add netcat cos it&#39;s a really useful# network troubleshooting toolRUN apt-get updateRUN apt-get install -y netcat# Install the Confluent Kafka python libraryRUN pip install confluent_kafka# Add our scriptADD python_kafka_test_client.py /ENTRYPOINT [ &quot;python&quot;, &quot;/python_kafka_test_client.py&quot;]위의 메니페스트를 이용해 클라이언트 이미지를 만든다.docker build -t python_kafka_test_client .카프카 브로커를 생성하자.docker network create rmoff_kafkadocker run --network=rmoff_kafka --rm --detach --name zookeeper -e ZOOKEEPER_CLIENT_PORT=2181 confluentinc/cp-zookeeper:5.5.0docker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0쥬키퍼와 카프카 브로커가 컨테이너로 돌아가고 있다.$ docker psIMAGE                              STATUS              PORTS                          NAMESconfluentinc/cp-kafka:5.5.0        Up 32 seconds       0.0.0.0:9092-&amp;gt;9092/tcp         brokerconfluentinc/cp-zookeeper:5.5.0    Up 33 seconds       2181/tcp, 2888/tcp, 3888/tcp   zookeeper위에서 우리는 우리만의 도커 네트워크를 만들었고 이제 이 네트워크를 통해 클라이언트와 브로커가 통신하도록 해보자$ docker run --network=rmoff_kafka --rm --name python_kafka_test_client \        --tty python_kafka_test_client broker:9092결과를 보면 초기 연결은 성공하지만, 메타데이터로 localhost를 돌려주기 때문에 프로듀서와 클라이언트의 연결은 실패된다.이를 해결하려면 advertise.listeners의 호스트네임을 컨테이너 이름으로 바꿔줘야 한다.# 수정 전-e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://localhost:9092 \# 수정 후 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \최종적으로 브로커 설정을 다음과 같이 고칠 수 있다.docker stop brokerdocker run --network=rmoff_kafka --rm --detach --name broker \           -p 9092:9092 \           -e KAFKA_BROKER_ID=1 \           -e KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181 \           -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:9092 \           -e KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1 \           confluentinc/cp-kafka:5.5.0Scenario 3: Kafka in Docker container with a client running locally위의 Scenario 2와 비교하여 클라이언트가 컨테이너화 되어 있다가 여기서는 따로 컨테이너화 되지 않고 로컬 머신 위에 있다. (이러한 차이로 위에서 하던 방식이 왜 안되는 건지 모르겠다…)로컬에 실행하는 클라이언트는 따로 네트워크가 구성되어 있지 않다. 그렇기 때문에 따로 특정 트래픽을 받기 위해서는 로컬의 포트를 열어 이를 통해 통신해야 한다. 아래 그림과 같이 9092:9092 포트를 열었다고 해보자. 클라이언트가 로컬의 9092포트 엔드포인트로 접근하기 위해서는 bootstrap_servers=&#39;localhost:9092&#39;로 해야 한다. advertised.listeners는 broker:9092로 해야 한다(클라이언트와 localhost관계가 아니므로).문제는 클라이언트 입장에서 broker:9092는 resolvable하지 않다.Adding a new listener to the broker이 문제를 해결하는 방법은 다수의 리스너를 만드는 것이다....    ports:      - &quot;19092:19092&quot;    environment:      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:9092,CONNECTIONS_FROM_HOST://localhost:19092      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,CONNECTIONS_FROM_HOST:PLAINTEXT...Scenario 4: Kafka running locally with a client in Docker container이런 상황이 잘 있지는 않지만, 어쨋든 이런 경우에 대한 해결책은 있다. 다만 좀 임시방편적일 뿐이다.만약 맥에서 도커가 동작하고 있다면, host.docker.internal을 이용할 수 있다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-31T21:01:35+09:00'>31 Jan 2022</time><a class='article__image' href='/kafka-series6'> <img src='/images/kafka_54.png' alt='Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series6'>Kafka Series [Part6]: Client Won’t Connect to Apache Kafka Cluster in Docker/AWS/My Laptop.[번역]</a> </h2><p class='article__excerpt'>클라이언트와 카프카간의 메세지를 주고받기 위해서는 두 가지의 연결이 반드시 선행되어야 한다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part5]: Kafka Listeners – Explained[번역]",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series5",
      "date"     : "Jan 30, 2022",
      "content"  : "Table of Contents  Kafka Listeners  Why can I connect to the broker, but the client still fails?  HOW TO: Connecting to Kafka on Docker  HOW TO: Connecting to Kafka on IaaS/Cloud          Option 1: External address is resolvable locally      Option 2: External address is NOT resolvable locally        Exploring listeners with Docker원문: Confluent블로그읽어보면 좋은 포스트Kafka Listeners카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners(또는 도커 이미지를 사용할 경우 KAFKA_ADVERTISED_LISTENERS)를 external IP 주소로 설정해야 합니다.아파치 카프카는 분산 시스템입니다. 데이터는 리더 파티션으로부터 쓰고 읽어지며 리더 파티션은 어떤 브로커에도 있을 수 있습니다. 그래서 클라이언트가 카프카에 연결되기 위해서는 해당 리더 파티션을 가지고 있는 브로커가 누구인지에 대한 메타데이터를 요청합니다. 이 메타데이터에는 리더 파티션을 가지는 브로커의 엔드포인트 정보를 포함하고 있으며 클라이언트는 이 정보를 이용해 카프카와 연결될 것입니다.만약 카프카가 도커와 같은 가상머신이 아닌 bare metal 위에서 동작한다면 이 엔드포인트는 그저 hostname이나 localhost 정도가 될 것입니다. 하지만 조금 더 복잡한 네트워크 환경 또는 멀티 노드 환경으로 오게 되면 조금 더 주의가 필요하게 됩니다.초기에 브로커가 연결되면 실제로 리더 파티션을 가지는 브로커의 host와 IP의 정보를 돌려줍니다. 이러한 과정은 단일 노드 환경에서도 마찬가지입니다.KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXTKAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOBserver.properties  KAFKA_LISTENERS: 카프카가 리스닝하기 위해 노출하는 host/IP와 port  KAFKA_ADVERTISED_LISTENERS: 클라이언트에게 알려주는 리스너의 host/IP와 port 리스트  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: 각 리스너들이 사용하는 security protocol  KAFKA_INTER_BROKER_LISTENER_NAME: 브로커들 간의 통신을 위해 사용하는 리스너브로커에 연결되면 연결된 리스너가 반환됩니다. kafkacat은 이러한 정보를 알아보는 유용한 툴입니다. -L을 이용하면 연결된 리스너에 관한 메타데이터를 얻을 수 있습니다.# 9092포트로 연결시, localhost:9092 리스너가 반환$ kafkacat -b kafka0:9092 \           -LMetadata for all topics (from broker -1: kafka0:9092/bootstrap):1 brokers:  broker 0 at localhost:9092# 29092포트로 연결시, kafka0:29092 리스너가 반환$ kafkacat -b kafka0:29092 \           -LMetadata for all topics (from broker 0: kafka0:29092/0):1 brokers:  broker 0 at kafka0:29092Why can I connect to the broker, but the client still fails?초기 브로커 연결에 성공했다고 하더라도, 브로커가 반환하는 메타데이터 안에 있는 주소로 여전히 클라이언트가 접근하지 못하는 경우가 있습니다.      AWS EC2 인스턴스에 브로커를 만들어 로컬 머신에서 EC2에 있는 브로커로 메세지를 보내보려고 합니다. external hostname은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com입니다. 로컬 머신과 EC2가 포트포워딩을 통해 연결되었는지 확인해보겠습니다.        우리의 로컬 머신은 ec2-54-191-84-122.us-west-2.compute.amazonaws.com을 54.191.84.122으로 성공적으로 리졸브(resolve) 합니다.  $ kafkacat -b ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 -LMetadata for all topics (from broker -1: ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092/bootstrap):1 brokers:  broker 0 at ip-172-31-18-160.us-west-2.compute.internal:9092      hostname이 ip-172-31-18-160.us-west-2.compute.internal인 리스너를 반환합니다.        하지만 인터넷을 통해 ip-172-31-18-160.us-west-2.compute.internal은 not resolvable해서 클라이언트는 브로커에 메세지 전송을 실패합니다.  $ echo &quot;test&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;[2018-07-30 15:08:41,932] ERROR Error when sending message to topic test with key: null, value: 4 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)org.apache.kafka.common.errors.TimeoutException: Expiring 1 record(s) for test-0: 1547 ms has passed since batch creation plus linger time  브로커가 설치된 서버의 클라이언트로는 문제없이 동작한다.$ echo &quot;foo&quot;|kafka-console-producer --broker-list ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test&amp;gt;&amp;gt;$ kafka-console-consumer --bootstrap-server ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092 --topic test --from-beginningfoo이러한 일이 발생하는 이유는 9092포트로 연결하는 리스너가 내부 리스너이기 때문이라고 한다. 그래서 브로커가 설치된 서버의 내부에서만 resolvable한 hostname인 ip-172-31-18-160.us-west-2.compute.internal을 리턴한다.HOW TO: Connecting to Kafka on Docker도커에서 동작하기 위해서는 카프카의 두 개의 listener를 지정해야 한다.      도커 네트워크 내에서의 통신: 이것은 브로커간의 통신 또는 도커 안의 다른 컴포넌트와의 통신을 의미한다. 이를 위해서는 도커 네트워크 안에 있는 컨테이너의 호스트네임을 사용해야 한다. 각각의 브로커는 컨테이너의 호스트네임을 통해 서로 통신하게 될 것이다.        도커가 아닌 네트워크로부터의 트래픽: 이것은 도커를 실행하는 서버에서 로컬로 동작하는 클라이언트가 될 수 있다. 이러한 경우 도커를 실행하는 서버(localhost)에서 컨테이너의 포트에 연결할 수 있다. 아래의 도커 컴포즈 스니펫을 한 번 보자.  […]kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      […]      # For more details see See https://rmoff.net/2018/08/02/kafka-listeners-explained/      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB[…]      도커 네트워크 내에 클라이언트가 있다면 클라이언트는 호스트네임 kafka0, 29092 포트를 이용한 BOB 리스너를 통해 브로커와 통신할 것입니다. 각각의 컨테이너(클라이언트, 브로커)는 kafka0를 도커 내부 네트워크를 통해 resolve합니다.        도커를 실행하는 호스트 머신(VM)에 있는 외부 클라이언트의 경우, 호스트 네임 localhost, 9092 포트를 이용한 FRED 리스너를 통해 브로커와 통신한다.        도커를 실행하는 호스트 머신(VM) 밖에 있는 외부 클라이언트는 위의 리스너를 통해 통신할 수 없다. 왜냐하면 kafka0도 localhost도 모두 resolvable하지 않기 때문이다.  HOW TO: Connecting to Kafka on IaaS/Cloud도커와의 차이점은, 도커에서 외부의 연결은 단순히 localhost에서 이루어진 반면, 클라우드 호스트 기반의 카프카는 클라이언트가 localhost에 존재하지 않는다는 것이다.더 복잡한 것은 도커 네트워크가 호스트의 네트워크와는 크게 분리되어 있지만 IaaS에서는 외부 호스트 이름이 내부적으로 확인 가능한 경우가 많기 때문에 이러한 문제가 실제로 발생할 경우 호스트 이름이 잘못될 수 있다.브로커에 연결할 외부 주소가 브로커에게 로컬로 확인할 수 있는지 여부에 따라 두 가지 방법이 있다.Option 1: External address is resolvable locallyEC2 인스턴스의 IP 주소는 기본적으로 External IP. 만약 local에서 resolvable하다면, 로컬 내의 클라이언트, 외부 클라이언트 모두 이를 통해 통신 가능. 다만 외부 클라이언트는 밑의 설정만 추가해주면 된다.advertised.listeners=PLAINTEXT://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092Option 2: External address is NOT resolvable locally만약 로컬 내에서 resolvable하지 않다면, 두 가지 리스너가 필요하다.  VPC 내에서의 통신을 위해 local에서 resolvable한 Internal IP를 통해 내부에서 리슨한다  VPC 밖, 예를 들어 나의 노트북에서 접속하려는 경우 인스턴스의 External IP가 필요하다listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTadvertised.listeners=INTERNAL://ip-172-31-18-160.us-west-2.compute.internal:19092,EXTERNAL://ec2-54-191-84-122.us-west-2.compute.amazonaws.com:9092inter.broker.listener.name=INTERNALExploring listeners with Docker  Listener BOB (port 29092) for internal traffic on the Docker network  Listener FRED (port 9092) for traffic from the Docker host machine (localhost)  Listener ALICE (port 29094) for traffic from outside, reaching the Docker host on the DNS name never-gonna-give-you-up---version: &#39;2&#39;services:  zookeeper:    image: &quot;confluentinc/cp-zookeeper:5.2.1&quot;    environment:      ZOOKEEPER_CLIENT_PORT: 2181      ZOOKEEPER_TICK_TIME: 2000  kafka0:    image: &quot;confluentinc/cp-enterprise-kafka:5.2.1&quot;    ports:      - &#39;9092:9092&#39;      - &#39;29094:29094&#39;    depends_on:      - zookeeper    environment:      KAFKA_BROKER_ID: 0      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181      KAFKA_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://kafka0:9092,LISTENER_ALICE://kafka0:29094      KAFKA_ADVERTISED_LISTENERS: LISTENER_BOB://kafka0:29092,LISTENER_FRED://localhost:9092,LISTENER_ALICE://never-gonna-give-you-up:29094      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: LISTENER_BOB:PLAINTEXT,LISTENER_FRED:PLAINTEXT,LISTENER_ALICE:PLAINTEXT      KAFKA_INTER_BROKER_LISTENER_NAME: LISTENER_BOB      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;false&quot;      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100  kafkacat:    image: confluentinc/cp-kafkacat    command: sleep infinity",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-30T21:01:35+09:00'>30 Jan 2022</time><a class='article__image' href='/kafka-series5'> <img src='/images/kafka_36.png' alt='Kafka Series [Part5]: Kafka Listeners – Explained[번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series5'>Kafka Series [Part5]: Kafka Listeners – Explained[번역]</a> </h2><p class='article__excerpt'>카프카 클라이언트가 카프카에 연결되기 위해서는 advertised.listeners를 external IP 주소로 설정해야 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part2]: 컴퓨터의 기본 구조",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series2",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents  컴퓨터의 기본 구성          하드웨어의 구성      폰 노이만 구조      하드웨어 사양 관련 용어        CPU          CPU의 기본 구성      CPU의 명령어 처리 과정        컴퓨터 성능 향상 기술          버퍼      캐시      인터럽트        참고컴퓨터의 기본 구성하드웨어의 구성컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다. 이 중에 CPU와 메모리는 필수장치로 구분되고 나머지는 주변장치로 구분됩니다.폰 노이만 구조오늘날 대부분의 컴퓨터는 CPU, 메모리, 저장장치, 입출력장치가 버스로 연결되어 있는 폰 노이만 구조를 따르고 있습니다. 폰 노이만 구조 이전에는 컴퓨터가 하드와이어링(hard wiring) 형태로 용도에 맞게 매번 컴퓨터의 전선을 새로 연결해야 했습니다. 이러한 문제를 해결하기 위해 수학자 존 폰 노이만(John von Neumann)은 프로그램만 교체하여 메모리에 올리는 방법을 제안했습니다. 이러한 폰 노이만 구조 덕분에 오늘날에는 프로그래밍 기술을 이용해 컴퓨터로 다양한 작업을 할 수 있게 되었습니다.폰 노이만 구조의 가장 중요한 특징은, 모든 프로그램은 메모리에 올라와야 실행할 수 있다는 것입니다. 예를 들어 하드디스크에 워드 프로그램과 문서가 저장되어 있어도 실행을 하기 위해서는 메모리에 올라와야 합니다. 운영체제 또한 프로그램이기 때문에 메모리에 올라와야 실행이 가능합니다.하드웨어 사양 관련 용어  CPU 클럭(clock): 초당 CPU내의 트랜지스터가 열고 닫히는 횟수(사이클 수) 하나의 사이클에 여러 개의 명령어가 완료되는 경우도 있고, 하나의 명령어가 여러 사이클에 걸쳐서 완료되기도 함      바이트(byte): 저장장치의 기억 용량 단위                            용량 단위          용량                          1B          1byte                          1KB          2^10byte = 1024B                          1MB          2^20byte = 1024KB                          1GB          2^30byte = 1024MB                          1TB          2^40byte = 1024GB                      버스(bus): 시스템 버스는 메모리와 주변장치를 연결하는 버스로 메인보드의 클럭속도를 나타내는 지표이며, CPU 내부 버스는 CPU 내부 부품들을 연결하는 버스로 CPU 클럭 속도와 같음. CPU 버스 속도가 시스템 버스의 속도보다 훨씬 빠름CPUCPU(Central Processing Unit)은 중앙처리장치라고 하며 메모리에 올라온 프로그램의 명령어를 해석하여 실행하는 장치입니다. 따라서 중앙 처리 장치(CPU)는 컴퓨터 부품과 정보를 교환하면서 컴퓨터 시스템 전체를 제어하는 장치로, 모든 컴퓨터의 작동과정이 중앙 처리 장치(CPU)의 제어를 받기 때문에 컴퓨터의 두뇌에 해당한다고 할 수 있습니다.🦊 32bit CPU흔히 CPU를 얘기할 때 32bit CPU, 64bit CPU라고 하는데 이 때 32bit는 CPU가 메모리에서 데이터를 읽거나 쓸 때 한 번에 처리할 수 있는 데이터의 최대 크기를 말합니다.CPU의 기본 구성  산술논리 연산장치(ALU): 산술 연산(덧셈, 뺄셈 등)과 논리 연산(AND, OR 등)을 수행하는 부분  제어장치(control unit): 명령어를 해석해 제어 신호를 보냄으로써 작업을 지시하는 부분  레지스터(resister): CPU 내에 데이터를 임시로 보관하는 부분CPU의 명령어 처리 과정CPU는 메모리에 올라온 프로그램을 실행하기 위해서는 컴파일러를 이용해 코드를 기계어로 바꿔줘야 합니다. 이 기계어를 사람이 이해하기 쉽게 일대일 대응시켜 기호화한 어셈블리어가 있는데 어셈블리어를 살펴보면 CPU가 어떤 식으로 명령어를 내리고 처리하는지 볼 수 있습니다.# C언어int D2 = 2, D3 = 3, sum;sum = D2 + D3;# 어셈블리어LOAD mem(100), register 2; # 메모리 100번지에 있는 값을 레지스터2에 로드LOAD mem(120), register 3; # 메모리 120번지에 있는 값을 레지스터3에 로드ADD register 5, resister 2, register 3; # 레지스터2와 레지스터3에 저장된 값을 더해 레지스터5에 저장MOVE register 5, mem(160); # 레지스터5에 저장된 값(5)을 메모리 160번지로 이동위의 명령어는 명령어 레지스터에 저장되고 제어장치는 저장된 명령어를 해석하고 알맞은 제어 신호를 보냄으로써 동작을 수행합니다. 이러한 제어 신호는 제어버스를 통해 메모리와 주변장치에 전달합니다.컴퓨터 성능 향상 기술현재 컴퓨터 구조의 가장 큰 문제는 CPU와 다른 장치간의 작업 속도가 다르다는 것입니다. CPU 내부 버스의 속도가 시스템 버스의 속도보다 빠르기 때문에, 메모리를 비롯한 주변장치의 속도가 CPU의 속도를 따라가지 못하고 있습니다. 여기서는 이러한 속도 차이를 개선하기 위해 개발된 기술 중 운영체제와 관련된 기술을 살펴보겠습니다.버퍼버퍼(buffer)는 속도에 차이가 있는 두 장치 사이에서 그 차이를 완화하는 역할을 합니다. 예를 들어 저장장치에서 메모리로 데이터를 읽어올 때 데이터를 하나씩 전송하는 것보다 일정량의 데이터를 모아서 한꺼번에 전송하면 속도를 향상시킬 수 있습니다. (일상생활에서 물건을 하나씩 나르는 것보다 바구니에 물건을 일정량 담아서 옮기는 것이 더 빠릅니다. 특히 거리가 먼 경우에는 그 차이가 더 클 것입니다.) 버퍼는 이러한 바구니 역할을 합니다.캐시캐시(cache)는 메모리와 CPU간의 속도 차이를 완화하기 위한 용도로 메모리의 데이터를 미리 가져와 저장해두는 임시 장소입니다. 캐시 또한 버퍼의 일종으로 CPU가 앞으로 사용할 것으로 예상되는 데이터를 미리 가져다 놓습니다(prefetch).캐시는 CPU 안에 있으며 CPU 내부 버스의 속도로 동작합니다. 캐시는 메모리의 내용 중 일부를 미리 가져오고, CPU는 메모리에 접근하기 전에 캐시를 먼저 방문해 원하는 데이터가 있는지 찾아봅니다. 캐시에서 원하는 데이터를 찾은 경우를 캐시 히트(cache hit)라고 합니다. 일반적인 컴퓨터의 캐시 적중률은 약 90%입니다.캐시 적중률을 높이기 위해 캐시는 내부적으로 현재 위치와 가까이 위치한 데이터를 가져옵니다. 캐시 용량이 높은 캐시를 구매할 수도 있지만 가격이 비쌉니다.인터럽트초기의 컴퓨터 시스템에는 주변장치가 많지 않아 CPU가 직접 입출력장치에서 데이터를 가져오거나 보냈는데 이러한 방식을 폴링(polling)이라고 합니다. 오늘날에는 주변장치가 많아 CPU가 모든 입출력에 관여하면 작업 효율이 현저하게 떨어집니다. 이러한 문제를 해결하기 위해 등장한 것이 인터럽트(interrupt) 방식입니다.CPU는 데이터를 가져오거나 보낼 때 직접하지 않고, 입출력 관리자에게 명령을 보냅니다. 입출력 관리자가 메모리에 가지고 오거나 메모리의 데이터를 저장장치로 옮기는 동안 CPU는 계속 다른 작업을 할 수 있습니다. 입출력 관리자가 데이터 전송을 완료하고 나면 완료 신호를 CPU에 보내는데 이를 인터럽트라고 합니다.인터럽트 방식을 이용하면 데이터의 입출력이 이루어지는 동안 CPU는 다른 작업을 하고 있을 수 있습니다.참고  쉽게 배우는 운영체제 책 참고  i’m developer, not coder블로그 참고  인텔 홈페이지  위키백과",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/os-series2'> <img src='/images/os_4.png' alt='OS Series [Part2]: 컴퓨터의 기본 구조'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series2'>OS Series [Part2]: 컴퓨터의 기본 구조</a> </h2><p class='article__excerpt'>컴퓨터 하드웨어는 크게 CPU, 메모리, 메인보드, 저장장치, 입출력장치로 구성됩니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part7]: StatefulSet과 Headless의 조합",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series7",
      "date"     : "Jan 29, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-29T21:01:35+09:00'>29 Jan 2022</time><a class='article__image' href='/kubernetes-series7'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part7]: StatefulSet과 Headless의 조합'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series7'>Kubernetes Series [Part7]: StatefulSet과 Headless의 조합</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part6]: ConfigMap과 Secret",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series6",
      "date"     : "Jan 28, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-28T21:01:35+09:00'>28 Jan 2022</time><a class='article__image' href='/kubernetes-series6'> <img src='/images/kubernetes_logo.png' alt='Kubernetes Series [Part6]: ConfigMap과 Secret'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series6'>Kubernetes Series [Part6]: ConfigMap과 Secret</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "OS Series [Part1]: 운영체제의 개요",
      "category" : "",
      "tags"     : "OS",
      "url"      : "/os-series1",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  운영체제 소개  운영체제의 구조          커널과 인터페이스                  시스템 호출          드라이버          커널의 구조                      참고운영체제 소개운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다. 이뿐만 아니라 스마트 워치, 스마트 TV에도 성능은 낮지만 임베디드 운영체제가 있습니다.운영체제가 하는 역할은 크게 다음과 같습니다.  프로세스 관리  자원(CPU, 메모리) 관리 및 분배  파일시스템 관리  사용자 인터페이스 제공 (CLI, GUI)  하드웨어 인터페이스 제공🦊 인터페이스인터페이스는 사용자가 컴퓨터를 더욱 편하게 사용할 수 있도록 제공하는 것으로 크게 두 가지 기능을 제공합니다.  사용자에게 사용 편의성 제공  하드웨어로의 무분별한 접근으로 생길 수 있는 장애를 차단운영체제의 구조커널과 인터페이스  커널(kernel): 프로세스 관리, 메모리 관리, 저장장치 관리와 같은  운영체제의 핵심적인 기능을 모아놓은 것입니다.  인터페이스(interface): 커널과 사용자 사이에서 명령을 전달하고, 실행 결과를 보여주는 역할을 합니다.운영체제는 커널과 인터페이스로 구분되어 있으며, 같은 커널에도 다양한 형태의 인터페이스를 사용할 수 있습니다. 예를 들어 리눅스에서는 인터페이스를 쉘(shell)이라고 하는데 쉘에는 배시쉘(bash), 지쉘(zsh)과 같은 여러 종류의 쉘이 있습니다.시스템 호출시스템 호출(system call)은 커널에 있는 인터페이스 중 하나로 시스템 자원의 사용과 관련한 함수를 제공합니다. 응용 프로그램이 하드웨어 자원에 접근하려 할 때는 시스템 호출을 사용함으로써 (예를 들어 read(), write() 함수) 직접적인 접근을 막아줍니다. 만약 직접적인 접근이 허용되게 된다면 두 응용 프로그램이 같은 위치에 데이터를 저장하게 될 수도 있고, 이로 인해 저장되어 있던 데이터가 지워질 수도 있습니다. 이러한 오류를 막아주기 위해 커널에서는 시스템 호출이라는 인터페이스를 제공합니다.드라이버예전과 다르게 하드웨어의 종류도 다양해지고, 제품을 만드는 회사도 굉장히 많기 때문에 제품별로 각각 다양한 특징과 기능이 존재합니다. 이러한 변동성을 커널이 가지고 있는 기본적인 기능만으로는 해결하기가 힘들기 때문에, 각 회사에서는 자신의 제품(하드웨어)과 운영체제의 커널이 잘 상호작용할 수 있도록 인터페이스를 제공하고 있습니다. 이를 드라이버 또는 디바이스 드라이버라고 합니다. 위의 그림에 드라이버가 커널과 하드웨어의 사이 전체를 감싸고 있지 않는 이유는 모든 하드웨어가 드라이버를 필요로 하지는 않기 때문입니다.커널의 구조  단일형 구조  계층형 구조  마이크로 구조참고  쉽게 배우는 운영체제 책 참고",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/os-series1'> <img src='/images/os_1.png' alt='OS Series [Part1]: 운영체제의 개요'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/os-series1'>OS Series [Part1]: 운영체제의 개요</a> </h2><p class='article__excerpt'>운영체제는 컴퓨터 전원을 켜면 가장 먼저 만나게 되는 소프트웨어로, 대표적인 예로 컴퓨터의 윈도우와 맥, 리눅스, 모바일의 iOS, 안드로이드가 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part4]: Kafka on Kubernetes",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series4",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  Kafka on Kubernetes  쿠버네티스 클러스터 구축          단일 노드 클러스터                  minikube          Docker Desktop                    멀티 노드 클러스터                  kind          클라우드 환경(GKE, EKS)                    쿠버네티스 GUI 도구: Lens        카프카 메니페스트 작성          LoadBalancer 생성      Zookeeper 설치      Kafka Broker 설치        카프카 클라이언트  카프카 모니터링  참고자료Kafka on Kubernetes쿠버네티스 클러스터 구축쿠버네티스 클러스터를 구축하는 방법에 대해서는 Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기를 참고하시면 됩니다.단일 노드 클러스터minikube미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)멀티 노드 클러스터kindkind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드 환경(GKE, EKS)쿠버네티스 GUI 도구: Lensbrew install lens카프카 메니페스트 작성LoadBalancer 생성kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yamlkubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yamlkubectl create -f ./metallb/configmap.yaml# configmap.yamlapiVersion: v1kind: ConfigMapmetadata:  namespace: metallb-system  name: configdata:  config: |    address-pools:    - name: default      protocol: layer2      addresses:      - 192.168.72.102Zookeeper 설치# deployment.yamlkind: DeploymentapiVersion: apps/v1metadata:  name: zookeeper-deployspec:  replicas: 2  selector:    matchLabels:      app: zookeeper-1  template:    metadata:      labels:        app: zookeeper-1    spec:      containers:      - name: zoo1        image: zookeeper:latest        ports:        - containerPort: 2181        env:        - name: ZOOKEEPER_ID          value: &quot;1&quot;---# zooservice.yamlapiVersion: v1kind: Servicemetadata:  name: zookeeper-service  labels:    app: zookeeper-1spec:  ports:  - name: client    port: 2181    protocol: TCP  - name: follower    port: 2888    protocol: TCP  - name: leader    port: 3888    protocol: TCP  selector:    app: zookeeper-1Kafka Broker 설치# kafkaservice.yamlapiVersion: v1kind: Servicemetadata:  name: kafka-service  annotations:    metallb.universe.tf/address-pool: default    metallb.universe.tf/allow-shared-ip: shared  labels:    name: kafkaspec:  type: LoadBalancer  ports:  - name: kafka-port    protocol: TCP    port: 9092    targetPort: 9092  selector:    app: kafka    id: &quot;0&quot;---# deployment.yamlapiVersion: apps/v1kind: Deploymentmetadata:  name: kafka-broker0spec:  replicas: 1  selector:    matchLabels:        app: kafka        id: &quot;0&quot;  template:    metadata:      labels:        app: kafka        id: &quot;0&quot;    spec:      hostname: kafka-host0      containers:      - name: kafka        image: wurstmeister/kafka        ports:        - containerPort: 9092        env:        - name: KAFKA_LISTENERS          value: INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092        - name: KAFKA_ADVERTISED_LISTENERS          value: INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092        - name: KAFKA_INTER_BROKER_LISTENER_NAME          value: INTERNAL_LISTENER        - name: KAFKA_LISTENER_SECURITY_PROTOCOL_MAP          value: INTERNAL_LISTENER:PLAINTEXT, EXTERNAL_LISTENER:PLAINTEXT        - name: KAFKA_ZOOKEEPER_CONNECT          value: zookeeper-service:2181        - name: KAFKA_BROKER_ID          value: &quot;0&quot;        - name: KAFKA_CREATE_TOPICS          value: admintome-test:1:1# 위치: opt/kafka_버전# 주키퍼 실행bin/zookeeper-server-start.sh ./config/zookeeper.properties# 카프카 실행bin/kafka-server-start.sh ./config/server.properties# 토픽 생성bin/kafka-topics.sh --create --zookeeper zookeeper-service:2181 --replication-factors 1 --partitions 1 --topic test-topic카프카 클라이언트# 카프카 클라이언트 파이썬 버전 설치pip install kafka-python# producer.pyfrom kafka import KafkaProducerproducer = KafkaProducer(security_protocol=&quot;PLAINTEXT&quot;, bootstrap_servers=[&#39;192.168.111.2:9092&#39;], api_version=(0,1,0))producer.send(&#39;test&#39;, b&#39;finally working kafka&#39;) # 현재 이부분에서 안넘어감 (bootstrap_servers의 host에 어떤거 넣어야 할지 모르겠음)producer.flush()카프카 모니터링참고자료  옥탑방의 일상로그 블로그  Towards Data Science 블로그  pcjayasinghe 깃허브  PharosProduction 깃허브  Deploy Apache Kafka and Zookeeper Cluster on Kubernetes 블로그 글",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T21:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/kafka-series4'> <img src='/images/kube_24.png' alt='Kafka Series [Part4]: Kafka on Kubernetes'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series4'>Kafka Series [Part4]: Kafka on Kubernetes</a> </h2><p class='article__excerpt'>Kubernetes 환경에서 Kafka를 띄워보려고 합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose를 이용해 데이터 파이프라인 구축하기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series5",
      "date"     : "Jan 27, 2022",
      "content"  : "Table of Contents  참고Elasticsearch 6.x는 arm 아키텍처 지원 X (도커 기준)나는 m1이라서 Elasticsearch 7.x 이상 써야됨Elasticsearch-Hadoop은 어떤 버전에서든 7.x 정식 지원 안함.MongoDB로 생각해보자……………ES와 Spark 컨테이너 실행version: &#39;3.2&#39;services:  elasticsearch:    image: elasticsearch:7.16.2    hostname: elasticsearch    ports:      - &quot;9200:9200&quot;      - &quot;9300:9300&quot;    environment:      ES_JAVA_OPTS: &quot;-Xmx256m -Xms256m&quot;      ELASTIC_PASSWORD: changeme      ELASTIC_USERNAME: elastic      discovery.type: single-node    spark:    image: kimziont:spark    hostname: spark    ports:      - &quot;4040:4040&quot;    tty: trueSpark에서 ElasticSearch를 쓰려면 커넥터가 필요한 것 같다. Elastic에서 이를 위해 elasticsearch-hadoop를 제공하는데 우선 다운을 받아야 한다.wget https://artifacts.elastic.co/downloads/elasticsearch-hadoop/elasticsearch-hadoop-6.4.1.zipunzip elasticsearch-hadoop-6.4.1.zipmkdir -p /app/spark/jars/ext/elasticsearch-hadoopmv elasticsearch-hadoop-6.4.1 /app/spark/jars/ext/elasticsearch-hadoop/6.4.1설치가 완료되었으면 스파크를 실행할 때 설치한 경로를 실행 인자로 주면되는 것 같다.pyspark에서 사용할 때는 아래와 같다.# pyspark 실행 커맨드pyspark --driver-class-path=/app/spark/jars/ext/elasticsearch-hadoop/6.4.1/dist/elasticsearch-hadoop-6.4.1.jardf = spark.read.format(&quot;org.elasticsearch.spark.sql&quot;).option(&quot;es.read.field.as.array.include&quot;, &quot;NerArray&quot;).option(&quot;es.nodes&quot;,&quot;localhost:9200&quot;).option(&quot;es.nodes.discovery&quot;, &quot;true&quot;).load(&quot;index명&quot;) df.registerTempTable(&quot;ner&quot;) spark.sql(&quot;show tables&quot;).show() spark.sql(&quot;select * from ner&quot;).show()spark-shell을 사용할 때는 아래와 같다.spark-shell --jars /app/spark/jars/ext/elasticsearch-hadoop/6.4.1/dist/elasticsearch-hadoop-6.4.1.jar참고      Docker 공식문서    Spark와 ElasticSearch 연동하기  elasticsearch-hadoop  Jason Heo’s Blog: Spark 3.0에서 elasticsearch hadoop 사용하기",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-27T00:01:35+09:00'>27 Jan 2022</time><a class='article__image' href='/docker-series5'> <img src='/images/datapipeline_logo.png' alt='Docker Compose를 이용해 데이터 파이프라인 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series5'>Docker Compose를 이용해 데이터 파이프라인 구축하기</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker Compose 속성 알아보기",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series4",
      "date"     : "Jan 26, 2022",
      "content"  : "Table of Contents  도커 컴포즈(Docker Compose)  Top-level keys  services  참고도커 컴포즈(Docker Compose)보통 하나의 프로젝트에는 여러 가지 서비스를 포함하고 있습니다. 예를 들어 웹 서버 기반의 프로젝트를 진행한다고 할 때 프로젝트의 기본적인 아키텍처는 다음과 같습니다.(Netflix Tech Blog 참고)위의 그림을 보면 웹 서버뿐만 아니라 데이터 웨어하우스, 데이터베이스, 검색 서비스 등과 같은 것들이 서로 유기적으로 연결되어 있습니다. 이런 경우 프로젝트를 상용화 하기 위해서는 어떻게 해야 할까요? 각각의 서비스를 도커를 이용해 컨테이너로 띄우고 서로를 연결시켜주어야 합니다. 이는 앞에서 배웠던 도커 이미지를 만들기 위한 Dockerfile 단계에서는 할 수 없고 도커 컨테이너를 실행(docker run)하는 단계에서 컨테이너를 띄우는 순서, 데이터 공유, 환경변수 등을 설정해서 실행해야 합니다.우리가 도커를 배우는 이유는 서비스를 컨테이너화하고 간결한 코드로 관리하여 어디서든 배포가능 하도록 하기 위한 것인데 컨테이너 실행 단계에서 이렇게 복잡한 과정이 필요하다면 생각보다 힘이 빠질 것 같습니다.이러한 문제를 해결하기 위해 도커에서는 도커 컴포즈라는 도구를 제공하였습니다. 도커 컴포즈는 기존의 여러 서비스를 연결하기 위해 컨테이너 실행 단계에서 복잡한 옵션을 주는 상황에서 벗어나 처음부터 YAML 파일 형태로 관리하여 docker-compose.yml 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜주는 것입니다. 다음은 웹 서버와 데이터베이스를 연결해 하나의 프로젝트로 배포하도록 해주는 도커 컴포즈 파일의 예시입니다.version: &#39;3&#39;services:  db:    image: postgres    volumes:      - ./docker/data:/var/lib/postgresql/data    environment:      - POSTGRES_DB=sampledb      - POSTGRES_USER=sampleuser      - POSTGRES_PASSWORD=samplesecret      - POSTGRES_INITDB_ARGS=--encoding=UTF-8  django:    build:      context: .      dockerfile: ./compose/django/Dockerfile-dev    environment:      - DJANGO_DEBUG=True      - DJANGO_DB_HOST=db      - DJANGO_DB_PORT=5432      - DJANGO_DB_NAME=sampledb      - DJANGO_DB_USERNAME=sampleuser      - DJANGO_DB_PASSWORD=samplesecret      - DJANGO_SECRET_KEY=dev_secret_key    ports:      - &quot;8000:8000&quot;    command:       - python manage.py runserver 0:8000    volumes:      - ./:/app/(44bits 블로그 참고)지금부터 도커 컴포즈 파일의 구성요소를 한 번 살펴보도록 하겠습니다.Top-level keys도커 컴포즈 파일의 최상위 키값에는 version, services, networks, volumes과 같은 항목이 있습니다.  version: 도커 컴포즈 파일 포맷의 버전을 지정 (도커 엔진과의 호환성 체크 문서)  services: 컨테이너 각각에 대한 세부사항 설정으로 도커 컴포즈에서 가장 많은 비중 차지  networks: 컴포즈가 제공하는 디폴트 네트워크가 아닌 커스텀 네트워크 생성을 원할 때 지정 (참고)  volumes: 볼륨에 이름을 지정할 수 있으며, 여러 컨테이너에서 공유하는 볼륨을 제공services  image원하는 컨테이너의 이미지를 이미지 저장소에서 불러와 설정합니다. 불러오는 방법은 다음과 같은 방법이 있습니다.# 이미지 이름image: redis# 이미지 이름:태그image: ubuntu:18.04# 이미지 작성자/이름image: tutum/influxdb# 이미지 urlimage: example-registry.com:4000/postgresql  buildbuild를 사용하면 이미지를 불러오지 않고, Dockerfile을 이용해 빌드할 수 있습니다.          context: Dockerfile이 위치한 디렉토리 또는 깃 레포지토리의 url      dockerfile: context에 Dockerfile이 없을 경우 사용할 대체 Dockerfile (단독으로 못쓰고 context필요)      args: 이미지 빌드 단계에서만 사용되는 환경변수의 값 (Dockerfile ARG 명령어에 미리 명시되어야함)      ARG buildnoRUN echo &quot;Build number: $buildno&quot;services:  webapp:    build:      context: ./dir      dockerfile: Dockerfile-alternate      args:        buildno: 1build와 image가 함께 표기된 경우 build로 이미지를 만들고 이미지의 이름에 image 값을 사용합니다.build: ./dirimage: webapp:tag  environment컨테이너에 환경변수를 추가해줍니다. 아래의 SESSION_SECRET와 같이 value를 표기하지 않으면 컴포즈가 실행중인 머신 안에서 정의된 value로 해석가능 합니다. 이것은 value를 secret하게 할 수 있으며 또한 host-specific value로 사용할 수 있습니다.environment:  RACK_ENV: development  SHOW: &#39;true&#39;  SESSION_SECRET:  ports포트를 노출해줍니다. 간단한 short syntax와 추가적으로 필드를 추가할 수 있는 long syntax가 있습니다.          Short Syntax 표기법보통 HOST PORT: CONTAINER PORT 방법으로 표기합니다번호 하나만 작성되면 CONTAINER PORT를 의미합니다IP 주소를 표기하여 해당 IP 주소의 트래픽만 허용할 수도 있습니다 (default: 0.0.0.0)쌍따옴표를 안쓰면 시간으로 인식되기 때문에 반드시 쌍따옴표로 감싸줍니다          ports:    - &quot;3000&quot;    - &quot;3000-3005&quot;    - &quot;8000:8000&quot;    - &quot;9090-9091:8080-8081&quot;    - &quot;49100:22&quot;    - &quot;127.0.0.1:8001:8001&quot;    - &quot;127.0.0.1:5000-5010:5000-5010&quot;    - &quot;127.0.0.1::5000&quot;    - &quot;6060:6060/udp&quot;    - &quot;12400-12500:1240&quot;                    Long Syntax 표기법                  target: the port inside the container          published: the publicly exposed port          protocol: the port protocol (tcp or udp)          mode: host for publishing a host port on each node, or ingress for a swarm mode port to be load balanced.                  ports:    - target: 80      published: 8080      protocol: tcp      mode: host                      volumes호스트 경로의 디렉토리 또는 네임드 볼륨을 컨테이너에 마운트합니다. 하나의 서비스를 위한 볼륨으로는 호스트 경로를 사용해도 괜찮지만, 여러 서비스가 공유하는 볼륨이 필요하다면 top-level volumes에 네임드 볼륨을 정의해야 합니다.          Short Syntax 표기법                              일반적인 표기방법: [SOURCE:]TARGET[:MODE]              volumes:      # Just specify a path and let the Engine create a volume      - /var/lib/mysql      # Specify an absolute path mapping      - /opt/data:/var/lib/mysql      # Path on the host, relative to the Compose file      - ./cache:/tmp/cache      # Named volume      - datavolume:/var/lib/mysql                                          Long Syntax 표기법                  type: 마운트 타입. volume, bind, tmpfs, npipe (참고)                          volume: 도커에 의해 관리되는 볼륨을 마운트하는 경우              bind: 호스트 머신의 파일 또는 디렉토리를 컨테이너에 마운트하는 경우                                source: 마운트 하고자 하는 호스트 경로의 디렉토리 또는 네임드 볼륨                      target: 볼륨이 마운트 될 컨테이너에서의 경로              version: &quot;3.9&quot;  services:    web:      image: nginx:alpine      ports:        - &quot;80:80&quot;      volumes:        - type: volume          source: mydata          target: /data          volume:            nocopy: true        - type: bind          source: ./static          target: /opt/app/static  networks:    webnet:  volumes:    mydata:                                            depends_on서비스간의 실행순서를 통해 디펜던시를 지키도록 해줍니다. 이 설정은 종속되는 다른 서비스가 실행되기만을 기다릴 뿐 준비(ready)가 되었는지 까지는 고려하지 않습니다. 준비 상태가 필요하다면 추가적인 설정이 필요합니다. (참고)아래 예시는 web 서비스 컨테이너가 실행되기 전에 db와 redis의 실행을 기다립니다. 보통 데이터베이스, 주키퍼와 같은 선행되어야 하는 서비스가 있는 경우 많이 사용합니다.version: &quot;3.9&quot;services:  web:    build: .    depends_on:      - db      - redis  redis:    image: redis  db:    image: postgres  command기본 커맨드 명령어를 오버라이딩합니다.command: bundle exec thin -p 3000command: [&quot;bundle&quot;, &quot;exec&quot;, &quot;thin&quot;, &quot;-p&quot;, &quot;3000&quot;]참고  Docker Compose 공식문서  44bits님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-26T21:01:35+09:00'>26 Jan 2022</time><a class='article__image' href='/docker-series4'> <img src='/images/docker_8.jpeg' alt='Docker Compose 속성 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series4'>Docker Compose 속성 알아보기</a> </h2><p class='article__excerpt'>도커 컴포즈는 파일 하나만 실행하면 프로젝트 배포에 필요한 모든 서비스를 실행하고 연결 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series3",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  WORKDIR  VOLUME  COPY  ADD  참고Dockerfile instruction  COPY  ADD  VOLUME  WORKDIRWORKDIRWORKDIR 명령은 Docker 파일에서 이어지는 모든 RUN, CMD, ENTRIPOINT, COPY 및 ADD 명령에 대한 작업 디렉토리를 설정합니다. WORKDIR이 존재하지 않으면 이후 Dockerfile 명령어에 사용되지 않더라도 생성됩니다.WORKDIR 명령은 Docker 파일에서 여러 번 사용할 수 있습니다. 상대 경로가 제공되는 경우 이전 WORKDIR 명령의 경로에 상대적입니다. 예를 들어 다음 명령어의 결과는 /a/b/c입니다.WORKDIR /aWORKDIR bWORKDIR cRUN pwd또한 ENV를 이용해 Dockerfile에서 명시한 환경 변수의 경우 WORKDIR 명령어에서 해석할 수 있습니다. 아래 예를 보면 DIRPATH는 Dockerfile에서 정의를 했기 때문에 /path로 인식되고, DIRNAME은 해석되지 않아 /path/$DIRNAME과 같은 결과가 나옵니다.ENV DIRPATH=/pathWORKDIR $DIRPATH/$DIRNAMERUN pwdVOLUMEVOLUME 명령은 지정된 이름으로 마운트 지점을 생성하고 네이티브 호스트 또는 다른 컨테이너와 마운트됩니다.docker run 명령어를 실행하면 기본 이미지 내의 디렉토리 중 명시된 디렉토리에 있는 파일들로 마운트된 디렉토리를 초기화합니다.VOLUME 명령어로 볼륨을 생성한 뒤 이후의 빌드과정에서 생기는 볼륨의 변경값은 모두 무시됩니다.호스트 디렉터리는 컨테이너를 생성하거나 실행할 때 지정해야 합니다.  호스트 디렉토리(마운트 지점)는 본질적으로 호스트에 종속됩니다. 이는 지정된 호스트 디렉토리를 모든 호스트에서 사용할 수 있다고 보장할 수 없기 때문에 이미지 이식성을 유지하기 위한 것입니다. 따라서 Dockerfile 내에서 호스트 디렉토리를 마운트할 수 없습니다.COPYThe COPY instruction copies new files or directories from  and adds them to the filesystem of the container at the path .The  is an absolute path, or a path relative to WORKDIR, into which the source will be copied inside the destination container.# t로 시작하는 모든 txt파일을 &amp;lt;WORKDIR&amp;gt;/relativeDir/ 로 복사한다COPY t*.txt relativeDir/# test.txt, teso.txt, tesi.txt과 같은 파일을 /absoluteDir/ 로 복사한다COPY tes?.txt /absoluteDir/ADD참고  Docker 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series3'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series3'>Dockerfile을 이용한 이미지 빌드(3): COPY ADD VOLUME WORKDIR</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series2",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  RUN  CMD  ENTRYPOINT  CMD vs ENTRYPOINT  참고Dockerfile instruction  RUN  CMD  ENTRYPOINTRUNRUN 명령어 작성요령은 다음과 같이 2가지 형태가 있습니다.  shell form: RUN &amp;lt;command&amp;gt; (the command is run in a shell, Linux default: /bin/sh -c)  exec form: RUN [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]RUN instruction은 어떠한 명령어든 최근 이미지에 새로운 레이어에서 실행됩니다. 그리고 실행 결과는 이미지에 커밋됩니다. 커밋된 새로운 이미지는 Dockerfile의 다음 단계에 계속 사용됩니다.원한다면 RUN 명령어 중 만들어지는 커밋된 이미지를 이용해 컨테이너를 생성할 수 있습니다.  shell form: RUN &amp;lt;command&amp;gt;shell form의 기본 shell은 /bin/zsh -c echo Test 과 같이 직접 표기를 통해 바꿀 수 있습니다. 또한 \를 통해 여러 개의 RUN 명령어를 하나로 압축할 수 있습니다.RUN /bin/zsh -c echo $HOMERUN apt-get -y update \&amp;amp;&amp;amp; apt-get -y install vim  exec formexec form은 /bin/sh -c 가 필요하지 않은 경우 사용 가능한 형태입니다.RUN pip install -r requirements.txtCMDCMD 명령어 작성요령은 다음과 같이 3가지 형태가 있습니다.  CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;] (exec form, this is the preferred form)  CMD [&quot;param1&quot;,&quot;param2&quot;] (as default parameters to ENTRYPOINT)  CMD command param1 param2 (shell form)CMD 명령어는 오직 한 개의 명령어만 효과가 있습니다. 만약 아래와 같이 여러 번에 걸쳐서 작성하면 마지막 명령어 CMD echo &quot;B&quot;만 실행됩니다.CMD echo &quot;A&quot; CMD echo &quot;B&quot; CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다. 예를 들어 설명해보겠습니다.  exec form: CMD [&quot;executable&quot;,&quot;param1&quot;,&quot;param2&quot;]# executable과 params의 조합이 하나의 디폴트CMD [&quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo&quot;, &quot;Hello&quot;] --------------# 컨테이너를 실행할 때 별다른 명령어를 입력하지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello# 명령어를 입력하면 CMD의 디폴트는 실행되지 않습니다docker run -it --rm &amp;lt;image-name&amp;gt; echo &quot;Good morning&quot;-&amp;gt; Good morning참고로 exec form은 shell processsing을 지원하지 않습니다. 그래서 CMD [ &quot;echo&quot;, &quot;$HOME&quot; ]은 $HOME을 대체해서 출력하지 않습니다.🦊shell processing이 필요한 경우 두 가지 방법이 있습니다.# shell을 직접 실행한다CMD [ &quot;/bin/bash&quot;, &quot;-c&quot;, &quot;echo $HOME&quot; ]# shell form을 사용한다CMD echo $HOME  only params: CMD [&quot;param1&quot;,&quot;param2&quot;]이 경우에는 반드시 ENTRYPOINT 명령어를 명시해줘야 합니다. 왜냐하면 인자값만 줬을 뿐 아무런 실행 가능한 것도 표기하지 않았기 때문입니다. 이 방법은 ENTRYPOINT 명령어에 디폴트 파라미터를 제공하기 위한 것입니다.ENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]--------------  # 컨테이너를 실행할 때 디폴트 인자값 주지 않아 CMD 명령어가 실행된 경우 docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# 실행 시 인자 값을 주어 CMD 명령어가 실행되지 않은 경우docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello ME  shell form: CMD command param1 param2shell form을 사용하면 command가 /bin/sh -c 를 통해 실행되게 됩니다. 그래서 만약 .py와 같은 파이썬 파일을 실행할 때는 shell form이 아닌 exec form을 사용해야 합니다.CMD echo &quot;Hello&quot;--------------docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hellodocker run -it --rm &amp;lt;image-name&amp;gt; echo Bye-&amp;gt; ByeCMD는 보다시피 컨테이너 실행 시 디폴트 값을 줄 뿐 반드시 실행된다는 보장을 할 수 없다. 항상 실행을 보장하고 싶을 때에는 ENTRYPOINT를 사용하면 된다.ENTRYPOINTENTRYPOINT에도 2가지 표현 방법이 있습니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]  shell form: ENTRYPOINT command param1 param2ENTRYPOINT 명령어는 docker run --entrypoint을 사용하는 경우를 제외하고는 오버라이딩 되지 않고 반드시 실행된다는 특징이 있습니다. 예를 들어 만약 docker run &amp;lt;image&amp;gt; -d 식으로 컨테이너를 실행했다면 -d는 ENTRYPOINT의 exec form 뒤에 붙게 됩니다.shell form은 어떠한 CMD 명령어나 run 커맨드라인 인자값도 사용되지 않도록 합니다. 단점은 CMD의 경우와 마찬가지로 무조건 /bin/sh -c로 시작할 수 밖에 없다는 점입니다.ENTRYPOINT 명령어도 마지막 것만 실행됩니다.  exec form: ENTRYPOINT [&quot;executable&quot;, &quot;param1&quot;, &quot;param2&quot;]FROM ubuntuENTRYPOINT [&quot;/bin/echo&quot;, &quot;Hello&quot;]CMD [&quot;world&quot;]----------------------# ENTRYPOINT, CMD 모두 실행docker run -it --rm &amp;lt;image-name&amp;gt;-&amp;gt; Hello world# ENTRYPOINT, run argument 실행docker run -it --rm &amp;lt;image-name&amp;gt; ME-&amp;gt; Hello MECMD vs ENTRYPOINT  CMD, ENTRYPOINT 명령어는 마지막 하나만 실행된다  CMD 명령어는 도커 컨테이너 실행할 때 디폴트 값을 주기 때문에 오버라이딩 될 수 있다  항상 실행되는 명령어를 원한다면 ENTRYPOINT를 사용하자  항상 실행되는 명령어와 오버라이딩 되는 인자를 원한다면 CMD와 ENTRYPOINT를 함께 써보자  CMD와 ENTRYPOINT의 조합 결과는 다음과 같다참고  Docker 공식문서  스뎅(thDeng)님 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series2'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series2'>Dockerfile을 이용한 이미지 빌드(2): RUN CMD ENTRYPOINT</a> </h2><p class='article__excerpt'>CMD 명령어의 가장 큰 목적은 컨테이너가 실행될 때 디폴트 명령어, 또는 인자값을 주고 싶은 경우입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series1",
      "date"     : "Jan 25, 2022",
      "content"  : "Table of Contents  Dockerfile  Dockerfile Instructions          FROM                  Multi-Stage Builds                    LABEL      ARG      ENV        참고Dockerfile instruction  FROM  LABEL  ARG  ENVDockerfileDockerfile InstructionsFROMFROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다. 그렇기 때문에 유효한 Dockerfile은 반드시 FROM 명령어로부터 시작해야 합니다.  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt; [AS &amp;lt;name&amp;gt;]  FROM [--platform=&amp;lt;platform&amp;gt;] &amp;lt;image&amp;gt;[:&amp;lt;tag&amp;gt;] [AS &amp;lt;name&amp;gt;]# python:3.8-buster 이미지를 기본 이미지로 만듭니다FROM python:3.8-buster# 현재 Dockerfile이 있는 위치에 있는 모든 파일을 새로 만든 이미지의 디렉토리 위치로 복사합니다COPY . .# 복사된 requirements.txt 파일에 있는 라이브러리를 설치RUN pip install -r requirements.txt# deeplearning:pytorch 라는 새로운 이미지를 만듭니다docker build -t deeplearning:pytorch .Multi-Stage Builds이미지를 빌드할 때 가장 중요한 것은 이미지의 사이즈를 줄이는 것입니다. Dockerfile에서 각각의 명령어는 이미지의 layer를 하나씩 늘려나가게 됩니다. 이를 경량화하는 방법으로 RUN 명령어 사용시 Bash에서 &amp;amp;&amp;amp; 연산자를 사용할 수 있습니다.또한 만약 여러 개의 이미지로부터 새로운 이미지를 불러와야 하는 상황이라면 FROM 과 COPY를 사용해 이미지를 경량화 할 수 있습니다. 이를 이용하면 각각의 이미지에서 원하는 파일만 선택적으로 복사해 다음 이미지로 전달시키고 필요없는 파일(다운로드 과정에서 필요한 코드와 같은 부수적인 파일들)은 제거할 수 있습니다. 이 방법을 Multi-Stage Builds라고 하는데 이 방법은 여러 개의 이미지로 부터 새로운 이미지를 생성할 때 여러 개의 Dockerfile이 필요없이 하나의 파일에 관리할 수 있다는 장점도 있습니다.Multi Stage Builds 방법으로 이미지를 만드는 코드의 형태는 다음과 같습니다.# &amp;lt;image&amp;gt; 를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# &amp;lt;image2&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image2&amp;gt;...# &amp;lt;image&amp;gt;의 빌드 결과로 생성된 파일 중 원하는 파일만 복사COPY --from=apple /dir/you/want/from/apple /dir/of/image2# 이 방법은 기본 이미지는 &amp;lt;image&amp;gt; 하나만 필요하지만 원하는 파일만 가져오고 싶은 경우 사용하는 것 같다# &amp;lt;image&amp;gt;를 기본 이미지로 한다FROM &amp;lt;image&amp;gt; AS apple...# apple로 생성된 이미지를 기본 이미지로 한다FROM apple AS apple_juice...# &amp;lt;image&amp;gt;에서 필요한 파일만 복사COPY /dir/you/want/from/apple /dir/of/apple_juice# 사용 예시FROM golang:1.16 AS builderWORKDIR /go/src/github.com/alexellis/href-counter/RUN go get -d -v golang.org/x/net/html  COPY app.go    ./RUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROM alpine:latest  RUN apk --no-cache add ca-certificatesWORKDIR /root/COPY --from=builder /go/src/github.com/alexellis/href-counter/app ./CMD [&quot;./app&quot;]  LABELLABEL 명령어는 이미지에 메타데이터를 추가하기 위해 사용됩니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot;LABEL com.example.label-with-value=&quot;foo&quot;LABEL version=&quot;1.0&quot;LABEL description=&quot;This text illustrates \that label-values can span multiple lines.&quot;하나의 명령어에 여러 데이터를 추가하면 이미지 크기를 줄일 수 있습니다.LABEL &quot;com.example.vendor&quot;=&quot;ACME Incorporated&quot; \com.example.label-with-value=&quot;foo&quot; \version=&quot;1.0&quot; \ description=&quot;This text illustrates \that label-values can span multiple lines.&quot;ARG빌드 단계에서만 사용하기 위한 변수입니다. 밑에서 배울 ENV와 같은 변수를 지정하게 되면 ENV가 ARG를 오버라이딩합니다.ENVENV 명령어는 환경 변수를 키:밸류 형태로 지정하도록 해줍니다. 설정된 환경 변수는 설정 이후의 모든 빌드 단계와 런타임 단계에서 사용됩니다.만약 밸류로 띄어쓰기가 필요하다면 쌍따옴표로 감싸주면 됩니다.ENV MY_NAME=&quot;John Doe&quot;ENV MY_CAT=fluffy참고  Docker 공식문서  Docker 공식문서2  stack overfolw Nagev 답변  EARTHLY 블로그: Docker Multistage Builds  geeksforgeeks 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-25T21:01:35+09:00'>25 Jan 2022</time><a class='article__image' href='/docker-series1'> <img src='/images/docker_4.png' alt='Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series1'>Dockerfile을 이용한 이미지 빌드(1) FROM LABEL ARG ENV</a> </h2><p class='article__excerpt'>FROM은 빌드를 위한 stage를 초기화하고 이후의 명령어를 위한 기본 이미지를 만듭니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part3]: Fault tolerance in Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series3",
      "date"     : "Jan 24, 2022",
      "content"  : "Table of Contents  Fault tolerance in Kafka          카프카 리플리케이션(Replication)      리더(Leader)와 팔로워(Follower)      컨트롤러(Controller)      리플리케이션 과정        참고자료Fault tolerance in Kafka카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.카프카 리플리케이션(Replication)카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, 다른 브로커에 파티션을 복제해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 리더와 팔로워가 있습니다.(shwitha B G 블로그 참고)리더(Leader)와 팔로워(Follower)카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.컨트롤러(Controller)리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 컨트롤러라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.(shwitha B G 블로그 참고)컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. 주키퍼(Zookeeper) 개념이 잠깐 등장합니다.(Zookeeper is the centralized service for storing metadata of topic, partition, and broker)  주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.  브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다  컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.  컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.리플리케이션 과정마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 ISR(InSyncReplica)이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 하이워터마크(high water mark)라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message 3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 message 3을 읽어갔지만, 이제는 더이상 message 3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.참고자료  Hackernoon 블로그  Ashwitha B G 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-24T21:01:35+09:00'>24 Jan 2022</time><a class='article__image' href='/kafka-series3'> <img src='/images/kafka_15.png' alt='Kafka Series [Part3]: Fault tolerance in Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series3'>Kafka Series [Part3]: Fault tolerance in Kafka</a> </h2><p class='article__excerpt'>이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part5]: Kubernetes networking for developers [번역]",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series5",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Basics  Localhost (IP address 127.0.0.1)  Pod network  Service network원문: Kubernetes networking for developers - IBM developers불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.Kubernetes Basics컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 파드(Pod)로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)Localhost (IP address 127.0.0.1)같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 공유되는 네트워크 네임스페이스를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.Pod network파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.사용되는 IP 주소는 파드 네트워크라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.Service network쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해 트래픽을 전송하도록 해줍니다.kind: ServiceapiVersion: v1metadata:  name: web  namespace: my-appspec:  selector:    app: web-server  ports:  - name: web    protocol: TCP    port: 80    targetPort: 80위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.  ClusterIP  NodePort  LoadBalancer  ExternalName",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series5'> <img src='/images/kube_35.png' alt='Kubernetes Series [Part5]: Kubernetes networking for developers [번역]'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series5'>Kubernetes Series [Part5]: Kubernetes networking for developers [번역]</a> </h2><p class='article__excerpt'>컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series4",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  로컬환경          미니큐브(minikube)      Docker Desktop      kind(Kubernetes in Docker)        클라우드환경          GKE(Google Kubernetes Engine)      EKS(Elastic Kubernetes Service)        참고자료로컬환경쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 테스트, 개발 환경에 적합합니다.미니큐브(minikube)미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 단일 노드 구성이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.brew install minikubeminikube version# minikube version: v1.25.1minikube start --driver=docker # --kubernetes-version 옵션으로 버전 선택 가능--------------------------------------------------------------------------------😄  Darwin 12.1 (arm64) 의 minikube v1.25.1✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중🚜  베이스 이미지를 다운받는 중 ...💾  쿠버네티스 v1.23.1 을 다운로드 중 ...    &amp;gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB    &amp;gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/🔥  Creating docker container (CPUs=2, Memory=7903MB) ...🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중    ▪ kubelet.housekeeping-interval=5m    ▪ 인증서 및 키를 생성하는 중 ...    ▪ 컨트롤 플레인이 부팅...    ▪ RBAC 규칙을 구성하는 중 ...🔎  Kubernetes 구성 요소를 확인...    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5🌟  애드온 활성화 : storage-provisioner, default-storageclass🏄  끝났습니다! kubectl이 &quot;minikube&quot; 클러스터와 &quot;default&quot; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.minikube status--------------------minikubetype: Control Planehost: Runningkubelet: Runningapiserver: Runningkubeconfig: Configuredminikube ip# 192.168.49.2정지하고 삭제하는 명령어도 간단합니다.minikube stopminikube deleteDocker DesktopDocker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)kind(Kubernetes in Docker)minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 멀티 노드 클러스터를 구축할 수 있습니다.(kind 공식문서 참고)brew install kindkind version--------------------kind v0.11.1 go1.17.2 darwin/arm64잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)# kind로 클러스터 구축을 위한 kind.yamlapiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦 ✓ Configuring the external load balancer ⚖️ ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✗ Joining worker nodes 🚜 실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.apiVersion: kind.x-k8s.io/v1alpha4kind: Clusternodes:- role: control-plane  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1- role: worker  image: kindest/node:v1.23.1kind create cluster --config kind.yaml --name kindcluster----------------------------------------------------------------------Creating cluster &quot;kindcluster&quot; ... ✓ Ensuring node image (kindest/node:v1.23.1) 🖼 ✓ Preparing nodes 📦 📦 📦 ✓ Writing configuration 📜 ✓ Starting control-plane 🕹️ ✓ Installing CNI 🔌 ✓ Installing StorageClass 💾 ✓ Joining worker nodes 🚜Set kubectl context to &quot;kind-kindcluster&quot;You can now use your cluster with:kubectl cluster-info --context kind-kindclusterHave a nice day! 👋클러스터가 성공적으로 구축되었습니다.쿠버네티스에서 실행중인 노드를 확인해보겠습니다.kubectl get nodes----------------------------------------------------------------------------NAME                        STATUS   ROLES                  AGE   VERSIONkindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1클러스터는 다음 명령어로 삭제하시면 됩니다.kind delete cluster --name kindcluster------------------------------------------Deleting cluster &quot;kindcluster&quot; ...클라우드환경GKE(Google Kubernetes Engine)EKS(Elastic Kubernetes Service)참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series4'> <img src='/images/kube_24.png' alt='Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series4'>Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기</a> </h2><p class='article__excerpt'>쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part3]: Kubernetes Service",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series3",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kubernetes Network  ClusterIP  NodePort  LoacBalancer  Ingress          클러스터 외부의 로드 밸런서만을 이용한 Ingress      클러스터 내부의 Ingress 파드를 곁들인 Ingress        참고자료Kubernetes Network쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 localhost로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 파드의 IP 주소로 통신하면 됩니다. 또한 노드 간의 통신은 VXLAN이나 L2 Routing을 이용할 수 있습니다.이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러나 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.  로드 밸런싱  서비스 디스커버리  클러스터 내부 DNSClusterIPClusterIP는 서비스의 기본 타입입니다. ClusterIP 서비스를 생성하면 클러스터 내부에서만 통신 가능한 가상 IP가 할당됩니다. kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송합니다.NodePortNodePort는 모든 노드의 IP주소:포트에서 수신한 트래픽을 컨테이너에 전송하는 형태로 외부와 통신할 수 있습니다. NodePort는 전체 노드 N개 중 임의의 노드의 IP주소를 외부에 노출합니다. 그럼에도 ClusterIP를 통해 다른 노드의 파드로 통신하는데에는 문제 없습니다. 그러나 노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)이 되기 때문에 NodePort만을 이용해 외부와 통신하는 것은 분명한 한계점이 있습니다. 또한 NodePort는 쿠버네티스에서 지정한 범위(30000~32767) 안에서만 지정할 수 있기 때문에 서비스로 활용하기에는 포트 번호가 예쁘지는 않습니다. 노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양합니다.LoacBalancerLoadBalancer에서는 NodePort와 다르게 별도로 외부 로드 밸런서를 사용하기 때문에 노드 장애가 발생해도 크게 문제가 되지 않습니다. 노드에 장애가 발생한 경우 해당 노드를 목적지에서 제외 처리하고 트래픽을 전송하지 않게됩니다. LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 할당됩니다. 실제 서비스 운영 환경에서는 외부로부터 요청을 수신하는 External IP 주소를 DNS 설정 등의 이유로 고정하는 것을 선호하고 LoadBalancer 서비스는 이를 지원합니다.Ingress인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, kind: Ingress타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.클러스터 외부의 로드 밸런서만을 이용한 Ingress  GKE 인그레스외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.클러스터 내부의 Ingress 파드를 곁들인 Ingress  Nginx 인그레스클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.참고자료  쿠버네티스 완벽 가이드 책  subicura님의 kubenetes안내서  NodePort vs LoadBalancer stackoverflow  Google Kubernetes Engine 가이드  Confluent 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kubernetes-series3'> <img src='/images/kube_26.png' alt='Kubernetes Series [Part3]: Kubernetes Service'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series3'>Kubernetes Series [Part3]: Kubernetes Service</a> </h2><p class='article__excerpt'>쿠버네티스에서는 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part2]: Main elements of Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series2",
      "date"     : "Jan 23, 2022",
      "content"  : "Table of Contents  Kafka의 주요 구성요소          Topic, Partition, Segment      Producer                  메세지 전송과정          라운드 로빈(Round-Robbin) 방식          스티키 파티셔닝(Sticky Partitioning) 방식          중복 없는 전송          정확히 한 번 전송                    Broker      Consumer                  컨슈머 오프셋 관리          그룹 코디네이터          파티션 할당 전략                          라운드 로빈 파티션 할당 전략              스티키 파티션 할당 전략              협력적 스티키 파티션 할당 전략                                            마치며  참고자료Kafka의 주요 구성요소Kafka는 크게 3가지로 이루어 있습니다.  Producer: Kafka로 메시지를 보내는 모든 클라이언트  Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버  Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트(참고: cloudkarafka)Topic, Partition, SegmentKafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.  Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)  Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)  Segment: 메시지가 실제로 저장되는 파일(참고: cloudkarafka)카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.Producer프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.(Dzone 블로그 참고)메세지 전송과정프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 ProducerRecord라고 하는 형태로 전송되며, Topic과 Value는 필수값이며, Partition과 Key는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은  프로듀서에서 send() 메소드 호출  Serializer는 JSON, String, Avro 등의 object를 bytes로 변환  ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달  Partition이 지정되지 않았을 때, Key값이 지정되었다면 Partitioner가 Key값을 바탕으로 해당 파티션에 전달  Partition, Key값이 모두 없으면 라운드 로빈(Round-Robbin)방식 또는 스티키 파티셔닝(Sticky Partitioning) 방식으로 메세지를 파티션에 할당  파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 배치로 전송할 수도 있음라운드 로빈(Round-Robbin) 방식프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.스티키 파티셔닝(Sticky Partitioning) 방식라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)중복 없는 전송정확히 한 번 전송Broker브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.Consumer컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.  파티션 할당 전략  프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가  컨슈머의 개수가 파티션보다 많지는 않은가  컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가컨슈머 오프셋 관리컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 __consumer_offsets라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, 컨슈머가 다음으로 읽어야 할 위치를 말합니다.참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.그룹 코디네이터컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.이렇게 컨슈머 그룹내의 변화를 감지하기 위해 트래킹하는 것이 바로 그룹 코디네이터입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.파티션 할당 전략라운드 로빈 파티션 할당 전략라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.스티키 파티션 할당 전략스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.협력적 스티키 파티션 할당 전략이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.(컨플루언트 블로그 참고)마치며이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 장애 발생에 대처 가능한 안정적인 서비스를 제공해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 높은 처리량, 데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활하게 해주는 것이 좋습니다.참고자료  실전 카프카 개발부터 운영까지 책  Dzone 블로그  CodeX 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-23T21:01:35+09:00'>23 Jan 2022</time><a class='article__image' href='/kafka-series2'> <img src='/images/kafka_30.png' alt='Kafka Series [Part2]: Main elements of Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series2'>Kafka Series [Part2]: Main elements of Kafka</a> </h2><p class='article__excerpt'>프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Docker란 무엇인가?",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/docker-series0",
      "date"     : "Jan 22, 2022",
      "content"  : "Table of Contents  도커 소개  도커의 장점  도커의 구조  도커의 구성요소          도커 데몬      도커 클라이언트      도커 오브젝트                  이미지          컨테이너                      참고도커 소개Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다. 컨테이너 기술은 가상화를 위한 방법 중 하나인데 이에 관한 더 자세한 내용은 추후에 다루어 보도록 하겠습니다.개발자들에게 있어 골칫거리 중 하나는 새로 만든 애플리케이션을 개발 환경에서 테스트 환경으로, 테스트 환경에서 운영환경으로 옮길 때마다 온갖 이상한 오류를 만난다는 것입니다. 그 이유는 인프라 환경마다 네트워크 기술과 보안 정책, 스토리지가 모두 제각각이어서 그렇습니다. 그래서 ‘소프트웨어를 한 컴퓨팅 환경에서 다른 컴퓨팅 환경으로 이동하면서도 안정적으로 실행하는 방법이 없을까?’라는 고민이 커졌고 그 대답이 바로 컨테이너였습니다.개념은 간단합니다. 애플리케이션과 그 실행에 필요한 라이브러리, 바이너리, 구성 파일 등을 패키지로 묶어 배포하는 것입니다. 이렇게 하면 노트북-테스트 환경-실제 운영환경으로 바뀌어도 실행에 필요한 파일이 함께 따라다니므로 오류를 최소화할 수 있습니다. 운영체제를 제외하고 애플리케이션 실행에 필요한 모든 파일을 패키징한다는 점에서 운영체제 위에서 구현된 가상화, 즉 ‘운영체제 레벨 가상화’라고 부르기도 합니다.참고로 도커 이전에도 컨테이너 기술을 이용한 운영체제 레벨의 가상화는 있었습니다. 구글에서는 도커가 등장하기 전부터 이러한 기술을 회사 내부적으로 이용하고 있었다고 합니다. 그러나 기술적으로 높은 진입 장벽 때문에 대중화되지 않았던 것 뿐입니다.이러한 상황 속에서 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되었고, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 필요한 모든  애플리케이션들을 도커파일을 이용해 이미지를 만들고 컨테이너로 배포하는 게 흔한 개발 프로세스가 되었습니다.도커의 장점  애플리케이션을 인프라 환경에 상관없이 빠르게 배포할 수 있습니다.  어플리케이션을 실행하기 위한 독립적인 컨테이너 환경을 제공해 서비스간 디펜던시 오류를 해결해줍니다.  별다른 운영체제 소프트웨어가 필요없어 가볍습니다.도커의 구조  도커의 아키텍처는 클라이언트-서버 아키텍처입니다.  도커 클라이언트(docker)는 도커 (REST) API를 사용해 도커 데몬(dockerd)에게 요청 메시지를 보냅니다.  dockerd은 요청을 받으면 이미지, 컨테이너, 네트워크, 볼륨과 같은 도커 오브젝트를 생성하고 관리합니다.  도커 레지스트리는 public한 곳(docker hub)도 있고, private(AWS의 ECR)한 곳도 있습니다.도커의 구성요소도커 데몬도커 데몬(dockerd)은 클라이언트로부터 API 요청을 수신하고 Image, Container, Network 및 Volume과 같은 Docker Object를 관리합니다.도커 클라이언트도커 클라이언트(docker)는 사용자가 도커 데몬과 통신하는 주요 방법입니다. docker run과 같은 명령을 사용하면 도커 클라이언트는 해당 명령을 도커 데몬에게 전송하고 도커 데몬은 명령을 수행하게 됩니다.도커 오브젝트(출처: 오웬의 개발 이야기)이미지도커 이미지는 도커 컨테이너를 생성하기 위한 읽기 전용 템플릿입니다. 기본 이미지 위에 원하는 커스터마이징을 통해 새로운 이미지를 만들 수도 있으며, 이렇게 만들어진 이미지는 Docker Registry에 Push하여 공유할 수 있습니다. 이미지를 만들때에는 Dockerfile에 필요한 명령어를 정의하여 만들 수 있습니다. Dockerfile에 정의된 각각의 명령어들은 이미지의 Layer를 생성하며, 이러한 Layer들이 모여 이미지를 구성합니다. Dockerfile을 변경하고 이미지를 다시 구성하면 변경된 부분만 새로운 Layer로 생성됩니다. 이러한 Image의 Layer구조는 Docker가 타 가상화 방식과 비교할 때, 매우 가볍고 빠르게 기동할 수 있는 요인이 됩니다.컨테이너컨테이너는 Docker API 사용하여 생성, 시작, 중지, 이동 또는 삭제 할 수 있는 이미지의 실행가능한 인스턴스를 나타냅니다. 컨테이너를 하나 이상의 네트워크에 연결하거나, 저장 장치로 묶을 수 있으며, 현재 상태를 바탕으로 새로운 이미지를 생성할 수도 있습니다. 기본적으로 컨테이너는 Host 또는 다른 컨테이너로부터 격리되어 있습니다. 컨테이너가 제거될 때는 영구 저장소에 저장되지 않은 변경 사항은 모두 해당 컨테이너와 같이 사라집니다.도커 내부에 대해 조금 더 알아보고 싶으시다면 [Docker의 아키텍처 이해하기]포스트를 참고하셔도 좋습니다.참고  도커 공식문서  Rain.i님의 도커 컨테이너 까보기(1) – Protocol, Registry 포스트  yjs0997님의 [Docker 기본(2/8)] Docker’s Skeleton 포스트  ITWorld 용어풀이: 컨테이너(container), IT World",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-22T21:01:35+09:00'>22 Jan 2022</time><a class='article__image' href='/docker-series0'> <img src='/images/docker_3.svg' alt='Docker란 무엇인가?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/docker-series0'>Docker란 무엇인가?</a> </h2><p class='article__excerpt'>Docker는 컨테이너 기술을 이용해 개개인의 인프라 환경에 상관없이 모든 애플리케이션을 독립적으로 배포, 실행 가능하도록 해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kafka Series [Part1]: What is Kafka",
      "category" : "",
      "tags"     : "Kafka",
      "url"      : "/kafka-series1",
      "date"     : "Jan 17, 2022",
      "content"  : "Table of Contents  Apache Kafka 소개  Event  기업 사례: 잘란도(Zalando)  Kafka의 핵심 기능          순서 보장      적어도 한 번 전송 방식      Pull based approach      강력한 Partitioning      비동기식 방식        정리  참고자료Apache Kafka 소개  Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.  실시간 데이터를 스트리밍하는 분산환경의 publish-subscribe 메시지 플랫폼  복잡한 데이터 파이프라인 구조를 간단하게 해주며 파이프라인의 확장성을 높여준다Event모든 기업에게 있어 데이터는 중요한 자산입니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 그 가치가 더욱 큽니다. 이러한 데이터에는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동(배송, 결제, 송금 등) 그 밖에 모든 것들이 데이터가 될 수 있습니다.Kafka에서는 Event, Data, Record, Message를 모두 혼용해서 쓰고 있습니다. Event는 어떠한 행동이나, 사건도 모두 될 수 있습니다. 다음과 같은 것들이 있습니다.  웹 사이트에서 무언가를 클릭하는 것  센서의 온도/압력 데이터  청구서  배송 물건의 위치 정보이렇게 세상에 있는 모든 정보를 실시간으로 저장하고, 처리하기 위해서는 높은 throughput, 낮은 latency가 요구됩니다. Kafka는 최대 600MB/s의 throughput과 200MB에 대해 5ms의 낮은 latency를 제공하고 있습니다.(Benchmarking Kafka vs. Pulsar vs. RabbitMQ: Which is Fastest? 참고)지금까지는 Kafka가 높은 throughput과 낮은 latency로 엄청난 양의 데이터를 실시간으로 처리해주는 플랫폼이라고 배웠습니다. 이제 이러한 개념을 가지고 조금 더 앞으로 나가보겠습니다. 다음은 Kafka를 설명하는 좋은 문장이라고 생각되어 가져와 봤습니다. (Apache Kafka Series [Part 1]: Introduction to Apache Kafka)  Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.Kafka를 이용하면 특정 Source에서 특정 Destination으로 데이터를 흘려보내는 것이 아니라, Publisher들이 실시간으로 언제든 데이터를 저장할 수 있으며, Subscriber는 언제든 저장된 데이터를 가지고 올 수 있습니다. 이러한 구조를 Pub/Sub 모델이라고 합니다. Pub/sub은 Messaging platform의 architecture를 훨씬 간단하게 만들고, 확장성을 용이하게 해줍니다.기업 사례: 잘란도(Zalando)Kafka는 현재 Fortune 100대 기업 중 80% 이상이 사용하고 있는 데이터 플랫폼의 핵심 기술입니다. 해외의 링크드인, 트위터, 아마존, 넷플릭스, 우버를 포함해 국내에서는 대표적으로 카카오와 라인 등이 Kafka를 이용하고 있습니다. 제가 여기서 소개드릴 사례는 유럽의 대표 온라인 쇼핑몰 잘란도(Zalando)입니다. (참고: Event First Development - Moving Towards Kafka Pipeline Applications)잘란도는 회사의 규모가 점점 커지고 사업이 다각화되면서 내부적으로 데이터에 대한 문제가 점점 대두되었습니다. 처리해야 할 데이터 양의 증가, 복잡해져가는 데이터 파이프라인(데이터를 Produce하는 곳과 Consume하는 곳의 다양화), 데이터 수집 장애로 인한 신뢰도 하락과 같은 문제로 잘란도에서는 이벤트 드리븐 시스템을 도입하기로 결정하였습니다.  The aim here was the democratization of data for all potential users on the new platform.(참고: https://realtimeapi.io/hub/event-driven-apis/)결과적으로 잘란도는 Kafka를 도입함으로써 내부의 데이터 처리 파이프라인을 간소화하고, 확장을 용이하게 했으며, 스트림 데이터 처리량을 높일 수 있었습니다. 이러한 결과를 얻을 수 있었던 것은 Kafka에서 제공하는 몇 가지 핵심 기능 덕분이었습니다.Kafka의 핵심 기능순서 보장이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.적어도 한 번 전송 방식분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(itempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다. 최근에는 ‘정확히 한 번’ 전송 방식이 도입되어 카프카내에서 중복성을 제거하는 방법이 많이 사용되고 있습니다.Pull based approach카프카에서 데이터를 소비하는 클라이언트는 풀 방식으로 동작합니다. 풀 방식의 장점은 자기 자신의 속도로 데이터를 처리할 수 있다는 점입니다. 푸시 방식은 브로커가 보내주는 속도에 의존해야 한다는 한계가 있습니다.강력한 Partitioning파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.비동기식 방식데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)정리  Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.  데이터를 분산처리하여 높은 throughput과 낮은 latency를 제공한다.  심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.참고자료  실전 카프카 개발부터 운영까지 책  CodeX 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-17T21:01:35+09:00'>17 Jan 2022</time><a class='article__image' href='/kafka-series1'> <img src='/images/kafka_12.png' alt='Kafka Series [Part1]: What is Kafka'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kafka-series1'>Kafka Series [Part1]: What is Kafka</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Apache Spark Series [Part1]: What is Apache Spark?",
      "category" : "",
      "tags"     : "Spark",
      "url"      : "/spark-series1",
      "date"     : "Jan 15, 2022",
      "content"  : "Table of Contents  Spark Introduction  RDD          파티션(Partition)      불변성(Immutable)      게으른 연산(Lazy Operation)        Cluster Mode          드라이버 프로그램      클러스터 매니저      워커 노드        알아두면 좋은 것들          Shuffling      Passing Functions to Spark        질문  참고Spark Introduction스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다. 쉽게 말해 대용량 데이터를 여러 컴퓨터에 나누어서 동시에 처리한다고 할 수 있습니다. 이런 방법이 스파크 이전에 없었던 것은 아닙니다. 스파크 이전에 하둡(Hadoop)이 이와 유사한 기능을 제공했었습니다. 참고로 하둡은 더그 커팅(Doug Cutting)이라는 사람이 구글이 발표했던 두 개의 논문(The Google File System_2003, MapReduce: simplified data processing on large clusters_2008)을 직접 구현해 만든 프레임워크입니다. 이처럼 구글에서는 예전부터 대용량의 데이터를 고속 분산처리 하기 위해 노력했었고, 현재 스파크는 대부분의 기업들이 사용하고 있는 소프트웨어입니다.지금부터는 스파크와 하둡을 비교하며 스파크의 특징에 어떤 것이 있는지 알아보겠습니다.            차이점      하둡      스파크              기반      디스크 기반      메모리 기반              처리방식      Map-Reduce      RDD              프로그래밍언어      자바      스칼라, 자바, 파이썬, R              라이브러리      -      다양한 라이브러리(Spark streaming, MLlib, GraphX 등) 제공      처리방식에 대해 조금 더 이야기 해보겠습니다. 맵리듀스(MapReduce)는 2004년 구글에서 대용량 데이터 처리를 분산환경에서 처리하기 위한 목적의 소프트웨어 프레임워크입니다. 맵리듀스는 함수형 프로그래밍에서 일반적으로 사용되는 Map과 Reduce라는 함수를 기반으로 만들어졌습니다. Map은 각각의 분산된 환경에서 독립적으로 실행되는 함수의 일종, Reduce는 분산 환경에서의 데이터를 하나로 모으는 함수라고 생각할 수 있습니다.맵리듀스는 분산된 환경에서 데이터가 처리되는데 필요한 많은 함수들을 제공해주지만, 현업에서 필요한 기능들을 모두 커버하기에는 무리가 있었습니다. 그래서 이러한 단점을 보완하기 위해 2009년 UC Berkeley 대학에서 연구를 시작해 2012년 미국 NSDI 학회에서 스파크의 핵심 개념인 RDD(Resilient Distributed Dataset) 에 대한 논문을 발표하였습니다.RDD  RDD is a fault-tolerant collection of elements that can be operated on in parallel.(아파치 스파크 공식문서 참고)다시 말하면 RDD란 스파크에서 정의한 분산 데이터 모델로서 병렬 처리가 가능한 요소로 구성되며 데이터를 처리하는 과정에서 장애가 발생하더라도 스스로 복구할 수 있는 능력을 가진 데이터 모델이라는 뜻입니다. RDD는 분산 데이터에 대한 모델로서 단순히 값으로 표현되는 데이터만 가리키는 것이 아니고, 분산된 데이터를 다루는 방법까지 포함하는 일종의 클래스와 같은 개념입니다.RDD에서 중요한 특징은 다음과 같습니다.  파티션(Partition): 분산 처리 기능 제공  불변성: 장애 복구 기능 제공  게으른 연산(Lazy operation): 연산 최적화 기능 제공파티션(Partition)RDD는 분산 데이터 요소로 구성된 데이터 집합입니다. 여기서 분산 데이터 요소를 파티션이라고 합니다. 스파크는 작업을 수행할 때 바로 이 파티션 단위로 나눠서 병렬로 처리합니다. 여기서 제가 헷갈렸던 것은 파티션이 분산처리와 병렬처리 중 어떤 것을 기준으로 나뉘어진 단위인가 라는 것 이었습니다. 공식문서(아파치 스파크 공식문서 참고)를 살펴본 결과 파티션은 병렬 처리가 되는 기준이었습니다. 여러 서버에 분산할 때 보통 하나의 서버 당 2~4개 정도의 파티션을 설정합니다. 이 기준은 개인의 클러스터 환경에 따라 기본 설정 값이 다르며 이 값은 원하는 값으로 바꿀 수 있습니다. 구글에서 이미지를 살펴보았을 때는 다들 task당 한개의 파티션이라고 합니다.불변성(Immutable)한 개의 RDD가 여러 개의 파티션으로 나뉘고 다수의 서버에서 처리되다 보니 작업 도중 일부 파티션 처리에 장애가 발생해 파티션 처리 결과가 유실될 수 있습니다. 하지만 스파크에서 RDD는 불변성이기 때문에 생성 과정에 사용되었던 연산들을 다시 실행하여 장애를 해결할 수 있습니다. 여기서 불변성이라는 말은 RDD에서 어떤 연산을 적용해 다른 RDD가 될 때 무조건 새로 RDD를 생성합니다(RDD는 불변이다). 이러한 방식 덕분에 장애가 발생해도 기존의 RDD 데이터에 다시 연산을 적용해 장애를 해결할 수 있는 것입니다(RDD는 회복 탄력성이 좋다(resilient)).게으른 연산(Lazy Operation)RDD의 연산은 크게 트랜스포메이션 과 액션이라는 두 종류로 나눌 수 있습니다.  트랜스포메이션: RDD1 -&amp;gt; RDD2 이런식으로 새로운 RDD를 만들어내는 연산, 대표적으로 map 함수  액션: RDD -&amp;gt; 다른 형태의 데이터를 만들어내는 연산, 대표적으로 reduce 함수(아파치 공식문서 참고)트랜스포메이션 연산은 보통 분산된 서버 각각에서 독립적으로 수행할 수 있는 연산입니다. 그리고 액션은 분산된 서버에 있는 데이터가 서로를 참조해야 하는 연산입니다. 그래서 액션은 서버 네트워크간의 이동이 발생하게 됩니다. 이런 현상을 셔플링(Shuffling)이라고 하고, 보통 네트워크에서 읽어오는 연산은 메모리에 비해 100만배 정도 느립니다.그렇기 때문에 셔플링이 발생하는 연산을 할 때에는 그 전에 최대한 데이터를 간추리는 것이 중요한데 스파크의 중요한 특징 중 하나가 바로 게으른 연산을 한다는 것입니다. 게으른 연산이라는 말은 RDD가 액션연산을 수행할 때에 비로소 모든 연산이 한꺼번에 실행된다는 것입니다. 이러한 방식의 장점은 데이터를 본격적으로 처리하기 전에 어떤 연산들이 사용되었는지 알 수 있고, 이를 통해 최종적으로 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그중에서 가장 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다. 이렇게 되면 셔플링이 최대한 작은 사이즈로 발생할 수 있도록 합니다.스파크는 RDD를 사용함으로써 처리 속도도 높이고, 장애 복구도 가능해졌다Cluster Mode이번에는 스파크를 구동시키는 환경에 대해서 알아보겠습니다. 스파크는 단일 서버로 동작시키는 로컬 모드와, 클러스터 환경에서 동작시키는 클러스터 모드가 있습니다.로컬 모드는 위의 클러스터 환경에 있는 구성 요소들을 모두 하나의 서버에 놓는 것(Executor는 1개)과 같기 때문에 여기서는 분산 처리를 가능하게 해주는 클러스터 모드에 대해서 조금 더 자세히 알아보겠습니다. 구성 요소는 크게 다음과 같습니다.  드라이버 프로그램  클러스터 매니저  워커 노드여기서 드라이버 프로그램과 워커 노드를 보통 애플리케이션이라고 하고, 클러스터 매니저는 외부 서비스로 애플리케이션과 연동합니다.드라이버 프로그램  Spark driver is a program that runs on the master node of the machine which declares transformations and actions on knowledge RDDs. In easy terms, the driver in Spark creates SparkContext, connected to a given Spark Master.It conjointly delivers the RDD graphs to Master, wherever the standalone cluster manager runs.드라이버 프로그램의 역할은 다음과 같습니다.  클러스터 매니저와의 connection을 위한 스파크 컨텍스트 객체를 생성  스파크 컨텍스트를 이용해 RDD 생성  스파크 컨텍스트를 이용해 연산 정의  정의된 연산은 DAG 스케줄러에게 전달되고 스케줄러는 연산 실행 계획 수립 후 클러스터 매니저에 전달클러스터 매니저클러스터 매니저에는 다음과 같은 것들이 있습니다.  Standalone: a simple cluster manager included with Spark that makes it easy to set up a cluster  YARN: the resource manager in Hadoop 2  Kubernetes: an open-source system for automating deployment, scaling, and management of containerized applications클러스터 매니저의 종류마다 지원하는 범위가 세부적으로 다르지만 대략적인 역할은 다음과 같습니다.  스파크 컨텍스트 생성시 설정된 Executer의 개수, Executer의 메모리를 바탕으로 자원을 할당  이용 가능한 워커에 태스크를 할당하기 위해 노드를 모니터링워커 노드스파크 컨텍스트는 워커 노드에 Executer를 생성하도록 클러스터 매니저에 요청을 하고 클러스터는 그에 맞춰 Executer를 생성합니다. Executer가 생성되면 드라이버 프로그램은 정의된 연산을 수행합니다.이 때 작업을 실제로 수행하는 것은 아니고 액션 연산의 수만큼 잡(Job)을 생성하고 잡은 셔플링이 최대한 적게 일어나는 방향으로 스테이지(Stage)를 나눕니다. 나누어진 스테이지는 다시 여러 개의 태스크(Task)로 나누어진 후 워커 노드에 생성된 Executer에 할당됩니다. 워커 노드는 Executer를 이용해 태스크를 처리하고, 데이터를 나중에 재사용 할 수 있도록 메모리에 저장도 합니다.  Executer: 작업을 수행하기 위해 스파크에서 실행하는 프로세스, 자원할당 단위, 하나의 노드에 여러 개 Executer 가능  Job: 액션 연산의 수  Task: 잡을 적당한 단위로 나누어 실제로 익스큐터에 할당하는 작업 단위알아두면 좋은 것들ShufflingPassing Functions to Spark질문  RDD가 파티션으로 나뉘어지는 시점은 RDD가 생성되는 순간일까 아니면 연산이 실행되는 순간일까?  어떤 기준으로 RDD를 파티셔닝할까?  셔플링은 액션 연산에서만 발생할까?참고  빅데이터 분석을 위한 스파크2 프로그래밍 책",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-15T21:01:35+09:00'>15 Jan 2022</time><a class='article__image' href='/spark-series1'> <img src='/images/spark_6.png' alt='Apache Spark Series [Part1]: What is Apache Spark?'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/spark-series1'>Apache Spark Series [Part1]: What is Apache Spark?</a> </h2><p class='article__excerpt'>스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Github Actions Series [Part1]: Understanding GitHub Actions",
      "category" : "",
      "tags"     : "Git",
      "url"      : "/github_action_series1",
      "date"     : "Jan 13, 2022",
      "content"  : "Table of Contents  Github Actions 소개  Github Actions 주요 구성요소          Workflow      Event      Job      Action      Runner        Workflow 예제  참고Github Actions 소개GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.Github Actions 주요 구성요소You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.(Github Actions 공식문서 참고)Workflow자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.Workflow 파일은 YAML으로 작성되고, Github Repository의 .github/workflows 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.EventWorkflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.JobJob은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.ActionAction은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.RunnerRunner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.Workflow 예제name: learn-github-actionson: [push]jobs:  check-bats-version:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - uses: actions/setup-node@v2        with:          node-version: &#39;14&#39;      - run: npm install -g bats      - run: bats -v  yaml 파일 하나가 Workflow이다  on이 Event이다  jobs 안에 정의된 이름이 각각의 Job이다  steps안에 정의된 uses가 Action이다  step안에 정의된 run이 쉘 명령어이다  job안에 정의된 runs-on이 Runner이다참고  Github Actions 공식문서  ggong.log 블로그",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-13T21:01:35+09:00'>13 Jan 2022</time><a class='article__image' href='/github_action_series1'> <img src='/images/github-actions_logo.png' alt='Github Actions Series [Part1]: Understanding GitHub Actions'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/github_action_series1'>Github Actions Series [Part1]: Understanding GitHub Actions</a> </h2><p class='article__excerpt'>GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part2]: Kubernetes Resource",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series2",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  쿠버네티스의 리소스          Workload Resources                  Pod          ReplicaSet          Deployment                    Service관련 리소스                  Service          Ingress                    Config and Storage관련 리소스                  ConfigMap          Volume                      참고자료쿠버네티스의 리소스Workload Resources  Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.(Rancher문서 참고)PodPod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다. Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다. 컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.Pod가 생성되는 과정은 다음과 같습니다.Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 할당합니다. 그러면 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.🐨 오브젝트 생성을 위한 YAML파일Pod를 포함해 쿠버네티스의 오브젝트를 만들기 위해서는 YAML파일이 필요합니다. YAML파일에 오브젝트를 위한 설정들을 작성할 수 있는데, 이 때 필수적으로 사용되는 key값들이 있습니다.            Key      설명      예              apiVersion      오브젝트 버전      v1, app/v1, ..              kind      오브젝트 종류      Pod, ReplicaSet, Deployment, ..              metadata      메타데이터      name, label, ..              spec      오브젝트 별 상세 설정      오브젝트마다 다름      apiVersion: v1kind: Podmetadata:  name: echo  labels:    app: echospec:  containers:    - name: app      image: ghcr.io/subicura/echo:v1Pod의 spec에는 containers, volumes, restartPolicy, hostname, hostNetwork 등이 있습니다.(Pod공식문서 참고)ReplicaSetReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트입니다. 단일 노드 환경이면 Pod는 모두 단일 노드에서 생성되고, 여러개의 노드를 가지고 있는 상황이면, Pod는 노드에 각각 분산되어 배포됩니다.(노드 장애 대비) 이 때 Pod를 어떤 노드에 배치할지는 스케줄러가 결정하게 됩니다. 보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많습니다.ReplicaSet은 다음과 같이 동작합니다.ReplicaSet controller가 desired state에 맞춰 Pod를 생성합니다. 그러면 Scheduler는 생성된 Pod를 노드에 할당해줍니다.apiVersion: apps/v1kind: ReplicaSetmetadata:  name: echo-rsspec:  replicas: 3  selector:    matchLabels: # app: echo이고 tier: app인 label을 가지는 파드를 관리      app: echo      tier: app  template: # replicaset이 만드는 pod의 템플릿    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v1ReplicaSet의 spec에는 replicas, selector, template, minReadySeconds가 있습니다.(ReplicaSet 공식문서 참고)DeploymentDeployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트입니다. ReplicaSet을 이용하여 Pod을 업데이트하고 이력을 관리하여 롤백Rollback하거나 특정 버전revision으로 돌아갈 수 있습니다.Deployment 오브젝트가 Pod의 버전을 관리하는 과정은 다음과 같습니다.Deployment Controller가 Deploy 조건을 체크하면서 원하는 버전에 맞게 Pod의 버전을 맞춥니다. 이 때 ReplicaSet에 있는 Pod들을 보통 한 번에 바꾸지 않고 조건에 맞게(예를 들어, 25%씩) 바꿔나감으로써 버전을 바꾸더라도 중간에 서비스가 중단되지 않도록 합니다. (무중단배포)apiVersion: apps/v1kind: Deploymentmetadata:  name: echo-deployspec:  strategy:    type: RollingUpdate    rollingUpdate:      maxSurge: 1      maxUnavailable: 0  replicas: 4  selector:    matchLabels:      app: echo      tier: app  template:    metadata:      labels:        app: echo        tier: app    spec:      containers:        - name: echo          image: ghcr.io/subicura/echo:v2spec에는 replicas, selector, template, strategy  등이 있습니다.(Deployment 공식문서 참고)Service관련 리소스  In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.ServiceService는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod와 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.  ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할  NodePort: 외부에서 접근가능하도록 하는 포트 역할  LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.# ClusterIP# redis라는 Deployment 오브젝트에 IP할당apiVersion: v1kind: Servicemetadata:  name: redisspec:  ports:    - port: 6379 # clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)      protocol: TCP  selector: # 어떤pod로 트래픽을 전달할지 결정    app: counter    tier: db# NodePortapiVersion: v1kind: Servicemetadata:  name: counter-npspec:  type: NodePort  ports:    - port: 3000 # ClusterIP, Pod IP의 포트      protocol: TCP      nodePort: 31000 # Node IP의 포트  selector:    app: counter    tier: app# LoadBalancerapiVersion: v1kind: Servicemetadata:  name: counter-lbspec:  type: LoadBalancer  ports:    - port: 30000      targetPort: 3000      protocol: TCP  selector:    app: counter    tier: appIngressIngress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 nginx, haproxy, traefik, alb등이 있습니다.minikube를 이용할 경우 다음 명령어로 설치할 수 있습니다.# nginx ingress controllerminikube addons enable ingressapiVersion: networking.k8s.io/v1kind: Ingressmetadata:  name: echo-v1spec:  rules:    - host: v1.echo.192.168.64.5.sslip.io      http:        paths:          - path: /            pathType: Prefix            backend:              service:                name: echo-v1                port:                  number: 3000# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라spec에는 rules, defaultBackend(어느 rule에도 속하지 않을 경우) 등이 있습니다.(Ingress 공식문서 참고)Config and Storage관련 리소스ConfigMapConfigMap은 설정, 환경 변수들을 담는 오브젝트입니다. 예를 들어 개발/운영에 따라 환경 변수값이 다른 경우, ConfigMap 을 활용해 Pod 생성시 넣어줄 수 있습니다.ConfigMap을 다양한 방법으로 만들 수 있습니다.  ConfigMap yaml 파일로 오브젝트 생성  환경 변수 설정을 담고 있는 yaml파일을 ConfigMap 오브젝트로 생성  그냥 환경 변수를 담고 있는 임의의 파일을 ConfigMap 오브젝트로 생성# ConfigMap yaml파일apiVersion: v1 # 참고로 v1이면 core API groupkind: ConfigMapmetadata:  name: my-configdata:  hello: world  kuber: neteskubectl apply -f config-map.yml# 환경 변수 설정을 담고 있는 yaml파일global:  scrape_interval: 15sscrape_configs:  - job_name: prometheus    metrics_path: /prometheus/metrics    static_configs:      - targets:          - localhost:9090# yaml 파일로 ConfigMap 파일 생성kubectl create cm my-config --from-file=config-file.yml# ConfigMap 적용kubectl apply -f my-config.yml# config-env.yml파일 (yml파일 아니지만 그냥 확장자 yml로 해놓아도됨)hello=worldhaha=hoho# 임의의 파일로 ConfigMap 파일 생성kubectl create cm env-config --from-env-file=config-env.yml# ConfigMap 적용kubectl apply -f env-config.yml여러 가지 방법으로 ConfigMap을 Pod에 적용할 수 있습니다.  디스크 볼륨 마운트  환경변수로 사용# ConfigMap yaml파일이 있는 볼륨 마운트apiVersion: v1kind: Podmetadata:  name: alpinespec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      volumeMounts:        - name: config-vol          mountPath: /etc/config  volumes:    - name: config-vol      configMap:        name: my-config# ConfigMap yaml파일 직접 환경변수로 설정apiVersion: v1kind: Podmetadata:  name: alpine-envspec:  containers:    - name: alpine      image: alpine      command: [&quot;sleep&quot;]      args: [&quot;100000&quot;]      env:        - name: hello          valueFrom:            configMapKeyRef:              name: my-config              key: helloVolumeVolume은 저장소와 관련된 오브젝트입니다. 지금까지 만들었던 컨테이너는 Pod을 제거하면 컨테이너 내부에 저장했던 데이터도 모두 사라집니다. MySQL과 같은 데이터베이스는 데이터가 유실되지 않도록 반드시 별도의 저장소에 데이터를 저장하고 컨테이너를 새로 만들 때 이전 데이터를 가져와야 합니다.저장소를 호스트 디렉토리를 사용할 수도 있고 EBS 같은 스토리지를 동적으로 생성하여 사용할 수도 있습니다. 사실상 인기 있는 대부분의 저장 방식을 지원합니다.저장소의 종류에는 다음과 같은 것들이 있습니다.  임시 디스크          emptyDir                  Pod 이 생성되고 삭제될 때, 같이 생성되고 삭제되는 임시 디스크          생성 당시에는 아무 것도 없는 빈 상태          물리 디스크(노드), 메모리에 저장                      로컬 디스크          hostpath                  노드가 생성될 때 이미 존재하고 있는 디렉토리                      네트워크 디스크          awsElasticBlockStore, azureDisk 등      # emptydirapiVersion: v1kind: Podmetadata:  name: shared-volumes spec:  containers:  - name: redis    image: redis    volumeMounts:    - name: shared-storage      mountPath: /data/shared  - name: nginx    image: nginx    volumeMounts:    - name: shared-storage      mountPath: /data/shared  volumes:  - name : shared-storage    emptyDir: {}# hostpathapiVersion: v1kind: Podmetadata:  name: host-logspec:  containers:    - name: log      image: busybox      args: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;sleep infinity&quot;]      volumeMounts:        - name: varlog          mountPath: /host/var/log  volumes:    - name: varlog      hostPath:        path: /var/log참고자료  subicura님의 kubenetes안내서  하나씩 점을 찍어나가며 블로그  Kubernetes 공식문서  Rancher 공식문서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/kubernetes-series2'> <img src='/images/kube_22.png' alt='Kubernetes Series [Part2]: Kubernetes Resource'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series2'>Kubernetes Series [Part2]: Kubernetes Resource</a> </h2><p class='article__excerpt'>Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container",
      "category" : "",
      "tags"     : "Docker",
      "url"      : "/container-series1",
      "date"     : "Jan 9, 2022",
      "content"  : "Table of Contents  물리서버(Bare Metal)  가상머신(Virtual Machine)          호스트 가상화      하이퍼바이저 가상화        컨테이너(Container)  참고이번 글에서는 IT업계에서 그동안 어떤 방법으로 컴퓨팅 자원을 사용해서 애플리케이션을 개발해왔는지에 대해서 살펴보겠습니다. 순서는 크게 물리서버, 가상머신, 컨테이너 순으로 알아보도록 하겠습니다.물리서버(Bare Metal)물리서버는 서버가 가지고 있는 컴퓨팅 자원을 오직 하나의 사용자(여기서 사용자는 서비스 사용자가 아니라 자원을 사용하는 사람)에게 할당하는 것입니다. 그렇기 때문에 물리서버에서는 자원을 전혀 분리해서 사용하지 않습니다.자원을 분리하지 않고 사용하는 것에는 장단점이 있습니다.먼저 장점으로는 하드웨어간 네트워크 이동이 발생하지 않기 때문에 네트워크 지연율을 최소화할 수 있고, 보안적인 측면에서도 훨씬 안전하다고 할 수 있습니다. 또한 별도의 하드웨어 에뮬레이션이 없기 때문에 하드웨어에 루트 레벨의 접근이 가능하고 이를 통해 뛰어난 커스터마이징을 할 수 있습니다. 그래서 보안을 최우선으로 생각하는 뱅킹시스템이나 데이터베이스 시스템은 이러한 Bare Metal 형태로 컴퓨팅 자원을 사용합니다.단점으로는 서비스 개발에 있어 여러 개의 애플리케이션을 독립적으로 개발하기가 힘듭니다. 그 이유는 자원이 분리되어 있지 않고 그로 인해 각각의 애플리케이션에 필요한 라이브러리 종속성도 해결하기가 어려워집니다. 한마디로 디펜던시 지옥에 빠질 수 있습니다 또한 사용자가 가지고 있는 Bare Metal 서버에 최적화된 방식으로 서비스를 개발하다보니 다른 서버에 배포가 힘들어집니다. 이러한 단점은 Scale Out 방식이 아닌 Scale Up으로 밖에 확장성을 가지지 못한다는 점으로 이어집니다.가상머신(Virtual Machine)약 1990년까지 하드웨어는 급속도로 성장해 왔습니다. 인텔의 설립자인 고든 무어가 1965년에 무어의 법칙을 얘기했다는 사실을 생각해보면 대략 30년이 지났으므로 2년간 2배씩 성장한다고 했을 떄 약 3만배(2의 15승) 가까이 성장을 한 것입니다. 하지만 그당시 소프트웨어의 발전은 한참 뒤쳐져 있는 상황이었습니다. 그래서 1990년대 중순에 처음으로 가상화 기술에 대한 요구가 생겨나기 시작했습니다.호스트 가상화처음 시중에 등장한 가상화 기술은 호스트 가상화(Type2 Hypervisor)였습니다. 호스트가상화는 Host OS위에 컴퓨팅 자원이 격리된 가상 머신을 띄우고 그 안에 Guest OS가 구동되는 방식입니다. 종류로는 VM Workstation, VMware Server, VMware Player, MS Virtual Sever, Virtual Box 등이 있습니다. 호스트 가상화는 일반 사용자들이 자신의 컴퓨터 위에 가상 머신을 실행시키는 방식으로 간단하게 가상화할 수 있지만, 하드웨어 에뮬레이팅을 위한 하이퍼바이저(Hypervisor)와 Host OS라는 두 개의 소프트웨어를 추가로 실행시켜야 하는 오버헤드가 있습니다.하이퍼바이저 가상화그다음 등장한 것이 하이퍼바이저 가상화(Type1 Hypervisor)입니다. 현재 서버 가상화 기술에서는 주류 방식으로 사용되고 있습니다. 종류로는 Xen, KVM 등이 있습니다. 이러한 방식의 가상화는 Host OS 없이 사용하는 가상화 방식이기 때문에 불필요한 오버헤드를 줄여줍니다. 아마존 AWS와 같은 클라우드의 컴퓨팅 서비스가 대표적으로 이러한 방식의 가상화 기술을 사용합니다. 하지만 Host OS가 없다는 사실에서 생기는 문제는 각각 다른 Guest OS가 하드웨어에 접근할 수 있어야 한다는 것입니다. 이를 가능하게 하는 방법에는 2가지가 있습니다.첫 번째는 하이퍼바이저가 구동될 때 DOM0라고 하는 관리용 가상머신을 하나 실행시켜 DOM0가 중개하는 전가상화 방식입니다. DOM0의 역할은 각각의 Guest OS에서 발생하는 요청을 하이퍼바이저가 해석할 수 있도록 컴파일해서 하이퍼바이저에 전달하는 것입니다. 이 방법은 호스트 OS보다는 가벼운 DOM0를 실행한다는 점에서 오버헤드가 줄게되지만 여전히 성능상의 단점이 있습니다.두 번째는 반가상화 방식(Bare Metal Hypervisor)입니다. 반가상화는 DOM0를 없애고, 각각의 Guest OS가 하이퍼바이저에게 직접 요청(Hypercall)할 수 있도록 Guest OS의 커널을 수정하는 방법입니다. 이 방법은 별도의 레이어가 필요없기 때문에 가장 오버헤드가 적게 발생합니다. 하지만 이 방법은 OS의 커널을 수정해야하기 때문에 오픈 소스의 OS에서만 가능하고 macOS나 windows같은 운영체제에서는 불가능합니다.하지만, 어디까지나 분류는 분류일 뿐 Type 1 하이퍼바이저들이 모두 전가상화에만 속하거나 반가상화에만 속하는 것은 아닙니다. 최근에는 하이퍼바이저에서 전가상화와 반가상화의 경계가 별 의미가 없어졌습니다.  VMware나 KVM이 대표적인 전가상화 제품에 속하고, Xen이 대표적인 반가상화 제품에 속했다(과거형).  전가상화는 모든 CPU 명령어를 가상화(애뮬레이션)하므로 아키텍처에 제한을 받지 않지만 느리다.  반가상화는 꼭 필요한 CPU 명령어만 가상화한다.          꼭 필요한 명령어만을 가상화 요청(Hyper Call)하도록 커널 수정 필요        Xen은 반 가상화 하이퍼바이저로 등장했지만 오래 전부터 전 가상화도 지원한다.  VMWare이나 KVM도 전 가상화 하이퍼바이저이지만 반 가상화 기능을 제공한다.  전 가상화와 반 가상화 하이퍼바이저의 경계가 거의 없어짐.컨테이너(Container)컨테이너 기술은 가상화의 꽃이라고 할 수 있습니다. 컨테이너는 애플리케이션 가상화로, VM과 달리 OS를 포함하지 않습니다. 즉, 하드웨어와 호스트 OS는 그대로 둔 채 애플리케이션 영역만 캡슐화하여 격리하는 방식입니다. VM에 비해 가볍고, 배포가 빠르며, 자원을 효율적으로 사용할 수 있다는 장점이 있어 최근에 많이 활용되고 있습니다. 이 컨테이너 기술이 정말 대중적인 가상화 기술로 자리잡게될 수 있었던 데에는 2013년 발표된 도커(Docker) 덕분일 것입니다.참고로 도커 이전에도 컨테이너 기반의 가상화는 있었습니다. 컨테이너의 역사에 대해 간략히 살펴보겠습니다.  2000년, Unix OS 인 FreeBSD 에서 OS 가상화 기능인 FreeBSD Jail를 발표합니다.  2001년, Linux에서 커널에 Linux-Vserver 라는 기능을 추가하여 OS 가상화 환경을 이용할 수 있게 되었습니다.  2006년, Google은 cgroup는 프로세스 자원 이용량을 제어하는 기능을 발표합니다.  2008년, Red Hat 에서 논리적으로 시스템 자원을 분할하는 Namespace를 발표합니다.  비슷한 시기에 IBM에서 LXC (LinuX Containers)를 발표합니다.  LXC가 cgroup 과 Namespace를 사용하여 구현한 최초의 Linux 컨테이너 엔진입니다.  2013년, 도커라는 회사에서 LXC를 아주 잘 활용할 수 있도록 도커( Docker) 라는 기술을 오픈소스로 발표합니다.  도커는 Dockerfile이란 메니페스트를 만들고, Container Hub를 만들면서, Container기술은 급속히 발전하게 됩니다.  2015년, Google에서 컨테이너를 통합하여 오케스트레이션하는 쿠버네티스라는 프로젝트를 오픈소스로 발표합니다.  2016년, 구글이 쿠버네티스를 CNCF 재단에 기증하면서 클라우드네이티브 시대의 서막을 알리게 됩니다.  이후 Containerd 와 CRI-O 그리고 PODMAN 등 컨테이너는 표준기술 중심으로 발전하고 있습니다.  이외에도 rht, OCI, CRI-O 등 표준 기술들이 발전하였고, 레드햇은 Kubernetes 기반으로 OpenShift를 개발했습니다.다음 포스트에서는 도커에 대해 공부해보도록 하겠습니다.참고  KT Cloud: Cloud 인프라 Intro - 물리서버와 가상서버  opennaru: 물리서버 , 가상화 , 컨테이너 기술 진화의 역사  phoenixnap: The Definitive Guide to Bare Metal Servers for 2021  phoenixnap: Bare Metal Vs VM: What Performs Better  IT Opening: Xen Kvm 반가상화 전가상화 차이 비교 분석  NDS: [소개] 가상화의 종류3가지  하드웨어 가상화(Virtualization) 뜻, 가상화 기술 종류 4가지, 가상머신(Virtual Machine)의 단점 3가지  Rain.i: 하이퍼바이저(Hypervisor)의 종류  openmaru: 컨테이너 기술의 발전과 역사",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-09T21:01:35+09:00'>09 Jan 2022</time><a class='article__image' href='/container-series1'> <img src='/images/container_5.png' alt='Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/container-series1'>Container Series [Part1]: 가상화 기술의 역사 Bare metal, Virtual machine, Container</a> </h2><p class='article__excerpt'>도커 이전에도 컨테이너 기반의 가상화는 있었습니다. 컨테이너의 역사에 대해 간략히 살펴보겠습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Kubernetes Series [Part1]: Kubernetes Intro",
      "category" : "",
      "tags"     : "Kubernetes",
      "url"      : "/kubernetes-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  들어가기 전에          도커의 등장        Kubernetes          쿠버네티스 소개      쿠버네티스 아키텍쳐      Desired State        마치며  참고자료들어가기 전에도커의 등장2013년 도커가 등장하기 전까지 서버 관리는 굉장히 어렵고 컨트롤하기 어려운 것으로 여겨졌습니다. 하나의 서비스를 제공하기 위해서는 보통 수십에서 수백개의 애플리케이션이 서로 연결되어 동작하는데, 이 때 오류가 나게 되면 어디서 문제가 생긴건지 파악하기가 쉽지 않았습니다.이러한 문제를 해결하기 위해 사람들은 가상화 기술을 이용해 서버를 애플리케이션별로 격리시키고자 하였습니다. 이 때 크게 두가지 방법으로 접근할 수 있는데, 하나는 가상머신을 이용해 컴퓨팅 리소스를 따로 분리하여 사용하도록 하는 것이었습니다. 하지만 이 방법은 컴퓨팅 성능을 떨어트립니다.두 번째 방법은 LXC(LinuX Containers)라는 리눅스 커널 기술로 기존의 하드웨어 레벨에서 하던 방식을 운영체제 레벨에서 해결하도록 했습니다. 이렇게 하면 컴퓨팅 성능도 떨어트리지 않으면서, 파일시스템, 리소스(CPU, 메모리, 네트워크)를 분리할 수 있습니다. 하지만 이 방법은 사용하기에는 운영체제에 대한 깊은 이해를 필요로 해서 많은 개발자들이 쉽게 쓰기는 힘들었습니다.이 때 등장한 것이 바로 도커입니다. 도커가 등장하게 되면서 컨테이너 기술에 대한 접근성이 훨씬 좋아지게 되자, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 수많은 애플리케이션이 컨테이너로 배포되고 도커파일을 만들어 이미지를 빌드하고 컨테이너를 배포하는 게 흔한 개발 프로세스가 되었습니다.(도커에 관한 더 자세한 내용은 여기를 참고하시기 바랍니다)이제 모든 것들을 컨테이너화하기 시작하면서 우리의 서비스는 다음과 같은 모습을 가지게 되었습니다.이렇게 서비스 하나를 배포하기 위해 수많은 컨테이너를 띄우고, 연결하고, 버전업을 해야하는 상황이 생긴겁니다. 그래서 개발자들은 이제 컨테이너들을 동시에 띄우고 관리까지 해주는 컨테이너 오케스트레이션기술이 필요해지게 되었습니다.Kubernetes쿠버네티스 소개쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 합니다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작합니다.쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화합니다.쿠버네티스 아키텍쳐  마스터: 전체 클러스터를 관리하는 서버  노드: 컨테이너가 배포되는 서버쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.  컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 Desired 상태를 유지, API서버 요청 처리  스케줄러: 새로 생성되는 Pod(컨테이너와 비슷)가 있는지 계속 체크, 생성되면 가장 적절한 노드 선택컨트롤러는 자신이 맡고 있는 오브젝트의 상태를 계속 체크하고 상태를 유지합니다. 또한 API 서버에서 어떤 새로운 상태를 요구할 경우, 맞춰서 또 상태를 바꿔서 유지하고 이 때 새롭게 Pod가 생성되거나 삭제되면 스케줄러가 그에 맞춰서 노드에서 삭제, 할당합니다.Desired State쿠버네티스에서 가장 중요한 것은 desired state(원하는 상태)라는 개념입니다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말합니다.쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있습니다.이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청합니다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 됩니다.마치며쿠버네티스는 여러 컨테이너를 자동으로 배포해주고 관리해준다는 점에서 정말 좋은 기술입니다. 그리고 마이크로서비스, 클라우드 환경과도 정말 잘 어울리기 때문에 배워두면 정말 쓸모가 많을 것 같습니다. 하지만 쿠버네티스는 많은 영역을 커버하다보니 배워야할 것들이 굉장히 많습니다. 그리고 컨테이너들을 띄우는 서버를 관리하기 위한 서버를 더 사용하게 되는 것이기 때문에, 컴퓨팅 자원이 충분하지 않다면 사용하는 것이 적절하지 않을 수도 있습니다. (쿠버네티스를 운영환경에 설치하기 위해선 최소 3대의 마스터 서버와 컨테이너 배포를 위한 n개의 노드 서버가 필요)다음 포스트에서는 서버가 넉넉하지 않은 상황에서 사용할 수 있는 minikube를 설치, 그리고 쿠버네티스에 명령어를 전달할 때 사용하는 kubectl 설치해보겠습니다.그리고 도커에서는 컨테이너를 띄우지만 쿠버네티스에서는 컨테이너를 관리할 수 있도록 조금 더 패키징한 다양한 오브젝트를 띄우게 되는데 이 때 어떠한 오브젝트들이 있는지도 배워보도록 하겠습니다.참고자료  subicura님의 kubenetes안내서",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/kubernetes-series1'> <img src='/images/kube_23.svg' alt='Kubernetes Series [Part1]: Kubernetes Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/kubernetes-series1'>Kubernetes Series [Part1]: Kubernetes Intro</a> </h2><p class='article__excerpt'>쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Javascript Series [Part1]: Javascript Intro",
      "category" : "",
      "tags"     : "Javascript",
      "url"      : "/javascript-series1",
      "date"     : "Jan 8, 2022",
      "content"  : "Table of Contents  자바스크립트의 탄생  자바스크립트의 표준화  자바스크립트의 역사          Ajax      jQuery      자바스크립트 엔진 V8      Node.js      SPA        ECMAScript  자바스크립트의 특징  자바스크립트 실행 환경자바스크립트의 탄생1995년 웹 브라우저 시장을 지배하고 있던 넷스케이프 커뮤니케이션즈는 웹 페이지의 보조적인 기능을 수행하기 위해 브라우저에서 동작하는 경량 프로그래밍 언어를 도입하기로 결정한다. 그래서 탄생한 것이 브렌던 아이크(Brendan Eich)가 개발한 자바스크립트다.자바스크립트의 표준화1996년 마이크로소프트에서 마이크로소프트에서 인터넷 익스플로러의 점유율을 높이고자 자바스크립트의 파생 버전인 Jscript를 인터넷 익스플로러에 탑재했다. 이로 인해 브라우저에 따라 웹페이지가 정상적으로 동작하지 않는 크로스 브라우징 이슈가 발생하기 시작했다.넷스케이프 커뮤니케이션즈는 컴퓨터 시스템의 표준을 관리하는 비영리 표준화 기구인 ECMA 인터내셔널에 자바스크립트의 표준화를 요청한다.1997년 ECMA-262라 불리는 표준화된 자바스크립트 초판이 완성되었고, 상표권 문제로 자바스크립트는 ECMAScript로 명명되었다.            버전      출시연도      특징              ES1      1997      초판              ES2      1998      ISO/IEC 16262 국제 표준과 동일한 규격을 적용              ES3      1999      정규표현식, try … catch              ES5      2009      HTML5와 함께 출연한 표준안, JSON, strict mode, 접근자 프로퍼티, 프로퍼티 어트리뷰트 제어, 향상된 배열 조작 기능(forEach, map, filter, reduce, some, every)              ES6(ECMAScript 2015)      2015      let/const, 클래스, 화살표 함수, 템플릿 리터럴, 디스트럭처링 할당, 스프레드 문법, rest파라미터, 심벌, 프로미스, Map/Set, 이터러블, for…of, 제너레이터, Proxy, 모듈 import/export      자바스크립트의 역사초창기 자바스크립트는 웹페이지의 보조적인 기능을 수행하기 위한 한정적인 용도로 사용되었다. 이 시기에 대부분의 로직은 주로 웹 서버에서 실행되었고, 브라우저는 서버로부터 전달받은 HTML과 CSS를 단순히 렌더링하는 수준이었다.Ajax1999년, 자바스크립트를 이용해 서버와 브라우저가 비동기방식으로 데이터를 교환할 수 있는 통신 기능인 Ajax(Asynchoronous JavaScript and XML)가 XMLHttpRequest라는 이름으로 등장했다.이전의 웹페이지는 html 태그로 시작해서 html 태그로 끝나는 완전한 HTML 코드를 서버로부터 다시 전송받아 웹페이지 전체를 렌더링하는 방식으로 동작했다. 이러한 방식은 변경할 필요가 없는 부분까지 서버로부터 코드를 다시 전송받기 때문에 성능면에서 부족한 점이 있었다.Ajax의 등장 이후, 웹 페이지에서 변경할 필요가 없는 부분은 다시 렌더링하지 않고, 필요한 부분만 렌더링하는 방식이 가능해졌다. 이로써 웹 브라우저에서도 데스크톱 애플리케이션과 유사한 빠른 성능과 부드러운 화면 전환이 가능해졌다.jQuery2006년 jQery의 등장으로 다소 번거로웠던 DOM(Document Object Model)을 더욱 쉽게 제어할 수 있게 되었고, 크로스 브라우징 이슈도 어느 정도 해결되었다. jQuery는 많은 사용자 층을 확보하게 되었고, 다소 배우기 까다로웠던 자바스크립트보다 jQuery를 더 선호하는 개발자가 양산되기도 했다.자바스크립트 엔진 V8그동안 웹 애플리케이션은 데스크톱 애플리케이션에 비해 성능상의 한계점이 있다는 인식이 있어왔지만 Ajax의 등장으로 웹 애플리케이션의 가능성을 확인하게 되었고, 이 후 자바스크립트로 웹 애플리케이션을 구축하려는 시도가 늘면서 자바 스크립트를 구동하는 자바스크립트 엔진의 성능을 더 높이고자 하는 요구가 생기게 되었다.이에 구글은 2008년 V8이라는 자바스크립트 엔진을 개발하였고 V8의 등장으로 자바스크립트를 이용해 개발한 웹 애플리케이션이 기존의 데스크톱 애플리케이션과 유사한 UX를 제공할 수 있게 되었다.Node.jsNode.js는 라이언 달(Ryan Dahl)이 2009년 개발한 자바스크립트 엔진 V8로 빌드된 자바스크립트 런타임 환경이다.Node.js는 브라우저의 자바스크립트 엔진에서만 동작하던 자바스크립트를 브라우저 이외의 환경에서도 동작할 수 있도록 했다.Node.js는 다양한 플랫폼에 적용할 수 있지만 서버 사이드 애플리케이션 개발에 주로 사용되며, 이에 필요한 모듈, 파일 시스템, HTTP 등 빌트인 API를 제공한다.프론트엔드와 백엔드 영역을 모두 자바스크립트로 개발할 수 있다는 동형성(isomorphic)은 개발 속도를 향상시켰다.그동안 브라우저에서만 동작하는 반쪽짜리 프로그래밍 언어 취급을 받았지만 Node.js의 등장으로 서버 사이드 애플리케이션 개발에도 사용할 수 있게 됨에따라 현재는 프론트영역 백엔드 영역을 아우르는 웹 프로그래밍 언어의 표준으로 자리잡았다.Node.js는 비동기 I/O을 지원하며, 단일 스레드 이벤트 루프 기반으로 동작함으로써 요청 처리 성능이 좋다. 따라서 Node.js는 데이터를 실시간으로 처리하기 위해 I/O이 빈번하게 발생하는 SPA(Single Page Application)에 적합하다.SPA자바스크립트의 발전으로 웹 어플리케이션을 이용한 개발이 활발해지다보니 복잡한 규모의 개발에 점점 대처하기가 어려워졌다. 이러한 요구에 발맞춰 여러 기업에서는 CBD(Component Based Development) 방법론을 기반으로 하는 SPA가 대중화 되면서 Angular, React, Vue.js등 다양한 프레임워크/라이브러리가 등장하게 되었다ECMAScriptECMAScript는 자바스크립트의 표준 사양인 ECMA-262를 말합니다. 각 브라우저 제조사는 ECMAScript 사양을 준수해 브라우저의 자바스크립트 엔진을 구현한다.자바스크립트는 ECMAScript와 브라우저가 별도 지원하는 클라이언트 사이드 Web API(DOM, XMLHttpRequest, fetch 등)을 아우르는 개념이다.자바스크립트의 특징자바스크립트는 웹브라우저에서 동작하는 유일한 프로그래밍 언어다.자바스크립트는 개발자가 별도의 컴파일 작업을 수행하지 않는 인터프리터 언어다.자바스크립트는 명령형, 함수형, 프로토타입 기반 객체지향 프로그래밍을 지원하는 멀티 패러다임 프로그래밍 언어다.자바스크립트 실행 환경자바스크립트를 실행하기 위해서는 자바스크립트 엔진이 필요한데 이는 브라우저와 Node.js에만 있다.자바스크립트를 개발/테스트할 때는 주로 크롬의 개발자 도구, Node.js, 비주얼 스튜디오 코드의 Live Server 확장 플러그인을 사용한다. Live Server를 사용하면 별도의 가상 서버가 기동되고 서버에 있는 브라우저에 HTML 파일을 로딩한다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-08T21:01:35+09:00'>08 Jan 2022</time><a class='article__image' href='/javascript-series1'> <img src='/images/js_logo.jpg' alt='Javascript Series [Part1]: Javascript Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/javascript-series1'>Javascript Series [Part1]: Javascript Intro</a> </h2><p class='article__excerpt'>이 시리즈는 이웅모님의 모던 자바스크립트 Deep Dive 책을 읽고 정리한 내용입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part4]: ElasticSearch 검색",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-search",
      "date"     : "Jan 7, 2022",
      "content"  : "Table of Contents  검색 API  Query DSL검색 APIQuery DSL",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/elasticsearch-search'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part4]: ElasticSearch 검색'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-search'>ElasticSearch Series [Part4]: ElasticSearch 검색</a> </h2><p class='article__excerpt'>Kafka는 이러한 데이터를 수집, 가공, 저장해주는 Event streaming platform입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "AWS Series [Part1]: AWS Intro",
      "category" : "",
      "tags"     : "AWS",
      "url"      : "/aws-series1",
      "date"     : "Jan 7, 2022",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-07T21:01:35+09:00'>07 Jan 2022</time><a class='article__image' href='/aws-series1'> <img src='/images/aws_logo.png' alt='AWS Series [Part1]: AWS Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/aws-series1'>AWS Series [Part1]: AWS Intro</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part3]: ElasticSearch Modeling",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-modeling",
      "date"     : "Jan 6, 2022",
      "content"  : "Table of Contents  Elasticsearch 모델링          데이터 타입                  Keyword 데이터 타입          Text 데이터 타입                    매핑 파라미터      Elasticsearch 모델링엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.  필드에 지정할 데이터 타입  필드별 매핑 파라미터예를 들어, 영화 정보를 엘라스틱서치를 이용해 저장하고 싶다면,            필드명      필드 타입              movieTitle      text              jenre      keyword              nation      keyword              produceYear      integer              releaseDate      date              actor      keyword      이렇게 필드별로 text, keyword, integer, date 등의 데이터 타입을 설정할 수 있습니다.데이터 타입# 대표적인 데이터 타입- 문자열 관련한 데이터 타입: keyword, text- 일반적인 데이터 타입: integer, long, double, boolean- 특수한 데이터 타입: date, ip, geo_point, geo_shapeKeyword 데이터 타입Keyword 데이터 타입은 문자열 데이터를 색인할 때 자주 사용하는 타입 중 하나로, 별도의 분석기 없이 원문 그대로가 저장된다는 것이 특징입니다. 예를 들어 ‘elastic search’라는 문자열을 keyword 타입으로 저장한다면, ‘elastic’이나 ‘search’로는 검색이 되지 않고 정확히 ‘elastic search’라고해야만 검색됩니다. 이러한 데이터 타입은 주로 카테고리형 데이터의 필드 타입으로 적절하며, 문자열을 필터링, 정렬, 집계할 때는 keyword타입을 이용해야 합니다.Text 데이터 타입반지의 제왕 영화 시리즈에는 ‘반지의 제왕: 반지 원정대’, ‘반지의 제왕: 두 개의 탑’, ‘반지의 제왕: 왕의 귀환’이 있습니다. 근데 저는 부제목까지는 기억이 안나고 ‘반지의 제왕’만 기억이 납니다. 그래서 저는 ‘반지의 제왕’이라고만 검색해도 위의 영화들이 나왔으면 좋겠습니다. 이럴 때는 text데이터 타입을 이용합니다. Text타입은 전문 검색이 가능하다는 점이 가장 큰 특징입니다. Text타입으로 데이터를 색인하면 전체 텍스트가 토큰화되어 역색인(inverted index)됩니다.더 자세한 내용은 공식문서를 참고해주시면 좋을 것 같습니다. (엘라스틱서치 공식문서 참고)매핑 파라미터문서를 색인하는 과정은 당연 엘라스틱서치에서 가장 중요한 부분입니다. 그렇기 때문에 엘라스틱서치에서는 매핑 파라미터를 통해 색인 과정을 커스텀하도록 도와줍니다. 예를 들어 어떤 영화 데이터는 장르가 없다고 하면 색인할 때 필드를 생성하지 않습니다. 이럴 때 null_value를 ‘데이터 없음’이라고 설정했다면, 필드가 생성 되고 값에 ‘데이터 없음’이라는 값이 들어갑니다. 또 다른 예시는 특정 필드를 색인에 포함할지 말지를 결정하는 enabled, index 파라미터도 있습니다. (둘의 차이는 stack overflow 참고)# 매핑 파라미터- 문자열에 자주 사용: analyzer, search_analyzer, similarity, term_vector, normalizer- 저장 관련: enabled, index, store, copy_to, doc_values- 색인 방식과 관련: ignore_above, ignore_malformed, coerce, dynamic, null_value- 필드 안의 필드를 정의할 때 사용: properties, fields- 그 밖: position_increment_gap, format(엘라스틱서치 공식문서 참고)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-06T21:01:35+09:00'>06 Jan 2022</time><a class='article__image' href='/elasticsearch-modeling'> <img src='/images/elastic_logo.png' alt='ElasticSearch Series [Part3]: ElasticSearch Modeling'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-modeling'>ElasticSearch Series [Part3]: ElasticSearch Modeling</a> </h2><p class='article__excerpt'>엘라스틱서치에서 문서(데이터)를 어떤 형태로 색인할 것인지 설정하는 것을 모델링이라고 합니다. 엘라스틱서치에서 모델링할 때에 중요한 요소는 다음과 같습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-element",
      "date"     : "Jan 5, 2022",
      "content"  : "Table of Contents  Elasticsearch를 구성하는 개념          인덱스      샤드      타입      문서      필드      매핑        Elasticsearch에서 제공하는 주요 API          인덱스 관련 API      문서 관련 API      검색 API      집계 API      Elasticsearch를 구성하는 개념“Elasticsearch에서 데이터는 인덱스 안의 특정 타입 에 문서 로 저장되는데 이 때 문서는 필드 를 가지고 있으며, 이러한 필드는 매핑 프로세스로 정의된 이름과 속성을 보통 따른다. 그리고 이 때 모든 문서들은 정의된 샤드 의 개수에 각각 골고루 분배되어 분산처리된다. 또한 장애를 대비해 레플리카 의 개수만큼 복제해 놓기도 한다.”위의 내용은 Elasticsearch의 데이터 저장방식에 관한 글입니다. 하지만 Elasticsearch가 익숙하지 않은 사람에게는 낯선 용어들도 있고 익숙하지만 이해하기 힘든 용어들도 있습니다. 그래도 Elasticsearch에서 사용하는 용어와 RDBMS에서 사용하는 용어를 비교하며 살펴보고 다시 한번 읽어보면 조금 더 이해가 갈 것입니다.            Elasticsearch      RDBMS              인덱스      데이터베이스              타입      테이블              문서      행              필드      열              매핑      스키마              샤드      파티션      인덱스인덱스는 논리적 데이터 저장 공간을 뜻하며, 하나의 물리적인 노드에 여러 개의 인덱스를 생성할 수도 있습니다. 이는 곧 멀티테넌시를 지원한다는 뜻이기도 합니다. 만약 Elasticsearch를 분산 환경으로 구성했다면 하나의 인덱스는 여러 노드에 분산 저장되며 검색 시 더 빠른 속도를 제공합니다.샤드분산 환경으로 저장되면 인덱스가 여러 노드에 분산 저장된다고 했는데, 이렇게 물리적으로 여러 공간에 나뉠 때의 단위를 샤드라고 합니다. 이 때 샤드는 레플리카의 단위가 되기도 합니다.타입타입은 보통 카테고리와 비슷한 의미로 노래를 K-pop, Classic, Rock처럼 장르별로 나누는 것과 같습니다. 하지만 6.1 버전 이후 인덱스 당 한 개의 타입만 지원하고 있습니다.문서한 개의 데이터를 뜻하며, 기본적으로 JSON 형태로 저장됩니다.필드필드는 문서의 속성을 나타내며 데이터베이스의 컬럼과 비슷한 의미입니다. 다만 컬럼의 데이터 타입은 정적이고, 필드의 데이터 타입은 좀 더 동적이라고 할 수 있습니다.매핑매핑은 필드와, 필드의 타입을 정의하고 그에 따른 색인 방법을 정의하는 프로세스입니다.Elasticsearch에서 제공하는 주요 APIfrom elasticsearch import ElasticsearchES_URL = &#39;localhost:9200&#39;ES_INDEX = &#39;first_index&#39;DOC_TYPE = &#39;_doc&#39;es = Elasticsearch(ES_URL)인덱스 관련 API# 인덱스 메타데이터, 매핑 정의index_settings = {    &#39;settings&#39;: {        &#39;number_of_shards&#39;: 2,        &#39;number_of_replicas&#39;: 1    },    &#39;mappings&#39;: {         &#39;properties&#39;: {            &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},            &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},            &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}        }            }}# 인덱스 생성es.indices.create(index=ES_INDEX, **index_settings)--------------------------------------------------------{&#39;acknowledged&#39;: True, &#39;shards_acknowledged&#39;: True, &#39;index&#39;: &#39;first_index&#39;}# 인덱스 메타 데이터 확인es.indices.get_settings()--------------------------------------------------------{&#39;first_index&#39;: {&#39;settings&#39;: {&#39;index&#39;: {&#39;routing&#39;: {&#39;allocation&#39;: {&#39;include&#39;: {&#39;_tier_preference&#39;: &#39;data_content&#39;}}},    &#39;number_of_shards&#39;: &#39;2&#39;,    &#39;provided_name&#39;: &#39;first_index&#39;,    &#39;creation_date&#39;: &#39;1641728644368&#39;,    &#39;number_of_replicas&#39;: &#39;1&#39;,    &#39;uuid&#39;: &#39;3QtIZXthRcGtCdV40WmUCg&#39;,    &#39;version&#39;: {&#39;created&#39;: &#39;7160299&#39;}}}}}# 인덱스 매핑 확인es.indices.get_mapping(index=ES_INDEX)--------------------------------------------------------{&#39;first_index&#39;: {&#39;mappings&#39;: {&#39;properties&#39;: {&#39;address&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;age&#39;: {&#39;type&#39;: &#39;long&#39;},    &#39;company&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;email&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;gender&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;name&#39;: {&#39;type&#39;: &#39;text&#39;},    &#39;phone&#39;: {&#39;type&#39;: &#39;text&#39;}}}}}# 인덱스 삭제es.indices.delete(ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}# 인덱스 존재 유무es.indices.exists(ES_INDEX)--------------------------------------------------------True# 매핑 업데이트new_field = {  &quot;properties&quot;: {    &quot;school&quot; : {      &quot;type&quot;: &quot;text&quot;    }  }}es.indices.put_mapping(new_field, index=ES_INDEX)--------------------------------------------------------{&#39;acknowledged&#39;: True}문서 관련 API# 문서 삽입unit_document = {    &#39;name&#39;: &#39;Jay Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,    &#39;phone&#39;: &#39;010-1234-5678&#39;}es.index(index=ES_INDEX, doc_type=DOC_TYPE, id=1, document=unit_document)--------------------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 4, &#39;_primary_term&#39;: 1}# 문서 정보 확인es.get(index=ES_INDEX, doc_type=DOC_TYPE, id=1)-----------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 1, &#39;_seq_no&#39;: 0, &#39;_primary_term&#39;: 1, &#39;found&#39;: True, &#39;_source&#39;: {&#39;name&#39;: &#39;Jay Kim&#39;,  &#39;age&#39;: 28,  &#39;gender&#39;: &#39;male&#39;,  &#39;email&#39;: &#39;abc@gmail.com&#39;,  &#39;address&#39;: &#39;부산 해운대 앞바다&#39;,  &#39;phone&#39;: &#39;010-1234-5678&#39;}}# 문서를 가지는 인덱스 생성 (이미 있으면 삽입)unit_document = {    &#39;name&#39;: &#39;Jae yeong Kim&#39;,    &#39;age&#39;: 28,    &#39;gender&#39;: &#39;male&#39;,    &#39;email&#39;: &#39;abc@gmail.com&#39;,    &#39;address&#39;: &#39;경북 구미 형곡동&#39;,    &#39;phone&#39;: &#39;010-3321-5668&#39;}es.create(index=ES_INDEX, id=2, body=unit_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 1, &#39;result&#39;: &#39;created&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 1, &#39;_primary_term&#39;: 1}# 문서 삭제es.delete(index=ES_INDEX, id=2)---------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;2&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;deleted&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 2, &#39;_primary_term&#39;: 1}# query로 삭제body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.delete_by_query(index=ES_INDEX, body=body)----------------------------------------------{&#39;took&#39;: 23, &#39;timed_out&#39;: False, &#39;total&#39;: 1, &#39;deleted&#39;: 1, &#39;batches&#39;: 1, &#39;version_conflicts&#39;: 0, &#39;noops&#39;: 0, &#39;retries&#39;: {&#39;bulk&#39;: 0, &#39;search&#39;: 0}, &#39;throttled_millis&#39;: 0, &#39;requests_per_second&#39;: -1.0, &#39;throttled_until_millis&#39;: 0, &#39;failures&#39;: []}# 문서 수정# &quot;&quot;doc&quot;&quot; is essentially Elasticsearch&#39;s &quot;&quot;_source&quot;&quot; fieldupdate_document = {&#39;doc&#39;: {         &#39;address&#39;: &#39;경북 구미 송정동&#39;,         &#39;age&#39;: 28,         &#39;email&#39;: &#39;ziont0510@gmail.com&#39;,    }}es.update(index=ES_INDEX, id=1, body=update_document)------------------------------------------------------------{&#39;_index&#39;: &#39;first_index&#39;, &#39;_type&#39;: &#39;_doc&#39;, &#39;_id&#39;: &#39;1&#39;, &#39;_version&#39;: 2, &#39;result&#39;: &#39;updated&#39;, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 1, &#39;failed&#39;: 0}, &#39;_seq_no&#39;: 5, &#39;_primary_term&#39;: 1}검색 API# 매칭되는 문서 개수body = {    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}es.count(body=body, index=ES_INDEX)---------------------------------------------------------------{&#39;count&#39;: 2, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}}# 문서 검색body = {    &#39;size&#39;:10,    &#39;query&#39;:{        &#39;match_all&#39;:{}    }}    es.search(body=body, index=ES_INDEX)-------------------------------------------------------{&#39;took&#39;: 6, &#39;timed_out&#39;: False, &#39;_shards&#39;: {&#39;total&#39;: 2, &#39;successful&#39;: 2, &#39;skipped&#39;: 0, &#39;failed&#39;: 0}, &#39;hits&#39;: {&#39;total&#39;: {&#39;value&#39;: 0, &#39;relation&#39;: &#39;eq&#39;},  &#39;max_score&#39;: None,  &#39;hits&#39;: []}}집계 API",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-05T21:01:35+09:00'>05 Jan 2022</time><a class='article__image' href='/elasticsearch-element'> <img src='/images/elastic_4.png' alt='ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-element'>ElasticSearch Series [Part2]: ElasticSearch에서 사용되는 간단한 용어와 파이썬 코드</a> </h2><p class='article__excerpt'>Elasticsearch에서 데이터는 인덱스 안의 특정 타입에 문서로 저장되는데..</p></div></div></div>"
    } ,
  
    {
      "title"    : "MongoDB Series [Part1]: MongoDB Intro",
      "category" : "",
      "tags"     : "MongoDB",
      "url"      : "/mongodb-intro",
      "date"     : "Jan 4, 2022",
      "content"  : "Table of Contents  참고참고  프로그래머 YD: Docker - 도커로 MongoDB 컨테이너 설치하는 방법을 알아보자  프리킴: [MongoDB] 몽고DB 기본 명령어",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-04T21:01:35+09:00'>04 Jan 2022</time><a class='article__image' href='/mongodb-intro'> <img src='/images/mongodb_logo.png' alt='MongoDB Series [Part1]: MongoDB Intro'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mongodb-intro'>MongoDB Series [Part1]: MongoDB Intro</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "ElasticSearch Series [Part1]: ElasticSearch Installation",
      "category" : "",
      "tags"     : "Elasticsearch",
      "url"      : "/elasticsearch-intro",
      "date"     : "Jan 3, 2022",
      "content"  : "Table of Contents  Elasticsearch 소개          Elasticsearch를 사용하는 이유                  Elasticsearch는 빠릅니다          Elasticsearch는 본질상 분산적입니다.          Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.          그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.                      Elasticsearch 설치          Docker를 이용한 설치      github + Docker를 이용한 설치      Linux에 직접 설치      마치며      Elasticsearch 소개  Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진  Apache Lucene을 기반으로 구축  Elastic Stack의 핵심 구성요소(Elasticsearch, Logstash, Kibana)  기본적으로는 검색엔진이지만, MongoDB나 HBase처럼 대용량 스토리지로도 활용Elasticsearch를 사용하는 이유Elasticsearch는 빠릅니다Elasticsearch는 Lucene을 기반으로 구축되기 때문에, 전문(Full-text) 검색에 뛰어납니다. Elasticsearch는 또한 거의 실시간 검색 플랫폼입니다. 이것은 문서가 색인될 때부터 검색 가능해질 때까지의 대기 시간이 아주 짧다는 뜻입니다. 이 대기 시간은 보통 1초입니다. 결과적으로, Elasticsearch는 보안 분석, 인프라 모니터링 같은 시간이 중요한 사용 사례에 이상적입니다.🐱 전문 검색: 내용 전체를 색인해서 특정 단어가 포함된 문서를 검색하는 것🐱 Lucene기반의 검색 엔진: 역색인을 지원하는 검색 엔진으로 보통 책 마지막 부분의 단어별 페이지 수를 적어놓은 것과 비슷Elasticsearch는 본질상 분산적입니다.Elasticsearch에 저장된 문서는 샤드라고 하는 여러 다른 컨테이너에 걸쳐 분산되며, 이 샤드는 복제되어 하드웨어 장애 시에 중복되는 데이터 사본을 제공합니다. Elasticsearch의 분산적인 특징은 수백 개(심지어 수천 개)의 서버까지 확장하고 페타바이트의 데이터를 처리할 수 있게 해줍니다.Elasticsearch는 광범위한 기능 세트와 함께 제공됩니다.속도, 확장성, 복원력뿐 아니라, Elasticsearch에는 데이터 롤업, 인덱스 수명 주기 관리 등과 같이 데이터를 훨씬 더 효율적으로 저장하고 검색할 수 있게 해주는 강력한 기본 기능이 다수 탑재되어 있습니다.그 밖에, RESTful API, 멀티 테넌시 등을 지원합니다.RESTful API를 지원하고, 요청과 응답에 JSON 형식을 사용해, 개발 언어에 관계없이 이용 가능하며, 서로 다른 인덱스(테이블)일지라도 필드명(컬럼)만 같으면 한번에 여러 개의 인덱스(테이블)을 조회할 수 있습니다.🐱 Elasticsearch의 약점  완전한 실시간이 아니다  롤백 기능을 제공하지 않는다  업데이트 될 때마다 문서를 새로 생성한다Elasticsearch 설치🐱 사용환경  Macbook M1  Docker on mac  Local python 3.8.9Docker를 이용한 설치먼저 Elasticsearch 이미지를 Dockerhub에서 다운 받아옵니다. (참고: Dockerhub)제가 사용하고 있는 맥북의 M1칩은 linux/arm64 OS architecture을 지원하기 때문에 7.16.2 버전의 이미지를 가져오려고 합니다.docker pull elasticsearch:7.16.2이제 이미지를 가지고 컨테이너를 생성합니다. Elasticsearch는 보통 성능상의 이유로 분산환경에서 실행하는 것을 권장하지만 단순 테스트 또는 공부를 목적으로 한다면 단일 노드 환경에서 실행하여도 문제가 없습니다. Elasticsearch 문서에서는 두 가지 방법에 대한 도커 명령어를 제공하므로, 사용 목적에 맞게 실행시키면 됩니다. (참고: Elasticsearch 공식 문서)  Single-node    docker run -p 127.0.0.1:9200:9200 -p 127.0.0.1:9300:9300 -e &quot;discovery.type=single-node&quot; docker.elastic.co/elasticsearch/elasticsearch:7.16.2        Multi-node# Create a docker-compose.yml fileversion: &#39;2.2&#39;services:  es01:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es01    environment:      - node.name=es01      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es02,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data01:/usr/share/elasticsearch/data    ports:      - 9200:9200    networks:      - elastic  es02:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es02    environment:      - node.name=es02      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es03      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data02:/usr/share/elasticsearch/data    networks:      - elastic  es03:    image: docker.elastic.co/elasticsearch/elasticsearch:7.16.2    container_name: es03    environment:      - node.name=es03      - cluster.name=es-docker-cluster      - discovery.seed_hosts=es01,es02      - cluster.initial_master_nodes=es01,es02,es03      - bootstrap.memory_lock=true      - &quot;ES_JAVA_OPTS=-Xms512m -Xmx512m&quot;    ulimits:      memlock:        soft: -1        hard: -1    volumes:      - data03:/usr/share/elasticsearch/data    networks:      - elasticvolumes:  data01:    driver: local  data02:    driver: local  data03:    driver: localnetworks:  elastic:    driver: bridge이렇게 하고나면 Elasticsearch 설치가 완료되었습니다. localhost의 9200번 포트로 Elasticsearch에 접근할 수 있습니다. 이제 클라이언트 모드로 Elasticsearch를 사용할 수 있습니다.github + Docker를 이용한 설치깃허브에 Elasticsearch뿐만 아니라, Kibana와 Logstash를 함께 설치해주는 코드가 있어서 공유드립니다.(참고: deviantony/docker-elk)Linux에 직접 설치이 방법은 제가 전에 부스트캠프에서 프로젝트를 진행할 때 팀원 중 한 분이 공유해주셨었는데, 잘 동작하여서 공유드립니다.apt-get update &amp;amp;&amp;amp; apt-get install -y gnupg2wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | apt-key add -apt-get install apt-transport-httpsecho &quot;deb https://artifacts.elastic.co/packages/7.x/apt stable main&quot; | tee /etc/apt/sources.list.d/elastic-7.x.listapt-get update &amp;amp;&amp;amp; apt-get install elasticsearchservice elasticsearch startcd /usr/share/elasticsearchbin/elasticsearch-plugin install analysis-noriservice elasticsearch restart마치며Elasticsearch에서는 다양한 프로그래밍 언어로 Elasticsearch Client API를 제공하고 있습니다.  Java  Python  Node.js  C#  Go  Ruby  PHP  Perl다음 포스트부터는 파이썬으로 Client API를 사용해 실습과 함께 포스트를 작성하도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-03T21:01:35+09:00'>03 Jan 2022</time><a class='article__image' href='/elasticsearch-intro'> <img src='/images/elastic_1.png' alt='ElasticSearch Series [Part1]: ElasticSearch Installation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/elasticsearch-intro'>ElasticSearch Series [Part1]: ElasticSearch Installation</a> </h2><p class='article__excerpt'>Elasticsearch는 정형 및 비정형 데이터 등 모든 유형의 데이터를 위한 무료 검색 및 분석 엔진</p></div></div></div>"
    } ,
  
    {
      "title"    : "Pytorch Series [Part1]: torch",
      "category" : "",
      "tags"     : "Pytorch",
      "url"      : "/pytorch-torch",
      "date"     : "Jan 2, 2022",
      "content"  : "1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2022-01-02T21:01:35+09:00'>02 Jan 2022</time><a class='article__image' href='/pytorch-torch'> <img src='/images/pytorch_logo.webp' alt='Pytorch Series [Part1]: torch'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/pytorch-torch'>Pytorch Series [Part1]: torch</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part5]: 자바 조금 더 알아보기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series5",
      "date"     : "Jul 8, 2021",
      "content"  : "Table of Contents  1. 상속  2. 캐스팅과 제네릭          1) 캐스팅      2) 제네릭        3. 인터페이스와 추상 클래스          1) 인터페이스      2) 추상 클래스      1. 상속클래스간 공통된 속성과 기능이 많이 있을 경우 만약 이 공통된 부분을 클래스마다 다시 쓴다면 프로그래밍의 중요한 법칙 중 하나인 ‘DRY(Don’t Repeat Yourself; 중복 배체)’를 어기게 되는 것입니다. 자바의 ‘클래스 상속(Class Inheritance)’ 기능이 이 문제를 해결해줍니다.public class 자식클래스 extends 부모클래스 {    ...}접근 제어자가 private이 아니면 자식클래스에서도 변수와 메소드를 그대로 사용할 수 있습니다.자식 클래스가 부모 클래스가 가지고 있는 메소드를 덮어 쓰고 싶을 때는(기존 부모 클래스의 메소드와는 독립적인 메소드로 만들고 싶을 때) ‘메소드 오버라이딩(Method Overriding)’을 해줘야 합니다. 메소드 정의 위에 써져있는 @Override가 메소드 오버라이딩을 표시해줍니다. @Override와 같이 골뱅이(@)가 붙어있는 문법을 ‘어노테이션(Annotation)’이라고 합니다. 주석(Comment)과 어느정도 비슷하지만, 어노테이션은 자바에서 추가적인 기능을 제공합니다. 예를 들어서 @Override를 써줬는데 부모 클래스에 같은 이름의 메소드가 없는 경우, 오류가 나오게 됩니다.public class MinimumBalanceAccount extends BankAccount {    private int minimum;    @Override    public boolean withdraw(int amount) {        if (getBalance() - amount &amp;lt; minimum) {            System.out.println(&quot;적어도 &quot; + minimum + &quot;원은 남겨야 합니다.&quot;);            return false;        }        setBalance(getBalance() - amount);        return true;    }}이번에는 부모 클래스가 가지고 있는 메소드에서 몇 가지를 추가해서 쓰고 싶은 경우에는 super를 사용하면 됩니다.public class TransferLimitAccount extends BankAccount {    private int transferLimit;    @Override    boolean withdraw(int amount) {        if (amount &amp;gt; transferLimit) {            return false;        }        return super.withdraw(amount);    }}  2. 캐스팅과 제네릭1) 캐스팅ArrayList&amp;lt;BankAccount&amp;gt; accounts = new ArrayList&amp;lt;&amp;gt;();accounts.add(ba);accounts.add(mba);accounts.add(sa);for (BankAccount account : accounts) {    account.deposit(1000);}이렇게 하면 각 계좌가 BankAccount 타입으로 ‘캐스팅(Casting)’되고, 한꺼번에 묶어서 다룰 수 있습니다.sa에게는 이자를 붙여주고 싶은데, BankAccount 클래스에는 addInterest 메소드가 없습니다. 만약 여기서 SavingsAccount만 골라서 addInterest 메소드를 쓰고 싶으면 instanceof 키워드를 사용하면 됩니다.for (BankAccount account : accounts) {    account.deposit(1000);    if (account instanceof SavingsAccount) {        ((SavingsAccount) account).addInterest();    }}2) 제네릭아래 꺽쇠 기호(&amp;lt;&amp;gt;) 사이에 있는 T를 ‘타입 파라미터’라고 부릅니다. 그리고 이와 같이 타입 파라미터를 받는 클래스를 ‘제네릭 클래스(Generic Class)’라고 합니다.public class Box&amp;lt;T&amp;gt; {    private T something;    public void set(T object) {         this.something = object;    }    public T get() {        return something;    }}아래처럼 타입 파라미터로 String을 넘겨주면,Box&amp;lt;String&amp;gt; box = new Box&amp;lt;&amp;gt;();클래스에 있던 모든 T가 String으로 대체된다고 생각하면 됩니다.public class Box&amp;lt;String&amp;gt; {    private String object;    public void set(String object) {        this.object = object;    }    public String get() {        return object;    }}지금까지는 타입 파라미터로 아무 클래스나 넘길 수 있었는데요. extends 키워드를 이용하면 타입을 제한할 수도 있습니다.public class PhoneBox&amp;lt;T extends Phone&amp;gt; extends Box&amp;lt;T&amp;gt; {    public void handsFreeCall(String numberString) {        object.call(numberString);    }}3. 인터페이스와 추상 클래스1) 인터페이스클래스가 생성될 때, 특정 빈 메소드를 강제로 가지도록 하고 싶을 때 인터페이스를 이용합니다// 인터페이스public interface Shape {  // 빈 메소드  double getArea();  double getPerimeter();}// 특정 인터페이스를 따라야 하는 클래스public class Circle implements Shape {  ...  ...  public double getArea() {    return PI * radius * radius;  }  public double getPerimeter() {    return 2 * PI * radius;  }}2) 추상 클래스// 추상클래스public abstract class Shape {  // 변수  public double x, y;  // 메소드  public void move(doulbe x, double y) {    ...  }  // 빈 메소드 (추상 메소드)  public abstract double getArea();  public abstract double getPerimeter();}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-08T21:01:35+09:00'>08 Jul 2021</time><a class='article__image' href='/java-series5'> <img src='/images/java_logo.png' alt='Java Series [Part5]: 자바 조금 더 알아보기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series5'>Java Series [Part5]: 자바 조금 더 알아보기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part4]: 자바의 자료형 특징",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series4",
      "date"     : "Jul 7, 2021",
      "content"  : "Table of Contents  1. 기본형 참조형  2. null  3. final  4. 클래스 변수와 클래스 메소드          1) 클래스 변수      2) 클래스 메소드        5. Wrapper class  6. ArrayList  7. HashMap1. 기본형 참조형// 참조형의 경우 == 연산자는 같은 인스턴스를 가리키는지를 물어봄String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase() == &quot;abc&quot;);-----------------------------------------------------------------false// 인스턴스가 가지는 값이 같은지를 확인하고 싶으면 equals() 메소드를 사용해야함String myString = &quot;aBc&quot;;System.out.println(myString.toLowerCase().equals(&quot;abc&quot;));----------------------------------------------------------------true2. null어떤 언어들에서는 ‘비어있음’을 None으로 표현하고, 또 어떤 언어들에서는 nil로 표현합니다. 자바에서는 ‘비어있음’이 null이라는 값으로 표현됩니다. 단, null은 참조형 변수(Reference Type)만 가질 수 있는 값입니다.3. final변수를 정의할 때 final을 써주면, 그 변수는 ‘상수’가 됩니다. 즉, 한 번 정의하고 나서 다시 바꿀 수 없다는 것이죠.public final double pi = 3.141592;4. 클래스 변수와 클래스 메소드1) 클래스 변수클래스 변수는 인스턴스가 생성될 때마다 값이 초기화 되는 것이 아니라, 모든 인스턴스들이 함께 공유하는 변수입니다. 클래스 변수를 정의하기 위해서는 static이라는 키워드를 붙여주면 됩니다.자주 접하게 되는 클래스 변수는 바로 상수입니다. final을 공부할 때 상수를 보긴 했지만, 상수를 더 상수답게 쓰려면 static과 함께 쓰는 것이 좋습니다. 상수는 인스턴스에 해당되는 것이 아니며, 여러 복사본 대신 한 값만 저장해두는 것이 맞기 때문입니다. 상수 이름은 보통 모두 대문자로 쓰고, 단어가 여러 개인 경우 _로 구분 짓습니다.public class CodeitConstants {    public static final double PI = 3.141592653589793;    public static final double EULERS_NUMBER = 2.718281828459045;    public static final String THIS_IS_HOW_TO_NAME_CONSTANT_VARIABLE = &quot;Hello&quot;;    public static void main(String[] args) {        System.out.println(CodeitConstants.PI + CodeitConstants.EULERS_NUMBER);    }}2) 클래스 메소드마찬가지로, 클래스 메소드는 인스턴스가 아닌 클래스에 속한 메소드입니다. 클래스 메소드는 언제 사용할까요? 인스턴스 메소드는 인스턴스에 속한 것이기 때문에, 반드시 인스턴스를 생성해야 사용할 수 있습니다. 하지만 클래스 메소드는 클래스에 속한 것이기 때문에, 인스턴스를 생성하지 않고도 바로 실행할 수 있습니다.예를 들어,  Math.abs(), Math.max() 등을 사용하면, 자바에서 미리 만들어 둔 수학 관련 기능을 활용할 수 있습니다. 하지만 우리는 Math 클래스의 인스턴스를 생성하지는 않습니다. 필요하지 않기 때문이죠. 단지 Math 클래스의 기능(메소드)만 활용하면 됩니다.사실 우리가 가장 먼저 접한 ‘클래스 메소드’는 바로 main 메소드입니다. main은 자바 프로그램의 시작점이라고 했습니다. 첫 번째로 실행되는 코드이니, 어떤 인스턴스도 생성되어 있지 않습니다. 따라서 main 메소드 역시 인스턴스를 생성하지 않고 실행하는 ‘클래스 메소드’입니다. 클래스 메소드도 동일하게 static이라는 키워드로 정의할 수 있습니다.5. Wrapper class‘Wrapper 클래스’는 기본 자료형을 객체 형식로 감싸는 역할을 합니다. Integer 클래스는 int형을, Double 클래스는 double을, Long 클래스는 long을, Boolean 클래스는 boolean을 wrapping할 수 있습니다. 그런데 이런 Wrapper 클래스가 왜 필요할까요?기본형 자료형(Primitive Type)을 참조형(Reference Type)처럼 다루어야할 때 Wrapper 클래스를 사용하면 됩니다. 예를 들어서 ArrayList같은 컬렉션을 사용할 때는 꼭 참조형을 사용해야 합니다.// 생성자로 생성하는 방법Integer i = new Integer(123);// 리터럴로 생성하는 방법Integer i = 123;6. ArrayListimport java.util.ArrayList// ArrayList&amp;lt;안에 넣은 객체의 클래스&amp;gt;ArrayList&amp;lt;String&amp;gt; nameList = new ArrayList&amp;lt;&amp;gt;();nameList.add(&quot;Jay&quot;);nameList.add(&quot;Mike&quot;);nameList.remove(1);nameList.contains(&quot;Jay&quot;);for (String name : nameList) {  System.out.println(name)}7. HashMapHashMap&amp;lt;String, Integer&amp;gt; check = new HashMap&amp;lt;&amp;gt;();check.put(&quot;사과&quot;,new Integer(1))check.get(&quot;사과&quot;)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-07T21:01:35+09:00'>07 Jul 2021</time><a class='article__image' href='/java-series4'> <img src='/images/java_logo.png' alt='Java Series [Part4]: 자바의 자료형 특징'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series4'>Java Series [Part4]: 자바의 자료형 특징</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part3]: 자바와 객체지향 프로그래밍",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series3",
      "date"     : "Jul 5, 2021",
      "content"  : "Table of Contents  1. 객체 만들기  2. 객체 설계하기          1) 접근 제어자(Access Modifier)                  a) private          b) protected          c) public final                    2) 메소드 오버로딩(Method Overloading)      3) 생성자      1. 객체 만들기자바는 객체 단위로 동작하는 객체 지향 프로그래밍이기 때문에, Hello world 한 줄을 출력하더라도 클래스로 작성해야 합니다.클래스는 변수와 메소드를 갖는 객체(인스턴스)를 만드는 설계도라고 생각하시면 됩니다.public class BankAccount {    // 변수    private int balance;    private Person owner;        // 메소드    public void setBalance(int newBalance){        this.balance = newBalance;    }    public int getBalance(){        return balance;    }    public void setOwner(Person newOwner){        this.owner = newOwner;    }    public Person getOwner(){        return this.owner;    }        boolean deposit(int amount){        if (amount &amp;lt; 0 || owner.getCashAmount() &amp;lt; amount){            System.out.println(&quot;입금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance += amount;            owner.setCashAmount(owner.getCashAmount()-amount);            System.out.println(amount + &quot;원 입금하였습니다. 잔고: &quot; + balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;                    }    }    boolean withdraw(int amount){        if ( amount &amp;lt; 0 || getBalance() &amp;lt; amount) {            System.out.println(&quot;출금 실패입니다. 잔고: &quot;+balance+&quot;원, 현금: &quot;+owner.getCashAmount()+&quot;원&quot;);            return false;        }        else{            balance -= amount;            owner.setCashAmount(owner.getCashAmount()+amount);            System.out.println(amount + &quot;원 출금하였습니다. 잔고: &quot;+ balance + &quot;원, 현금: &quot; + owner.getCashAmount() + &quot;원&quot;);            return true;        }    }    }2. 객체 설계하기1) 접근 제어자(Access Modifier)a) privatepublic class Person {    private int age;    public void setAge(int newAge) {        if (newAge &amp;gt; 0) {            this.age = newAge;        }    }    public int getAge() {        return this.age;    }}age 변수를 public으로 하면 외부에서 Person 객체를 생성했을 때 age 변수에 직접 접근이 가능하다 (p1.age = -10 이런 식으로) 따라서 외부에서 무분별하게 접근하는 것을 막고자 접근 제어자를 public이 아닌 private으로 작성합니다.b) protected접근 제어자에는 public, private 그리고 protected가 있습니다. protected를 사용하면 자식 클래스에 한해서 변수에 직접적으로 접근이 가능합니다. 그렇게 함으로써 private을 사용했을 때 setter, getter 메소드를 작성해야하는 불편함을 해소해줍니다.public class Person {    protected int age;}public class Student extends Person {  public void olderAge() {    age = age + 1  }}c) public final자식 클래스 뿐 아니라 다른 곳에서도 사용되지만 수정은 안되도록 하는 ( 좀 더 일반적인) 방법은 public final입니다.public final double pi = 3.142) 메소드 오버로딩(Method Overloading)‘메소드 오버로딩(Method Overloading)’은 클래스 내에 같은 이름의 메소드를 2개 이상 정의할 수 있게 해주는 기능입니다.public class Calculator {    int add(int a, int b) {        return a + b;    }    int add(int a, int b, int c) {        return a + b + c;    }    double add(double a, double b) {        return a + b;    }}지금까지 써왔던 System.out.println()도 메소드오버로딩되어 있는 메소드입니다.3) 생성자‘생성자(Constructor)’는 크게 두 가지 역할이 있습니다  인스턴스를 만들고,  인스턴스의 속성(인스턴스 변수)들을 초기화시켜줍니다.생성자를 한 개도 정의 안 했을 경우에는 자바에서 자동으로 기본 생성자를 제공해줍니다.Person p1 = new Person();생성자를 하나라도 정의하면 위의 기본 생성자는 사용할 수 없습니다.public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public static void main(String[] args) {    Person p1 = new Person(&quot;Jay&quot;, 27);    }}  생성자 오버로딩도 가능합니다.// 생성자 오버로딩public class Person {    String name;    int age;    public Person(String pName, int pAge) {        this.name = pName;        this.age = pAge;    }    public Person(String pName) {    this.name = pName;    this.age = 12;    // 12살을 기본 나이로 설정    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-05T21:01:35+09:00'>05 Jul 2021</time><a class='article__image' href='/java-series3'> <img src='/images/java_logo.png' alt='Java Series [Part3]: 자바와 객체지향 프로그래밍'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series3'>Java Series [Part3]: 자바와 객체지향 프로그래밍</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part2]: 자바 시작하기",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series2",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  Hello World  변수와 연산  조건문  반복문  배열Hello Worldpublic class HelloWorld {    /*main: 프로그램을 실행하면 가장 먼저 실행되는 메소드    void: 리턴값 없음    String[] args: args라는 이름의 문자열 변수가 메소드에 전달*/    public static void main(String[] args) {        // System: 클래스, out: 클래스 변수, println: 메소드        System.out.println(&quot;Hello World&quot;);    }}변수와 연산public class Variables {    public static void main(String[] args) {        // 선언 방법        // 1)        int age;        age = 27;        // 2)        double num = 12.5;        // 자료형        // primitive type        int myInt = 123;        long myLong = 12345678910L;        double myDouble = 3.14; // double이 더 높은 정밀도, 소수형의 기본 타입        float f = 3.14f;        char a = &#39;a&#39;; // 쌍따옴표로 감싸면 String으로 인식함        char aPrime = 97; // 아스키 값 97 == &#39;a&#39;        char b = &#39;가&#39;;        boolean myBoolean = true;        // 객체형 type        String myString = &quot;jay kim&quot;;    }}조건문public class IfElse {    public static void main(String[] args) {        int temp = 15;        if (temp &amp;lt; 0) {            System.out.println(&quot;오늘의 날씨는 영하입니다: &quot; + temp +&quot;도&quot;);        } else if (temp &amp;lt; 5){            System.out.println(&quot;오늘의 날씨는 0도 이상 5도 미만입니다: &quot; + temp +&quot;도&quot;);        } else {            System.out.println(&quot;오늘의 날씨는 5도 이상입니다: &quot; + temp +&quot;도&quot;);        }    }}public class Switch {    public static void main(String[] args) {        int score = 80;        String grade;        switch (score / 10) {            case 10:                grade = &quot;A+&quot;;                break;            case 9:                grade = &quot;A&quot;;                break;            default:                grade = &quot;F&quot;;                break;        }        System.out.println(&quot;학점은 &quot; + grade + &quot;입니다.&quot;);    }}반복문public class For {    public static void main(String[] args) {        int sum = 0;        // i++는 실행 부분이 실행되고 나서 실행된다        for (int i = 1; i &amp;lt;= 5; i++) {            sum += i;            System.out.println(i);        }    }}public class While {    public static void main(String[] args ) {        int i = 1;        int sum = 0;        while (i &amp;lt;= 3) {            sum = sum + i;            i = i + 1;        }        System.out.println(sum);    }}배열public class Array {    public static void main(String[] args) {        // 배열 생성하는 첫 번째 방법        int[] intArray = new int[5];        intArray[0] = 2;        intArray[1] = 3;        intArray[2] = 5;        intArray[3] = 7;        intArray[4] = 11;        // 배열 생성하는 두 번째 방법        int[] arr1 = {1, 2, 3, 4, 5};        int[] arr2 = arr1;        int[] arr3 = arr1.clone();        arr1[0] = 100;        System.out.println(arr2[0]);        System.out.println(arr3[0]);        for (double i : intArray) {            System.out.println(i);        }        int[][] multiArray = new int[3][4];        int[][] multiArray2 = { {1 ,2, 3, 4},                            {5, 6, 7, 8},                            {9, 10, 11, 12}                            };        System.out.println(multiArray2[0][1]);    }}",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series2'> <img src='/images/java_logo.png' alt='Java Series [Part2]: 자바 시작하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series2'>Java Series [Part2]: 자바 시작하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Java Series [Part1]: 자바와 가상머신",
      "category" : "",
      "tags"     : "Java",
      "url"      : "/java-series1",
      "date"     : "Jul 4, 2021",
      "content"  : "Table of Contents  자바와 가상머신자바와 가상머신  한 번만 작성하면, 어디서든 동작한다. (Write Once, Run Anywhere.)어떤 언어는 운영체제에 따라 동작이 달라집니다.분명히 윈도우즈에서는 잘 동작했는데, 맥에서 동작하지 않는 일이 발생합니다.그래서 우리가 개발할 때는 항상 운영체제를 신경써야 합니다.중간 중간 테스트도 해주어야 하고요.만약 휴대폰 애플리케이션을 개발한다면 어떨까요?최악의 경우, 모든 휴대폰 기종을 모아서 매번 테스트를 해봐야겠네요.자바는 이런 ‘호환성’문제를 해결해 줍니다.‘자바 가상머신’이라는 것만 설치되면, 어느 운영체제이든, 어느 디바이스이든, 동일하게 동작합니다.(자바 가상머신은 영어로 Java Virtual Machine, 줄여서 JVM 이라고 부릅니다.)이러한 자바의 높은 호환성은 애플리케이션의 특징과도 잘 맞아떨어지기 때문에, 애플리케이션 개발에 활발히 사용되고 있죠.JVM을 사용해서 마음껏 개발할 수 있는 환경을 JRE (Java Runtime Environment) 라고 부르며, 내 컴퓨터에 이런 환경을 만들기 위해서는 JDK (Java Development Kit) 라는 것을 설치하면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-07-04T21:01:35+09:00'>04 Jul 2021</time><a class='article__image' href='/java-series1'> <img src='/images/java_logo.png' alt='Java Series [Part1]: 자바와 가상머신'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/java-series1'>Java Series [Part1]: 자바와 가상머신</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Intermediate Series [Part1]: Iterable, Iterator, Generator",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-mid-context_manager",
      "date"     : "Jun 1, 2021",
      "content"  : "Table of Contents  Python tricks we MUST all use",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-06-01T21:01:35+09:00'>01 Jun 2021</time><a class='article__image' href='/python-mid-context_manager'> <img src='/images/python_intermediate_logo.png' alt='Python Intermediate Series [Part1]: Iterable, Iterator, Generator'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-mid-context_manager'>Python Intermediate Series [Part1]: Iterable, Iterator, Generator</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part11]: Advanced RNN",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_rnn",
      "date"     : "Apr 6, 2021",
      "content"  : "Table of Contents  Advanced RNN          바닐라 RNN의 한계점      LSTM (Long Short Term Memory)      GRU  (Gated Recurrent Unit)                  참조                    Advanced RNN바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다. 이를 장기 의존성 문제(the problem of Long-Term Dependencies)라고 합니다.LSTM (Long Short Term Memory)바닐라 RNN의 한계를 극복하기 위해 다양한 RNN의 변형이 나왔습니다. LSTM과 GRU가 대표적인 예입니다.긴 시퀀스를 다룰 때 LSTM이 바닐라 RNN과 비교해 어떤 점에서 더 좋은지 알기위해 우선 LSTM의 구조에 대해 먼저 살펴보겠습니다.꽤나 복잡하게 생겨서 머리가 아플 수 있지만 하나하나 살펴보면 충분히 가능하기 때문에, 천천히 살펴보도록 하겠습니다.우선 위로 지나가는 Cell state 부분부터 한번 보겠습니다.Cell state의 역할은 중요한 정보는 그대로 넘겨주고, 중요하지 않은 정보는 약하게 함으로써 중요한 정보만 계속 흘러갈 수 있도록 해줍니다. 이걸 가능하게 하는 것이 바로 게이트입니다. i(t) 게이트를 통해 중요한 정보는 흘러가고, f(t) 게이트를 통해 중요하지 않은 정보를 약하게 만듭니다. 그러면 어떤 정보가 중요하고 중요하지 않은지는 어떤 기준으로 정해지고 어떻게 설정해야 할까요? 그 기준은 바로 이전 셀의 h(t-1)의 값과 현재 층의 입력 x(t) 으로 정해지며, 설정하는 것은 우리의 몫이 아닌 신경망의 역할입니다. 신경망은 학습을 통해 알아서 중요한 정보와 중요하지 않은 정보를 잘 선택할 수 있도록 학습됩니다.위 과정을 통해 C(t)를 만듭니다.정보의 중요도에 따라 크기가 달라진 C(t)의 값을 tanh에 넣어서 이 값을 다시 -1과 1사이의 값으로 만들어 준 후 o(t)에 곱해 줌으로써 h(t)를 계산합니다.C(t)와 h(t)를 다음 셀에 전달해줍니다.그래서 이러한 LSTM이 어떤 점에서 바닐라 RNN이 가지는 한계를 극복하게 된걸까요? 그것은 바로 C(t)가 셀에서 tanh함수를 거치지 않기 때문에 중요한 정보가 셀을 거듭하더라도 약해지지 않고  정보를 잘 전달할 수 있다는 것입니다.GRU  (Gated Recurrent Unit)GRU(Gated Recurrent Unit)는 2014년 뉴욕대학교 조경현 교수님이 집필한 논문에서 제안되었습니다. GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트하는 계산을 줄였습니다. 다시 말해서, GRU는 성능은 LSTM과 유사하면서 복잡했던 LSTM의 구조를 간단화 시켰습니다.LSTM에서는 출력, 입력, 삭제 게이트라는 3개의 게이트가 존재했습니다. 반면, GRU에서는 업데이트 게이트와 리셋 게이트 두 가지 게이트만이 존재합니다. GRU는 LSTM보다 학습 속도가 빠르다고 알려져있지만 여러 평가에서 GRU는 LSTM과 비슷한 성능을 보인다고 알려져 있습니다.데이터 양이 적을 때는, 매개 변수의 양이 적은 GRU가 조금 더 낫고, 데이터 양이 더 많으면 LSTM이 더 낫다고 알려져 있습니다. GRU보다 LSTM에 대한 연구나 사용량이 더 많은데, 이는 LSTM이 더 먼저 나온 구조이기 때문입니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-06T21:01:35+09:00'>06 Apr 2021</time><a class='article__image' href='/advanced_rnn'> <img src='/images/LSTM_2.png' alt='Deep Learning Series [Part11]: Advanced RNN'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_rnn'>Deep Learning Series [Part11]: Advanced RNN</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part10]: RNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_rnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  RNN          시퀀스 데이터 vs 시계열 데이터      시퀀스 데이터      바닐라 RNN      평가      깊은 RNN 모델      양방향 RNN 모델      바닐라 RNN의 한계점                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.          참조                    RNN시퀀스 데이터 vs 시계열 데이터  시퀀스 데이터는 순서만 중요한 데이터 (문장, 음성)  시계열 데이터는 순서뿐 아니라 데이터가 발생한 시간도 중요한 데이터 (주식, 센서 데이터)시퀀스 데이터  시퀀스 데이터는 IID가정을 대체로 위배하기 때문에, 순서를 바꾸면 데이터의 확률 분포도 바뀌게 됩니다.  이전 시퀀스의 정보를 가지고 앞으로 발생할 데이터의 확률 분포를 다루기 위해 조건부 확률을 이용할 수 있습니다.  다음과 같은 문제를 해결하기 위한 모델을 만든다고 생각해 봅시다.이미지에 대한 설명문 달기주가 예측하기한국어 영어로 번역하기다음과 같은 문제는 입력과 출력이 시퀀스 형태를 가지고 있습니다. 이러한 시퀀스 데이터를 처리하기 위해 고안된 모델을 시퀀스 모델이라고 합니다. 그 중에서도 RNN은 딥러닝에서 가장 기본적인 시퀀스 모델입니다.one to one: 비 시퀀스 데이터를 다루는 경우one to many: 이미지 캡셔닝many to one: 주가 예측, 텍스트 분류 many to many: 번역바닐라 RNN그동안 신경망들은 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향했습니다. 이와 같은 신경망들을 피드 포워드 신경망(Feed Forward Neural Network)이라고 합니다. 그런데 그렇지 않은 신경망들도 있습니다. RNN(Recurrent Neural Network)이 그 중 하나입니다.RNN은 해당 층의 입력 데이터와 이전 층에서의 출력을 함께 입력으로 사용합니다.그리고 이전 층의 출력과 해당 층의 입력은 다음과 같이 결합되게 됩니다.(참고로 W_d와 W_h를 concatenation해서 쓸 수도 있습니다.)이를 모델에 적용해 다시 한 번 살펴보면 다음과 같습니다.이를 식으로 표현하면 다음과 같습니다.평가Loss함수 식을 보면 변수 t에 대해 theta값은 변하지 않는다 -&amp;gt; 펼쳐져 있지만 W_h와 W_d는 같은 레이어입니다.깊은 RNN 모델양방향 RNN 모델양방향 순환 신경망은 시점 t에서의 출력값을 예측할 때 이전 시점의 데이터뿐만 아니라, 이후 데이터로도 예측할 수 있다는 아이디어에 기반합니다.즉, RNN이 과거 시점(time step)의 데이터들을 참고해서, 찾고자하는 정답을 예측하지만 실제 문제에서는 과거 시점의 데이터만 고려하는 것이 아니라 향후 시점의 데이터에 힌트가 있는 경우도 많습니다. 그래서 이전 시점의 데이터뿐만 아니라, 이후 시점의 데이터도 힌트로 활용하기 위해서 고안된 것이 양방향 RNN입니다.바닐라 RNN의 한계점바닐라 RNN은 출력 결과가 이전의 계산 결과에 의존한다는 것을 언급한 바 있습니다. 하지만 바닐라 RNN은 비교적 짧은 시퀀스(sequence)에 대해서만 효과를 보이는 단점이 있습니다. 바닐라 RNN의 시점(time step)이 길어질 수록 앞의 정보가 뒤로 충분히 전달되지 못하는 현상이 발생합니다. 그 이유는 다음과 같이 셀을 거듭할수록 tanh함수의 출력값이 가지는 제한(절댓값의 크기가 1보다 같거나 작습니다) 때문입니다.만약 tanh가 아니라 relu를 쓴다면, 반대로 맨 앞에서 받았던 정보가 층을 거듭할수록 값이 너무 커져서 시퀀스 뒤에 위치한 데이터의 학습을 방해할 수 있습니다.이를 해결하기 위해 RNN의 advanced 버전인 LSTM과 GRU에 대해서는 다음 포스트에서 살펴보도록 하겠습니다.실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.참조  유튜브 카이스트 딥러닝 홀로서기  딥러닝을 이용한 자연어 처리",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/basic_of_rnn'> <img src='/images/basic_of_rnn_5.png' alt='Deep Learning Series [Part10]: RNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_rnn'>Deep Learning Series [Part10]: RNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part9]: Advanced CNN  ",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/advanced_cnn",
      "date"     : "Apr 5, 2021",
      "content"  : "Table of Contents  Advanced CNN          1. GoogLeNet                  1) GoogLeNet의 특징          2) Inception Module          3) GoogLeNet Network Structure                    2. ResNet                  1) Residual Learning          2) Residual Block          3) ResNet Architecture                    Advanced CNN1. GoogLeNet1) GoogLeNet의 특징  커널의 적절한 사이즈를 찾기 위해 고민하기 보다, 여러 가지 사이즈의 커널을 병렬로 이용함으로써 보다 풍부한 Feature Extraction 수행한다  1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시킨다.  1 X 1 이라서 Feature map dimension이 줄어든 것이 아니라, 1 X 1 필터의 채널 수를 작은 사이즈를 썼기 때문이다.  참고로 컨볼루션 커널의 사이즈, padding, striding이 Feature map의 사이즈를 결정한다.  컨볼루션 커널의 채널의 개수가 Feature map의 개수를 결정한다.2) Inception Module  위 그림과 같이 1 X 1 컨볼루션 필터를 이용해 Feature map의 dimension을 줄이고, 결과적으로 연산해야 할 파라미터 수를 감소시켰다.  Inception모듈을 보면 여러 필터가 병렬적으로 연산되고 모듈 끝에서 결과들이 Concatenation됩니다.  따라서 Feature map의 개수는 달라도 괜찮지만, Feature map의 사이즈는 같아야 합니다.3) GoogLeNet Network Structure  다음과 같이 Inception Module은 총 9개로 구성되어 있다  Concatenation까지가 Inception Module에 포함되고 그 후에 보통 Pooling layer를 거친 뒤 다시 Inception Module로 들어가는 것으로 반복된다.  Inception Module을 2개 거친 후 Pooling하기도 하고, 5개 거치고 Pooling 하기도 한다.2. ResNet  VGG 모델이 나온 이후 깊은 Network가 좋은 성능을 낸다는 인식이 생겼다.  하지만 비슷한 방식으로 Network를 더 깊게 만들었을 때 오히려 성능이 저하되었다.  그 원인으로는 Gradient Vanishing과 파라미터 수 증가에 따른 학습 속도 저하가 있다.  파라미터 수 증가는 앞에서와 같이 1 X 1 컨볼루션으로 해결하였다.  Gradient Vanishing 문제는 Residual Learning을 통해 해결하였다.1) Residual Learning  처음 Residual Learning이 나오기 전에 시도되던 방법은 Identity mapping이다.  Identity mapping은 층은 더 깊게 만들되, Gradient vanishing은 생기지 않도록 하기 위해 이전 값을 그대로 다시 통과시키는 방법이다.  비선형성은 있어야 층을 깊게 쌓는 의미가 있으므로 Relu()정도가 있어야 하는데, 이렇게 되면 identity한 mapping이 되기 어려워진다.  그래서 좀 더 쉬운 방법으로 제안된 것이 Residual Learning이다.  H(x)가 x가 되도록 하는 것이 아니라, F(x)가 0이 되도록 학습하는 것이 쉽다.2) Residual Block  ResNet 50을 포함한 이보다 깊은 네트워크(50/101/152)에서는 1 X 1 Conv를 이용해 파라미터 갯수를 줄였다.  Residual Block 내에서는 Feature map 사이즈는 동일하고 Filter수만 변함3) ResNet Architecture",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-05T21:01:35+09:00'>05 Apr 2021</time><a class='article__image' href='/advanced_cnn'> <img src='/images/googlenet_1.png' alt='Deep Learning Series [Part9]: Advanced CNN  '> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/advanced_cnn'>Deep Learning Series [Part9]: Advanced CNN  </a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part8]: CNN의 기초",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/basic_of_cnn",
      "date"     : "Apr 4, 2021",
      "content"  : "Table of Contents  CNN의 기초          CNN의 발단      CNN 구조                  1. 컨볼루션 연산          2. 커널(채널), 필터          3. 스트라이드, 패딩                          스트라이드(stride)              패딩(padding)                                4. 풀링 연산                    특성맵의 크기 구하기      CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기                  실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.                    CNN의 기초CNN의 발단그동안 앞에서 설명한 신경망은 모두 기본 전제가 노드가 서로 완전 연결되어 있는 경우였습니다. 이를 완전 연결층(Fully connected layer)라고 합니다. 이는 목적한 바를 잘 이룰 수 있도록 특징 공간 갖는 모델을 만들 수 있지만, 가중치가 너무 많아 복잡도가 너무 높습니다. 이는 모델의 학습 속도를 더디게 하며 또한 과잉적합에 빠질 가능성을 높이게 됩니다. 컨볼루션 신경망(CNN)은 부분 연결 구조로 이러한 문제를 잘 해결하며 또한 특징을 잘 추출하도록 해줍니다. 이러한 이유로 CNN은 이미지, 비전 분야에서 매우 뛰어난 성능을 발휘했으며, 음성인식이나 자연어 처리같은 다른 작업에도 사용됩니다.실제로 1958년에 데이비드 허블의 연구에서는 인간의 시각 뉴런은 부분연결 구조를 가진다는 사실을 밝혀냈습니다. 이를 조금 더 자세히 설명하면, 눈의 시각 피질 안의 많은 뉴런은 작은 국부 수용장(Local receptive field)를 가지며 이는 시야의 영역에서 작은 특정 패턴에 뉴런이 반응한다는 것입니다. 예를 들어,눈이 처음 무언가를 봤을 때는 국부 수용장에 해당하는 작은 영역이 가지는 패턴에 먼저 반응을 하고, 시각 신호가 연속적으로 뇌의 뉴런들을 통과하며 다음 뉴런들은 점점 더 큰 수용장에 있는 복잡한 패턴에 반응을 합니다. 이러한 점에서 CNN은 사람의 눈과 비슷한 원리를 가지고 있다고 할 수 있습니다.CNN은 눈의 원리와 비슷하다.부분 연결 구조로 되어 있어 비교적 구해야 하는 가중치의 갯수가 적다.학습 속도가 훨씬 빠르며, 과잉적합에 빠질 가능성 또한 낮아진다.CNN 구조보통 이미지 분류를 위한 CNN의 구조는 다음과 같습니다.1. 컨볼루션 연산사실 CNN에서의 합성곱은 실제의 합성곱과는 다릅니다. 실제의 합성곱은 필터를 뒤집어야 하지만 CNN에서는 필터를 뒤집지 않습니다. 그 이유는 어차피 필터의 가중치 값은 처음에 보통 랜덤으로 초기화하게 됩니다. 따라서 가중치를 굳이 뒤집지 않아도 상관이 없습니다. 어쨋든 CNN에서의 컨볼루션 연산은 필터가 옆으로 움직이면서 데이터와 각각 원소별 곱셈을 진행하고 그 곱셈의 결과들을 하나의 값으로 합하면 됩니다.밑의 예시를 살펴보면 3×1 + 1×(-1) + 1×1 + 7×(-1) + 2×1 + 5×(-1) = -7 이 됩니다.다음의 컨볼루션 연산은 필터의 사이즈, 스트라이드의 크기, 패딩 여부 등에 따라 결과(특성 맵)이 달라집니다.2. 커널(채널), 필터여기서 CNN에서 정말 헷갈리지만 또 중요한 개념이 등장합니다. 바로 커널(채널)과 필터입니다. 느낌이 비슷해서 헷갈릴 수 있지만 엄연히 구분되어 사용되어야 하기 때문에 여기서 한 번 짚고 넘어가도록 하겠습니다.커널은 채널과 비슷한 의미로 데이터의 커널의 수(RGB채널의 경우 3)와 필터의 커널 수는 항상 같아야 합니다.필터는 카메라 필터와 비슷하게 shape을 위한 필터, curve를 위한 필터와 같은 필터를 의미합니다.예를 들어 채널이 1(Gray scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.만약 채널이 3(RGB scale)이고 필터가 두 개인 경우 그림은 다음과 같습니다.따라서 필터의 커널의 수는 항상 데이터의 커널의 수를 따르게 되고,결과(특성 맵)의 커널 수는 항상 필터의 수를 따르게 됩니다.CNN의 목표는 특징을 잘 추출해주는 필터의 가중치를 찾는 것입니다.3. 스트라이드, 패딩스트라이드(stride)  스트라이드는 필터의 미끄러지는 간격을 조절하는 것을 말합니다.  기본은 1이지만, 2를(2pixel 단위로 Sliding window 이동) 적용하면 입력 특성 맵 대비 출력 특성 맵의 크기를 대략 절반으로 줄여줍니다.  stride 를 키우면 공간적인 feature 특성을 손실할 가능성이 높아지지만, 이것이 중요 feature 들의 손실을 반드시 의미하지는 않습니다.  오히려 불필요한 특성을 제거하는 효과를 가져 올 수 있습니다. 또한 Convolution 연산 속도를 향상 시킵니다.패딩(padding)  패딩은 데이터 양 끝에 빈 원소를 추가하는 것을 말합니다.      패딩에는 밸리드(valid) 패딩, 풀(full) 패딩, 세임(same) 패딩이 있습니다. 패딩 각각의 역할은 다음과 같습니다.                            패딩          역할                                      밸리드          평범한 패딩으로 원소별 연산 참여도가 다르다                          풀          데이터 원소의 연산 참여도를 갖게 만든다                          세임          특성 맵의 사이즈가 기존 데이터의 사이즈와 같도록 만든다                      세임패딩을 적용하면 Conv 연산 수행 시 출력 특성 맵 이 입력 특성 맵 대비 계속적으로 작아지는 것을 막아줍니다.4. 풀링 연산  풀링층은 특성 맵의 사이즈를 줄여주는 역할을 합니다.  보통 최대 풀링 또는 평균 풀링을 많이 사용합니다.    보통은 Conv연산, ReLU activation함수를 적용한 후에 풀링을 적용합니다.    풀링은 비슷한 feature 들이 서로 다른 이미지에서 위치가 달라지면서 다르게 해석되는 현상을 중화 시켜 줍니다.  일반적으로는 Sharp 한 feature 가 보다 Classification 에 유리하여 최대 풀링이 더 많이 사용됩니다.  풀링의 경우 특정 위치의 feature 값이 손실 되는 이슈로 인해 최근 Advanced CNN에서는 Stride만 이용하여 모델을 구성하는 경향입니다.특성맵의 크기 구하기  5×5 입력, 3×3 필터가 스트라이드가 1이고 패딩이 없는 경우, 특성 맵의 크기는 3×3이 됩니다.CNN 모델의 구조에 따른 파라미터 수와 특성 맵 차원 생각해보기실제 코드를 활용하는 방법은 여기를 참고하시면 됩니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-04T21:01:35+09:00'>04 Apr 2021</time><a class='article__image' href='/basic_of_cnn'> <img src='/images/cnn_3.png' alt='Deep Learning Series [Part8]: CNN의 기초'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/basic_of_cnn'>Deep Learning Series [Part8]: CNN의 기초</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-heapq",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/python-component-heapq'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-heapq'>Python Basic Series [Part8]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 heapq</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/regulation",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝의 규제를 통한 일반화          1. 모델의 일반화      2. 규제의 필요성과 원리                  규제의 정의                    3. 규제 기법                  1) 데이터 증가(Data Augmentation)                          a) Salt &amp;amp; Pepper Noise              b) Rotation, Flipping Shifting              c) Dropping, Exchanging for Text augmentation              d) 새로운 데이터 생성                                2) 가중치 벌칙                          L2놈                                3) 드롭아웃(Dropout)          4) 조기 멈춤(Early stopping)                    딥러닝의 규제를 통한 일반화1. 모델의 일반화모델의 일반화란 무엇일까요? 모델이 특정 데이터에만 잘 맞는 것이 아니라 전례가 없는 데이터에 대해서도 높은 정확도를 유지하도록 하는 것을 모델의 일반화라고 합니다. 그러기 위해서는 우선  우리가 해결하고자 하는 문제가 가지고 있는 데이터의 특징 공간을 모델이 충분히 잘 수용할 수 있어야 합니다.모델의 용량은 데이터를 나타낼 수 있는 모델의 차원이라고 생각합니다. 따라서 모델이 데이터의 특징을 잘 찾아내기 위해서는, 모델의 차원이 데이터의 특징 공간의 차원 보다는 높아야 합니다. 그럼 무조건 모델의 용량이 크면 다 해결되는 걸까요? 그렇지는 않습니다. 왜냐하면 데이터에는 우리가 정말로 원하는 성분 말고도 우리가 원치 않는 노이즈가 함께 존재하는 경우가 대부분이기 때문입니다. 용량이 너무 크면 모델은 노이즈에 대한 잘못된 특징도 수용하기 때문에 성능이 떨어지게 됩니다. 이러한 현상을 모델이 데이터에 과잉적합되었다고 합니다. 따라서 현대 기계 학습은 용량이 충분히 큰 모델을 선택한 후, 선택한 모델이 노이즈에 민감하지 않도록 하기 위해 여러 가지 규제 기법을 적용하는 접근방법을 채택합니다.용량이 충분히 큰 모델에 여러 가지 규제 기법을 적용해 일반화 능력을 높일 수 있다.2. 규제의 필요성과 원리따라서 우리는 모델이 주어진 문제에 대해 최소한의 역할을 하기 위해 용량을 크게 하는 것이 좋으며(과소 적합 방지), 전례가 없는 데이터에 대해서도 높은 정확도를 가지는 성능이 좋은 모델을 만들기 위해 모델을 잘 일반화 시켜야 합니다(과잉 적합 방지).그리고 모델을 일반화 시키기 위한 방법을 우리는 규제라고 합니다.규제의 정의  일반화 오류를 줄이기 위해 학습 알고리즘을 수정하는 모든 방법3. 규제 기법현대 기계 학습은 아주 다양한 규제 기법을 사용합니다.  데이터를 통해          Data Augmentation -&amp;gt; Noise injection        Loss 함수를 통해          Weight Decay -&amp;gt; L1, L2 규제        Neural network layer를 통해          Dropout        학습 방식, 추론 방식을 통해          Early Stopping      Bagging &amp;amp; Ensemble      1) 데이터 증가(Data Augmentation)핵심 특징을 간직한 채, noise를 더하여 데이터를 확장하는 방법으로 보통 핵심 특징을 보존하기 위해 휴리스틱한 방법을 사용합니다.이를 통해 더욱 noise robust한 모델을 얻을 수 있습니다. 규칙을 통해 데이터를 증가시키려고 하면 모델이 그 규칙을 배우게 되기 때문에규칙이 아닌 Randomness를 통해 데이터를 증가시켜야 합니다.a) Salt &amp;amp; Pepper Noise  Adding RGB(255, 255, 255) noise  Adding RGB(0, 0, 0) noiseb) Rotation, Flipping Shiftingc) Dropping, Exchanging for Text augmentation  임의의 단어를 생략한다  임의로 특정 단어를 주변 단어와 위치를 바꾼다d) 새로운 데이터 생성  Autoencoder, GAN을 통해 데이터를 학습 후 새로운 데이터 생성2) 가중치 벌칙가중치 𝛉가 커지게 되면 R항이 커지게 되고 그러면 손실 함수 J가 증가하게 됩니다. 우리의 학습 알고리즘은 손실 함수가 작아지도록 하므로 R항은 가중치의 크기에 제약을 가하는 역할을 한다고 볼 수 있습니다. 규제 항 R은 가중치를 작은 값으로 유지하므로 모델의 용량을 제한하는 역할을 한다고 볼 수 있습니다. 𝜆는 층마다 다르게 할 수도 있고 같게 할 수도 있습니다.하지만 실제로 사용하게 되면 성능이 오히려 떨어져 잘 사용하진 않습니다.L2놈규제 항 R로 가장 널리 쓰이는 것은 L2놈이며 이를 가중치 감쇠 기법이라고 합니다.목적 함수가 달라졌으므로, 그래디언트와 가중치 또한 바뀌게 됩니다.3) 드롭아웃(Dropout)드롭아웃이란, 입력층과 은닉층의 모든 노드에 대해 일정 확률로 노드를 임의로 제거하는 것입니다. 해당되는 노드의 들어오고 나가는 엣지들을 모두 제거합니다.(0을 출력합니다) 보통 드롭아웃될 확률의 0.1~0.5로 합니다.여기서 이렇게 하는 것이 과연 어떤 의미가 있는지 궁금해 하시는 분들이 있을 것 같아 예를 한 가지 들어보도록 하겠습니다. 어떤 회사에서 직원들이 아침마다 일정 확률로 회사를 쉰다면 어떻게 될까요? 회사가 이런 식으로 운영된다면 어떠한 업무도 한 사람에게 전적으로 의지할 수 없게 되고, 전문성이 여러 사람에게 나뉘어져 있어야 합니다. 그렇기에 이 회사는 유연성이 훨씬 더 높아질 것입니다. 한 직원이 직장을 떠나도 크게 달라지는 것이 없을 것입니다.신경망 또한 마찬가지입니다. 노드들은 몇 개의 노드에만 지나치게 의존할 수 없습니다. 모든 노드에 주의를 기울여야 합니다. 그러므로 입력값의 작은 변화에 덜 민감해집니다. 결국 더 안정적인 네트워크가 되어 일반화 성능이 좋아집니다.테스트시에는 드롭아웃을 사용하지 않는 보통 신경망처럼 전방 계산을 수행하기 때문에 출력이 학습할 떄에 비해 1/p배 더 큽니다. 따라서 W에 p를 곱하여 이를 상쇄시켜 줘야합니다.일반적으로 (출력층을 제외한) 맨 위의 층부터 세 번째 층까지 있는 노드에만 드롭아웃을 적용합니다. 또한 많은 최신 신경망 구조는 마지막 은닉층 뒤에만 드롭아웃을 사용합니다.4) 조기 멈춤(Early stopping)보통 학습을 오래 시킬수록 더 최적점에 접근합니다. 하지만 어떤 시점을 넘어서면 모델이 훈련 데이터에만 너무 최적화가 되어 검증집합에 대해서는 오히려 성능이 떨어지기 시작합니다. 다시 말해, 일반화 능력이 하락하기 시작하는 것입니다. 따라서 일반화 능력이 최고인 지점, 즉 검증집합의 오류가 최저인 지점에서 학습을 멈추는 전략을 조기 멈춤이라고 합니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/regulation'> <img src='/images/regulation_1.png' alt='Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/regulation'>Deep Learning Series [Part7]: 딥러닝의 규제를 통한 일반화</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/performance",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  딥러닝 성능 향상을 위한 요령          1. 딥러닝 성능 향상의 방향성      2. 데이터 전처리      3. 가중치 초기화(Initialization)      4. 배치 정규화(Batch Normalization)      5. 그래디언트 모멘텀(Gradient Momentum)      6. 적응적 학습률(Adaptive Learning-rate)      딥러닝 성능 향상을 위한 요령1. 딥러닝 성능 향상의 방향성딥러닝의 성능을 향상시킨다고 할 때는 보통 다음과 같은 두 가지를 말합니다.과잉 적합 방지를 통한 일반화 능력 극대화 (실전에 배치되었을 때의 성능을 극대화)학습 알고리즘의 수렴 속도 향상 (더 빠른 학습은 결국 더 좋은 성능을 가져다 준다)2. 데이터 전처리데이터 전처리는 데이터 정규화를 의미합니다.데이터가 양수, 음수 값을 골고루 갖도록 한다 =&amp;gt; 평균: 0특성 scale이 같도록 한다 =&amp;gt; 표준편차: 13. 가중치 초기화(Initialization)역전파 알고리즘은 출력층에서 입력층으로 오차 그래디언트를 전파하면서 진행됩니다. 그런데 알고리즘이 하위층으로 진행될수록 그래디언트가 작아지는 경우가 많습니다. 이 문제를 그래디언트 소실이라고 합니다. 어떤 경우엔 반대로 그래디언트가 점점 커져 여러 층이 비정상적으로 큰 가중치로 갱신되면 알고리즘은 발산합니다. 이 문제를 그래디언트 폭주라고 하며 순환 신경망에서 주로 나타납니다. 일반적으로 불안정한 그래디언트는 심층 신경망 훈련을 어렵게 만듭니다. 층마다 학습 속도가 달라질 수 있기 때문입니다.기계 학습 초기에는 가중치 초기화를 정규분포 형태(평균:0, 분산:1)를 갖도록 초기화하였습니다. 하지만 입력층의 노드 수가 많다면 출력층의 분포는 밑에 그림과 같이 값들이 대부분 0이나 1로 수렴하게 됩니다. 문제는 0과 1근처에서 활성화 함수의 그래디언트가 거의 0에 가깝다는 것 입니다. 그렇기 때문에 아래층까지 역전파가 진행되기도 전에 이미 그래디언트가 거의 소실됩니다.그렇다면 출력층의 분포가 어떤게 좋을까요? 출력층의 값이 고르게 분포해 시그모이드 함수의 비선형성과 적당한 그래디언트를 갖도록 하는 것이 좋을 것입니다.그렇다면 왜 이런 분포가 안되는 걸까요? 아마 그 이유는 입력층(각 층은 다음 층의 입력층이므로 결국 모든 층)의 노드 수가 많으면 가중치와 데이터가 정규분포를 갖는다고 하더라도 모두 더하게 되면 출력층에 sum(wx)의 값이 치우치게 되고 그러면 sigmoid(sum(wx))는 0또는 1로 주로 분포하게 될 것 입니다. 따라서 이를 완화시켜주기 위해서는 입력층의 노드 수가 많다면 그만큼 가중치의 분산을 작게 하여 최대한 작은 값을 갖도록 하면 sum(wx)의 값이 치우치게 되지 않도록 해줄 것입니다. 이와 관련한 몇 가지 초기화 방법을 살펴보겠습니다.🔔 가중치 초기화는 Gradient vanishing문제를 완화시켜줍니다.평균이 0인 정규 분포를 갖도록 한다표준편차 크면 그리고 노드의 갯수도 많으면 값이 특정 부분에 몰리게 된다.4. 배치 정규화(Batch Normalization)배치 정규화는 각 층에서 활성화 함수를 통과하기 전이나 후에 입력을 정규화한 다음, 두 개의 새로운 파라미터(𝛾, 𝛽)로 결과값의 스케일을 조정하고 이동시킵니다. 정규화 하기 위해서는 평균과 표준편차를 구해야 합니다. 이를 위해 현재 미니배치에서 입력의 평균과 표준편차를 평가합니다. 테스트 시에는 어떻게 할까요? 간단한 문제는 아닙니다. 아마 샘플의 배치가 아니라 샘플 하나에 대한 예측을 만들어야 합니다. 이 경우 입력의 평균과 표준편차를 계산할 방법이 없습니다. 샘플의 배치를 사용한다 하더라도 매우 작거나 독립 동일 분포(IID)조건을 만족하지 못할 수도 있습니다.케라스에서는 이를 층의 입력 평균과 표준편차의 이동 평균(moving average)을 사용해 훈련하는 동안 최종 통계를 추정함으로써 해결합니다. 케라스의 BatchNormalization층은 이를 자동으로 수행합니다.정리하면 배치 정규화 층마다 네 개의 파라미터 벡터가 학습됩니다.  𝛾(출력 스케일 벡터)와 𝛽(출력 이동 벡터)는 일반적인 역전파를 통해 학습됩니다. 𝜇(최종 입력 평균 벡터)와 𝜎(최종 입력 표준편차 벡터)는 지수 이동 평균을 사용하여 추정됩니다. 𝜇와 𝜎는 훈련하는 동안 추정되지만 훈련이 끝난 후에 사용됩니다.(배치 입력 평균과 표준편차를 대체하기 위해)다음과 같은 배치 정규화는Gradient vanishing문제를 완화시켜준다.Learning rate 높여도 학습이 잘된다.일반화 능력이 좋아진다.5. 그래디언트 모멘텀(Gradient Momentum)모멘텀은 학습을 좀 더 안정감 있게 하도록 해줍니다. 데이터에 의해 Gradient를 계산할 때 만약 Noisy한 데이터인 경우, Gradient가 잘못된 방향으로 갈 가능성이 큽니다. 그렇기 때문에 그 동안 누적된 Gradient를 감안하여 Gradient가 Noisy한 데이터에 의한 안 좋은 영향을 줄여준다. 영어로 잘 설명된 부분이 있어 가져와 보았습니다.  Momentum method can accelelerate gradient descent by taking accounts of previous gradients in the update rule equation in every iteration6. 적응적 학습률(Adaptive Learning-rate)가중치 업데이트의 척도가 되는 학습률을 각 가중치의 학습 진행 정도에 따라 다르게 바꿔주는 것을 적응적 학습률이라고 한다.적응적 학습률은가중치의 업데이트가 많이 이루어질수록 점점 학습률을 줄여나간다.특성마다 업데이트가 많이 된 특성은 학습률을 줄이고, 적게된 특성은 학습률을 늘린다.※ 적응적 학습률과 학습률 스케줄링의 차이가 뭘까? 둘 중 하나만 사용하면 되는걸까?",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/performance'> <img src='/images/performance_2.png' alt='Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/performance'>Deep Learning Series [Part6]: 딥러닝 성능 향상을 위한 요령</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part5]: 다층 신경망 이론",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/multi_layer_perceptron",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  다층 신경망 이론          1. 인공신경망                  퍼셉트론                    2. 비선형 분류 문제                  다층 퍼셉트론(MLP)                    3. 딥러닝의 등장      4. 다층 신경망에서의 경사 하강법      다층 신경망 이론1. 인공신경망새가 비행기의 발명에 영감이 되었다면, 사람의 뇌는 인공신경망의 영감이 되었습니다. 인공 신경망은 뇌에 있는 생물학적 뉴런의 네트워크에서 영감을 받은 머신러닝 모델입니다. 그러나 비행기가 새처럼 날개를 펄럭거릴 필요가 없듯이, 인공 신경망이 사람의 뇌와 똑같이 동작해야 할 이유는 없습니다. 최근 연구자들은 인공 신경망 연구의 창의성을 위해 이러한 비교 자체를 모두 버려야 한다고 주장합니다.인공 신경망은 딥러닝의 핵심입니다. 인공 신경망은 다재다능하고 강력하며 확장성 또한 좋아서 이미지 분류, 음성 인식, 비디오 추천 등 아주 복잡한 문제를 다루는 데 적합합니다.퍼셉트론퍼셉트론은 가장 간단한 인공 신경망 구조 중 하나로 1957년 프랑크 로젠블라트가 제안했습니다. 퍼셉트론은 TLU(Threshhold Logic Unit)이라고 불리는 인공 뉴런을 기반으로 합니다.그렇다면 퍼셉트론은 어떻게 훈련 될까요? 여기서 실제 생물학적 뉴런에 약간의 영감을 받았다고 할 수 있습니다. 도널드 헤브는 1949년에 ‘서로 활성화되는 세포가 서로 연결된다.’라는 아이디어를 제안합니다. 즉 두 뉴런이 동시에 활성화될 때마다 이들 사이의 연결 가중치가 증가한다는 것입니다. 여기서 퍼셉트론은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련됩니다. 퍼셉트론 학습규칙은 오차가 감소되도록 연결을 강화시킵니다.그러나 이러한 퍼셉트론은 XOR과 같은 비선형 분류 문제를 해결하지 못하는 약점이 지적되었습니다.2. 비선형 분류 문제XOR문제는 대표적인 비선형 분류 문제입니다. 이 문제는 1970년대 민스키의 『Perceptrons』에서 지적되었고, 이 후 한동안 신경망 연구가 정체기를 겪었습니다. 이 후 이를 해결하기 위한 방법이 몇 가지 발표되며 신경망 연구가 부활하는 계기가 되었습니다. 새로 도입한 기법을 요약하면,은닉층을 둔다.시그모이드 활성함수를 도입한다.오류 역전파 알고리즘을 사용한다.시그모이드 함수와 역전파 알고리즘은 살펴봤기 때문에 여기서는 은닉층의 필요성에 대해 알아보겠습니다. 다시 XOR문제로 돌아와 보면, XOR문제는 주어진 x1, x2 공간에서는 데이터를 분류하는 모델을 만들 수 없습니다. 따라서 분류가 가능하도록 해주는 특징공간으로 옮겨야 하는데, 이를 가능하게 해주는 것이 바로 은닉층의 역할입니다. 밑에 그림과 같이 두 개의 퍼셉트론을 이용해 새로운 특징공간 z1, z2로 옮기면 우리의 데이터를 분류할 수 있게 됩니다.다층 퍼셉트론(MLP)퍼셉트론을 여러 개 쌓아올린 인공 신경망을 다층 퍼셉트론이라 합니다. 다층 퍼셉트론은 하나의 입력층, 하나 이상의 은닉층 그리고 출력층으로 구성됩니다.3. 딥러닝의 등장은닉층을 여러 개 쌓아 올린 인공 신경망을 심층 신경망이라고 합니다. 딥러닝은 이러한 심층 신경망을 연구하는 분야입니다. 깊은 층을 통해 비선형 분류 문제를 해결하게 되며 이를 계기로 다양한 문제에 층을 깊이 쌓은 신경망 구조가 주목을 받기 시작했습니다. 우리의 생각으로는 뚜렷한 구별 방법이 떠오르지 않지만, 신경망을 깊게 쌓음으로써 기계가 여러 가지 특징 공간에서 데이터를 볼 수 있게 되었고 이 방법은 실제로 비정형 데이터(음성, 사진 등)를 다루는 데에 굉장한 성능을 보여주었습니다. 또한 깊은 층의 의미가 있기 위해 각 층마다 활성화 함수를 사용했는데, 미분 연산이 간단한 ReLU함수가 등장하게 되면서 층을 더 깊이 쌓는 것이 실제로 가능해지게 되었습니다. 이 때 부터 본격적으로 비약적인 발전을 하게 되었습니다.4. 다층 신경망에서의 경사 하강법다음과 같이 구한 W1, W2에 대한 각각의 그래디언트를 이용해 각각의 가중치를 업데이트 한다. 행렬로 표기된 이유는 배치 경사 하강법을 가정했기 때문이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/multi_layer_perceptron'> <img src='/images/multi_layer_3.png' alt='Deep Learning Series [Part5]: 다층 신경망 이론'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/multi_layer_perceptron'>Deep Learning Series [Part5]: 다층 신경망 이론</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part4]: 배치 경사 하강법",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/batch_gradient_descent",
      "date"     : "Apr 3, 2021",
      "content"  : "Table of Contents  배치 경사 하강법          1. 확률적 경사 하강법(Stochastic Gradient Descent)      2. 배치 경사 하강법(Batch Gradient Descent)                  배치 경사 하강법 수식 과정                          (1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다              (2) Error를 구한다              (3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다              (4) 가중치를 업데이트 한다                                          배치 경사 하강법1. 확률적 경사 하강법(Stochastic Gradient Descent)확률적 경사 하강법은 데이터 세트에서 무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산합니다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용합니다. 그렇기 때문에 굉장히 빠른 속도로 가중치를 업데이트 할 수 있게 됩니다. 하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌이 납니다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것입니다. 그래서 느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법입니다.2. 배치 경사 하강법(Batch Gradient Descent)배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 64, 128개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용합니다. 다시 말해 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 64개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 겁니다.또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정을 겪지 않았습니다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능합니다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어입니다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 됩니다.확률적 경사 하강법과 배치 경사 하강법  배치 경사 하강법 수식 과정(1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다Forward propagation은 앞에서 했던 데이터와 가중치를 곱하고 합하는 과정들을 일컫는 말입니다.(2) Error를 구한다(3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다(4) 가중치를 업데이트 한다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-03T21:01:35+09:00'>03 Apr 2021</time><a class='article__image' href='/batch_gradient_descent'> <img src='/images/batch_1.png' alt='Deep Learning Series [Part4]: 배치 경사 하강법'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/batch_gradient_descent'>Deep Learning Series [Part4]: 배치 경사 하강법</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-itertools",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/python-component-itertools'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-itertools'>Python Basic Series [Part7]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 itertools</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part3]: 분류를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/classification",
      "date"     : "Apr 2, 2021",
      "content"  : "Table of Contents  분류(Logistic regression)          1. 로지스틱 회귀(Logistic regression)      2. 활성화 함수: 시그모이드(Sigmoid)      3. 손실 함수: 크로스 엔트로피(Cross entropy)      4. 가중치 업데이트      분류(Logistic regression)1. 로지스틱 회귀(Logistic regression)앞에서 연속적인 값을 예측하는 모델에 대해 공부했습니다. 이번 포스트에서는 고양이, 개를 분류하는 이진 분류, 2가지 이상을 분류하는 다중 분류에 대해 공부해 보겠습니다. 그럼 회귀가 아니라 분류라고 해야 하지 않는가? 왜 로지스틱 회귀라고 할까? 여기서부터는 저의 생각이니 편하게 보고 그냥 넘기셔도 됩니다. 우선 로지스틱 회귀는 앞에서 봤던 회귀 모델과 같이 연속적인 값을 예측합니다. 다만 뒤에 시그모이드 함수와 합성함수를 취해주게 됩니다. 그럼 결과는 어떨까요? 시그모이드 함수의 결과는 0과 1사이의 실수값입니다. 그렇기 때문에 여전히 연속적인 값을 예측하는 것이라는 점에서 회귀라고 할 수 있는 것이지요. 다만 0과 1 사이의 값을 리턴하니까 예를 들면, 다음 사진이 고양이인지 아닌지에 대한 확률로 이용할 수 있을 것 같다는 생각이 듭니다. 그렇기 때문에 로지스틱 회귀라는 회귀 모델이지만 분류에 사용되는 것 같습니다.로지스틱 회귀는 0과 1사이의 실수값을 리턴해주므로 회귀다0과 1사이의 값을 분류를 위한 확률로 사용될 수 있기에 분류 모델에 사용된다.고양이, 개 사진 분류하기2. 활성화 함수: 시그모이드(Sigmoid)여기서부터 인공지능에서 중요한 개념 중 하나인 활성화 함수에 대해 알아보겠습니다. 활성화 함수의 종류는 다양합니다. 그 중 여기서는 Sigmoid 함수에 대해 알아보겠습니다. 그 밖에도 대표적으로 ReLU(Rectified Linear Unit) 함수가 있으면 ReLU는 최근 딥러닝에 많이 사용되는 대표적인 활성화 함수입니다.Sigmoid 함수는 0과 1사이의 연속적인 값을 리턴하기 때문에 확률로 사용하기 적합합니다. 그래서 이진 분류의 출력층에 활성화 함수로 많이 사용됩니다. ReLU 함수는 값이 0또는 입력값(x)이기 때문에 미분 계산 다른 활성화 함수보다 훨씬 간단합니다. 그러한 이유로 은닉층에 많이 이용되고 있습니다.보통 각 층에 있는 노드는 서로 같은 활성화 함수를 사용합니다. 밑에 그림은 입력층과 은닉층에는 ReLU함수를 사용했고, 출력층에는 이진 분류를 위해 Sigmoid함수를 사용하였습니다.은닉층에 활성화 함수가 없다면 층을 깊게 쌓아도 결국 가중치와 데이터의 곱과 합의 형태를 갖는 하나의 층에 불과하기 때문에 각 은닉층에는 ReLU와 같은 활성화 함수를 사용해야 층을 깊게 쌓는 의미가 있게 됩니다.활성화 함수는 각 층마다 활성화 함수를 가지고 있다.만약 각 층 간에 활성화 함수가 없다면 층이 깊어져도 선형 함수이기 때문에 층이 깊어져도 의미가 없다.보통 각 은닉층에는 ReLU함수가 사용되고, 이진 분류를 위해 출력층에는 Sigmoid 함수가 사용된다.다중 분류에는 출력층으로 Softmax 함수가 사용된다.3. 손실 함수: 크로스 엔트로피(Cross entropy)앞에서 최종적으로 모델의 예측 값을 얻었습니다. 이제 우리가 얻은 값과 실제 값 사이를 비교해 최적화를 하기 위해 손실 함수를 정의 해야 합니다.이 전 회귀 모델에서는 MSE를 사용했습니다. 분류를 위한 손실 함수로는 어떤 것을 선택하는 것이 좋을까요? 그대로 MSE를 쓴다면 어떨지 먼저 생각해봅시다.선형 회귀는 정답과 예상값의 오차 제곱이 최소가 되는 가중치를 찾는 것이 목표였습니다. 그렇다면 분류의 목표는 무엇일까요? 올바르게 분류된 데이터의 비율을 높이는 것이 분류의 목표입니다. 하지만 올바르게 분류된 샘플의 비율은 미분 가능한 함수가 아니기 때문에 경사 하강법의 손실 함수로 사용할 수가 없습니다. 대신 비슷한 역할을 하는 함수가 있습니다. 바로 그 함수가 로지스틱 손실 함수입니다.이진 분류를 위한 크로스 엔트로피 함수다중 분류를 위한 크로스 엔트로피 함수정답이 0인 경우 우리의 출력값이 1에 가까워지면 손실함수가 증가하게 됩니다. 반면에 정답이 1인 경우에는 출력값이 0에 가까워 질수록 손실함수가 증가합니다.4. 가중치 업데이트선형 회귀 모델에서는 가중치를 업데이트 할 때 손실함수를 바로 가중치에 대해 미분할 수 있었다. 하지만 실제로 은닉층이 생기고 활성화 함수가 추가되면 더 이상 바로 미분할 수가 없다. 그래디언트를 구하기 위해서는 우선 활성화 함수에 대해 편미분을 해야한다. 이렇게 합성함수를 순서대로 편미분해 곱한 것을 Chain rule이라고 한다.과정은 선형회귀보다 조금 복잡하지만 결과는 같게 나왔다.크로스 엔트로피 손실함수를 활성화 함수를 고려해 그래디언트를 구한 결과 여전히 err*x 이다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-02T21:01:35+09:00'>02 Apr 2021</time><a class='article__image' href='/classification'> <img src='/images/dog_cat.png' alt='Deep Learning Series [Part3]: 분류를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/classification'>Deep Learning Series [Part3]: 분류를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-collections",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/python-component-collections'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-collections'>Python Basic Series [Part6]: 코딩테스트를 위한 파이썬의 유용한 내장 모듈 collections</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/linear_regression",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  선형 회귀          1. 선형 회귀(Linear regression)      2. 손실함수: MSE(Mean Squared Error)      3. 가중치 업데이트      선형 회귀1. 선형 회귀(Linear regression)딥러닝을 배울 때 출발점으로 좋은 모델이 바로 선형 회귀입니다. 왜냐하면 뒤에서 배우게 될 모델들은 보통 이러한 선형 회귀에서 약간 변형하고, 추가하는 방법을 통해 만들어 지기 때문입니다.선형 회귀에서선형: 모델이 가중치와 데이터의 곱과 합으로 구성되어 있다. 이러한 경우를 데이터의 선형 조합(linear combination)이라고 한다.회귀: 어떤 연속적인 값을 예측하는 것을 회귀 라고 한다.선형 회귀 모델2. 손실함수: MSE(Mean Squared Error)우리의 모델의 예측 값(y’)이 정답(y)에 가깝도록 하는 것이 목표일 때 손실함수를 어떻게 정의하면 좋을까? 바로 머릿 속에 떠오르는 방법은 둘 간의 오차로 정의하는 것입니다. 이것을 우리는 Error라고 한다. 근데 Error 값은 양수일 수도 있고, 음수일 수도 있습니다. 그 상태에서 Error들을 합하면 실제 우리가 생각하는 것보다 작을 것입니다.그렇기 때문에 항상 각각의 Error가 양수가 되도록 제곱을 취하도록 하겠습니다. 이것을 우리는 MSE라고 합니다. 아마 더 깊이 공부하다 보면 이런 MSE 손실함수로는 부족할 수 도 있다고 생각이 듭니다. 하지만 MSE는 처음에 인공지능을 시작할 때 사용하기 좋은 손실함수이기 때문에 여기서는 회귀 모델에서는 MSE를 손실함수로 사용한다 라고 까지만 하고 마치도록 하겠습니다.MSE를 통해 정의한 손실함수 L3. 가중치 업데이트가중치(w)를 업데이트해 우리의 모델을 최적화시켜보도록 하겠습니다. 앞에서 우리는 가중치를 업데이트 하는 방법으로 경사하강법(Gradient descent)를 사용한다고 했습니다. 그렇기 때문에 우선 각 가중치의 Gradient를 구해야합니다. 그리고 Gradient는 함수값을 가장 가파르게 증가하는 방향이므로 (-)를 취해 손실함수 값을 가장 빠르게 감소시키는 방향으로 가중치를 업데이트 하겠습니다.손실함수를 각각의 가중치에 대해 편미분한다.각각의 편미분에 (-)를 취해 해당 가중치 지점에서 가장 빨리 손실함수를 감소시키는 방향으로 가중치를 업데이트한다.(알파는 학습률이라는 파라미터인데 얼마나 크게/작게 가중치를 업데이트할 것인지 정하는 값이다.)",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/linear_regression'> <img src='/images/mse.png' alt='Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/linear_regression'>Deep Learning Series [Part2]: 선형회귀를 위한 딥러닝</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Deep Learning Series [Part1]: 딥러닝이 처음이라면",
      "category" : "",
      "tags"     : "DL_theory",
      "url"      : "/intro",
      "date"     : "Apr 1, 2021",
      "content"  : "Table of Contents  딥러닝이 처음이라면          1. 미래를 바꿀 핵심 기술      2. 딥러닝이란 무엇인가?      3. 인공지능, 머신러닝, 딥러닝                  인공지능          머신러닝          딥러닝                    4. 딥러닝을 해보자                  학습          검증          결론                    딥러닝이 처음이라면1. 미래를 바꿀 핵심 기술그 동안 컴퓨터는 단순한 계산에서 복잡한 계산, 정보 전달을 가능하게 했고, 또 인간이 하던 단순한 작업 이를 테면 스팸 메일 분류와 같은 것들을 해왔습니다.그러나 최근 컴퓨터 하드웨어의 발달과, 축적된 데이터를 통해 사람들은 더 많은 시도를 해왔습니다. 그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다. 지금부터 이것들을 가능하게 한 딥러닝이 어떤 건지 하나씩 살펴 보도록 하겠습니다.2. 딥러닝이란 무엇인가?딥러닝이란 무엇일까요? 위키 백과에서는 다음과 같이 정의합니다.  딥 러닝은 여러 비선형 변환기법의 조합을 통해 높은 수준의 추상화(abstractions, 다량의 데이터나 복잡한 자료들 속에서 핵심적인 내용 또는 기능을 요약하는 작업)를 시도하는 기계 학습 알고리즘의 집합으로 정의되며, 큰 틀에서 사람의 사고방식을 컴퓨터에게 가르치는 기계학습의 한 분야라고 이야기할 수 있다.좀 더 자세하고 전문적인 지식은 위키백과에서 찾아 보도록 하고 저는 이해하기 쉽도록 간단하게 정의해 보겠습니다.딥러닝은 특별한 규칙이나 문제를 푸는 방법을 컴퓨터에게 알려주지 않습니다.우리는 그저 복잡하고 설명하기 힘든 특성을 컴퓨터가 잘 잡아낼 수 있도록 신경망을 만듭니다. 그리고 그 신경망에 데이터를 제공함으로써 기계가 숨겨진 특성을 잘 찾아낼 수 있도록 잘 학습시킵니다. 이러한 과정이 바로 딥러닝입니다.3. 인공지능, 머신러닝, 딥러닝(출처: Samstory)인공지능인공지능은 인간이 뇌를 통해 하는 행동들을, 컴퓨터도 마치 생각하는 것처럼 유사하게 동작하는 것을 인공지능이라고 합니다. 머신러닝, 딥러닝도 결국 인공지능을 위한 하나의 방법인 겁니다.머신러닝머신러닝은 정해진 명령보다 데이터를 기반으로 예측이나 결정을 이끌어 내기 위해 특정한 알고리즘을 수행하는 모델을 구축하는 방식으로 모델을 구축함으로써 입력하지 않은 정보에 대해서다 판단이나 결정을 할 수 있게 됩니다.머신러닝 기법은 주로 정형 데이터를 다룹니다. 관계형 데이터베이스(Relational Database)나 엑셀 표로 정리되는 테이블 데이터를 생각하시면 되겠습니다. 의사결정에 필요한 데이터를 사람이 정리해 기계에 알려주면 기계는 이 정보를 토대로 판단이나 예측을 하는 경우입니다.딥러닝딥러닝은 머신러닝의 한 파트이지만 보다 조금 더 추상적인 모델입니다. 머신러닝과 같이 특정한 알고리즘을 수행하도록 모델을 만들기보다는, 사람이 잡아내기 힘든 추상적인 특성을 데이터에서 잘 추출하도록 단일 신경망을 그저 깊게 쌓는 방식으로 모델을 만듭니다.이러한 이유로 딥러닝은 주로 비정형 데이터를 다룹니다. 비정형 데이터란 지정된 방식으로 정리되지 않은 정보를 말합니다. 간단히 말하자면 이미지, 비디오, 텍스트 문장이나 문서, 음성 데이터 등을 말합니다. 비정형 데이터는 인간이 그 특성을 잡아내기 매우 힘든 데이터이기 때문에 딥러닝이 큰 힘을 발휘하게 됩니다.4. 딥러닝을 해보자이번 챕터에서 딥러닝의 전체적인 과정을 정말 간략히만 훑어보도록 하겠습니다. 딥러닝을 포함해 인공지능의 목표는 어떻게 보면 새로운 데이터에 대한 예측이라고 생각하면 될 것 같습니다. 예를 들어 부동산 가격 예측, 주가 예측, 승패 예측, 날씨 예측과 같은 것들이 있겠죠. 앞의 두 가지는 연속적인 값을 예측하므로 회귀(regression)라고 하고, 뒤의 두 가지는 분류(classification)라고 합니다. 그럼 인공지능 모델은 크게 다음과 같이 두 가지 목적에 따라 분류된다고 할 수 있습니다.  회귀(regression)모델: 데이터의 일반적인 경향을 가장 잘 나타내는 모델을 만드는 것이 목표  분류(classification)모델: 데이터를 일반적으로 가장 잘 분류하는 decision boundary를 찾는 것이 목표학습그럼 이제 우리의 목표는 데이터를 이용해 다음과 같은 모델을 만드는 것입니다. 어떻게 만들 수 있을까요? 이것은 마치 어렸을 때 수학 시험을 위해 문제가 가득한 문제집을 무수히 많이 푸는 과정과 비슷하다고 할 수 있습니다. 컴퓨터는 우리의 데이터를 기반으로 최소한의 오차를 내기 위해 계속 학습하게 됩니다. 드디어 우리의 모델이 무엇을 해야할지 목표가 생겼습니다.목표: 오차 함수를 최솟값으로 만드는 것입니다.목표가 생겼으니 이제 목표를 향해 어떻게 나아갈 지를 생각해봐야 합니다. 어떻게 나아가야 할까요? 우리의 컴퓨터가 목표(최솟값)가 어디 있는지 한 번에 알면 좋겠지만 그렇지는 않습니다. 마치 다음과 같습니다.  앞이 보이지 않습니다. (어디가 최솟값인지 알 수 없습니다)      힌트는 최솟값이 우리의 목표라는 것입니다.      깜깜한 상태에서 가장 밑으로 내려가기 위해서는 발을 더듬으며 내리막길 중 어디가 가장 가파른지를 찾을 것입니다. 그쪽으로 가야 가장 빨리 내려갈 수 있겠죠. 우리의 모델도 이와 비슷한 방법으로 학습을 시킬 수 있습니다. 이 방법을 경사 하강법(Gradient descent)이라고 합니다.방법: 경사하강법을 통해 학습할 것입니다.결론: 학습이란 경사하강법을 통해 오차가 최소가 되도록 하는 것입니다.💡 경사하강법이란 함수의 특정 지점에서의 그래디언트(가장 가파르게 증가하는 방향) 반대 방향으로 우리의 모델을 조금씩 수정해 나가는 것으로, 그래디언트 반대 방향인 이유는 그래디언트 가장 가파르게 증가하는 방향이기 때문에 부호(-)를 취해줌으로써 정확히 반대 방향으로 가면 오차함수를 가장 빨리 감소시키는 방향으로 모델을 수정할 수 있다.검증우리는 데이터를 이용해 경사하강법을 사용함으로써 오차함수를 최소로 하도록 모델을 수정하면 된다고 배웠습니다. 하지만 우리는 수능을 앞두고 있는 상황에서 항상 문제집만 풀지는 않습니다. 자칫 잘못하면 문제집에 나오는 문제들은 너무 완벽하게 공부했지만 수능에 출제될 문제와는 전혀 다를 수도 있습니다. 우리는 언제까지 문제집을 푸는게 도움이 되는지 판단을 할 수 있어야 합니다. 매번 문제집을 풀며 중간 중간 모의고사를 통해 모의고사 성적을 확인합니다. 그러다가 어느 순간 모의고사 성적이 오히려 나빠진다면 문제집을 더이상 풀면 안됩니다. (이를 과대적합(overfitting)이라고 합니다.)훈련마다 검증 데이터를 통해 언제 훈련을 멈출 지 정한다.검증 데이터에서 가장 좋은 성능을 갖는 모델을 우리의 모델로 선택한다.결론지금까지 살펴본 과정들이 딥러닝의 간략한 모습이라고 할 수 있습니다.1. 목적에 맞게 회귀 또는 분류 모델을 선택한다. 2. 그에 맞는 목적함수를 설정한다.  3. 경사하강법을 통해 학습한다.  4. 검증 데이터를 통해 언제 학습을 종료할지 결정한다.  5. 검증 데이터에서 가장 좋은 성능을 낸 모델을 선택한다.다음 포스트에서 부터 하나씩 자세하게 다뤄보도록 하겠습니다.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-04-01T21:01:35+09:00'>01 Apr 2021</time><a class='article__image' href='/intro'> <img src='/images/deep_learning.png' alt='Deep Learning Series [Part1]: 딥러닝이 처음이라면'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/intro'>Deep Learning Series [Part1]: 딥러닝이 처음이라면</a> </h2><p class='article__excerpt'>그렇게 시작된 딥러닝 기술의 발달은 최근 추천 알고리즘, 알파고, 파파고, 자율 주행 등 많은 분야에서 엄청난 변화를 가져오고 있습니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series6",
      "date"     : "Mar 25, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-25T21:01:35+09:00'>25 Mar 2021</time><a class='article__image' href='/mysql-series6'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series6'>MySQL을 이용한 데이터베이스 모델링(2): 정규화를 이용해 테이블의 구조 최적화하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series5",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series5'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series5'>MySQL을 이용한 데이터베이스 모델링(1): 논리적 모델링: 카디널리티와 ERM</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series4",
      "date"     : "Mar 22, 2021",
      "content"  : "Table of Contents  테이블 다루기          테이블의 컬럼 구조 확인하기      컬럼 추가, 이름 변경      컬럼 삭제      컬럼 타입 변경      컬럼 속성 변경      테이블에 제약 사항 걸기      테이블의 제약 사항 삭제      컬럼 순서 앞으로 당기기      컬럼 순서 정하기      컬럼명 속성 동시에 바꾸기      테이블 복제하기      테이블 뼈대만 복제하기        외래키 설정하기          외래키 설정      외래키 정책      외래키 삭제      외래키 파악      이번 포스트에서는 테이블을 처음 구축할 때 필요한 설정을 하기 위한 SQL문에 대해 배워보겠습니다.테이블 다루기테이블의 컬럼 구조 확인하기DESCRIBE [테이블 이름]컬럼 추가, 이름 변경ALTER TABLE [테이블 이름] ADD [추가할 컬럼] CHAR(10) NULL;ALTER TABLE [테이블 이름]RENAME COLUMN [원래 컬럼명] TO [바꿀 컬럼명];컬럼 삭제ALTER TABLE [테이블 이름]DROP COLUMN [삭제할 컬럼명];컬럼 타입 변경ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT;컬럼 속성 변경# NOT NULL 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL;# DEFAULT 속성ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT NOT NULL DEFAULT [주고 싶은 default값];# DATETIME, TIMESTAMP 타입에 줄 수 있는 특별한 속성# DEFAULT CURRENT_TIMESTAMP: 값 입력 안되면 default로 현재 시간 입력ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP;# 처음 default로 현재 시간 넣어주고, 데이터 갱신될 때 마다 갱신된 시간 넣어줌  ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP;# UNIQUE 속성# UNIQUE는 PRIMARY KEY와 다르게 NULL 허용ALTER TABLE [테이블 이름]MODIFY [변경할 컬럼명] INT UNIQUE;테이블에 제약 사항 걸기ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍] CHECK [제약 사항(ex. age &amp;lt; 100)];테이블의 제약 사항 삭제ALTER TABLE [테이블 이름]DROP CONSTRAINT [제약 사항 이름];컬럼 순서 앞으로 당기기ALTER TABLE [테이블 이름]MODIFY [컬럼명] INT FIRST;컬럼 순서 정하기ALTER TABLE [테이블 이름]MODIFY [뒤에 올 컬럼명] INT AFTER [앞에 있는 컬럼명];컬럼명 속성 동시에 바꾸기ALTER TALBE [테이블 이름]CHANGE [원래 컬럼명] [바꿀 컬럼명] VARCHAR(10) NOT NULL;테이블 복제하기CREATE TABLE [복제한 테이블의 이름]AS SELECT * FROM [원본 테이블의 이름]테이블 뼈대만 복제하기CREATE TABLE [복제한 테이블의 이름]LIKE [원본 테이블의 이름]외래키 설정하기외래키(Foreign Key)란 한 테이블의 컬럼 중에서 다른 테이블의 특정 컬럼을 식별할 수 있는 컬럼을 말합니다. 그리고 외래키에 의해 참조당하는 테이블을 부모 테이블(parent table), 참조당하는 테이블(referenced table)이라고 합니다. 외래키를 이용하면 테이블간의 참조 무결성을 지킬 수 있습니다. 참조 무결성이란 아래 그림과 같이 두 테이블 간에 참조 관계가 있을 때 각 데이터 간에 유지되어야 하는 정확성과 일관성을 의미합니다.예를 들어, 강의 평가인 review 테이블에는 ‘컴퓨터 개론’에 관한 평가 데이터가 남아있지만, 강의 목록을 나타내는 course 테이블에는 ‘컴퓨터 개론’ 과목이 삭제된다면 이상한 상황이 벌어질 것입니다. 이 때 외래키를 통해 지정해 놓으면 이런 상황을 해결할 수 있습니다.이렇게 외래키는 두 개 이상의 테이블에서 중요한 역할을 하기 때문에 외래키 속성을 어떻게 설정하는지는 굉장히 중요한 문제입니다.외래키 설정ALTER TABLE [테이블 이름]ADD CONSTRAINT [제약 사항 네이밍]    FOREIGN KEY (자식테이블의 컬럼)    REFERENCES 부모테이블 (부모테이블의 컬럼)    ON DELETE [DELETE정책]    ON UPDATE [UPDATE정책];외래키 정책  RESTRICT: 자식 테이블에서 삭제/갱신해야만 부모 테이블에서도 삭제/갱신 가능  CASCADE: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터도 같이 삭제/갱신  SET NULL: 부모 테이블의 데이터 삭제/갱신하면 관련 자식 테이블 데이터의 컬럼에 NULL 지정외래키 삭제ALTER TABLE [테이블 이름]DROP FOREIGN KEY [제약 사항이 걸린 테이블];외래키 파악SELECT    i.TABLE_SCHEMA, i.TABLE_NAME, i.CONSTRAINT_TYPE, i.CONSTRAINT_NAME,    k.REFERENCED_TABLE_NAME, k.REFERENCED_COLUMN_NAMEFROM information_schema.TABLE_CONSTRAINTS iLEFT JOIN information_schema.KEY_COLUMN_USAGE kUSING(CONSTRAINT_NAME)WHERE i.CONSTRAINT_TYPE = &#39;FOREIGN KEY&#39;;",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-22T21:01:35+09:00'>22 Mar 2021</time><a class='article__image' href='/mysql-series4'> <img src='/images/sql_2.png' alt='MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series4'>MySQL을 이용한 데이터 관리(2): 테이블 설정을 통해 데이터 관리하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part6]: 파이썬의 네임스페이스",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-namespace",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents파이썬 식별자, 스코프, 네임스페이스",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-namespace'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part6]: 파이썬의 네임스페이스'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-namespace'>Python Basic Series [Part6]: 파이썬의 네임스페이스</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-component-file_directory",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/python-component-file_directory'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-component-file_directory'>Python Basic Series [Part5]: 파이썬의 파일과 디렉토리 관련 모듈</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series3",
      "date"     : "Mar 21, 2021",
      "content"  : "Table of Contents  테이블 생성 및 삭제          SQL문 데이터 타입        데이터 CRUD          데이터 추가      데이터 갱신      데이터 삭제      앞에서 저희가 배웠던 SQL문들은 모두 이미 테이블이 주어졌고 그 테이블에 데이터가 쌓여있는 상태에서 원하는 데이터를 조회하는 방법에 관한 것들이었습니다.하지만 저희가 직접 테이블을 생성하고 데이터를 쌓아야 하는 순간도 있을 것입니다. 이번 포스트에서는 이러한 순간에 필요한 SQL문에 대해 배워 보겠습니다.테이블 생성 및 삭제# 데이터베이스 생성CREATE DATABASE [생성할 데이터베이스 이름]CREATE DATABASE IF NOT EXISTS [생성할 데이터베이스 이름]# 데이터베이스 지정USE [생성한 데이터베이스 이름]# 테이블 생성CREATE TABLE [데이터베이스 이름].[생성할 테이블 이름] (    [컬럼1] INT NOT NULL AUTO_INCREMENT PRIMARY KEY,    [컬럼2] VARCHAR(20) NULL,    [컬럼3] VARCHAR(15) NULL,    또는 PRIMARY KEY ([&#39;컬럼1&#39;]));# 테이블 삭제DROP TABLE [테이블 이름]SQL문 데이터 타입            종류      타입              정수형      TINYINT, SMALLINT, MEDIUMINT, INT, BIGINT              실수형      DECIMAL, FLOAT, DOUBLE              날짜 및 시간      DATE, TIME, DATETIME, TIMESTAMP              문자열      CHAR, VARCHAR, TEXT            TINYINTsigned: -128 ~ 127unsigned: 0 ~ 255        INTsigned: -2147483648 ~ 2147483647unsigned: 0 ~ 4294967295        DECIMALDECIMAL(M, D): M은 전체 숫자의 최대 자리수, D는 소수점 자리 숫자의 최대 자리수DECIMAL(5, 2): -999.99 ~ 999.99M은 최대 65까지 가능, D는 최대 30까지 가능        FLOAT-3.4 * 10^38 ~ 3.4 * 10^38        DOUBLE-1.7 * 10^308 ~ 1.7 * 10^308FLOAT와 비교해 범위도 더 넓고, 정밀도 또한 더 높음(더 많은 소수점 자리 수 지원)        DATE날짜를 저장하는 데이터 타입’2021-03-21’ 이런 형식의 연, 월, 일 순        TIME시간을 저장하는 데이터 타입’09:27:31’ 이런 형식의 시, 분, 초        DATETIME날짜와 시간을 저장하는 데이터 타입’2021-03-21 09:30:27’ 이런 식으로 연, 월, 일, 시, 분, 초        TIMESTAMPDATETIME과 같다차이점은 TIMESTAMP는 타임 존 정보도 포함        CHARCHAR(30): 최대 30자의 문자열을 저장 (0~255까지 가능)차지하는 용량이 항상 숫자값에 고정됨데이터의 길이가 크게 변하지 않는 상황에 적합        VARCHARVARCHAR(30): 최대 30자의 문자열을 저장 (0~65536까지 가능)차지하는 용량이 가변적. 30이어도 그 이하의 길이면 용량도 적게 차지함해당 값의 사이즈를 나타내는 부분(1byte 또는 2byte)이 저장 용량에 추가데이터 길이가 크게 들쑥날쑥해지는 경우에 적합        TEXT문자열이 아주 긴 상황에 적합  데이터 CRUD데이터 추가# 데이터 추가INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼2, 컬럼3, ...)VALUES (컬럼1의 데이터, 컬럼2의 데이터, 컬럼3의 데이터, ...);# 특정 컬럼에만 데이터 넣을 수도 있다INSERT INTO [사용할 테이블 이름] (컬럼1, 컬럼3)VALUES (컬럼1의 데이터, 컬럼3의 데이터);데이터 갱신# 데이터 갱신UPDATE [사용할 테이블 이름]    SET 컬럼1 = [갱신 데이터] WHERE [조건]; # 기존 값을 기준으로 갱신UPDATE [사용할 테이블 이름]SET 컬럼1 = [컬럼1 + 3] WHERE [조건]; 데이터 삭제DELETE FROM [사용할 테이블 이름]WHERE [조건]",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-21T21:01:35+09:00'>21 Mar 2021</time><a class='article__image' href='/mysql-series3'> <img src='/images/mysql_logo.webp' alt='MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series3'>MySQL을 이용한 데이터 관리(1): CRUD를 이용해 데이터 관리하기</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-dictionary",
      "date"     : "Mar 16, 2021",
      "content"  : "Table of Contents  1. 딕셔너리 자료형의 특징  2. 딕셔너리 생성          2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법      2-2 중괄호를 사용하는 방법      2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법      2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법        3. 딕셔너리 메소드          3-1 확인하기: .keys(), .values(), .items()      3-2 제거하기: .pop(), .popitem()      3-3 복사하기: .copy()      3-4 결합하기: .update(), {**dict1, **dict2}        KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()          get(key, default)      setdefault(key, default)      defaultdict(자료형)      1. 딕셔너리 자료형의 특징딕셔너리 자료형은 순서가 없는 시퀀스형 자료형입니다.key, value를 쌍으로 갖고 있습니다.key값은 중복이 불가능하고 value값은 중복이 가능합니다.2. 딕셔너리 생성딕셔너리 형태의 데이터를 생성하는 방법을 알아보도록 하겠습니다.만드는 방법도 여러가지가 있기 때문에 하나씩 살펴보도록 하겠습니다.2-1 딕셔너리 객체 생성 후 값을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict()&amp;gt;&amp;gt;&amp;gt; a[&#39;a&#39;] = &#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;b&#39;] = &#39;banana&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;c&#39;] = &#39;car&#39;2-2 중괄호를 사용하는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;, &#39;c&#39;:&#39;car&#39;}2-3 딕셔너리 객체 생성 후 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])2-4 내장함수 zip()을 사용해 (key, value)쌍을 넣어주는 방법&amp;gt;&amp;gt;&amp;gt; dict1 = dict(list(zip([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;], [&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;])))🔔 딕셔너리는 key값은 중복이 안되고 value는 중복이 가능합니다&amp;gt;&amp;gt;&amp;gt; dict1 = dict([(&#39;a&#39;, &#39;apple phone&#39;), (&#39;a&#39;, &#39;apple car&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;banana&#39;)])&amp;gt;&amp;gt;&amp;gt; dict1{&#39;a&#39;: &#39;apple car&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;banana&#39;} #key값 &#39;a&#39;는 중복이 불가, value값 &#39;banana&#39;는 중복 가능🔔 key값을 통해 value 값을 접근하고 수정하는 것은? 가능하다dict1[&#39;a&#39;] = &#39;apple pods&#39;🔔 value값을 통해 key 값에 접근하고 수정하는 것은? 간단하지 않다딕셔너리의 자료구조 특성(해시테이블)상 key를 통한 value의 접근은 O(1), 그 반대는 O(n)for i in range(len(dict1)):    if &#39;car&#39; in list(dict1.items())[i]:        wanted_key = list(dict1.items())[i][0]a.pop(wanted_key)a[&#39;c_new&#39;] = &#39;car&#39;3. 딕셔너리 메소드&amp;gt;&amp;gt;&amp;gt; dict1 = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;}&amp;gt;&amp;gt;&amp;gt; dir(dict1)[&#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;, &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;,  &#39;__lt__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;,  &#39;__subclasshook__&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;fromkeys&#39;, &#39;get&#39;, &#39;items&#39;, &#39;keys&#39;, &#39;pop&#39;, &#39;popitem&#39;, &#39;setdefault&#39;, &#39;update&#39;, &#39;values&#39;]3-1 확인하기: .keys(), .values(), .items()  .keys()딕셔너리의 key값들을 보여주는 dict_keys객체를 생성합니다. 이는 메모리 낭비를 방지하기 위함 입니다.값 하나하나를 읽어오기 위해서는 list로 형 변환 시켜줘야 합니다.    &amp;gt;&amp;gt;&amp;gt; dict1.keys()dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])&amp;gt;&amp;gt;&amp;gt; list(dict1.keys())[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;]        .values()딕셔너리의 value들을 보여줍니다.마찬가지로 값을 하나씩 읽어오기 위해서는 list로 형 변환 시켜줍니다.    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.values()dict_values([&#39;apple&#39;, &#39;banana&#39;, &#39;car&#39;, &#39;dragon&#39;, &#39;epsilon&#39;])        .items()딕셔너리의 (key, value)쌍들을 보여줍니다.    &amp;gt;&amp;gt;&amp;gt; dict1.items()dict_items([(&#39;a&#39;, &#39;apple&#39;), (&#39;b&#39;, &#39;banana&#39;), (&#39;c&#39;, &#39;car&#39;)])      3-2 제거하기: .pop(), .popitem()  .pop()없애고자 하는 (key, value)쌍의 key값을 입력해주면 value값을 리턴하고 해당하는 쌍을 pop해줍니다    &amp;gt;&amp;gt;&amp;gt; d = {&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}&amp;gt;&amp;gt;&amp;gt; d.pop(&#39;a&#39;)&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; d{&#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}        .popitem()한 번 실행할 때 마다 가장 뒤에 저장된 (key, value)쌍을 리턴하고 딕셔너리에서 pop해줍니다.(딕셔너리는 순서가 없는데 어떤 쌍이 가장 뒤에 있는 값인지 어떻게 알까?-&amp;gt; 파이썬 3.x 버전 업데이트 이후로는 순서대로 저장된다)3-3 복사하기: .copy()  .copy() (얕은 복사)딕셔너리와 같은 데이터를 갖는 새로운 딕셔너리를 생성해줍니다.  얕은 복사이므로 딕셔너리의 값이 mmutable한 경우 문제가 된다.🔔 이 밖에도 변수를 이용한 복사, 깊은 복사가 있습니다. (복사에 관한 포스팅)3-4 결합하기: .update(), {**dict1, **dict2}&amp;gt;&amp;gt;&amp;gt; a = dict()&amp;gt;&amp;gt;&amp;gt; a.update({&#39;a&#39;:&#39;apple&#39;})&amp;gt;&amp;gt;&amp;gt; a{&#39;a&#39;:&#39;apple&#39;}&amp;gt;&amp;gt;&amp;gt; b = dict()&amp;gt;&amp;gt;&amp;gt; b.update({&#39;b&#39;:&#39;banana&#39;})&amp;gt;&amp;gt;&amp;gt; b{&#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; c = {**a, **b}&amp;gt;&amp;gt;&amp;gt; c{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}&amp;gt;&amp;gt;&amp;gt; {**{&#39;a&#39;:&#39;apple&#39;, &#39;b&#39;:&#39;banana&#39;}, **{&#39;c&#39;:&#39;car&#39;, &#39;d&#39;:&#39;dragon&#39;, &#39;e&#39;:&#39;epsilon&#39;}}{&#39;a&#39;: &#39;apple&#39;, &#39;b&#39;: &#39;banana&#39;, &#39;c&#39;: &#39;car&#39;, &#39;d&#39;: &#39;dragon&#39;, &#39;e&#39;: &#39;epsilon&#39;}KeyError를 막기 위한 3가지 방법: get(), setdefault(), defaultdict()KeyError를 해결하기 위해 try, except구문을 써도 되지만 다음과 같은 방법으로 코드를 더 간결하게 작성할 수 있습니다.get(key, default)딕셔너리 자료형의 get() 메소드는 원하는 key값의 value를 조회할 때, key값이 없을 경우 default 값을 주어 KeyError를 해결합니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# 일반적인 값 조회dic[&#39;a&#39;]-------------1# 일반적인 값 조회는 key값이 없으면 오류가 발생dic[&#39;d&#39;]-------------KeyError# get메소드 이용dic.get(&#39;d&#39;)-------------None# get메소드의 default 인자 이용dic.get(&#39;d&#39;, 0)-------------0setdefault(key, default)setdefault는 get과 거의 비슷해 보이지만 제가 알고있는 한 가지 다른점은 없는 key값의 default값을 리턴만 하는 get()메소드와는 다르게, setdefault()메소드는 key값이 없으면 딕셔너리에 저장도 해준다는 것입니다. 바로 예시를 보겠습니다.dic = {&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# get메소드는 default갑 리턴만 해줍니다dic.get(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3}# setdefault메소드는 저장도 합니다.dic.setdefault(&#39;d&#39;, 0)dic--------------------{&#39;a&#39;: 1, &#39;b&#39;: 2, &#39;c&#39;: 3, &#39;d&#39;: 0}# key값이 없으면 defalut인 빈 리스트를 값으로 생성 dic = {&#39;a&#39;: [&#39;apple&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;a&#39;, []).append(&#39;alphago&#39;)dic---------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;]}dic.setdefault(&#39;d&#39;, []).append(&#39;dog&#39;)dic--------------------------------------------------------------{&#39;a&#39;: [&#39;apple&#39;, &#39;alphago&#39;], &#39;b&#39;: [&#39;banana&#39;, &#39;bulgaria&#39;], &#39;c&#39;: [&#39;car&#39;], &#39;d&#39;: [&#39;dog&#39;]}defaultdict(자료형)defaultdict는 collections모듈에 있는 함수로 default를 가지는 딕셔너리를 생성할 때 활용됩니다.from collections import defaultdictdic = defaultdict(int)dic[&#39;a&#39;]-------------0dic-------------{&#39;a&#39;: 0}dic = defaultdict(list)dic[&#39;a&#39;]------------[]dic[&#39;b&#39;].append(&#39;banana&#39;)dic-----------{&#39;a&#39;:[], &#39;b&#39;:[&#39;banana&#39;]}# 여기서 setdefault를 이용할 수도 있습니다.dic.setdefault(&#39;c&#39;, 0)dic-------------------{&#39;a&#39;: [], &#39;b&#39;: [&#39;banana&#39;], &#39;c&#39;: 0}# 이렇게 dic의 default가 list였음에도 int형 0을 default로 할 수 있습니다.  ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-16T21:01:35+09:00'>16 Mar 2021</time><a class='article__image' href='/python-data-type-dictionary'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-dictionary'>Python Basic Series [Part4]: 파이썬 딕셔너리/셋 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part3]: 파이썬 리스트 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-list",
      "date"     : "Mar 15, 2021",
      "content"  : "Table of Contents  리스트          1. 리스트 자료형의 특징      2. 리스트의 장점      3. 리스트 생성      4. 인덱싱, 슬라이싱      5. 리스트 메소드                  5-1 .append(), .extend(), .insert(), .copy()          5-2 .pop(), .remove(), .clear()          5-3 .sort(), .reverse()          5-4 .count(), .index()                    6. 리스트에서 주목할 만한 것들                  6-1 List &amp;amp; Range          6-2 리스트 표현식 (List comprehension)          6-3 리스트와 문자열 넘나들기          6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)                    7. 리스트 주요 연산들의 시간 복잡도      리스트파이썬 언어는 개발자의 편의성, 생산성, 가독성에 가장 초점을 맞춘 언어입니다. 그래서 파이썬에서는 기존 언어에서 동적 배열이라는 기초 자료형의 불편함을 개선해 리스트라는 파이썬만의 강력한 자료형을 제공합니다.1. 리스트 자료형의 특징  파이썬에서 가장 자주 사용하는 자료형  원소들의 순서가 있는 시퀀스  원소들의 변경이 가능 (Mutable)  다양한 타입의 원소 저장 가능  동적배열로 구현됨2. 리스트의 장점  임의의 원소에 O(1) 접근 가능: 이것은 기존 동적배열이 제공해주는 기능입니다  다양한 타입의 원소 저장 가능: 리스트가 값이 아닌 값을 가진 객체의 주소를 동적배열로 저장하고 있기 때문입니다.  왠만한 추상 자료형은 리스트로 구현 가능: 리스트 자료형이 가지고 있는 많은 메소드로 스택, 큐, 트리, 그래프 등 거의 모든 추상 자료형을 구현할 수 있습니다.3. 리스트 생성리스트는 여러 가지 자료형을 가질 수 있는 시퀀스형 자료형입니다.또한 값을 변경할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = [3.1, 2.5, 7]&amp;gt;&amp;gt;&amp;gt; c = [&quot;Hello&quot;, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; d = [1, 4.5, &quot;Good&quot;]&amp;gt;&amp;gt;&amp;gt; a[0] = 100&amp;gt;&amp;gt;&amp;gt;a[100, 2, 3, 4]🔔 리스트를 곱하거나 더하면 값이 반복되거나 추가됩니다&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a + [5][1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a + 5 -&amp;gt; Error&amp;gt;&amp;gt;&amp;gt; a * 2[1, 2, 3, 4, 1, 2, 3, 4]4. 인덱싱, 슬라이싱이번에는 위에서 만들어진 리스트 데이터를 가지고 원하는 부분만 가져올 수 있도록 해주는 인덱싱, 슬라이싱에 대해 알아보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&quot;&quot;&quot;인덱싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0]1&amp;gt;&amp;gt;&amp;gt; a[2]3&amp;gt;&amp;gt;&amp;gt; a[4] = 10 -&amp;gt; Error  (a[50] = 10 이런식으로 하면 그 사이의 인덱스에 값을 표시할 수 없어서 무조건 차례대로 값을 채워넣어야 함 -&amp;gt; 더하기 또는 append 메소드)&quot;&quot;&quot;슬라이싱&quot;&quot;&quot;&amp;gt;&amp;gt;&amp;gt; a[0:3] # 0에서 부터 3앞까지 -&amp;gt; 인덱스 0~2[1, 2, 3]&amp;gt;&amp;gt;&amp;gt; a[:][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::][1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a[::-1] #처음부터 끝까지 거꾸로 슬라이싱 (중요)[4, 3, 2, 1]5. 리스트 메소드리스트 데이터는 프로그래밍을 하다보면 정말 자주 만나게 되는 자료형 중에 하나입니다.그렇기 때문에 문자열 객체의 메소드를 잘 활용할 줄 아는 것이 굉장히 중요합니다.먼저 어떤 메소드가 있는지 확인해 보겠습니다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__add__&#39;, &#39;__class__&#39;, &#39;__contains__&#39;, &#39;__delattr__&#39;, &#39;__delitem__&#39;, &#39;__dir__&#39;, &#39;__doc__&#39;, &#39;__eq__&#39;, &#39;__format__&#39;,  &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getitem__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__iadd__&#39;, &#39;__imul__&#39;, &#39;__init__&#39;, &#39;__init_subclass__&#39;,   &#39;__iter__&#39;, &#39;__le__&#39;, &#39;__len__&#39;, &#39;__lt__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__new__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;,    &#39;__repr__&#39;, &#39;__reversed__&#39;, &#39;__rmul__&#39;, &#39;__setattr__&#39;, &#39;__setitem__&#39;, &#39;__sizeof__&#39;, &#39;__str__&#39;, &#39;__subclasshook__&#39;,    &#39;append&#39;, &#39;clear&#39;, &#39;copy&#39;, &#39;count&#39;, &#39;extend&#39;, &#39;index&#39;, &#39;insert&#39;, &#39;pop&#39;, &#39;remove&#39;, &#39;reverse&#39;, &#39;sort&#39;]실제로 코딩을 하실 때는 기억이 안나면 그 때마다 dir() 함수를 사용해 어떤게 있는지 살펴보면 됩니다.5-1 .append(), .extend(), .insert(), .copy().append()리스트 맨 끝에 인자로 넣어준 값 하나를 추가해준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.append(100)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100]여러 개를 추가하고 싶어서 인자로 값을 여러 개 준다면? -&amp;gt; 에러가 난다그래서 [100, 101, 102] 이런 식으로 추가하면? -&amp;gt; 에러는 안나지만 리스트가 추가되어 원하는 모습과는 다르다..extend()iterable한 객체를 인자로 넣어주면 그 안의 원소들이 모두 차례대로 리스트에 추가된다.&amp;gt;&amp;gt;&amp;gt; a.extend([101, 102, 103]) # 리스트와 같은 iterable한 객체를 인자로 주어야 한다.&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 100, 101, 102, 103]이제는 맨 뒤가 아니라 원하는 인덱스에 값을 추가(교체)하고 싶다..insert()인자로 인덱스와 값을 넣어주면 인덱스에 값을 넣어준다.인덱스에 이미 값이 있으면 바꿔주고 리스트 길이보다 인덱스 값이 크거나 같으면 리스트 맨 뒤에 값을 넣어준다.-&amp;gt; 길이 신경쓰지 않고 해줘도 오류는 안난다. (내가 원하는 인덱스에 값이 들어가지 않을 수도 있지만)&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1, 10)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.insert(1000, 7)&amp;gt;&amp;gt;&amp;gt; a[1, 10, 3, 4, 7].copy()객체와 똑같은 값을 가지는 리스트를 복사한다. 변수를 지정해주면 새로운 메모리에 저장된다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b = a.copy()&amp;gt;&amp;gt;&amp;gt; b.append(5)&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; b[1, 2, 3, 4, 5]5-2 .pop(), .remove(), .clear().pop()리스트의 가장 끝에 있는 원소를 뽑아 리턴해준다.&amp;gt;&amp;gt;&amp;gt; a = [&#39;banana&#39;, &#39;lemon&#39;, &#39;apple&#39;]&amp;gt;&amp;gt;&amp;gt; a.pop()&#39;apple&#39;&amp;gt;&amp;gt;&amp;gt; a[&#39;banana&#39;, &#39;lemon&#39;].remove()인자로 받은 값은 값을 제거해준다.&amp;gt;&amp;gt;&amp;gt; a.remove(2)&amp;gt;&amp;gt;&amp;gt; a[1, 3, 4].clear()리스트를 싹 비운다.&amp;gt;&amp;gt;&amp;gt; a.clear()&amp;gt;&amp;gt;&amp;gt; a[]5-3 .sort(), .reverse().sort()  리스트를 작은 값부터 순서대로 정렬해준다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a = [&#39;안녕&#39;, &#39;Hello&#39;, &#39;Hi&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a.sort(key=len)&amp;gt;&amp;gt;&amp;gt; a[&#39;안녕&#39;, &#39;Hi&#39;, &#39;Hello&#39;, &#39;안녕하십니까&#39;]&amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: x**2)&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]# 같은 제곱값에 대해 양수가 먼저 나오게 하려면 양수가 논리연산 시 False가 되면 되므로 기준을 0보다 작은지로 하면 된다 &amp;gt;&amp;gt;&amp;gt; a = [3, -9, -1, 1, 2, 11]&amp;gt;&amp;gt;&amp;gt; a.sort(key=lambda x: (x**2, x&amp;lt;=0))&amp;gt;&amp;gt;&amp;gt; a[-1, 1, 2, 3, -9, 11]&amp;gt;&amp;gt;&amp;gt; a =[False, True, False, True, True, False]&amp;gt;&amp;gt;&amp;gt; a.sort()&amp;gt;&amp;gt;&amp;gt; a[False, False, False, True, True, True]🔔 sorted() 함수  sorted() 함수는 정렬된 값을 리턴해줄 뿐 인자로 받은 리스트를 정렬하지는 않는다.  또 한가지 중요한 특징은 sorted()함수는 리스트 뿐 아니라 모든 iterable한 값들을 정렬시켜 준다는 것입니다.&amp;gt;&amp;gt;&amp;gt; a = [3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted(a)[1, 2, 3, 4, 5]&amp;gt;&amp;gt;&amp;gt; a[3, 5, 1, 2, 4]&amp;gt;&amp;gt;&amp;gt; sorted({1: &#39;D&#39;, 2: &#39;B&#39;, 3: &#39;B&#39;, 4: &#39;E&#39;, 5: &#39;A&#39;})[1, 2, 3, 4, 5]🔔 .sort()와 sorted() 모두 key, reverse 인자를 갖는다  key: 정렬을 목적으로 하는 함수를 값으로 넣는다. lambda를 이용할 수 있다. key 매개 변수의 값은 단일 인자를 취하고 정렬 목적으로 사용할 키를 반환하는 함수(또는 다른 콜러블)여야 합니다.  reverse: bool값을 넣는다. 기본값은 reverse=False(오름차순)이다..reverse()리스트의 원소의 순서를 뒤집어준다.&amp;gt;&amp;gt;&amp;gt; a = [1, 2, 3, 4]&amp;gt;&amp;gt;&amp;gt; a.reverse()&amp;gt;&amp;gt;&amp;gt; a[4, 3, 2, 1]🔔 reversed() 함수뒤집은 값을 리턴해줄 뿐 인자로 받은 리스트는 그대로다.🔔 문자열을 뒤집는 방법a.reverse()a = list(reversed(a))a = a[::-1]5-4 .count(), .index().count()인자로 받은 값이 등장하는 횟수를 리턴해준다.a = [1, 1, 1, 2, 3, 4]a.count(1)---------------------3.index()인자로 받은 값의 인덱스를 리턴해준다.a = [1, 3, 5, 7]a.index(7)--------------------36. 리스트에서 주목할 만한 것들6-1 List &amp;amp; Range1부터 1000까지 값을 하나씩 출력하는 코드를 짠다고 할 때for i in [1, 2, 3, 4, 5, 6, 7, 8, ..., 1000]:  print(i)로 하게 되면 위의 코드를 실행하기 위해 1000개의 요소를 적어서 리스트를 만드는 것은 너무 비효율적입니다.이를 개선시키는 방법으로for i in range(1000):  print(i)이렇게 해주면 훨씬 짧고 간결한 코드를 작성할 수 있습니다.range(start, end, step)range(1000) =&amp;gt; 0, 1, 2, 3, ..., 999range(1, 1000) =&amp;gt; 1, 2, 3, ..., 999range(1, 1000, 2) =&amp;gt; 1, 3, 5, 7, ..., 9996-2 리스트 표현식 (List comprehension)&amp;gt;&amp;gt;&amp;gt; a = []&amp;gt;&amp;gt;&amp;gt; for i in range(100):        if i % 3 == 0 and i % 5 == 0:          a.append(i)&amp;gt;&amp;gt;&amp;gt; [i for i in range(100) if i % 3 == 0 and i % 5 == 0]6-3 리스트와 문자열 넘나들기문자열을 리스트로 바꿔야 하는 경우문자열은 값을 바꿀 수가 없기 때문에 예를 들어 스펠링을 고치기 위해서는리스트로 바꿔서 고친 후 다시 문자열로 변환해줘야 한다.&amp;gt;&amp;gt;&amp;gt; name = &#39;kinziont&#39;&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39; -&amp;gt; 에러&amp;gt;&amp;gt;&amp;gt; name = list(name)&amp;gt;&amp;gt;&amp;gt; name[2] = &#39;m&#39;&amp;gt;&amp;gt;&amp;gt; name[&#39;k&#39;, &#39;i&#39;, &#39;m&#39;, &#39;z&#39;, &#39;i&#39;, &#39;o&#39;, &#39;n&#39;, &#39;t&#39;]&amp;gt;&amp;gt;&amp;gt; name = str(name)&amp;gt;&amp;gt;&amp;gt; name&#39;kimziont&#39;문자열 데이터를 단어 단위 또는 문장 단위로 토크나이징하기 위해 문자열 메소드인 .split()을 쓰면자동으로 리스트로 변환된다.6-4 리스트를 이용한 다차원 데이터 표현하기 (Tensor)a = [1, 2, 3, 4] # 1*4 vectorb = [[1, 2], [3, 4]] # 2*2 matrixc = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]] # 2*2*2 tensora[0] -&amp;gt; 1b[0] -&amp;gt; [1, 2]c[0] -&amp;gt; [[1, 2], [3, 4]]c[0][1] -&amp;gt; [3, 4]c[0][1][0] -&amp;gt; 37. 리스트 주요 연산들의 시간 복잡도            연산      시간 복잡도      설명              len(a)      O(1)      전체 요소의 개수를 리턴              a[i]      O(1)      인덱스 i의 요소를 가져온다              a[i:j]      O(k)      객체 k개에 대한 조회가 필요하므로 O(k)이다              x in a      O(n)      정렬되어 있지 않은 a 이므로 순차 탐색              a.append(x)      O(1)      동적배열의 특징              a.pop(x)      O(1)      동적배열의 특징              a.pop(0)      O(n)      배열의 특성상 앞의 원소가 추가/삭제 되면 그 뒤의 모든 원소들의 이동이 발생              del a[i]      O(n)      i에 따라 다르다. 최악의 경우 O(n)이다              a.sort()      O(nlogn)      파이썬에서는 팀소트(Timsort)를 사용              min(a), max(a)      O(n)      최소, 최대값 찾기 위해서는 선형 탐색 해야함              a.reverse()      O(n)      선형 이동하면서 처음과 끝 원소 바꾼다      ",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-15T21:01:35+09:00'>15 Mar 2021</time><a class='article__image' href='/python-data-type-list'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part3]: 파이썬 리스트 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-list'>Python Basic Series [Part3]: 파이썬 리스트 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series2",
      "date"     : "Mar 14, 2021",
      "content"  : "Table of Contents  조인          서로 구조가 다른 테이블간의 조인                  LEFT OUTER JOIN          RIGHT OUTER JOIN          INNER JOIN                    구조가 같은 테이블간의 조인                  UNION          UNION ALL          INTERSECT          MINUS                      서브쿼리          뷰        마치며조인여러 테이블을 합쳐서 하나의 테이블인 것처럼 보는 행위를 ‘조인(join)’이라고 합니다. 실무에서는 이 조인을 잘해야 제대로된 데이터 분석을 할 수 있습니다. 조인은 SQL을 얼마나 잘 쓰는지 판단하는 척도 중 하나일만큼 정말 중요한 개념입니다.서로 구조가 다른 테이블간의 조인  서로 구조가 다른 테이블을 특정 컬럼을 기준으로 조인SELECT     p.name    p.team    r.team    r.regionFROM player AS p LEFT OUTER JOIN region AS rON p.team = r.team # ON 대신 USING(team) 이렇게 할 수도 있음LEFT OUTER JOINRIGHT OUTER JOININNER JOIN구조가 같은 테이블간의 조인UNION  중복을 허용하지 않는 합집합SELECT * FROM old_playerUNIONSELECT * FROM new_player;UNION ALL  중복을 허용하는 합집합INTERSECT  교집합  MySQL에서는 지원 하지 않음  INNER JOIN으로 해결MINUS  차집합  MySQL에서는 지원 하지 않음  LEFT/RIGHT OUTER JOIN으로 해결서브쿼리  SELECT문의 결과로 나온 값/열/테이블을 적재적소에 맞게 다른 SELECT문의 입력으로 사용할 수 있습니다뷰  때에 따라 서브쿼리가 이중 중첩, 삼중 중첩되는 경우도 있습니다.  이 때 생기는 SELECT문의 복잡성을 줄이고자 뷰를 사용할 수 있습니다.  특정 역할을 하는 SELECT문들을 뷰로 저장해둡니다.  코드 스니펫처럼 필요할 때마다 가져와서 사용할 수 있습니다.  백엔드 개발자들의 자산과도 같습니다마치며지쳐서 너무 대충해버렸다…생각날 때마다 조금씩 보충해야겠다",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-14T21:01:35+09:00'>14 Mar 2021</time><a class='article__image' href='/mysql-series2'> <img src='/images/sql_5.png' alt='MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series2'>MySQL을 이용한 데이터 조회 및 분석(2): 조인(JOIN)과 서브쿼리(SUBQUERY)</a> </h2></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part2]: 파이썬 숫자 자료형",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-number",
      "date"     : "Mar 13, 2021",
      "content"  : "Table of Contents  1. 숫자 자료형의 종류  2. 파이썬의 특별한 점  3. 2진법, 8진법, 16진법  4. 부동 소수점 연산 오류  5. 숫자 자료형 관련 메소드1. 숫자 자료형의 종류파이썬에는 세 가지 다른 숫자 형이 있습니다: 정수 (integers), 실수 (floating point numbers), 복소수 (complex numbers)또한 최댓값, 최솟값이 없고 자동으로 메모리를 할당해줍니다. 그래서 사용하기에는 간편하지만 다른 언어에 비해서는 조금 비효율적이라고 할 수 있겠습니다. (C++과 비교해 약 10배 정도 느리다고 합니다)2. 파이썬의 특별한 점      느린 실행 속도를 보완하고자 파이썬에서는 1~256의 값을 메모리에 static하게 저장합니다. 따라서 1~256 사이의 값을 어떤 변수에 할당할 경우, 새로운 메모리를 할당하지 않고 기존에 저장된 값의 주소를 변수가 가리키도록 합니다.    a = 1b = 1c = 4a == b # true (값을 비교한다)a is b # true (주소를 비교한다)b = b + 3b == c # true (값을 비교한다)b is c # true (주소를 비교한다)-------------------------------a = 260b = 260a == b # turea is b # false (값이 같더라도 256이 넘는 숫자에 대해서는 새로운 메모리가 할당된다)------------------------------id(a) = 2592314943888id(b) = 2592314943824            파이썬2에서는 int의 크기는 CPU에 따라 32비트나 64비트로 제한되었습니다. long은 64비트까지 허용하는 정수형 데이터 타입이었습니다. 그러나 파이썬 3에서는 long이 사라지고, int가 arbitrary precision을 지원하여 오버플로우가 생기지 않습니다.🔔 arbitrary-precision은 사용할 수 있는 메모리양이 정해져 있는 기존의 fixed-precision과 달리, 현재 남아있는 만큼의 가용 메모리를 모두 수 표현에 끌어다 쓸 수 있는 형태를 이야기하는 것 같다. 예를 들어 특정 값을 나타내는데 4바이트가 부족하다면 5바이트, 더 부족하면 6바이트까지 사용할 수 있게 유동적으로 운용한다는 것이다.  3. 2진법, 8진법, 16진법bin(), oct(), hex() 함수를 이용하면 정수형 자료를 2진법, 8진법, 16진법으로 표현된 문자열을 리턴해 줍니다.&amp;gt;&amp;gt;&amp;gt; bin(15)&#39;0b1111&#39;&amp;gt;&amp;gt;&amp;gt; oct(23)&#39;0o27&#39;&amp;gt;&amp;gt;&amp;gt; hex(13)&#39;0xd&#39;앞의 표기법을 제외한 값만을 얻고 싶을 때는 문자열 슬라이싱을 이용하면 됩니다.&amp;gt;&amp;gt;&amp;gt; bin(15)[2:]1111&amp;gt;&amp;gt;&amp;gt; oct(23)[2:]27반대로 2, 8, 16진법으로 표기된 숫자를 10진법으로 바꾸고 싶을 때는 다음과 같은 방법을 이용할 수 있습니다.&amp;gt;&amp;gt;&amp;gt; int(bin(15), 2)15&amp;gt;&amp;gt;&amp;gt; int(oct(23), 8)232, 8, 16진법으로 표기된 숫자를 사칙연산 하는 방법으로는 10진법으로 변환하여 사칙연산을 한 뒤 다시 해당하는 진법으로 변환합니다.bin(int(bin(15), 2) + int(oct(23), 8)) # 0b1111 + 0o27 을 계산하여 bin() 으로 감싸 결과를 2진법으로 변환한다4. 부동 소수점 연산 오류1부터 10까지 정수는 10개지만 실수는 무한히 많습니다.컴퓨터에서는 숫자를 비트로 표현하는데 실수는 유한개의 비트로 정확하게 표현할 수가 없습니다.따라서 실수는 유한개의 비트를 사용하여 근삿값으로 표현합니다.파이썬에서 0.1 + 0.2의 값은 0.3이 나올 것 같지만 실제로는 0.30000000000000004가 나옵니다.두 실수가 같은지 판단할 때는 ==을 사용하면 안 됩니다.&amp;gt;&amp;gt;&amp;gt; 0.1 + 0.2 == 0.3FalsePython 3.5 이상부터 math.isclose() 함수를 사용하여 두 실수가 같은지 확인할 수 있습니다.&amp;gt;&amp;gt;&amp;gt;import math&amp;gt;&amp;gt;&amp;gt;math.isclose(0.1 + 0.2, 0.3)True5. 숫자 자료형 관련 메소드dir() 내장 함수를 이용하면 해당 객체가 갖고 있는 변수와 메소드를 보여줍니다.(익숙하지 않은 객체를 사용해야할 경우 먼저 dir() 내장 함수를 통해 변수와, 메소드를 살펴볼 수 있어 굉장히 유용합니다.)&amp;gt;&amp;gt;&amp;gt; dir(a)[&#39;__abs__&#39;, &#39;__add__&#39;, &#39;__and__&#39;, &#39;__bool__&#39;, &#39;__ceil__&#39;, &#39;__class__&#39;, &#39;__delattr__&#39;, &#39;__dir__&#39;, &#39;__divmod__&#39;, &#39;__doc__&#39;,  &#39;__eq__&#39;, &#39;__float__&#39;,   &#39;__floor__&#39;, &#39;__floordiv__&#39;, &#39;__format__&#39;, &#39;__ge__&#39;, &#39;__getattribute__&#39;, &#39;__getnewargs__&#39;, &#39;__gt__&#39;, &#39;__hash__&#39;, &#39;__index__&#39;, &#39;__init__&#39;,   &#39;__init_subclass__&#39;, &#39;__int__&#39;, &#39;__invert__&#39;, &#39;__le__&#39;, &#39;__lshift__&#39;, &#39;__lt__&#39;, &#39;__mod__&#39;, &#39;__mul__&#39;, &#39;__ne__&#39;, &#39;__neg__&#39;, &#39;__new__&#39;, &#39;__or__&#39;,   &#39;__pos__&#39;,  &#39;__pow__&#39;, &#39;__radd__&#39;, &#39;__rand__&#39;, &#39;__rdivmod__&#39;, &#39;__reduce__&#39;, &#39;__reduce_ex__&#39;, &#39;__repr__&#39;, &#39;__rfloordiv__&#39;, &#39;__rlshift__&#39;, &#39;__rmod__&#39;,   &#39;__rmul__&#39;, &#39;__ror__&#39;, &#39;__round__&#39;, &#39;__rpow__&#39;, &#39;__rrshift__&#39;, &#39;__rshift__&#39;, &#39;__rsub__&#39;, &#39;__rtruediv__&#39;, &#39;__rxor__&#39;, &#39;__setattr__&#39;, &#39;__sizeof__&#39;,   &#39;__str__&#39;, &#39;__sub__&#39;, &#39;__subclasshook__&#39;, &#39;__truediv__&#39;, &#39;__trunc__&#39;, &#39;__xor__&#39;, &#39;bit_length&#39;, &#39;conjugate&#39;, &#39;denominator&#39;, &#39;from_bytes&#39;, &#39;imag&#39;,   &#39;numerator&#39;, &#39;real&#39;, &#39;to_bytes&#39;]이 중에 double undermethod ( ex). __abs__)를 제외한 속성에 유의해서 살펴보면 됩니다.예를 들어 bit_length 메소드의 경우 객체의 비트 길이를 리턴해줍니다.&amp;gt;&amp;gt;&amp;gt; a = 11&amp;gt;&amp;gt;&amp;gt; a.bit_length()4",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-13T21:01:35+09:00'>13 Mar 2021</time><a class='article__image' href='/python-data-type-number'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part2]: 파이썬 숫자 자료형'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-number'>Python Basic Series [Part2]: 파이썬 숫자 자료형</a> </h2><p class='article__excerpt'>Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative a...</p></div></div></div>"
    } ,
  
    {
      "title"    : "Python Basic Series [Part1]: 파이썬에서 데이터의 특성",
      "category" : "",
      "tags"     : "Python",
      "url"      : "/python-data-type-intro",
      "date"     : "Mar 12, 2021",
      "content"  : "Table of Contents  파이썬 데이터는 객체다          원시타입과 객체        타입  가변성  참조  복사          Alias      Shallow Copy      Deep Copy        참고파이썬 데이터는 객체다컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.원시타입과 객체C나 자바 같은 언어는 기본적으로 원시 타입을 제공합니다. 원시 타입은 메모리에 정확하게 타입 크기만큼의 공간을 할당하고 그 공간을 오로지 값으로 채워넣습니다. 배열이라면 요소들이 연속된 순서로 메모리에 배치될 것입니다.객체는 단순히 값 뿐만 아니라 여러 가지 정보를 함께 저장하고, 이를 이용해 여러 가지 작업(비트조작, 시프팅 등)을 수행할 수 있게됩니다. 하지만 이로 인해 메모리 점유율이 늘어나게 되고 계산 속도 또한 감소하게 되는 단점이 있습니다.타입            이름      타입      가변              불리언      bool      불변              정수      int      불변              부동소수점      float      불변              복소수      complex      불변              문자열      str      불변              튜플      tuple      불변              바이트      bytes      불변              프로즌 셋      frozenset      불변              리스트      list      가변              바이트 배열      bytearray      가변              셋      set      가변              딕셔너리      dict      가변      가변성값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)a = &quot;5&quot;print(type(a))print(id(a))a = int(a)print(type(a))print(id(a))------------------&amp;lt;class &#39;str&#39;&amp;gt;139861785283696&amp;lt;class &#39;int&#39;&amp;gt;139861784516640참조a = 5변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 a라는 변수에 5라는 값을 담는 것이 아니라 a라는 이름이 Int 객체 5를 참조하는 것 입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다. 차이는 객체가 불변이냐 가변이냐의 차이일 뿐입니다.a라는 이름과 객체의 메모리 주소의 매핑 관계는 네임스페이스에 키-밸류 형태로 저장되는데 이 때의 네임스페이스는 메모리 상에서 코드 영역 또는 데이터 영역에 저장된다고 합니다. (스택이나 힙 영역은 아니라고 함, 참고)예를 들어, 왼쪽 그림에서 a가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 a는 새로운 값을 참조합니다.반면 오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.복사복사와 관련해서 진짜 복사(copy) 기능을 하는 것이 있고 흉내만 내는 것도 있습니다. 대표적으로 3가지 케이스가 있는데 하나씩 살펴보도록 하겠습니다.복사란 기존의 값과 같은 값을 가지는 변수를 하나 더 생성하며 각각의 변수는 독립적이어야 한다Aliasa = [1, 2, 3]b = a대입 연산자(=)를 이용한 경우를 alias(별칭)라고 합니다. 말 그대로 ‘[1, 2, 3]이라는 리스트 객체가 a라는 이름을 가지고 있었는데 b라는 이름을 하나 더 가지게 되었다.’ 정도로 이해할 수 있습니다. 이렇게 되면 a가 가르키는 [1, 2, 3]이 바뀌게 되면 b도 따라서 바뀌게 됩니다.a[2] = 100a -&amp;gt; [1, 2, 100]b -&amp;gt; [1, 2, 100](예시: 가수 ‘비’가 있습니다. ‘비’의 본명은 ‘정지훈’입니다. 만약 ‘비’가 머리를 잘랐다면 ‘정지훈’의 머리도 잘립니다.)Shallow Copy리스트의 슬라이싱 기능인 :를 이용하면 alias보다 더 복사같이 느껴집니다. 이를 얕은 복사(Shallow Copy)라고 합니다. 얕은 복사는 어떤 경우에는 정말 복사의 기능을 합니다.a = [1, 2, 3]c = a[:]a[2] = 100a -&amp;gt; [1, 2, 100]c -&amp;gt; [1, 2, 3]이렇게 봤을 때는 충분히 복사의 기능을 하고 있습니다. 하지만 변경하고자 하는 요소가 가변 객체이면 진짜 복사가 아니었다는 것이 드러나게 됩니다.a = [1, 2, [3, 4, 5]]c = a[:]a[2][2] = 100a -&amp;gt; [1, 2, [3, 4, 100]]c -&amp;gt; [1, 2, [3, 4, 100]]새로운 리스트 객체를 생성하긴 하지만 원래의 리스트 객체와 같은 ob_item(요소들의 포인터목록)을 가지고 생성되기 때문에, 요소가 가변 객체일 경우 따라서 변하게 됩니다.Deep Copy파이썬의 내장 모듈인 copy를 사용하면 어떤 상황에서도 복사를 제공합니다. 이를 깊은 복사(Deep Copy)라고 합니다. 깊은 복사는 아예 요소 자체를 새로 생성하기 때문에 ob_item도 다른 값을 가지는, 다시 말해서 완전히 같은 값을 새로운 메모리에 할당한 복사가 일어나게 됩니다.깊은 복사는 어떠한 상황에서도 복사를 보장하기 때문에 안정된 코드를 제공하지만, 메모리 낭비가 발생할 수 있습니다.import copya = [1, 2, 3]d = copy.deepcopy(a)참고  파이썬 알고리즘 인터뷰 책  YABOONG: 자바 메모리 관리 - 스택 &amp;amp; 힙  nina, memory-management-in-python-the-basics",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-12T21:01:35+09:00'>12 Mar 2021</time><a class='article__image' href='/python-data-type-intro'> <img src='/images/python_logo.jpg' alt='Python Basic Series [Part1]: 파이썬에서 데이터의 특성'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/python-data-type-intro'>Python Basic Series [Part1]: 파이썬에서 데이터의 특성</a> </h2><p class='article__excerpt'>이를 변수를 통해 객체를 참조한다라고 합니다. 여기에는 예외가 없으며 심지어 문자와 숫자도 모두 객체입니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리",
      "category" : "",
      "tags"     : "MySQL",
      "url"      : "/mysql-series1",
      "date"     : "Mar 7, 2021",
      "content"  : "Table of Contents  DBMS          Database      DBMS      SQL      MySQL      DBMS의 종류      DBMS의 구조        SELECT문          SELECT      FROM      WHERE      ORDER BY      LIMIT      GROUP BY      HAVING      SELECT문의 작성순서와 실행순서      SQL에서 제공하는 함수      NULL 데이터 다루는 방법        마치며DBMS빅 데이터 시대에서 데이터 저장은 가장 중요한 부분 중 하나입니다. 힘들게 얻은 데이터를 저장하지 않는다면 큰 자원 낭비겠죠. 하지만 중요한 것은 단순히 저장에 그치는 것이 아니라, 어떤 식으로 저장해 그 후 데이터를 추가, 갱신, 삭제 할 때 문제(NULL, 중복 등)가 생기지 않도록 할 것인지에 대한 고민도 이루어져야 한다는 것 입니다. 이번 MySQL 시리즈에서 이런 문제들을 어떻게 해결할 것인지에 대해 공부해보도록 하겠습니다.Database데이터베이스는 데이터의 집합 또는 데이터 저장소라고 정의할 수 있습니다.DBMS데이터베이스를 보통 직접적으로 접근하지는 않습니다. 사용자들이 데이터베이스를 그냥 접근한다면 데이터의 일관성도 떨어질 것이고, 관리도 쉽지 않을 것 입니다. 이러한 이유로 데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS(DataBase Management System)이라고 합니다.DBMS는 데이터베이스를 구축하는 틀을 제공하고, 효율적으로 데이터를 검색하고 저장하는 기능을 제공합니다. 또한 응용 프로그램들이 데이터베이스에 접근할 수 있는 인터페이스를 제공하고, 장애에 대한 복구 기능, 사용자 권한에 따른 보안성 유지 기능 등을 제공합니다.SQLDBMS를 이용해 데이터베이스를 사용하게 됩니다. 그렇다면 저희는 DBMS와 소통하는 방법을 알아야 합니다. 여기서 DBMS와 소통하기 위한 언어를 SQL(Structured Query Language)라고 합니다. SQL을 이용하면 데이터베이스 조작에 필요한 모든 명령어를 DBMS에 전달함으로써 수행할 수 있습니다.MySQL처음 SQL이라는 언어는 IBM이라고 하는 회사에서 System/R이라는 DBMS와, 이것을 사용하기 위해 필요한 언어인 SEQUEL을 만들면서 처음 등장했습니다. 그런데 SEQUEL(Structured English Query Language)은 그 단어가 이미 다른 곳에서 사용되고 있다는 문제(상표권 문제) 때문에 그 이름이 SQL(Structured Query Language)로 변경되었습니다. 그러다가 1987년, 국제 표준화 기구(ISO)에서 SQL에 관한 국제 표준(ISO 9075:1987)이 제정되었습니다.하지만 우리가 실제로 사용하는 SQL은 이 국제 표준에 완벽히 부합하지는 않습니다. Oracle, Microsoft SQL Server, MySQL 등의 DBMS에서 지원되는 SQL이 표준을 완벽히 준수하지는 않는다는 뜻입니다. 그 이유는 다양하지만 일단 많은 DBMS 회사들이 성능 향상과 더 다양한 기능 제공을 위해서, 차라리 표준을 일부 벗어나는 것을 택하는 경우가 많기 때문입니다.MySQL은 가장 처음 MySQL AB라고 하는 스웨덴 회사에서 개발되었습니다. 현재는 인수 과정을 거쳐 Oracle의 소유입니다. 이로 인해 지금 Oracle에는 Oracle(회사명과 같은 DBMS)과 MySQL이라는 서비스를 함께 제공하고 있습니다.두 DBMS의 시장에서의 쓰임새를 보면 약간의 차이가 있습니다. 은행, 거래소 등과 같이 데이터 처리의 정확성, 운영의 안정성 등이 엄격하게 요구되는 분야에서는 오라클이 주로 사용되고 있고, 우리가 흔히 쓰는 앱, 웹 사이트 같은 서비스를 만들 때는 MySQL을 쓰는 경우가 많습니다.DBMS의 종류위와 같이 많은 회사에서 성능 향상과 목적에 맞게 SQL이라는 언어를 조금씩 변형, 개선하여 새로운 DBMS로 개발해왔습니다. 이러한 이유로 MySQL과 같이 ~SQL이라는 용어도 사실상은 그 언어를 지원하는 DBMS 자체를 의미하게 되었습니다. 그래서 약간 헷갈리지만 관계형 데이터를 위한 DBMS의 경우 RDBMS, 비 관계형 데이터를 위한 DBMS의 경우 NoSQL이라고 하게 되었습니다.RDBMS: MySQL, Oracle, MariaDB(MySQL 개발자들이 만든 오픈소스), SQLite 등NoSQL: MongoDB, ElasticSearch, Cassandra 등DBMS의 구조  client(클라이언트 프로그램): 유저의 데이터베이스 관련 작업을 위해, SQL을 입력할 수 있는 화면 등을 제공하는 프로그램  server(서버 프로그램): client로부터 SQL 문 등을 전달받아 데이터베이스 관련 작업을 직접 처리하는 프로그램MySQL에서 서버 프로그램의 이름은 mysqld, 클라이언트 프로그램 이름은 mysql입니다. mysql은 보통 CLI 환경에서 사용하는 프로그램입니다. CLI 환경이 아니라 GUI 환경에서 mysql을 사용하려면 mysql을 GUI 환경에서 사용할 수 있도록 해주는 프로그램을 사용하면 됩니다. 대표적으로 Oracle이 공식적으로 제공하는 MySQL Workbench라는 프로그램이 있습니다.SELECT문MySQL에서 데이터를 조회하거나 분석할 때 필요한 SELECT문에 대해서 간단히 정리해 보겠습니다.SELECT  특정 컬럼이나 컬럼의 연산 결과를 지정SELECT *SELECT addressSELECT height / weightSELECT MAX(age)SELECT MAX(age) AS max_ageSELECT     (CASE         WHEN age IS NOT NULL THEN age         ELSE &quot;N/A&quot;     END) AS ageFROM  기준이 되는 테이블 지정FROM customersFROM orders# 예시SELECT name FROM customers;WHERE  컬럼에 조건을 지정WHERE age = 20WHERE gender != &#39;m&#39;WHERE age &amp;gt;= 27WHERE age NOT BETWEEN 20 AND 30 # 20~30WHERE age IN (20, 30) # 20 or 30WHERE address LIKE &#39;서울%&#39;WHERE address LIKE &#39;%고양시%&#39;WHERE address LIKE BINARY &#39;%Kim%&#39; # Kim 매칭, kim 매칭 xWHERE email LIKE &#39;__@%&#39; # _는 임의의 문자 1개# 예시SELECT * FROM customers WHERE age &amp;gt; 25;ORDER BY  정렬 기준을 지정ORDER BY height ASCORDER BY height DESC# 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASC;LIMIT  보고자 하는 결과의 개수를 지정LIMIT 5 # 5개LIMIT 10, 5 # 10번째부터 5개# 예시SELECT name, age, height FROM customers WHERE MONTH(birthday) IN (4, 5, 6) ORDER BY height ASCLIMIT 3;GROUP BY  특정 컬럼의 값을 기준으로 그루핑  그루핑 하고나면 모든 함수연산 또한 그룹 단위로 실행GROUP BY genderGROUP BY countryGROUP BY country, genderGROUP BY SUBSTRING(address, 1, 2)# 예시SELECT genderFROM customersGROUP BY gender;SELECT gender, MAX(age)FROM customersGROUP BY gender;GROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUP# 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;;SELECT SUBSTRING(address, 1, 2) as region, gender, COUNT(*)FROM memberGROUP BY SUBSTRING(address, 1, 2), gender WITH ROLLUPHAVING region IS NOT NULLORDER BY region ASC, gender DESC;HAVING  그루핑된 결과에 조건을 지정HAVING region = &#39;서울&#39;# 예시SELECT    SUBSTRING(address, 1, 2) as region    COUNT(*)FROM customersGROUP BY SUBSTRING(address, 1, 2)HAVING region = &#39;서울&#39;🦊 WHERE과 HAVING의 차이점WHERE: 주어진 테이블의 전체 row에서 필터링을 하는 용도HAVING: GROUP BY 되고 난 후 row에서 필터링 하는 용도SELECT문의 작성순서와 실행순서작성 순서SELECT FROMWHEREGROUP BYHAVING ORDER BYLIMIT 실행 순서FROMWHERE GROUP BYHAVING SELECTORDER BYLIMIT SQL에서 제공하는 함수# 모든 데이터 타입COUNT(*)DISTINCT(gender)# 문자열 데이터 타입SUBSTRING(address, 1, 2) # address의 첫번째 문자에서 2개LENGTH(address)UPPER(address)LOWER(address)LPAD(address)RPAD(address)# 숫자 데이터 타입# 집계(aggregation) 함수MAX(height)MIN(weight)AVG(weight)# 산술(mathematical) 함수ABS(balance)CEIL(height)FLOOR(height)ROUND(height)# 날짜 및 시간 데이터 타입YEAR(birthday)MONTH(birthday)DAYOFMONTH(birthday)DATEDIFF(birthday, &#39;2002-01-01&#39;)# 예시SELECT * FROM customers WHERE MONTH(birthday) IN (4, 5, 6);NULL 데이터 다루는 방법WHERE address IS NULLWHERE address IS NOT NULL# COALESCE(a, b, c) 함수는 a, b, c 중 가장 먼저 NULL아닌 값 리턴COALESCE(height, &quot;키 정보 없음&quot;)COALESCE(height, weight * 2.5, &quot;키 정보 없음&quot;)# IFNULL(a, b) 함수는 a가 NULL 아니면 a, NULL이면 b 리턴IFNULL(height, &quot;키 정보 없음&quot;)# IF(condition, a, b) 함수는 condition이 True이면 a, False이면 b리턴IF(address IS NOT NULL, address, &quot;N/A&quot;)# CASE 함수CASE    WHEN address IS NOT NULL THEN address    ELSE N/AEND# 예시SELECT addressFROM customersWHERE address IS NOT NULL;SELECT COALESCE(height, &quot;키 정보 없음&quot;), COALESCE(gender, &quot;성별 정보 없음&quot;)FROM customers;SELECT IF(address IS NOT NULL, address, &quot;N/A&quot;)FROM customers;SELECT    CASE        WHEN address IS NOT NULL THEN address        ELSE N/A    ENDFROM customers;마치며여기까지 테이블 한 개에 대해서 데이터를 조회하고 분석하는 방법에 대해 살펴보았습니다. 다음 포스트에서는 테이블이 여러 개인 경우에 대해 데이터를 조회하고 분석할 때 필요한 문법에 대해 알아보겠습니다.MySQL 실습 제공 사이트",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2021-03-07T21:01:35+09:00'>07 Mar 2021</time><a class='article__image' href='/mysql-series1'> <img src='/images/mysql_1.png' alt='MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/mysql-series1'>MySQL을 이용한 데이터 조회 및 분석(1): SQL소개와 MySQL 문법 정리</a> </h2><p class='article__excerpt'>데이터베이스를 체계적으로 작동할 수 있도록 돕는 소프트웨어가 나오게 되었으며 이를 DBMS라고 한다</p></div></div></div>"
    } ,
  
    {
      "title"    : "Korean-to-French Translation",
      "category" : "",
      "tags"     : "",
      "url"      : "/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say",
      "date"     : "Nov 7, 2018",
      "content"  : "아직 글을 작성 중입니다.Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Sam Bark diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Sam Bark on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Sam Bark potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Sam Bark on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-11-07T00:00:00+09:00'>07 Nov 2018</time><a class='article__image' href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'> <img src='/images/08.jpg' alt='Korean-to-French Translation'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/what-you-do-speaks-so-loudly-that-I-cannot-hear-what-you-say'>Korean-to-French Translation</a> </h2><p class='article__excerpt'>한국어를 프랑스어로 번역해줍니다.</p></div></div></div>"
    } ,
  
    {
      "title"    : "Real Dubbing using Tacotron",
      "category" : "",
      "tags"     : "",
      "url"      : "/the-way-to-get-started-is-to-quit-talking-and-begin-doing",
      "date"     : "Apr 23, 2018",
      "content"  : "아직 글을 작성 중입니다.Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace Jeroen Bendeler diversity and empowerment.Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Synergistically evolvePodcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.Photo by Jeroen Bendeler on UnsplashLeverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.  The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable Jairph potentialities.PodcastingCollaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.Photo by Jairph on UnsplashCompletely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.",
      "article"  : "<div class='article col col-4 col-d-6 col-t-12 animate'> <div class='article__inner'> <div class='article__head'> <time class='article__date' datetime='2018-04-23T00:00:00+09:00'>23 Apr 2018</time><a class='article__image' href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'> <img src='/images/grafana_1.webp' alt='Real Dubbing using Tacotron'> </a></div><div class='article__content'> <h2 class='article__title'> <a href='/the-way-to-get-started-is-to-quit-talking-and-begin-doing'>Real Dubbing using Tacotron</a> </h2><p class='article__excerpt'>실제 배역 주인공의 목소리를 이용해 더빙을 합니다.</p></div></div></div>"
    } 
  
]

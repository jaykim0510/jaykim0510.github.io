<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Code Museum</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Thu, 25 Aug 2022 05:52:21 +0900</pubDate>
        <lastBuildDate>Thu, 25 Aug 2022 05:52:21 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>AWS Series [Part9]: AWS Analytics Service: Kinesis</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis&quot; id=&quot;markdown-toc-kinesis&quot;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis-data-streams&quot; id=&quot;markdown-toc-kinesis-data-streams&quot;&gt;Kinesis Data Streams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis-data-firehose&quot; id=&quot;markdown-toc-kinesis-data-firehose&quot;&gt;Kinesis Data Firehose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#youtube&quot; id=&quot;markdown-toc-youtube&quot;&gt;Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kinesis&quot;&gt;Kinesis&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time&lt;/li&gt;
  &lt;li&gt;Apache Kafka와 같은 용도로 사용한다&lt;/li&gt;
  &lt;li&gt;Apache Kafka와 아키텍처까지 비슷한 서비스로는  Managed Streaming for Apache Kafka(MSK)가 있다&lt;/li&gt;
  &lt;li&gt;Kinesis가 MSK보다 조금 더 일찍 서비스로 제공되었기 때문에 아직까지는 MSK 보다 통합성이 좋지만 점차 MSK도 나아지는 중&lt;/li&gt;
  &lt;li&gt;Kinesis가 더 일찍 서비스되었기 때문에 실무에서는 MSK보다 Kinesis를 더 많이 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;아래 영상은 Kinesis와 MSK의 특징과 차이를 자세히 설명해준다.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/9y-aCX5O3Ms&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;kinesis-data-streams&quot;&gt;Kinesis Data Streams&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Kafka의 Broker와 비슷한 용도&lt;/li&gt;
  &lt;li&gt;아래 그림은 설정 화면을 캡처한 것인데 크게 설정할게 없는 것 같다.
&lt;img src=&quot;/images/kinesis_1.png&quot; alt=&quot;&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;중복없는 전송과 같은 설정은 script(python의 boto3 라이브러리)로 해결하거나 Source에서 해결해야 하는 것 같다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kinesis Data Stream 서비스 자체에 Auto-scaling 설정 기능이 없다
    &lt;ul&gt;
      &lt;li&gt;Unlike some other AWS services, Kinesis does not provide a native auto-scaling solution like DynamoDB On-Demand or EC2 Auto Scaling. Therefore, there is a need for the right number of shards to be calculated for every stream based on the expected number of records and/or the size of the records&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아래와 같은 방법이 하나의 솔루션이 된다 
&lt;img src=&quot;/images/kinesis_3.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;kinesis-data-firehose&quot;&gt;Kinesis Data Firehose&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Kafka의 Connector와 비슷한 용도&lt;/li&gt;
  &lt;li&gt;Source로 설정가능한 값은 Kinesis Data Stream 또는 Direct PUT 뿐이다&lt;/li&gt;
  &lt;li&gt;Target으로 가능한 것은 S3, Redshift와 같은 것들이 있다&lt;/li&gt;
  &lt;li&gt;Source로 Direct PUT을 사용하면 Apache Kafka + Kinesis Firehose 조합도 가능할 것 같다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kinesis_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;youtube&quot;&gt;Youtube&lt;/h1&gt;

&lt;p&gt;Kinesis Data Stream과 Kinesis Data Firehose의 Use case를 포함해 데이터 파이프라인에 관한 좋은 인사이트를 제공해준다.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/S3vdTBbQ2YM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/&quot; target=&quot;_blank&quot;&gt;AWS docs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.whizlabs.com/blog/aws-kinesis-data-streams-vs-aws-kinesis-data-firehose/&quot; target=&quot;_blank&quot;&gt;whizlabs, AWS Kinesis Data Streams vs AWS Kinesis Data Firehose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=S3vdTBbQ2YM&amp;amp;list=LL&amp;amp;index=8&quot; target=&quot;_blank&quot;&gt;Youtube AWS Korea: AWS에서 데이터 분석을 시작하기 위한 실시간, 배치 데이터 수집 방법 알아보기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/slalom-data-ai/amazon-kinesis-data-streams-auto-scaling-the-number-of-shards-105dc967bed5&quot; target=&quot;_blank&quot;&gt;Brandon Stanley, Amazon Kinesis Data Streams: Auto-scaling the number of shards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series9</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series9</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part8]: AWS [Database, Analytics] Service: Redshift</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#use-case&quot; id=&quot;markdown-toc-use-case&quot;&gt;Use Case&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples&quot; id=&quot;markdown-toc-examples&quot;&gt;Examples&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-생성&quot; id=&quot;markdown-toc-클러스터-생성&quot;&gt;클러스터 생성&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#스키마-생성&quot; id=&quot;markdown-toc-스키마-생성&quot;&gt;스키마 생성&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#쿼리-및-분석&quot; id=&quot;markdown-toc-쿼리-및-분석&quot;&gt;쿼리 및 분석&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Amazon Redshift is a fully managed(setting up, operating, and scaling a data warehouse, provisioning capacity, monitoring and backing up the cluster), petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.&lt;/p&gt;

&lt;p&gt;The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today.&lt;/p&gt;

&lt;p&gt;Redshift is an OLAP-style (Online Analytical Processing) column-oriented database. It is based on PostgreSQL version 8.0.2. This means regular SQL queries can be used with Redshift. But this is not what separates it from other services. The fast delivery to queries made on a large database with exabytes of data is what helps Redshift stand out.&lt;/p&gt;

&lt;p&gt;Fast querying is made possible by Massively Parallel Processing design or MPP. The technology was developed by ParAccel. With MPP, a large number of computer processors work in parallel to deliver the required computations. Sometimes processors situated across multiple servers can be used to deliver a process.&lt;/p&gt;

&lt;h1 id=&quot;use-case&quot;&gt;Use Case&lt;/h1&gt;

&lt;p&gt;Amazon Redshift is used when the data to be analyzed is humongous. The data has to be at least of a petabyte-scale (1015 bytes) for Redshift to be a viable solution. The MPP technology used by Redshift can be leveraged only at that scale. Beyond the size of data, there are some specific use cases that warrant its use.&lt;/p&gt;

&lt;p&gt;(쿼리의 성능이 극대화됨 -&amp;gt; 대용량 데이터 or 실시간 분석에 적합 -&amp;gt; 그 외의 경우 요구사항 대비 지나친 성능으로 낭비가 될 수 있음)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;more than petabyte-scale&lt;/li&gt;
  &lt;li&gt;processing real-time analytics&lt;/li&gt;
  &lt;li&gt;combining multiple data sources&lt;/li&gt;
  &lt;li&gt;business intelligence&lt;/li&gt;
  &lt;li&gt;log analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;examples&quot;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&quot;클러스터-생성&quot;&gt;클러스터 생성&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;노드 유형, 개수는 작게 시작하는 것이 좋다 (Redshift는 비싸니까)&lt;/li&gt;
  &lt;li&gt;IAM 역할을 제대로 지정안하면 안됨 -&amp;gt; 나의 경우 Athena, Glue, S3의 FullAccess를 이용
    &lt;ul&gt;
      &lt;li&gt;처음에 RedshiftFullAccess도 추가해줬었는데 왜인지 모르겠지만 에러남&lt;/li&gt;
      &lt;li&gt;(왜 Redshift를 이용할 때 RedshiftFullAccess를 추가하면 에러가 날까)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;스키마-생성&quot;&gt;스키마 생성&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Glue의 Catalog가 있으면 Redshift를 사용할 때도 정말 편하다&lt;/li&gt;
  &lt;li&gt;Catalog 없으면 [Create Schema] -&amp;gt; [Create Table] -&amp;gt; [Load Data] 해줘야됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;쿼리-및-분석&quot;&gt;쿼리 및 분석&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MongoDB에서 저장할 때 requirements라는 속성을 array형태로 저장했었다&lt;/li&gt;
  &lt;li&gt;array가 있으면 쿼리시 에러가 난다 -&amp;gt; unnesting을 진행했다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Redshift&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/redshift/latest/dg/query-super.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Querying semistructured data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cloudzero.com/blog/aws-redshift&quot; target=&quot;_blank&quot;&gt;CLOUDZERO, AWS Redshift 101: What Is It and When Should You Use It?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series8</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series8</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part7]: AWS Database Service: DynamoDB</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-dynamodb&quot; id=&quot;markdown-toc-what-is-dynamodb&quot;&gt;What is DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#features-of-dynamodb&quot; id=&quot;markdown-toc-features-of-dynamodb&quot;&gt;Features of DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#components-of-dynamodb&quot; id=&quot;markdown-toc-components-of-dynamodb&quot;&gt;Components of DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storing-data-in-dynamodb&quot; id=&quot;markdown-toc-storing-data-in-dynamodb&quot;&gt;Storing Data in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#querying-data-in-dynamodb&quot; id=&quot;markdown-toc-querying-data-in-dynamodb&quot;&gt;Querying Data in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#configuration-in-dynamodb&quot; id=&quot;markdown-toc-configuration-in-dynamodb&quot;&gt;Configuration in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#use-case&quot; id=&quot;markdown-toc-use-case&quot;&gt;Use Case&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pricing&quot; id=&quot;markdown-toc-pricing&quot;&gt;Pricing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dynamodb-provisioned-capacity&quot; id=&quot;markdown-toc-dynamodb-provisioned-capacity&quot;&gt;DynamoDB Provisioned Capacity&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dynamodb-on-demand-pricing&quot; id=&quot;markdown-toc-dynamodb-on-demand-pricing&quot;&gt;DynamoDB On-demand Pricing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-dynamodb&quot;&gt;What is DynamoDB&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Amazon DynamoDB is a cloud-native fully managed NoSQL primarily key-value database.&lt;/p&gt;

&lt;p&gt;DynamoDB’s NoSQL design is oriented towards simplicity and scalability, which appeal to developers and devops teams respectively. It can be used for a wide variety of semistructured data-driven applications prevalent in modern and emerging use cases beyond traditional databases, from the Internet of Things (IoT) to social apps or massive multiplayer games. With its broad programming language support, it is easy for developers to get started and to create very sophisticated applications using DynamoDB.&lt;/p&gt;

&lt;p&gt;While we cannot describe exactly what DynamoDB is, we can describe how you interact with it. When you set up DynamoDB on AWS, you do not provision specific servers or allocate set amounts of disk. Instead, you provision throughput — you define the database based on provisioned capacity — how many transactions and how many kilobytes of traffic you wish to support per second. Users specify a service level of read capacity units (RCUs) and write capacity units (WCUs).&lt;/p&gt;

&lt;p&gt;DynamoDB needed to “provide fast performance at any scale,” allowing developers to “start small with just the capacity they need and then increase the request capacity of a given table as their app grows in popularity.” Predictable performance was ensured by provisioning the database with guarantees of throughput, measured in “capacity units” of reads and writes. “Fast” was defined as single-digit milliseconds, based on data stored in Solid State Drives (SSDs).&lt;/p&gt;

&lt;p&gt;DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don’t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.&lt;/p&gt;

&lt;p&gt;You can scale up or scale down your tables’ throughput capacity without downtime or performance degradation. You can use the AWS Management Console to monitor resource utilization and performance metrics.&lt;/p&gt;

&lt;p&gt;DynamoDB allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant. For more information, see Expiring items by using DynamoDB Time to Live (TTL).&lt;/p&gt;

&lt;h1 id=&quot;features-of-dynamodb&quot;&gt;Features of DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;NoSQL primarily key-value (and document using JSON) database service&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fully Managed Distributed Systems -&amp;gt; Stable Performance&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Managed&lt;/strong&gt; — provided ‘as-a-Service’ so users would not need to maintain the database&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt; — automatically provision hardware on the backend, invisible to the user&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Durable&lt;/strong&gt; and highly available — multiple availability zones for failures/disaster recovery&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integrates well with other AWS services&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Built-in support for ACID transactions&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encryption at rest&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;On-demand backup&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Point-in-time recovery&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;components-of-dynamodb&quot;&gt;Components of DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Attribute&lt;/strong&gt;: &lt;strong&gt;single field&lt;/strong&gt; that is attached to an item. Key-value pair&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Item&lt;/strong&gt;: &lt;strong&gt;unique set of attributes&lt;/strong&gt; in a table. set of key-value pair&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Table&lt;/strong&gt;: &lt;strong&gt;group of items&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Primary Key&lt;/strong&gt;: The primary key &lt;strong&gt;uniquely identifies each item in the table&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;therefore, no two items can have the same key&lt;/li&gt;
      &lt;li&gt;primary key could be just &lt;strong&gt;partition key&lt;/strong&gt; or &lt;strong&gt;partition key + sort key&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partiton Key&lt;/strong&gt;: key for determining physical storage in which the item will be stored
    &lt;ul&gt;
      &lt;li&gt;input to an internal hash function. the output from the hash function determines the partition&lt;/li&gt;
      &lt;li&gt;필수 지정값. primary key로 partition key만 지정되면 partition key는 고유한 값을 가져야함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sort Key&lt;/strong&gt;: 동일한 파티션 키를 공유하는 모든 항목을 정렬하거나 검색하는데 이용 (선택 사항)
    &lt;ul&gt;
      &lt;li&gt;partition key + sort key -&amp;gt; Referred to as a composite primary key&lt;/li&gt;
      &lt;li&gt;All items with the same partition key value are stored together, in sorted order by sort key value.&lt;/li&gt;
      &lt;li&gt;In a table that has a partition key and a sort key, it’s possible for multiple items to have the same partition key value. However, those items must have different sort key values.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Secondary Indexes&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesn’t require that you use indexes, but they give your applications more flexibility when querying your data. After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table.&lt;/li&gt;
      &lt;li&gt;Global secondary index – An index with a partition key and sort key that can be different from those on the table.&lt;/li&gt;
      &lt;li&gt;Local secondary index – An index that has the same partition key as the table, but a different sort key.&lt;/li&gt;
      &lt;li&gt;In the example Music table shown previously, you can query data items by Artist (partition key) or by Artist and SongTitle (partition key and sort key). What if you also wanted to query the data by Genre and AlbumTitle? To do this, you could create an index on Genre and AlbumTitle, and then query the index in much the same way as you’d query the Music table.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;storing-data-in-dynamodb&quot;&gt;Storing Data in DynamoDB&lt;/h1&gt;

&lt;p&gt;A key-value store holds for each key a single value. Arguably, if the value can be an entire document, you can call this database a “document store”. In this sense, DynamoDB is a document store. The DynamoDB API lets you conveniently store a JSON document as the value, and also read or writes part of this document directly instead of reading or writing the entire document (although, you actually pay for reading and writing the entire document).&lt;/p&gt;

&lt;p&gt;보통은 Key-value store로 쓴다. Document store로 사용하고 싶은 경우 MongoDB의 AWS 버전인 DocumentDB를 쓰는 것을 추천한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DynamoDB is a “wide column” style of NoSQL database. While the schema isn’t defined beyond the primary key at table construction time, the querying abilities are limited to primary keys or secondary indexes. Creating Global Secondary Indexes allows you to query against other attribute values. Local Secondary Indexes can be queried too, but they’re a bit of an odd duck. See here for a good comparison of the two secondary index types.&lt;/p&gt;

&lt;p&gt;If your needs do include querying inside the attributes, check out some of the “document-oriented” style of NoSQL databases, of which MongoDB is the one most people think of. If you’re already embedded in the AWS ecosystem and don’t want to break out of it, AWS offers DocumentDB as a MongoDB-compatible service managed by AWS.&lt;/p&gt;

&lt;p&gt;Wide-column and document-style data stores have different pro’s &amp;amp; cons. Generally-speaking, the wide-column approach is better for extreme scalability at consistent cost &amp;amp; speed, whereas the document-oriented approach gives more flexibility as your data access patterns evolve over time. Choose the one that suits your needs the best.&lt;/p&gt;

&lt;h1 id=&quot;querying-data-in-dynamodb&quot;&gt;Querying Data in DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;PartiQL (SQL like)&lt;/li&gt;
  &lt;li&gt;Primary Key 또는 Global Secondary Indexes(GSI)에 대해서만 쿼리 가능&lt;/li&gt;
  &lt;li&gt;Filter는 쿼리 이후 결과를 제한하는 용도&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;일반 Attribute에 대해서는 쿼리가 안된다.&lt;/p&gt;

&lt;p&gt;아래와 같이 원하는 Attribute를 GSI로 만들고 나면 쿼리가 가능해진다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;configuration-in-dynamodb&quot;&gt;Configuration in DynamoDB&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 설정은 우선 기본 설정으로 테이블을 만든뒤 이후 설정값으르 수정할 수 도 있다.&lt;/p&gt;

&lt;p&gt;그 밖에 테이블 생성 이후 설정할 수 있는 설정값은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;use-case&quot;&gt;Use Case&lt;/h1&gt;

&lt;p&gt;In this tutorial, you will create a bookstore application that showcases a product catalog. Products typically contain unique identifiers and attributes such as descriptions, quantities, locations, and prices. The method for retrieving these types of attributes (specifically, the access pattern) is often a key-value lookup based on the product’s unique identifier. This means that an application can retrieve these other attributes when a product’s unique identifier is provided.&lt;/p&gt;

&lt;p&gt;While the product catalog can start off with a few products, it should have the ability to scale to billions if needed without having to be re-architected or requiring a different database. It should also maintain fast, predictable performance at any scale for these key-value lookups. With these requirements in mind, Amazon DynamoDB is a good fit as the durable system of record for the bookstore because it offers low latency performance and scales as your application grows. Another benefit is that you do not need to manage any servers or clusters.&lt;/p&gt;

&lt;h1 id=&quot;pricing&quot;&gt;Pricing&lt;/h1&gt;
&lt;p&gt;DynamoDB can be extremely expensive to use. There are two pricing structures to choose from: provisioned capacity and on-demand capacity.&lt;/p&gt;

&lt;h2 id=&quot;dynamodb-provisioned-capacity&quot;&gt;DynamoDB Provisioned Capacity&lt;/h2&gt;
&lt;p&gt;In this Amazon DynamoDB Pricing Plan, you’re billed hourly per the use of operational capacity units (or read and write capacity units). You can control costs by specifying the maximum amount of resources needed by each database table being managed. The provisioned capacity provides autoscaling and dynamically adapts to an increase in traffic. However, it does not implement autoscaling for sudden changes in data traffic unless that’s enabled.&lt;/p&gt;

&lt;p&gt;You should use provisioned capacity when:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You have an idea of the maximum workload your application will have&lt;/li&gt;
  &lt;li&gt;Your application’s traffic is consistent and does not require scaling (unless you enable the autoscaling feature, which costs more)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dynamodb-on-demand-pricing&quot;&gt;DynamoDB On-demand Pricing&lt;/h2&gt;
&lt;p&gt;This plan is billed per request units (or read and write request units). You’re only charged for the requests you make, making this a truly serverless choice. This choice can become expensive when handling large production workloads, though. The on-demand capacity method is perfect for autoscaling if you’re not sure how much traffic to expect.&lt;/p&gt;

&lt;p&gt;Knowing which capacity best suits your requirements is the first step in optimizing your costs with DynamoDB. Here are some factors to consider before making your choice.&lt;/p&gt;

&lt;p&gt;You should use on-demand capacity when:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You’re not sure about the workload your application will have&lt;/li&gt;
  &lt;li&gt;You don’t know how consistent your application’s data traffic will be&lt;/li&gt;
  &lt;li&gt;You only want to pay for what you use&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.scylladb.com/learn/dynamodb/introduction-to-dynamodb/&quot; target=&quot;_blank&quot;&gt;scylladb, Introduction to DynamoDB [추천]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2k2GINpO308&amp;amp;list=LL&amp;amp;index=14&quot; target=&quot;_blank&quot;&gt;Youtube, Be A Better Dev: AWS DynamoDB Tutorial For Beginners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Core components of Amazon DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/67646412/how-is-it-possible-for-dynamodb-to-support-both-key-value-and-document-database&quot; target=&quot;_blank&quot;&gt;stackoverflow: How is it possible for DynamoDB to support both Key-Value and Document database properties at the same time&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/47521164/aws-dynamodb-query-based-on-non-primary-keys&quot; target=&quot;_blank&quot;&gt;stackoverflow: AWS DynamoDB Query based on non-primary keys&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cloudforecast.io/blog/dynamodb-pricing/&quot; target=&quot;_blank&quot;&gt;cloudforecast: DynamoDB Pricing and Cost Optimization Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series7</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series7</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part10]: AWS Analytics Service: Glue, Athena</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aws-glue&quot; id=&quot;markdown-toc-aws-glue&quot;&gt;AWS Glue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aws-athena&quot; id=&quot;markdown-toc-aws-athena&quot;&gt;AWS Athena&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#s3--glue-catalog--athena--glue-etl-환상의-조합&quot; id=&quot;markdown-toc-s3--glue-catalog--athena--glue-etl-환상의-조합&quot;&gt;S3 + Glue Catalog + Athena + Glue ETL: 환상의 조합&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#glue-catalog&quot; id=&quot;markdown-toc-glue-catalog&quot;&gt;Glue Catalog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#glue-etl&quot; id=&quot;markdown-toc-glue-etl&quot;&gt;Glue ETL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;데이터와 관련된 업무를 하다보면 실제로 데이터를 분석하고, 머신러닝과 같은 분야에 활용하는 시간은 약 30%, 나머지 시간은 수집, 적재, 변환과 같은 ETL 작업에 대부분의 시간을 할애하게 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 번째 문제(낮은 데이터 품질)를 해결하도록 도와주는 AWS 서비스에는 대표적으로 다음과 같은 것들이 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;aws-glue&quot;&gt;AWS Glue&lt;/h1&gt;

&lt;p&gt;AWS Glue is a fully managed ETL service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.&lt;/p&gt;

&lt;p&gt;AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so there’s no infrastructure to set up or manage.&lt;/p&gt;

&lt;p&gt;No schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you want&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;aws-athena&quot;&gt;AWS Athena&lt;/h1&gt;

&lt;p&gt;Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.&lt;/p&gt;

&lt;p&gt;Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automatically—running queries in parallel—so results are fast, even with large datasets and complex queries.&lt;/p&gt;

&lt;h1 id=&quot;s3--glue-catalog--athena--glue-etl-환상의-조합&quot;&gt;S3 + Glue Catalog + Athena + Glue ETL: 환상의 조합&lt;/h1&gt;

&lt;p&gt;데이터 웨어하우스로 가기 전 단계에 활용하기 좋은 것 같다. 보통 S3와 같은 데이터 레이크에는 raw-data가 많기 때문에 이러한 데이터를 클렌징하고, 가공하는 과정이 수반되어야 하는데 이러한 작업들을 AWS Glue가 해준다.&lt;/p&gt;

&lt;p&gt;또한 S3의 데이터를 다른 곳으로 옮기기 전에 먼저 데이터를 분석하고 싶은 경우가 많다. 어떤 데이터가 있고, 스키마가 어떻고, 어떤 데이터를 옮기면 좋을지, 어떤 데이터가 가치가 있을지를 먼저 S3에서 충분히 탐색해야 한다. 이러한 기능을 하는 것이 바로 AWS Athena이다. Athena를 이용하면 쿼리를 통해 S3의 데이터를 탐색/분석할 수 있다. 근데 Athena는 반드시 Data Catalog에서 쿼리를 진행한다. (카탈로그(Catalog): 데이터에 대한 하나의 단일화된 뷰)&lt;/p&gt;

&lt;p&gt;따라서 S3를 다른 데이터 웨어하우스로 옮기기 전에 먼저 Glue를 통해 데이터를 카탈로그화 하고, 그 카탈로그를 Athena를 이용해 분석하고 다시 Glue의 ETL 작업을 통해 클렌징, 가공해 데이터 웨어하우스로 옮겨주는 것이 데이터 파이프라인의 좋은 예이다.&lt;/p&gt;

&lt;p&gt;참고로 크롤러를 통해 카탈로그화 시킬 수 있는 데이터 소스는 S3뿐만 아니라 DynamoDB, DocumentDB, DataLake 등이 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;glue-catalog&quot;&gt;Glue Catalog&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;크롤러를 주기적으로 실행시킴으로써 스키마 변경을 감지하고 관리해줌&lt;/li&gt;
  &lt;li&gt;스키마의 버전을 관리하고 해당 스키마에 맞는 일관된 데이터 뷰 제공(대표적으로 Athena, EMR, Redshift에 제공)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;glue-etl&quot;&gt;Glue ETL&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ETL 작업을 그래프로 시각화하여 쉽게 파이프라인을 만들 수 있음&lt;/li&gt;
  &lt;li&gt;스케줄링 기능도 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(이상하게 S3에 저장하는 부분에서 데이터가 사라짐.. 실제로 S3에 저장은 되지만 크기가 0Byte..)&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html&quot; target=&quot;_blank&quot;&gt;AWS docs, What is AWS Glue?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=LkkgtNtuEoU&amp;amp;list=LL&amp;amp;index=24&quot; target=&quot;_blank&quot;&gt;Youtube, AWS Korea: AWS Glue를 통한 손쉬운 데이터 전처리 작업하기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dQnRP6X8QAU&amp;amp;list=LL&amp;amp;index=17&quot; target=&quot;_blank&quot;&gt;Youtube, Johny Chivers: AWS Glue Tutorial for Beginners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series10</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series10</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part28]: Distributed System</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-시스템이란&quot; id=&quot;markdown-toc-분산-시스템이란&quot;&gt;분산 시스템이란&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#장점&quot; id=&quot;markdown-toc-장점&quot;&gt;장점&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#performance&quot; id=&quot;markdown-toc-performance&quot;&gt;Performance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#scalability&quot; id=&quot;markdown-toc-scalability&quot;&gt;Scalability&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#availability&quot; id=&quot;markdown-toc-availability&quot;&gt;Availability&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#필요한-것&quot; id=&quot;markdown-toc-필요한-것&quot;&gt;필요한 것&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#partitioning&quot; id=&quot;markdown-toc-partitioning&quot;&gt;Partitioning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#replication&quot; id=&quot;markdown-toc-replication&quot;&gt;Replication&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#어려운-점&quot; id=&quot;markdown-toc-어려운-점&quot;&gt;어려운 점&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#극복&quot; id=&quot;markdown-toc-극복&quot;&gt;극복&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;분산-시스템이란&quot;&gt;분산 시스템이란&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/dis_sys_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;나는 분산 시스템을 다음과 같은 맥락으로 정리하려고 한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dis_sys_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;장점&quot;&gt;장점&lt;/h1&gt;

&lt;h2 id=&quot;performance&quot;&gt;Performance&lt;/h2&gt;

&lt;p&gt;여기서 Performance는 단일 시스템에서의 Performance와 비교해 가격대비 더 낫다는 의미이다. Performance의 절대적인 수치 자체가 더 오를 이유는 없다. 오히려 네트워크 비용으로 감소할 가능성은 있다. 그럼에도 분산 시스템을 쓰는 이유는 가격적인 측면에서 그만큼 값싼 장비를 여러 대 사용하는 것이 낫고, 성능적인 측면 이외에도 분산 시스템이 주는 장점이 있기 때문이다.&lt;/p&gt;

&lt;h2 id=&quot;scalability&quot;&gt;Scalability&lt;/h2&gt;

&lt;p&gt;분산 시스템은 서비스 규모, 트래픽량, 작업량에 따라 시스템의 크기를 조절해 이를 핸들링할 능력이 있다. 물론 단일 시스템에서도 Vertical-scaling이 가능하다. 하지만 Horizontal-scaling이 가격적인 측면과 확장이 용이하다는 점에서 이점이 있다.&lt;/p&gt;

&lt;h2 id=&quot;availability&quot;&gt;Availability&lt;/h2&gt;

&lt;p&gt;분산 시스템은 노드 일부에 장애가 발생하더라도 계속 같은 기능을 유지할 수 있다. 이는 24시간 내내 장애없는 서비스가 가능하다는 말이다. 물론 이를 위해 요구되는 조건들이 있는데 이 부분은 뒤에서 더 자세히 다룰 것이다.&lt;/p&gt;

&lt;p&gt;지금까지 분산 시스템의 장점에 대해서 얘기했다. 이러한 장점을 얻기위해 해야할 일이 있다. 우선 High-level에서 이에 대해 알아보겠다.&lt;/p&gt;

&lt;h1 id=&quot;필요한-것&quot;&gt;필요한 것&lt;/h1&gt;

&lt;h2 id=&quot;partitioning&quot;&gt;Partitioning&lt;/h2&gt;

&lt;p&gt;파티셔닝은 분산 시스템의 장점 중에서도 Scalability, Performance를 얻기 위해 필요한 핵심이다. 파티셔닝은 처리(또는 저장)해야 할 데이터를 작게 나누어서 이를 처리(또는 저장)해야 할 노드에게 할당해주는 작업이다.&lt;/p&gt;

&lt;p&gt;하지만 오히려 하나의 작업을 위해 네트워크를 거쳐 여러 노드에 접근해야 한다는 점에서 단점이 되는 경우도 있다. 그래서 데이터를 처리할 때는 최대한 네트워크 비용을 줄이는 것이 관건이다.&lt;/p&gt;

&lt;p&gt;파티셔닝은 크게 range partitioning, hash partitioning, consistent hashing가 있다. Apache HBase는 range partitioning을 쓰고, Apache Cassandra는 consistent hasing을 쓴다.&lt;/p&gt;

&lt;h2 id=&quot;replication&quot;&gt;Replication&lt;/h2&gt;

&lt;p&gt;복제(Replication)는 Availability를 위해 필요한 핵심이다. 복제는 같은 데이터를 여러 노드에 복수 저장함으로써, 노드 중 일부에 장애가 발생하더라도 계속 역할을 유지할 수 있게 한다.&lt;/p&gt;

&lt;p&gt;복제하는 것이 단순한 일은 아니다. 우선 복제 수 만큼 더 많은 저장용량이 필요하다. 또한 복사된 후에도 원본 데이터가 업데이트 될 때마다 동기화해야 한다.&lt;/p&gt;

&lt;p&gt;복제 방법 중 하나인 primary-backup replication에 대해 알아보자.&lt;/p&gt;

&lt;p&gt;We commonly refer to the remaining replicas as followers or secondaries. These can only handle read requests. Every time the leader receives an update, it executes it locally and also propagates the update to the other nodes. This ensures that all the replicas maintain a consistent view of the data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dis_sys_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;리더는 업데이트를 어떻게 팔로워들에게 전파할까&lt;/p&gt;

&lt;p&gt;There are two ways to propagate the updates: synchronously and asynchronously.&lt;/p&gt;

&lt;p&gt;Synchronous replication&lt;/p&gt;

&lt;p&gt;In synchronous replication, the node replies to the client to indicate the update is complete—only after receiving acknowledgments from the other replicas that they’ve also performed the update on their local storage. This guarantees that the client is able to view the update in a subsequent read after acknowledging it, no matter which replica the client reads from.&lt;/p&gt;

&lt;p&gt;Furthermore, synchronous replication provides increased durability. This is because the update is not lost even if the leader crashes right after it acknowledges the update.&lt;/p&gt;

&lt;p&gt;However, this technique can make writing requests slower. This is because the leader has to wait until it receives responses from all the replicas.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dis_sys_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Asynchronous replication&lt;/p&gt;

&lt;p&gt;In asynchronous replication, the node replies to the client as soon as it performs the update in its local storage, without waiting for responses from the other replicas.&lt;/p&gt;

&lt;p&gt;This technique increases performance significantly for write requests. This is because the client no longer pays the penalty of the network requests to the other replicas.&lt;/p&gt;

&lt;p&gt;However, this comes at the cost of reduced consistency and decreased durability. After a client receives a response for an update request, the client might read older (stale) values in a subsequent read. This is only possible if the operation happens in one of the replicas that have not yet performed the update. Moreover, if the leader node crashes right after it acknowledges an update, and the propagation requests to the other replicas are lost, any acknowledged update is eventually lost.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dis_sys_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Most widely used databases, such as PostgreSQL or MySQL, use a primary-backup replication technique that supports both asynchronous and synchronous replication.&lt;/p&gt;

&lt;p&gt;primary-backup replication에는 장점과 단점이 있습니다.&lt;/p&gt;

&lt;p&gt;장점&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is simple to understand and implement&lt;/li&gt;
  &lt;li&gt;Concurrent operations serialized in the leader node, remove the need for more complicated, distributed concurrency protocols. In general, this property also makes it easier to support transactional operations&lt;/li&gt;
  &lt;li&gt;It is scalable for read-heavy workloads, because the capacity for reading requests can be increased by adding more read replicas&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;단점&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is not very scalable for write-heavy workloads, because a single node (the leader)’s capacity determines the capacity for writes&lt;/li&gt;
  &lt;li&gt;It imposes an obvious trade-off between performance, durability, and consistency&lt;/li&gt;
  &lt;li&gt;Scaling the read capacity by adding more follower nodes can create a bottleneck in the network bandwidth of the leader node, if there’s a large number of followers listening for updates&lt;/li&gt;
  &lt;li&gt;The process of failing over to a follower node when the leader node crashes, is not instant. This may create some downtime and also introduce the risk of errors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;또한 primary-backup replication은 항상 리더가 존재해야 한다. 따라서 리더가 잘 살아있는지 체크하고, 리더가 죽었다면 리더를 새로 선출하는 Leader election 문제도 고려해야 한다.&lt;/p&gt;

&lt;h1 id=&quot;어려운-점&quot;&gt;어려운 점&lt;/h1&gt;

&lt;h1 id=&quot;극복&quot;&gt;극복&lt;/h1&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://thesecretlivesofdata.com/raft/&quot; target=&quot;_blank&quot;&gt;Video Demo,  Raft: Understandable Distributed Consensus&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/in-search-of-an-understandable-consensus-algorithm-a-summary-4bc294c97e0d/&quot; target=&quot;_blank&quot;&gt;Understanding the Raft consensus algorithm: an academic article summary&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-2-data-replication/&quot; target=&quot;_blank&quot;&gt;Building a Distributed Log from Scratch, Part 2: Data Replication&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Tue, 02 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series28</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series28</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part27]: Redis</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Redis: an open source, in-memory key-value database that supports abstract data structures like lists, hashes, strings, and sets&lt;/p&gt;

&lt;p&gt;in this article i’m going to be covering everything you need to know about redis. starting with what redis is moving on to how to install it all of the commands you need to know and then finally ending up with a real world example of how you would implement redis yourself. this is a really important video to watch because redis can be included in every single production level application and it’s going to make your application more performant. so let’s get started now.&lt;/p&gt;

&lt;p&gt;redis is essentially a type of database and more specifically it is a nosql database. but it’s not very similar to any other nosql databases. it’s definitely not similar to mongodb and it’s obviously very different than sql databases like postgres and mysql and this is because redis doesn’t really have any idea of tables or documents and instead all of the data in redis is
stored inside of key value pairs. so think about a json object you have the key name and you have the value kyle. this is essentially a key value pair and redis is just like one giant json object that has key value pairs and that’s all that you have inside of redis. so it’s not very good at storing a bunch of structured data like you have in sql .but it’s really good for storing you know individual key value pairs that you need to access or get data from another.&lt;/p&gt;

&lt;p&gt;important thing to note about redis is that unlike a normal database that runs on your disk and stores all your information on disk. redis actually runs inside of your working memory your ram on your computer and this means that redis is incredibly fast because it’s all working inside of ram. but it’s much more unstable because if all of a sudden your system crashes you’re going to lose everything that’s in redis unless you’re backing it up consistently, which is why redis is generally not used as like an actual persistent database store like you have with mongodb and postgres and instead it’s used more &lt;strong&gt;for caching&lt;/strong&gt; where you take things that are really you know things that you access a lot or things that take a long time to compute and you store those values inside of redis that way when you need to access them in the future. it’s incredibly quick since redis is in the memory already loaded. it’s milliseconds to get data as opposed to hundreds of milliseconds or even seconds of time to get data from a traditional database.&lt;/p&gt;

&lt;p&gt;really the important thing to realize about redis is that it’s going to be built on top of a traditional database. almost always you’re going to have your mongodb or postgres database in the background and you’re going to have redis sitting in front of that database and any time that you have a really long or slow query to your database or you have data you access all the time but doesn’t change that much what you’re going to do is you’re going to store that data inside of redis as well as inside your database. and then when you go to get that information if it’s already in redis you can access that data in milliseconds as opposed to going all the way to the database. computing the data and then coming all the way back which is going to take you hundreds to even thousands of milliseconds depending on how complex your data is. so redis is going to take your app and make it hundreds to even thousands of times faster when it comes to querying these pieces of information.&lt;/p&gt;

&lt;p&gt;let’s actually talk about how we can install redis. installing redis on your computer is really simple if you have a mac or linux computer. if you use mac just use homebrew to do the install and if you’re on linux just use your package manager of choice to install it. it’s just called redis it’s that simple. but if you’re on windows it’s a bit more complex. because there is no way to install redis on windows. instead you need to go through the windows subsystem for linux which is pretty simple to install.&lt;/p&gt;
</description>
                <pubDate>Tue, 02 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series27</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series27</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part26]: I 🤍 Logs(2) Data Integration</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#two-complications&quot; id=&quot;markdown-toc-two-complications&quot;&gt;Two Complications&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-is-more-diverse&quot; id=&quot;markdown-toc-data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-explosion-of-specialized-data-systems&quot; id=&quot;markdown-toc-the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#log-structured-data-flow&quot; id=&quot;markdown-toc-log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experience-at-linkedin&quot; id=&quot;markdown-toc-experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relationship-to-etl-and-data-warehouse&quot; id=&quot;markdown-toc-relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#etl-and-scalability&quot; id=&quot;markdown-toc-etl-and-scalability&quot;&gt;ETL and Scalability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#where-should-we-put-the-data-transformations&quot; id=&quot;markdown-toc-where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#decoupling-systems&quot; id=&quot;markdown-toc-decoupling-systems&quot;&gt;Decoupling Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scaling-a-log&quot; id=&quot;markdown-toc-scaling-a-log&quot;&gt;Scaling a Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Integration and Logs&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data Integration means making available all the data (that an organization has) to all the services and systems that need it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The more recognizable term ETL(populating a relational data warehouse) usually covers only a limited part of data integration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Capturing all the relevant data and being able to put it together in an applicable processing environment&lt;/li&gt;
  &lt;li&gt;This data has to be modeled in a uniform way to make it easy to read and process&lt;/li&gt;
  &lt;li&gt;Process this data in various ways: MapReduce, real-time query systems, ans so on&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Focus on step-by-step&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reliable and complete data flow&lt;/li&gt;
  &lt;li&gt;Refining data modeling and consistency&lt;/li&gt;
  &lt;li&gt;Better visualization, reporting, algorithmic processing and prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How can we build reliable data flow throughout all the data systems?&lt;/p&gt;

&lt;h1 id=&quot;two-complications&quot;&gt;Two Complications&lt;/h1&gt;

&lt;p&gt;Two things have made data integration an increasingly difficult proflem.&lt;/p&gt;

&lt;h2 id=&quot;data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/h2&gt;

&lt;p&gt;Transactional data - things that &lt;strong&gt;are&lt;/strong&gt;,  Event data - things that &lt;strong&gt;happen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Log = Data structure what event data is logged&lt;/p&gt;

&lt;p&gt;Event data is generated from Web service, Financial organization, IoT&lt;/p&gt;

&lt;p&gt;This type of event data shakes up traditional data integration approaches because it tends to be several orders of magnitude larger than transactional data.&lt;/p&gt;

&lt;h2 id=&quot;the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/h2&gt;

&lt;p&gt;ex. OLAP, Search service, Batch processing, Graph analysis&lt;/p&gt;

&lt;h1 id=&quot;log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/h1&gt;

&lt;p&gt;Log is the natural problem data structure for handling data flow between systems.&lt;/p&gt;

&lt;h1 id=&quot;experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/h1&gt;

&lt;h1 id=&quot;relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/h1&gt;

&lt;h1 id=&quot;etl-and-scalability&quot;&gt;ETL and Scalability&lt;/h1&gt;

&lt;h1 id=&quot;where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/h1&gt;

&lt;h1 id=&quot;decoupling-systems&quot;&gt;Decoupling Systems&lt;/h1&gt;

&lt;h1 id=&quot;scaling-a-log&quot;&gt;Scaling a Log&lt;/h1&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewEng.laf?barcode=9781491909386&amp;amp;ejkGb=BNT&amp;amp;mallGb=ENG&quot; target=&quot;_blank&quot;&gt;책 I Heart Logs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/logika-io/try-kill-batch-processing-with-unified-log-stream-processing-d92709117f74&quot; target=&quot;_blank&quot;&gt;Try Kill batch processing with unified log stream processing…&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series26</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series26</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part25]: I 🤍 Logs(1) Introduction</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-a-log&quot; id=&quot;markdown-toc-what-is-a-log&quot;&gt;What Is a Log?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-database&quot; id=&quot;markdown-toc-logs-in-database&quot;&gt;Logs in Database&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-distributed-system&quot; id=&quot;markdown-toc-logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#log-centric-design-pattern&quot; id=&quot;markdown-toc-log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-a-log&quot;&gt;What Is a Log?&lt;/h1&gt;

&lt;p&gt;Yet other than perhaps occasionally tailing a log file, most engineers don’t think much about logs. To help remedy that, I’ll give an overview of how logs work in distributed systems, and then give some practical applications of these concepts to a variety of common uses: data integration, enterprise architecture, real-time data processing, and data system design.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[2022-07-02 05:30:44] method=POST user=crazyboy0510 path=/movies/comment-create/ movie_id=16
[2022-07-02 05:30:57] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:15] method=GET user=crazyboy0510 path=/movies/movie-play/16 movie_id=16
[2022-07-02 05:31:18] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:19] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Every programmer is familiar with this kind of log - a series of loosely structured requests, errors, or other messages in a sequence of rotating text files.&lt;/p&gt;

&lt;p&gt;The purpose of logs quickly becomes an input to queries in order to understand behavior across many machines, something that English text in files is not nearly as appropriate for as the kind of structured log I’ll be talking about.&lt;/p&gt;

&lt;p&gt;The log I’ll be discussing is a little more general and closer to what in the database or systems called a ‘commit log’. It is append-only sequence of records ordered by time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each rectangle represents a record that was appended to the log. Records are stored in the order they were appended. The contents and format of the records aren’t important for the purposes of this discussion. To be concrete, we can just imagine each record to be a JSON blob.&lt;/p&gt;

&lt;p&gt;The log entry number can be thought of as the ‘timestamp’ of the entry. this is convenient property of being decoupled from any particular physical clock. This property is essential as we get to distributed systems.&lt;/p&gt;

&lt;p&gt;A log is just kind of table or file where the records are sorted by time&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;table: array of records  
file: array of bytes  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However it is important that we thing about the log as an &lt;strong&gt;abstract data structure, not a text file&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Logs have a specific purpose: &lt;strong&gt;they record what happened and when&lt;/strong&gt;. For distributed data systems, this is the heart of the problem.&lt;/p&gt;

&lt;h1 id=&quot;logs-in-database&quot;&gt;Logs in Database&lt;/h1&gt;

&lt;p&gt;The usage in databases has to do with keeping in sync a variety of data structures and indexes in the presence of crashes. To make this atomic and durable, a database uses a log to write out information about the records it will be modifying before applying the changes to all the various data structures that it maintains.&lt;/p&gt;

&lt;p&gt;The log is the record of what happened, and each table or index is a projection of this history into some useful data structure or index.&lt;/p&gt;

&lt;p&gt;Over time, the usage of the log grew from &lt;strong&gt;an implementation detail of the ACID database properties&lt;/strong&gt; to a &lt;strong&gt;method for replicating data between databases&lt;/strong&gt;. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a remote replica database in sync. Oracle, MySQL, PostreSQL, and MongoDB include log shipping protocols to transmit portions of a log to replica databases that act as slaves. The slaves can then apply the changes recorded in the log to their own local data structures to stay in sync with the master.&lt;/p&gt;

&lt;p&gt;In fact, the use of logs is variations on the two uses in database internals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The log is used as a publish/subscribe mechanism to transmit data to other replicas&lt;/li&gt;
  &lt;li&gt;The log is used as a consistency mechanism to order the updates that are applied to multiple replicas&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/h1&gt;

&lt;p&gt;The same problems that databases solve with logs (like distributing data to replicas and agreeing on update order) are among the most fundamental problems for all distributed systems.&lt;/p&gt;

&lt;p&gt;The log-centric approach to distributed systems arises from a simple observation&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Desterministic means that the processing isn’t timing dependent)&lt;/p&gt;

&lt;p&gt;The application to distributed computing is pretty obvious. You can reduce the problem of making multiple machines all do the same thing to the problem of implementaing a consistent log to feed input to theses processes. The purpose of the log here is to squeeze all the nondeterminism out of the input stream to ensure that each replica stays in sync.&lt;/p&gt;

&lt;p&gt;Discrete log entry numbers act as a clock for the state of the replicas - you can describe the state of each replica by a single number: the timestamp for the maximum log entry that it has processed. Two replicas at the same time will be in the same state.&lt;/p&gt;

&lt;h2 id=&quot;log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/h2&gt;

&lt;p&gt;There are many variations on how this principle can be applied., depending on what is put in the log. For example, we can log the incoming requests to a service and have each replica process these independently. Or we can have one instance that processed requests and log the state changes that the service undergoes in response to a request.&lt;/p&gt;

&lt;p&gt;Database people generally differentiate between physical and logical logging. Physical or row-based logging means logging the contents of each row that is changed (로우별 실제 변경된 데이터를 저장하는 것). Logical or statement logging  means logging the SQL commands that lead to the row changes (insert, update, and delete statements).&lt;/p&gt;

&lt;p&gt;The distributed systems distinguished two broad approaches to processing and replication. The &lt;strong&gt;state machine model&lt;/strong&gt; keep a log of the incoming requests and &lt;strong&gt;each replica processes each request&lt;/strong&gt; in log order. &lt;strong&gt;primary backup&lt;/strong&gt; elect one replica as the leader. This leader processes requests in the order they arrive and logs the changes to its state that occur as a result of processing the requests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series25</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series25</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Coding Test Series [Part22]: 행렬(Matrix) - 문제</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix&quot; id=&quot;markdown-toc-matrix&quot;&gt;Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;matrix&quot;&gt;Matrix&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://leetcode.com/tag/matrix/&quot; target=&quot;_blank&quot;&gt;Leetcode: Two-Pointers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;💟 ✅ ❎&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;문제 리스트
---------------------------------------------EASY 3문제
- Richest Customer Wealth
- Flipping an Image
- Matrix Diagonal Sum
- The K Weakest Rows in a Matrix
- Toeplitz Matrix
- Shift 2D Grid
- Transpose Matrix

---------------------------------------------MEDIUM 5문제
- Sort the Matrix Diagonally
- Remove All Ones With Row and Column Flips
- Candy Crush
- Max Area of Island
- Rotate Image
- Sparse Matrix Multiplication
- Game of Life
- Construct Quad Tree
- Spiral Matrix II
- Walls and Gates
- Number of Islands
- Rotting Oranges
- Shortest Path in Binary Matrix
- Word Search
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/matrix</link>
                <guid isPermaLink="true">http://localhost:4000/matrix</guid>
                
                <category>Coding_Test</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part15] IntelliJ IDE를 이용한 데이터베이스 시각화</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/help/idea/creating-diagrams.html&quot; target=&quot;_blank&quot;&gt;IntelliJ: Database diagrams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series15</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series15</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
    </channel>
</rss>
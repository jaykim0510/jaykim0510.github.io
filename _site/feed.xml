<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Jay Tech</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Thu, 27 Jan 2022 09:12:22 +0900</pubDate>
        <lastBuildDate>Thu, 27 Jan 2022 09:12:22 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>Kafka Series [Part3]: Fault tolerance in Kafka</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#fault-tolerance-in-kafka&quot; id=&quot;markdown-toc-fault-tolerance-in-kafka&quot;&gt;Fault tolerance in Kafka&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#카프카-리플리케이션replication&quot; id=&quot;markdown-toc-카프카-리플리케이션replication&quot;&gt;카프카 리플리케이션(Replication)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#리더leader와-팔로워follower&quot; id=&quot;markdown-toc-리더leader와-팔로워follower&quot;&gt;리더(Leader)와 팔로워(Follower)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#컨트롤러controller&quot; id=&quot;markdown-toc-컨트롤러controller&quot;&gt;컨트롤러(Controller)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#리플리케이션-과정&quot; id=&quot;markdown-toc-리플리케이션-과정&quot;&gt;리플리케이션 과정&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;fault-tolerance-in-kafka&quot;&gt;Fault tolerance in Kafka&lt;/h1&gt;
&lt;p&gt;카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;카프카-리플리케이션replication&quot;&gt;카프카 리플리케이션(Replication)&lt;/h2&gt;
&lt;p&gt;카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;다른 브로커에 파티션을 복제&lt;/code&gt;해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.&lt;/p&gt;

&lt;p&gt;그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;리더와 팔로워&lt;/code&gt;가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_14.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;(shwitha B G 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;리더leader와-팔로워follower&quot;&gt;리더(Leader)와 팔로워(Follower)&lt;/h2&gt;
&lt;p&gt;카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_15.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.&lt;/p&gt;

&lt;h2 id=&quot;컨트롤러controller&quot;&gt;컨트롤러(Controller)&lt;/h2&gt;
&lt;p&gt;리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;컨트롤러&lt;/code&gt;라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_16.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;(shwitha B G 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;주키퍼(Zookeeper)&lt;/code&gt; 개념이 잠깐 등장합니다.&lt;br /&gt;
(&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Zookeeper&lt;/code&gt; is the centralized service for storing metadata of topic, partition, and broker)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.&lt;/li&gt;
  &lt;li&gt;브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다&lt;/li&gt;
  &lt;li&gt;컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.&lt;/li&gt;
  &lt;li&gt;컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;리플리케이션-과정&quot;&gt;리플리케이션 과정&lt;/h2&gt;
&lt;p&gt;마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.&lt;br /&gt;
먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ISR(InSyncReplica)&lt;/code&gt;이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.&lt;br /&gt;
ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.&lt;/p&gt;

&lt;p&gt;그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)&lt;/p&gt;

&lt;p&gt;ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;하이워터마크(high water mark)&lt;/code&gt;라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_17.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message 3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 message 3을 읽어갔지만, 이제는 더이상 message 3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.&lt;/p&gt;
&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackernoon.com/apache-kafkas-distributed-system-firefighter-the-controller-broker-1afca1eae302&quot; target=&quot;_blank&quot;&gt;Hackernoon 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;Ashwitha B G 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 24 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series3</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series3</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#로컬환경&quot; id=&quot;markdown-toc-로컬환경&quot;&gt;로컬환경&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#미니큐브minikube&quot; id=&quot;markdown-toc-미니큐브minikube&quot;&gt;미니큐브(minikube)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#docker-desktop&quot; id=&quot;markdown-toc-docker-desktop&quot;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kindkubernetes-in-docker&quot; id=&quot;markdown-toc-kindkubernetes-in-docker&quot;&gt;kind(Kubernetes in Docker)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#클라우드환경&quot; id=&quot;markdown-toc-클라우드환경&quot;&gt;클라우드환경&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gkegoogle-kubernetes-engine&quot; id=&quot;markdown-toc-gkegoogle-kubernetes-engine&quot;&gt;GKE(Google Kubernetes Engine)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ekselastic-kubernetes-service&quot; id=&quot;markdown-toc-ekselastic-kubernetes-service&quot;&gt;EKS(Elastic Kubernetes Service)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;로컬환경&quot;&gt;로컬환경&lt;/h1&gt;
&lt;p&gt;쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;테스트, 개발 환경에 적합&lt;/code&gt;합니다.&lt;/p&gt;

&lt;h2 id=&quot;미니큐브minikube&quot;&gt;미니큐브(minikube)&lt;/h2&gt;
&lt;p&gt;미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;단일 노드 구성&lt;/code&gt;이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요&lt;/code&gt;합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;minikube

minikube version
&lt;span class=&quot;c&quot;&gt;# minikube version: v1.25.1&lt;/span&gt;

minikube start &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker &lt;span class=&quot;c&quot;&gt;# --kubernetes-version 옵션으로 버전 선택 가능&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------&lt;/span&gt;
😄  Darwin 12.1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;arm64&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 의 minikube v1.25.1
✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중
👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중
🚜  베이스 이미지를 다운받는 중 ...
💾  쿠버네티스 v1.23.1 을 다운로드 중 ...
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/
🔥  Creating docker container &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CPUs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2, &lt;span class=&quot;nv&quot;&gt;Memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;7903MB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중
    ▪ kubelet.housekeeping-interval&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5m
    ▪ 인증서 및 키를 생성하는 중 ...
    ▪ 컨트롤 플레인이 부팅...
    ▪ RBAC 규칙을 구성하는 중 ...
🔎  Kubernetes 구성 요소를 확인...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  애드온 활성화 : storage-provisioner, default-storageclass
🏄  끝났습니다! kubectl이 &lt;span class=&quot;s2&quot;&gt;&quot;minikube&quot;&lt;/span&gt; 클러스터와 &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube status
&lt;span class=&quot;nt&quot;&gt;--------------------&lt;/span&gt;
minikube
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

minikube ip
&lt;span class=&quot;c&quot;&gt;# 192.168.49.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;정지하고 삭제하는 명령어도 간단합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube stop

minikube delete
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;docker-desktop&quot;&gt;Docker Desktop&lt;/h2&gt;
&lt;p&gt;Docker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/kube_24.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;kindkubernetes-in-docker&quot;&gt;kind(Kubernetes in Docker)&lt;/h2&gt;
&lt;p&gt;minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;멀티 노드 클러스터&lt;/code&gt;를 구축할 수 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kind.sigs.k8s.io&quot; target=&quot;_blank&quot;&gt;(kind 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kind

kind version
&lt;span class=&quot;nt&quot;&gt;--------------------&lt;/span&gt;
kind v0.11.1 go1.17.2 darwin/arm64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.&lt;/p&gt;

&lt;p&gt;(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# kind로 클러스터 구축을 위한 kind.yaml&lt;/span&gt;
apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
- role: control-plane
  image: kindest/node:v1.23.1
- role: control-plane
  image: kindest/node:v1.23.1
- role: control-plane
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Creating cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
 ✓ Ensuring node image &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;kindest/node:v1.23.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 🖼
 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦
 ✓ Configuring the external load balancer ⚖️
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
 ✗ Joining worker nodes 🚜 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
- role: control-plane
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Creating cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
 ✓ Ensuring node image &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;kindest/node:v1.23.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 🖼
 ✓ Preparing nodes 📦 📦 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
 ✓ Joining worker nodes 🚜
Set kubectl context to &lt;span class=&quot;s2&quot;&gt;&quot;kind-kindcluster&quot;&lt;/span&gt;
You can now use your cluster with:

kubectl cluster-info &lt;span class=&quot;nt&quot;&gt;--context&lt;/span&gt; kind-kindcluster

Have a &lt;span class=&quot;nb&quot;&gt;nice &lt;/span&gt;day! 👋
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;클러스터가 성공적으로 구축되었습니다.&lt;br /&gt;
쿠버네티스에서 실행중인 노드를 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
----------------------------------------------------------------------------
NAME                        STATUS   ROLES                  AGE   VERSION
kindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1
kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;클러스터는 다음 명령어로 삭제하시면 됩니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind delete cluster &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;------------------------------------------&lt;/span&gt;
Deleting cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;클라우드환경&quot;&gt;클라우드환경&lt;/h1&gt;

&lt;h2 id=&quot;gkegoogle-kubernetes-engine&quot;&gt;GKE(Google Kubernetes Engine)&lt;/h2&gt;

&lt;h2 id=&quot;ekselastic-kubernetes-service&quot;&gt;EKS(Elastic Kubernetes Service)&lt;/h2&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791165216283&quot; target=&quot;_blank&quot;&gt;쿠버네티스 완벽 가이드 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/k8s/guide/&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series4</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series4</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part3]: Kubernetes Service</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes-network&quot; id=&quot;markdown-toc-kubernetes-network&quot;&gt;Kubernetes Network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#clusterip&quot; id=&quot;markdown-toc-clusterip&quot;&gt;ClusterIP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nodeport&quot; id=&quot;markdown-toc-nodeport&quot;&gt;NodePort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#loacbalancer&quot; id=&quot;markdown-toc-loacbalancer&quot;&gt;LoacBalancer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ingress&quot; id=&quot;markdown-toc-ingress&quot;&gt;Ingress&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-외부의-로드-밸런서만을-이용한-ingress&quot; id=&quot;markdown-toc-클러스터-외부의-로드-밸런서만을-이용한-ingress&quot;&gt;클러스터 외부의 로드 밸런서만을 이용한 Ingress&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-내부의-ingress-파드를-곁들인-ingress&quot; id=&quot;markdown-toc-클러스터-내부의-ingress-파드를-곁들인-ingress&quot;&gt;클러스터 내부의 Ingress 파드를 곁들인 Ingress&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kubernetes-network&quot;&gt;Kubernetes Network&lt;/h1&gt;
&lt;p&gt;쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost&lt;/code&gt;로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;파드의 IP 주소&lt;/code&gt;로 통신하면 됩니다. 또한 노드 간의 통신은 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VXLAN이나 L2 Routing&lt;/code&gt;을 이용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_25.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러나 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로드 밸런싱&lt;/li&gt;
  &lt;li&gt;서비스 디스커버리&lt;/li&gt;
  &lt;li&gt;클러스터 내부 DNS&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;clusterip&quot;&gt;ClusterIP&lt;/h1&gt;
&lt;p&gt;ClusterIP는 서비스의 기본 타입입니다. ClusterIP 서비스를 생성하면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;클러스터 내부에서만 통신 가능한 가상 IP&lt;/code&gt;가 할당됩니다. kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_26.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;nodeport&quot;&gt;NodePort&lt;/h1&gt;
&lt;p&gt;NodePort는 모든 노드의 IP주소:포트에서 수신한 트래픽을 컨테이너에 전송하는 형태로 외부와 통신할 수 있습니다. NodePort는 전체 노드 N개 중 임의의 노드의 IP주소를 외부에 노출합니다. 그럼에도 ClusterIP를 통해 다른 노드의 파드로 통신하는데에는 문제 없습니다. 그러나 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)&lt;/code&gt;이 되기 때문에 NodePort만을 이용해 외부와 통신하는 것은 분명한 한계점이 있습니다. 또한 NodePort는 쿠버네티스에서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;지정한 범위(30000~32767)&lt;/code&gt; 안에서만 지정할 수 있기 때문에 서비스로 활용하기에는 포트 번호가 예쁘지는 않습니다. 노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;loacbalancer&quot;&gt;LoacBalancer&lt;/h1&gt;
&lt;p&gt;LoadBalancer에서는 NodePort와 다르게 별도로 외부 로드 밸런서를 사용하기 때문에 노드 장애가 발생해도 크게 문제가 되지 않습니다. 노드에 장애가 발생한 경우 해당 노드를 목적지에서 제외 처리하고 트래픽을 전송하지 않게됩니다. LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 할당됩니다. 실제 서비스 운영 환경에서는 외부로부터 요청을 수신하는 External IP 주소를 DNS 설정 등의 이유로 고정하는 것을 선호하고 LoadBalancer 서비스는 이를 지원합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_28.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ingress&quot;&gt;Ingress&lt;/h1&gt;
&lt;p&gt;인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind: Ingress&lt;/code&gt;타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;클러스터-외부의-로드-밸런서만을-이용한-ingress&quot;&gt;클러스터 외부의 로드 밸런서만을 이용한 Ingress&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GKE 인그레스&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.&lt;br /&gt;
&lt;img src=&quot;../../images/kube_29.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;클러스터-내부의-ingress-파드를-곁들인-ingress&quot;&gt;클러스터 내부의 Ingress 파드를 곁들인 Ingress&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Nginx 인그레스&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.&lt;/p&gt;

&lt;p&gt;Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.&lt;br /&gt;
&lt;img src=&quot;../../images/kube_30.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791165216283&quot; target=&quot;_blank&quot;&gt;쿠버네티스 완벽 가이드 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/k8s/guide/&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/65887993/when-to-choose-loadbalancer-over-nodeport-service-typeor-vice-versa-in-kub&quot; target=&quot;_blank&quot;&gt;NodePort vs LoadBalancer stackoverflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview&quot; target=&quot;_blank&quot;&gt;Google Kubernetes Engine 가이드&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series3</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series3</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part2]: Main elements of Kafka</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kafka의-주요-구성요소&quot; id=&quot;markdown-toc-kafka의-주요-구성요소&quot;&gt;Kafka의 주요 구성요소&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#topic-partition-segment&quot; id=&quot;markdown-toc-topic-partition-segment&quot;&gt;Topic, Partition, Segment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#producer&quot; id=&quot;markdown-toc-producer&quot;&gt;Producer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#메세지-전송과정&quot; id=&quot;markdown-toc-메세지-전송과정&quot;&gt;메세지 전송과정&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#라운드-로빈round-robbin-방식&quot; id=&quot;markdown-toc-라운드-로빈round-robbin-방식&quot;&gt;라운드 로빈(Round-Robbin) 방식&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#스티키-파티셔닝sticky-partitioning-방식&quot; id=&quot;markdown-toc-스티키-파티셔닝sticky-partitioning-방식&quot;&gt;스티키 파티셔닝(Sticky Partitioning) 방식&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#중복-없는-전송&quot; id=&quot;markdown-toc-중복-없는-전송&quot;&gt;중복 없는 전송&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#정확히-한-번-전송&quot; id=&quot;markdown-toc-정확히-한-번-전송&quot;&gt;정확히 한 번 전송&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#broker&quot; id=&quot;markdown-toc-broker&quot;&gt;Broker&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#consumer&quot; id=&quot;markdown-toc-consumer&quot;&gt;Consumer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#컨슈머-오프셋-관리&quot; id=&quot;markdown-toc-컨슈머-오프셋-관리&quot;&gt;컨슈머 오프셋 관리&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#그룹-코디네이터&quot; id=&quot;markdown-toc-그룹-코디네이터&quot;&gt;그룹 코디네이터&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#파티션-할당-전략&quot; id=&quot;markdown-toc-파티션-할당-전략&quot;&gt;파티션 할당 전략&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#라운드-로빈-파티션-할당-전략&quot; id=&quot;markdown-toc-라운드-로빈-파티션-할당-전략&quot;&gt;라운드 로빈 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#스티키-파티션-할당-전략&quot; id=&quot;markdown-toc-스티키-파티션-할당-전략&quot;&gt;스티키 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#협력적-스티키-파티션-할당-전략&quot; id=&quot;markdown-toc-협력적-스티키-파티션-할당-전략&quot;&gt;협력적 스티키 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#마치며&quot; id=&quot;markdown-toc-마치며&quot;&gt;마치며&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kafka의-주요-구성요소&quot;&gt;Kafka의 주요 구성요소&lt;/h1&gt;
&lt;p&gt;Kafka는 크게 3가지로 이루어 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Producer: Kafka로 메시지를 보내는 모든 클라이언트&lt;/li&gt;
  &lt;li&gt;Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버&lt;/li&gt;
  &lt;li&gt;Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_7.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html&quot; target=&quot;_blank&quot;&gt;(참고: cloudkarafka)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;topic-partition-segment&quot;&gt;Topic, Partition, Segment&lt;/h2&gt;
&lt;p&gt;Kafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)&lt;/li&gt;
  &lt;li&gt;Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)&lt;/li&gt;
  &lt;li&gt;Segment: 메시지가 실제로 저장되는 파일&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_6.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html&quot; target=&quot;_blank&quot;&gt;(참고: cloudkarafka)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;producer&quot;&gt;Producer&lt;/h2&gt;
&lt;p&gt;프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_13.webp&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://dzone.com/articles/take-a-deep-dive-into-kafka-producer-api&quot; target=&quot;_blank&quot;&gt;(Dzone 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;메세지-전송과정&quot;&gt;메세지 전송과정&lt;/h3&gt;
&lt;p&gt;프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ProducerRecord&lt;/code&gt;라고 하는 형태로 전송되며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Topic&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Value&lt;/code&gt;는 필수값이며, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Partition&lt;/code&gt;과 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Key&lt;/code&gt;는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;프로듀서에서 send() 메소드 호출&lt;/li&gt;
  &lt;li&gt;Serializer는 JSON, String, Avro 등의 object를 bytes로 변환&lt;/li&gt;
  &lt;li&gt;ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달&lt;/li&gt;
  &lt;li&gt;Partition이 지정되지 않았을 때, Key값이 지정되었다면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Partitioner가 Key값을 바탕으로&lt;/code&gt; 해당 파티션에 전달&lt;/li&gt;
  &lt;li&gt;Partition, Key값이 모두 없으면 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;라운드 로빈(Round-Robbin)&lt;/code&gt;방식 또는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;스티키 파티셔닝(Sticky Partitioning)&lt;/code&gt; 방식으로 메세지를 파티션에 할당&lt;/li&gt;
  &lt;li&gt;파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;배치로 전송할 수도 있음&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;라운드-로빈round-robbin-방식&quot;&gt;라운드 로빈(Round-Robbin) 방식&lt;/h3&gt;
&lt;p&gt;프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_30.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.&lt;/p&gt;

&lt;h3 id=&quot;스티키-파티셔닝sticky-partitioning-방식&quot;&gt;스티키 파티셔닝(Sticky Partitioning) 방식&lt;/h3&gt;
&lt;p&gt;라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_29.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_31.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.confluent.io/en-gb/blog/apache-kafka-producer-improvements-sticky-partitioner/&quot; target=&quot;_blank&quot;&gt;(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;중복-없는-전송&quot;&gt;중복 없는 전송&lt;/h3&gt;

&lt;h3 id=&quot;정확히-한-번-전송&quot;&gt;정확히 한 번 전송&lt;/h3&gt;

&lt;h2 id=&quot;broker&quot;&gt;Broker&lt;/h2&gt;
&lt;p&gt;브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.&lt;/p&gt;

&lt;h2 id=&quot;consumer&quot;&gt;Consumer&lt;/h2&gt;
&lt;p&gt;컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;파티션 할당 전략&lt;/li&gt;
  &lt;li&gt;프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가&lt;/li&gt;
  &lt;li&gt;컨슈머의 개수가 파티션보다 많지는 않은가&lt;/li&gt;
  &lt;li&gt;컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;컨슈머-오프셋-관리&quot;&gt;컨슈머 오프셋 관리&lt;/h3&gt;
&lt;p&gt;컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것&lt;/code&gt;이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt;라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;컨슈머가 다음으로 읽어야 할 위치&lt;/code&gt;를 말합니다.&lt;/p&gt;

&lt;p&gt;참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;그룹-코디네이터&quot;&gt;그룹 코디네이터&lt;/h3&gt;
&lt;p&gt;컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.&lt;/p&gt;

&lt;p&gt;이렇게 컨슈머 그룹내의 변화를 감지하기 위해 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;트래킹하는 것이 바로 그룹 코디네이터&lt;/code&gt;입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더&lt;/code&gt;입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_28.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.&lt;/p&gt;

&lt;p&gt;이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;파티션-할당-전략&quot;&gt;파티션 할당 전략&lt;/h3&gt;

&lt;h4 id=&quot;라운드-로빈-파티션-할당-전략&quot;&gt;라운드 로빈 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.&lt;/p&gt;

&lt;h4 id=&quot;스티키-파티션-할당-전략&quot;&gt;스티키 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.&lt;/p&gt;

&lt;p&gt;동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_33.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.&lt;/p&gt;

&lt;h4 id=&quot;협력적-스티키-파티션-할당-전략&quot;&gt;협력적 스티키 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.&lt;/p&gt;

&lt;p&gt;이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/&quot; target=&quot;_blank&quot;&gt;(컨플루언트 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;마치며&quot;&gt;마치며&lt;/h1&gt;
&lt;p&gt;이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.&lt;/p&gt;

&lt;p&gt;카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;장애 발생에 대처 가능한 안정적인 서비스를 제공&lt;/code&gt;해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;높은 처리량&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활&lt;/code&gt;하게 해주는 것이 좋습니다.&lt;/p&gt;

&lt;p&gt;다음 포스트에서는 이러한 역할을 어떤 방식으로 제공해주었는지 살펴보며 그 과정에서 필요한 개념들을 하나씩 배워가도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;amp;ejkGb=KOR&amp;amp;barcode=9791189909345&quot; target=&quot;_blank&quot;&gt;실전 카프카 개발부터 운영까지 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/take-a-deep-dive-into-kafka-producer-api&quot; target=&quot;_blank&quot;&gt;Dzone 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/codex/apache-kafka-series-part-1-introduction-to-apache-kafka-9b890832002&quot; target=&quot;_blank&quot;&gt;CodeX 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series2</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series2</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part1]: What is Kafka</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#apache-kafka-소개&quot; id=&quot;markdown-toc-apache-kafka-소개&quot;&gt;Apache Kafka 소개&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#event&quot; id=&quot;markdown-toc-event&quot;&gt;Event&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#기업-사례-잘란도zalando&quot; id=&quot;markdown-toc-기업-사례-잘란도zalando&quot;&gt;기업 사례: 잘란도(Zalando)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kafka의-핵심-기능&quot; id=&quot;markdown-toc-kafka의-핵심-기능&quot;&gt;Kafka의 핵심 기능&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#--순서-보장&quot; id=&quot;markdown-toc---순서-보장&quot;&gt;- 순서 보장&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#--적어도-한-번-전송-방식&quot; id=&quot;markdown-toc---적어도-한-번-전송-방식&quot;&gt;- 적어도 한 번 전송 방식&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#--pull-based-approach&quot; id=&quot;markdown-toc---pull-based-approach&quot;&gt;- Pull based approach&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#--강력한-partitioning&quot; id=&quot;markdown-toc---강력한-partitioning&quot;&gt;- 강력한 Partitioning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#--비동기식-방식&quot; id=&quot;markdown-toc---비동기식-방식&quot;&gt;- 비동기식 방식&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#정리&quot; id=&quot;markdown-toc-정리&quot;&gt;정리&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;apache-kafka-소개&quot;&gt;Apache Kafka 소개&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Apache Kafka is an open-source distributed publish-subscribe messaging platform that has been purpose-built to handle real-time streaming data for distributed streaming, pipelining, and replay of data feeds for fast, scalable operations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;실시간 데이터를 스트리밍하는 분산환경의 publish-subscribe 메시지 플랫폼&lt;/li&gt;
  &lt;li&gt;복잡한 데이터 파이프라인 구조를 간단하게 해주며 파이프라인의 확장성을 높여준다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;event&quot;&gt;Event&lt;/h1&gt;

&lt;p&gt;모든 기업에게 있어 데이터는 중요한 자산입니다. 특히나 요즘과 같이 데이터를 이용해 새로운 비즈니스를 창출하는 시대에는 그 가치가 더욱 큽니다. 이러한 데이터에는, 로그 메세지가 될 수도 있고, 사용자의 정보나 활동(배송, 결제, 송금 등) 그 밖에 모든 것들이 데이터가 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;Kafka에서는 Event, Data, Record, Message를 모두 혼용해서 쓰고 있습니다. Event는 어떠한 행동이나, 사건도 모두 될 수 있습니다. 다음과 같은 것 들이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;웹 사이트에서 무언가를 클릭하는 것&lt;/li&gt;
  &lt;li&gt;센서의 온도/압력 데이터&lt;/li&gt;
  &lt;li&gt;청구서&lt;/li&gt;
  &lt;li&gt;배송 물건의 위치 정보&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이렇게 세상에 있는 모든 정보를 실시간으로 저장하고, 처리하기 위해서는 높은 throughput, 낮은 latency가 요구됩니다. Kafka는 최대 600MB/s의 throughput과 200MB에 대해 5ms의 낮은 latency를 제공하고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_3.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://www.confluent.io/blog/kafka-fastest-messaging-system/&quot; target=&quot;_blank&quot;&gt;(Benchmarking Kafka vs. Pulsar vs. RabbitMQ: Which is Fastest? 참고)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;지금까지는 Kafka가 높은 throughput과 낮은 latency로 엄청난 양의 데이터를 실시간으로 처리해주는 플랫폼이라고 배웠습니다. 이제 이러한 개념을 가지고 조금 더 앞으로 나가보겠습니다. 다음은 Kafka를 설명하는 좋은 문장이라고 생각되어 가져와 봤습니다. &lt;a href=&quot;https://medium.com/codex/apache-kafka-series-part-1-introduction-to-apache-kafka-9b890832002&quot; target=&quot;_blank&quot;&gt;(Apache Kafka Series [Part 1]: Introduction to Apache Kafka)&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Publish/subscribe messaging is a pattern that is characterized by that a piece of data (message) of the sender (publisher) is not directing to certain receiver. Instead, the publisher classifies the message somehow, and that receiver (subscriber) subscribes to receive certain classes of messages. Pub/sub systems often have a broker, a central point where messages are published, to facilitate this.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Kafka를 이용하면 특정 Source에서 특정 Destination으로 데이터를 흘려보내는 것이 아니라, Publisher들이 실시간으로 언제든 데이터를 저장할 수 있으며, Subscriber는 언제든 저장된 데이터를 가지고 올 수 있습니다. 이러한 구조를 Pub/Sub 모델이라고 합니다. Pub/sub은 Messaging platform의 architecture를 훨씬 간단하게 만들고, 확장성을 용이하게 해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;기업-사례-잘란도zalando&quot;&gt;기업 사례: 잘란도(Zalando)&lt;/h1&gt;

&lt;p&gt;Kafka는 현재 Fortune 100대 기업 중 80% 이상이 사용하고 있는 데이터 플랫폼의 핵심 기술입니다. 해외의 링크드인, 트위터, 아마존, 넷플릭스, 우버를 포함해 국내에서는 대표적으로 카카오와 라인 등이 Kafka를 이용하고 있습니다. 제가 여기서 소개드릴 사례는 유럽의 대표 온라인 쇼핑몰 잘란도(Zalando)입니다. &lt;a href=&quot;https://engineering.zalando.com/posts/2017/10/event-first-development---moving-towards-kafka-pipeline-applications.html&quot; target=&quot;_blank&quot;&gt;(참고: Event First Development - Moving Towards Kafka Pipeline Applications)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;잘란도는 회사의 규모가 점점 커지고 사업이 다각화되면서 내부적으로 데이터에 대한 문제가 점점 대두되었습니다. 처리해야 할 데이터 양의 증가, 복잡해져가는 데이터 파이프라인(데이터를 Produce하는 곳과 Consume하는 곳의 다양화), 데이터 수집 장애로 인한 신뢰도 하락과 같은 문제로 잘란도에서는 이벤트 드리븐 시스템을 도입하기로 결정하였습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The aim here was the democratization of data for all potential users on the new platform.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_5.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://realtimeapi.io/hub/event-driven-apis/&quot; target=&quot;_blank&quot;&gt;(참고: https://realtimeapi.io/hub/event-driven-apis/)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;결과적으로 잘란도는 Kafka를 도입함으로써 내부의 데이터 처리 파이프라인을 간소화하고, 확장을 용이하게 했으며, 스트림 데이터 처리량을 높일 수 있었습니다. 이러한 결과를 얻을 수 있었던 것은 Kafka에서 제공하는 몇 가지 핵심 기능 덕분이었습니다.&lt;/p&gt;

&lt;h1 id=&quot;kafka의-핵심-기능&quot;&gt;Kafka의 핵심 기능&lt;/h1&gt;

&lt;h2 id=&quot;--순서-보장&quot;&gt;- 순서 보장&lt;/h2&gt;
&lt;p&gt;이벤트 처리 순서가 보장되면서, 엔티티 간의 유효성 검사나 동시 수정 같은 무수한 복잡성들이 제거됨으로써 구조 또한 매우 간결해졌습니다.&lt;/p&gt;

&lt;h2 id=&quot;--적어도-한-번-전송-방식&quot;&gt;- 적어도 한 번 전송 방식&lt;/h2&gt;
&lt;p&gt;분산된 여러 네트워크 환경에서의 데이터 처리에서 중요한 것은 멱등성(itempotence)입니다. 멱등성이란 동일한 작업을 여러 번 수행하더라도 결과가 달라지지 않는 것을 의미합니다. 하지만 실시간 대용량 데이터 스트림에서 이를 완벽히 지켜내기란 쉽지 않습니다. 그래서 차선책으로 데이터가 중복은 되더라도, 손실은 일어나지 않도록 하는 방식이 ‘적어도 한 번’ 전송 방식입니다. 만약 백엔드 시스템에서 중복 메세지만 처리해준다면 멱등성을 위한 시스템 복잡도를 기존에 비해 훨씬 낮출 수 있게 되고, 처리량 또한 더욱 높아집니다. 최근에는 ‘정확히 한 번’ 전송 방식이 도입되어 카프카내에서 중복성을 제거하는 방법이 많이 사용되고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;--pull-based-approach&quot;&gt;- Pull based approach&lt;/h2&gt;
&lt;p&gt;카프카에서 데이터를 소비하는 클라이언트는 풀 방식으로 동작합니다. 풀 방식의 장점은 자기 자신의 속도로 데이터를 처리할 수 있다는 점입니다. 푸시 방식은 브로커가 보내주는 속도에 의존해야 한다는 한계가 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;--강력한-partitioning&quot;&gt;- 강력한 Partitioning&lt;/h2&gt;
&lt;p&gt;파티셔닝을 통해 확장성이 용이한 분산 처리 환경을 제공합니다.&lt;/p&gt;

&lt;h2 id=&quot;--비동기식-방식&quot;&gt;- 비동기식 방식&lt;/h2&gt;
&lt;p&gt;데이터를 제공하는 Producer와 데이터를 소비하는 Consumer가 서로 각기 원하는 시점에 동작을 수행할 수 있습니다. (데이터를 보내줬다고 해서 반드시 바로 받을 필요가 없습니다)&lt;/p&gt;

&lt;h1 id=&quot;정리&quot;&gt;정리&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;Kafka는 Pub/sub모델의 실시간 데이터 처리 플랫폼이다.&lt;/li&gt;
  &lt;li&gt;데이터를 분산처리하여 높은 throughput과 낮은 latency를 제공한다.&lt;/li&gt;
  &lt;li&gt;심플한 데이터 처리 파이프라인과 용이한 확장성을 제공한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;다음 포스트에서는 Kafka의 주요 구성 요소에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;amp;ejkGb=KOR&amp;amp;barcode=9791189909345&quot; target=&quot;_blank&quot;&gt;실전 카프카 개발부터 운영까지 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/codex/apache-kafka-series-part-1-introduction-to-apache-kafka-9b890832002&quot; target=&quot;_blank&quot;&gt;CodeX 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 17 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series1</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series1</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Apache Spark Series [Part1]: Apache Spark Introduction</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#spark-introduction&quot; id=&quot;markdown-toc-spark-introduction&quot;&gt;Spark Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#rdd&quot; id=&quot;markdown-toc-rdd&quot;&gt;RDD&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#파티션partition&quot; id=&quot;markdown-toc-파티션partition&quot;&gt;파티션(Partition)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#불변성immutable&quot; id=&quot;markdown-toc-불변성immutable&quot;&gt;불변성(Immutable)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#게으른-연산lazy-operation&quot; id=&quot;markdown-toc-게으른-연산lazy-operation&quot;&gt;게으른 연산(Lazy Operation)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cluster-mode&quot; id=&quot;markdown-toc-cluster-mode&quot;&gt;Cluster Mode&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#드라이버-프로그램&quot; id=&quot;markdown-toc-드라이버-프로그램&quot;&gt;드라이버 프로그램&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-매니저&quot; id=&quot;markdown-toc-클러스터-매니저&quot;&gt;클러스터 매니저&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#워커-노드&quot; id=&quot;markdown-toc-워커-노드&quot;&gt;워커 노드&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#알아두면-좋은-것들&quot; id=&quot;markdown-toc-알아두면-좋은-것들&quot;&gt;알아두면 좋은 것들&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#shuffling&quot; id=&quot;markdown-toc-shuffling&quot;&gt;Shuffling&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#passing-functions-to-spark&quot; id=&quot;markdown-toc-passing-functions-to-spark&quot;&gt;Passing Functions to Spark&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#질문&quot; id=&quot;markdown-toc-질문&quot;&gt;질문&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;spark-introduction&quot;&gt;Spark Introduction&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;스파크는 클러스터 기반의 분산 처리 기능을 제공하는 오픈소스 프레임워크입니다.&lt;/strong&gt; 쉽게 말해 대용량 데이터를 여러 컴퓨터에 나누어서 동시에 처리한다고 할 수 있습니다. 이런 방법이 스파크 이전에 없었던 것은 아닙니다. 스파크 이전에 하둡(Hadoop)이 이와 유사한 기능을 제공했었습니다. 
참고로 하둡은 더그 커팅(Doug Cutting)이라는 사람이 구글이 발표했던 두 개의 논문(&lt;a href=&quot;https://static.googleusercontent.com/media/research.google.com/ko//archive/gfs-sosp2003.pdf&quot; target=&quot;_blank&quot;&gt;&lt;strong&gt;The Google File System_2003&lt;/strong&gt;&lt;/a&gt;, &lt;a href=&quot;https://dl.acm.org/doi/10.1145/1327452.1327492&quot;&gt;&lt;strong&gt;MapReduce: simplified data processing on large clusters_2008&lt;/strong&gt;&lt;/a&gt;){:target=”_blank”}을 직접 구현해 만든 프레임워크입니다. 이처럼 구글에서는 예전부터 대용량의 데이터를 고속 분산처리 하기 위해 노력했었고, 현재 스파크는 대부분의 기업들이 사용하고 있는 소프트웨어입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;지금부터는 스파크와 하둡을 비교하며 스파크의 특징에 어떤 것이 있는지 알아보겠습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;차이점&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;하둡&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;스파크&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;기반&lt;/td&gt;
      &lt;td&gt;디스크 기반&lt;/td&gt;
      &lt;td&gt;메모리 기반&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;처리방식&lt;/td&gt;
      &lt;td&gt;Map-Reduce&lt;/td&gt;
      &lt;td&gt;RDD&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;프로그래밍언어&lt;/td&gt;
      &lt;td&gt;자바&lt;/td&gt;
      &lt;td&gt;스칼라, 자바, 파이썬, R&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;라이브러리&lt;/td&gt;
      &lt;td&gt;-&lt;/td&gt;
      &lt;td&gt;다양한 라이브러리(Spark streaming, MLlib, GraphX 등) 제공&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;처리방식에 대해 조금 더 이야기 해보겠습니다. 맵리듀스(MapReduce)는 2004년 구글에서 대용량 데이터 처리를 분산환경에서 처리하기 위한 목적의 소프트웨어 프레임워크입니다. 맵리듀스는 함수형 프로그래밍에서 일반적으로 사용되는 Map과 Reduce라는 함수를 기반으로 만들어졌습니다. Map은 각각의 분산된 환경에서 독립적으로 실행되는 함수의 일종, Reduce는 분산 환경에서의 데이터를 하나로 모으는 함수라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;맵리듀스는 분산된 환경에서 데이터가 처리되는데 필요한 많은 함수들을 제공해주지만, 현업에서 필요한 기능들을 모두 커버하기에는 무리가 있었습니다. 그래서 이러한 단점을 보완하기 위해 2009년 UC Berkeley 대학에서 연구를 시작해 2012년 미국 NSDI 학회에서 스파크의 핵심 개념인 &lt;strong&gt;RDD(Resilient Distributed Dataset)&lt;/strong&gt; 에 대한 논문을 발표하였습니다.&lt;/p&gt;

&lt;h1 id=&quot;rdd&quot;&gt;RDD&lt;/h1&gt;
&lt;blockquote&gt;
  &lt;p&gt;RDD is a fault-tolerant collection of elements that can be operated on in parallel.&lt;br /&gt;
&lt;a href=&quot;https://spark.apache.org/docs/3.2.0/rdd-programming-guide.html#resilient-distributed-datasets-rdds&quot; target=&quot;_blank&quot;&gt;(아파치 스파크 공식문서 참고)&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;다시 말하면 RDD란 스파크에서 정의한 &lt;strong&gt;분산 데이터 모델로서 병렬 처리가 가능한 요소&lt;/strong&gt; 로 구성되며 데이터를 처리하는 과정에서 &lt;strong&gt;장애가 발생하더라도 스스로 복구할 수 있는 능력&lt;/strong&gt; 을 가진 &lt;strong&gt;데이터 모델&lt;/strong&gt; 이라는 뜻입니다. RDD는 분산 데이터에 대한 모델로서 단순히 값으로 표현되는 데이터만 가리키는 것이 아니고, 분산된 &lt;strong&gt;데이터를 다루는 방법까지 포함&lt;/strong&gt; 하는 일종의 클래스와 같은 개념입니다.&lt;/p&gt;

&lt;p&gt;RDD에서 중요한 특징은 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;파티션(Partition)&lt;/li&gt;
  &lt;li&gt;불변성&lt;/li&gt;
  &lt;li&gt;게으른 연산(Lazy operation)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;파티션partition&quot;&gt;파티션(Partition)&lt;/h2&gt;
&lt;p&gt;RDD는 분산 데이터 요소로 구성된 데이터 집합입니다. 여기서 분산 데이터 요소를 파티션이라고 합니다. 스파크는 작업을 수행할 때 바로 이 파티션 단위로 나눠서 병렬로 처리합니다. 여기서 제가 헷갈렸던 것은 파티션이 분산처리와 병렬처리 중 어떤 것을 기준으로 나뉘어진 단위인가 라는 것 이었습니다. 공식문서&lt;a href=&quot;https://spark.apache.org/docs/3.2.0/rdd-programming-guide.html#parallelized-collections&quot; target=&quot;_blank&quot;&gt;(아파치 스파크 공식문서 참고)&lt;/a&gt;를 살펴본 결과 파티션은 병렬 처리가 되는 기준이었습니다. 여러 서버에 분산할 때 보통 하나의 서버 당 2~4개 정도의 파티션을 설정합니다. 이 기준은 개인의 클러스터 환경에 따라 기본 설정 값이 다르며 이 값은 원하는 값으로 바꿀 수 있습니다. 구글에서 이미지를 살펴보았을 때는 다들 task당 한개의 파티션이라고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;불변성immutable&quot;&gt;불변성(Immutable)&lt;/h2&gt;
&lt;p&gt;한 개의 RDD가 여러 개의 파티션으로 나뉘고 다수의 서버에서 처리되다 보니 작업 도중 일부 파티션 처리에 장애가 발생해 파티션 처리 결과가 유실될 수 있습니다. 하지만 스파크에서 RDD는 불변성이기 때문에 생성 과정에 사용되었던 연산들을 다시 실행하여 장애를 해결할 수 있습니다. 여기서 불변성이라는 말은 RDD에서 어떤 연산을 적용해 다른 RDD가 될 때 &lt;strong&gt;무조건 새로 RDD를 생성합니다(RDD는 불변이다).&lt;/strong&gt; 이러한 방식 덕분에 장애가 발생해도 기존의 RDD 데이터에 다시 연산을 적용해 장애를 해결할 수 있는 것입니다(RDD는 회복 탄력성이 좋다(resilient)).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;게으른-연산lazy-operation&quot;&gt;게으른 연산(Lazy Operation)&lt;/h2&gt;

&lt;p&gt;RDD의 연산은 크게 &lt;strong&gt;트랜스포메이션&lt;/strong&gt; 과 &lt;strong&gt;액션&lt;/strong&gt; 이라는 두 종류로 나눌 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;트랜스포메이션: RDD1 -&amp;gt; RDD2 이런식으로 새로운 RDD를 만들어내는 연산, 대표적으로 map 함수&lt;/li&gt;
  &lt;li&gt;액션: RDD -&amp;gt; 다른 형태의 데이터를 만들어내는 연산, 대표적으로 reduce 함수&lt;br /&gt;
&lt;a href=&quot;https://spark.apache.org/docs/3.2.0/rdd-programming-guide.html#transformations&quot; target=&quot;_blank&quot;&gt;(아파치 공식문서 참고)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;트랜스포메이션 연산은 보통 분산된 서버 각각에서 독립적으로 수행할 수 있는 연산입니다. 그리고 액션은 분산된 서버에 있는 데이터가 서로를 참조해야 하는 연산입니다. 그래서 액션은 서버 네트워크간의 이동이 발생하게 됩니다. 이런 현상을 셔플링(Shuffling)이라고 하고, 보통 네트워크에서 읽어오는 연산은 메모리에 비해 100만배 정도 느립니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇기 때문에 셔플링이 발생하는 연산을 할 때에는 그 전에 최대한 데이터를 간추리는 것이 중요한데 스파크의 중요한 특징 중 하나가 바로 게으른 연산을 한다는 것입니다. 게으른 연산이라는 말은 RDD가 액션연산을 수행할 때에 비로소 모든 연산이 한꺼번에 실행된다는 것입니다. 이러한 방식의 장점은 데이터를 본격적으로 처리하기 전에 어떤 연산들이 사용되었는지 알 수 있고, 이를 통해 최종적으로 실행이 필요한 시점에 누적된 변환 연산을 분석하고 그중에서 가장 최적의 방법을 찾아 변환 연산을 실행할 수 있습니다. 이렇게 되면 셔플링이 최대한 작은 사이즈로 발생할 수 있도록 합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;스파크는 RDD를 사용함으로써 처리 속도도 높이고, 장애 복구도 가능해졌다
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;cluster-mode&quot;&gt;Cluster Mode&lt;/h1&gt;
&lt;p&gt;이번에는 스파크를 구동시키는 환경에 대해서 알아보겠습니다. 스파크는 단일 서버로 동작시키는 로컬 모드와, 클러스터 환경에서 동작시키는 클러스터 모드가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/spark_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;로컬 모드는 위의 클러스터 환경에 있는 구성 요소들을 모두 하나의 서버에 놓는 것(Executor는 1개)과 같기 때문에 여기서는 분산 처리를 가능하게 해주는 클러스터 모드에 대해서 조금 더 자세히 알아보겠습니다. 구성 요소는 크게 다음과 같습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;드라이버 프로그램&lt;/li&gt;
  &lt;li&gt;클러스터 매니저&lt;/li&gt;
  &lt;li&gt;워커 노드&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;여기서 드라이버 프로그램과 워커 노드를 보통 애플리케이션이라고 하고, 클러스터 매니저는 외부 서비스로 애플리케이션과 연동합니다.&lt;/p&gt;

&lt;h2 id=&quot;드라이버-프로그램&quot;&gt;드라이버 프로그램&lt;/h2&gt;
&lt;p&gt;드라이버 프로그램의 역할은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;클러스터 매니저와의 connection을 위한 스파크 컨텍스트 객체를 생성&lt;/li&gt;
  &lt;li&gt;스파크 컨텍스트를 이용해 RDD 생성&lt;/li&gt;
  &lt;li&gt;스파크 컨텍스트를 이용해 연산 정의&lt;/li&gt;
  &lt;li&gt;정의된 연산은 DAG 스케줄러에게 전달되고 스케줄러는 연산 실행 계획 수립 후 클러스터 매니저에 전달&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;클러스터-매니저&quot;&gt;클러스터 매니저&lt;/h2&gt;
&lt;p&gt;클러스터 매니저에는 다음과 같은 것들이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Standalone&lt;/strong&gt;: a simple cluster manager included with Spark that makes it easy to set up a cluster&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;YARN&lt;/strong&gt;: the resource manager in Hadoop 2&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Kubernetes&lt;/strong&gt;: an open-source system for automating deployment, scaling, and management of containerized applications&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;클러스터 매니저의 종류마다 지원하는 범위가 세부적으로 다르지만 대략적인 역할은 다음과 같습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;스파크 컨텍스트 생성시 설정된 Executer의 개수, Executer의 메모리를 바탕으로 자원을 할당&lt;/li&gt;
  &lt;li&gt;이용 가능한 워커에 태스크를 할당하기 위해 노드를 모니터링&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;워커-노드&quot;&gt;워커 노드&lt;/h2&gt;
&lt;p&gt;스파크 컨텍스트는 워커 노드에 Executer를 생성하도록 클러스터 매니저에 요청을 하고 클러스터는 그에 맞춰 Executer를 생성합니다. Executer가 생성되면 드라이버 프로그램은 정의된 연산을 수행합니다. 이 때 작업을 실제로 수행하는 것은 아니고 액션 연산의 수만큼 잡(Job)을 생성하고 잡은 셔플링이 최대한 적게 일어나는 방향으로 스테이지(Stage)를 나눕니다. 나누어진 스테이지는 다시 여러 개의 태스크(Task)로 나누어진 후 워커 노드에 생성된 Executer에 할당됩니다.&lt;br /&gt;
워커 노드는 Executer를 이용해 태스크를 처리하고, 데이터를 나중에 재사용 할 수 있도록 메모리에 저장도 합니다.&lt;/p&gt;

&lt;h1 id=&quot;알아두면-좋은-것들&quot;&gt;알아두면 좋은 것들&lt;/h1&gt;

&lt;h2 id=&quot;shuffling&quot;&gt;Shuffling&lt;/h2&gt;

&lt;h2 id=&quot;passing-functions-to-spark&quot;&gt;Passing Functions to Spark&lt;/h2&gt;

&lt;h1 id=&quot;질문&quot;&gt;질문&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;RDD가 파티션으로 나뉘어지는 시점은 RDD가 생성되는 순간일까 아니면 연산이 실행되는 순간일까?&lt;/li&gt;
  &lt;li&gt;어떤 기준으로 RDD를 파티셔닝할까?&lt;/li&gt;
  &lt;li&gt;셔플링은 액션 연산에서만 발생할까?&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sat, 15 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/spark-series1</link>
                <guid isPermaLink="true">http://localhost:4000/spark-series1</guid>
                
                <category>Spark</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Github Actions Series [Part1]: Understanding GitHub Actions</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#github-actions-소개&quot; id=&quot;markdown-toc-github-actions-소개&quot;&gt;Github Actions 소개&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#github-actions-주요-구성요소&quot; id=&quot;markdown-toc-github-actions-주요-구성요소&quot;&gt;Github Actions 주요 구성요소&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#workflow&quot; id=&quot;markdown-toc-workflow&quot;&gt;Workflow&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#event&quot; id=&quot;markdown-toc-event&quot;&gt;Event&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#job&quot; id=&quot;markdown-toc-job&quot;&gt;Job&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#action&quot; id=&quot;markdown-toc-action&quot;&gt;Action&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#runner&quot; id=&quot;markdown-toc-runner&quot;&gt;Runner&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#workflow-예제&quot; id=&quot;markdown-toc-workflow-예제&quot;&gt;Workflow 예제&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;github-actions-소개&quot;&gt;Github Actions 소개&lt;/h1&gt;
&lt;p&gt;GitHub Actions은 CI/CD 플랫폼으로 build, test, deployment 파이프라인을 자동화 시켜줍니다. Github Actions은 Github repository에서 어떤 event(ex. push, pull request)가 발생했을 때 설정한 workflow를 실행하도록 할 수 있습니다. 이러한 workflow를 실행하기 위해 Github에서는 Linux, Windows, macOS와 같은 주요 운영체제 기반의 가상머신을 제공해주고, 원한다면 self-hosted runner를 이용할 수도 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;github-actions-주요-구성요소&quot;&gt;Github Actions 주요 구성요소&lt;/h1&gt;
&lt;p&gt;You can configure a GitHub Actions workflow to be triggered when an event occurs in your repository, such as a pull request being opened or an issue being created. Your workflow contains one or more jobs which can run in sequential order or in parallel. Each job will run inside its own virtual machine runner, or inside a container, and has one or more steps that either run a script that you define or run an action, which is a reusable extension that can simplify your workflow.&lt;br /&gt;
&lt;a href=&quot;https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions&quot;&gt;(Github Actions 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/github-actions_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;workflow&quot;&gt;Workflow&lt;/h2&gt;
&lt;p&gt;자동화된 전체 프로세스. 하나 이상의 Job으로 구성되고, Event에 의해 예약되거나 트리거될 수 있는 자동화된 절차를 말한다.
Workflow 파일은 YAML으로 작성되고, Github Repository의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.github/workflows&lt;/code&gt; 폴더 아래에 저장된다. Github에게 YAML 파일로 정의한 자동화 동작을 전달하면, Github Actions는 해당 파일을 기반으로 그대로 실행시킨다.&lt;/p&gt;

&lt;h2 id=&quot;event&quot;&gt;Event&lt;/h2&gt;
&lt;p&gt;Workflow를 트리거(실행)하는 특정 사건. 예를 들어, pull, push, creating issue와 같은 것들로 Workflow를 실행시킬 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;job&quot;&gt;Job&lt;/h2&gt;
&lt;p&gt;Job은 여러 Step으로 구성되고, 단일 가상 환경에서 실행된다. 다른 Job에 의존 관계를 가질 수도 있고, 독립적으로 병렬로 실행될 수도 있다. Step에서는 shell script를 실행시킬 수도 있고, action을 실행시킬 수도 있다.&lt;/p&gt;

&lt;h2 id=&quot;action&quot;&gt;Action&lt;/h2&gt;
&lt;p&gt;Action은 반복적인 코드를 하나로 묶어 재사용 가능하도록 만들어 놓은 블럭입니다. Action을 직접 커스텀하여 사용할 수도 있고, Github Marketplace에 올라와 있는 것을 사용해도 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;runner&quot;&gt;Runner&lt;/h2&gt;
&lt;p&gt;Runner는 Gitbub Action Runner 어플리케이션이 설치된 머신으로, Workflow가 실행될 인스턴스입니다.&lt;/p&gt;

&lt;h1 id=&quot;workflow-예제&quot;&gt;Workflow 예제&lt;/h1&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;learn-github-actions&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;push&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;jobs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;check-bats-version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;runs-on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ubuntu-latest&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;steps&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/checkout@v2&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;uses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;actions/setup-node@v2&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;node-version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;14'&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;npm install -g bats&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bats -v&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/github-actions_2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;../images/../../images/github-actions_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;yaml 파일&lt;/code&gt; 하나가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Workflow&lt;/code&gt;이다&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;on&lt;/code&gt;이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Event&lt;/code&gt;이다&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;jobs 안에 정의된 이름&lt;/code&gt;이 각각의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Job&lt;/code&gt;이다&lt;/li&gt;
  &lt;li&gt;steps안에 정의된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;uses&lt;/code&gt;가 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Action&lt;/code&gt;이다&lt;/li&gt;
  &lt;li&gt;step안에 정의된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;run&lt;/code&gt;이 쉘 명령어이다&lt;/li&gt;
  &lt;li&gt;job안에 정의된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runs-on&lt;/code&gt;이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Runner&lt;/code&gt;이다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/github-actions_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.github.com/en/actions/learn-github-actions/understanding-github-actions&quot;&gt;Github Actions 공식문서&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://velog.io/@ggong/Github-Action에-대한-소개와-사용법&quot;&gt;ggong.log 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 13 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/github_action_series1</link>
                <guid isPermaLink="true">http://localhost:4000/github_action_series1</guid>
                
                <category>Git</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part2]: Kubernetes Resource</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#쿠버네티스의-리소스&quot; id=&quot;markdown-toc-쿠버네티스의-리소스&quot;&gt;쿠버네티스의 리소스&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#workload-resources&quot; id=&quot;markdown-toc-workload-resources&quot;&gt;Workload Resources&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#pod&quot; id=&quot;markdown-toc-pod&quot;&gt;Pod&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#replicaset&quot; id=&quot;markdown-toc-replicaset&quot;&gt;ReplicaSet&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#deployment&quot; id=&quot;markdown-toc-deployment&quot;&gt;Deployment&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#service관련-리소스&quot; id=&quot;markdown-toc-service관련-리소스&quot;&gt;Service관련 리소스&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#service&quot; id=&quot;markdown-toc-service&quot;&gt;Service&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ingress&quot; id=&quot;markdown-toc-ingress&quot;&gt;Ingress&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#config-and-storage관련-리소스&quot; id=&quot;markdown-toc-config-and-storage관련-리소스&quot;&gt;Config and Storage관련 리소스&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#configmap&quot; id=&quot;markdown-toc-configmap&quot;&gt;ConfigMap&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#volume&quot; id=&quot;markdown-toc-volume&quot;&gt;Volume&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;쿠버네티스의-리소스&quot;&gt;쿠버네티스의 리소스&lt;/h1&gt;

&lt;h2 id=&quot;workload-resources&quot;&gt;Workload Resources&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;Workloads are objects that set deployment rules for pods. Based on these rules, Kubernetes performs the deployment and updates the workload with the current state of the application. Workloads let you define the rules for application scheduling, scaling, and upgrade.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://rancher.com/docs/rancher/v2.5/en/k8s-in-rancher/workloads/&quot; target=&quot;_blank&quot;&gt;(Rancher문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;pod&quot;&gt;Pod&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pod는 쿠버네티스에서 배포할 수 있는 가장 작은 단위의 오브젝트로 한 개 이상의 컨테이너와 스토리지, 네트워크 속성을 가집니다. Pod에 속한 컨테이너는 스토리지와 네트워크를 공유하고 서로 localhost로 접근할 수 있습니다. 컨테이너를 하나만 사용하는 경우도 반드시 Pod으로 감싸서 관리합니다.&lt;/p&gt;

&lt;p&gt;Pod가 생성되는 과정은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Scheduler는 계속 할당할 새로운 Pod가 있는지 체크하고 있으면 노드에 할당합니다. 그러면 노드에 있는 Kubelet은 컨테이너를 생성하고 결과를 API서버에 보고합니다.&lt;/p&gt;

&lt;p&gt;🐨 &lt;strong&gt;오브젝트 생성을 위한 YAML파일&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Pod를 포함해 쿠버네티스의 오브젝트를 만들기 위해서는 YAML파일이 필요합니다. YAML파일에 오브젝트를 위한 설정들을 작성할 수 있는데, 이 때 필수적으로 사용되는 key값들이 있습니다.&lt;/p&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;strong&gt;Key&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;설명&lt;/strong&gt;&lt;/td&gt;
      &lt;td&gt;&lt;strong&gt;예&lt;/strong&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;apiVersion&lt;/td&gt;
      &lt;td&gt;오브젝트 버전&lt;/td&gt;
      &lt;td&gt;v1, app/v1, ..&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;kind&lt;/td&gt;
      &lt;td&gt;오브젝트 종류&lt;/td&gt;
      &lt;td&gt;Pod, ReplicaSet, Deployment, ..&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;metadata&lt;/td&gt;
      &lt;td&gt;메타데이터&lt;/td&gt;
      &lt;td&gt;name, label, ..&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;spec&lt;/td&gt;
      &lt;td&gt;오브젝트 별 상세 설정&lt;/td&gt;
      &lt;td&gt;오브젝트마다 다름&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ghcr.io/subicura/echo:v1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Pod의 spec에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;containers&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;volumes&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;restartPolicy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostname&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostNetwork&lt;/code&gt; 등이 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/&quot; target=&quot;_blank&quot;&gt;(Pod공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;replicaset&quot;&gt;ReplicaSet&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_13.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ReplicaSet은 Pod을 여러 개(한 개 이상) 복제하여 관리하는 오브젝트입니다. 단일 노드 환경이면 Pod는 모두 단일 노드에서 생성되고, 여러개의 노드를 가지고 있는 상황이면, Pod는 노드에 각각 분산되어 배포됩니다.(노드 장애 대비) 이 때 Pod를 어떤 노드에 배치할지는 스케줄러가 결정하게 됩니다. 보통 직접적으로 ReplicaSet을 사용하기보다는 Deployment등 다른 오브젝트에 의해서 사용되는 경우가 많습니다.&lt;/p&gt;

&lt;p&gt;ReplicaSet은 다음과 같이 동작합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_14.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ReplicaSet controller가 desired state에 맞춰 Pod를 생성합니다. 그러면 Scheduler는 생성된 Pod를 노드에 할당해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReplicaSet&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo-rs&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# app: echo이고 tier: app인 label을 가지는 파드를 관리&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# replicaset이 만드는 pod의 템플릿&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ghcr.io/subicura/echo:v1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;ReplicaSet의 spec에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replicas&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selector&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;template&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minReadySeconds&lt;/code&gt;가 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/replica-set-v1/#ReplicaSetSpec&quot; target=&quot;_blank&quot;&gt;(ReplicaSet 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;deployment&quot;&gt;Deployment&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_18.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deployment는 쿠버네티스에서 가장 널리 사용되는 오브젝트입니다. ReplicaSet을 이용하여 Pod을 업데이트하고 이력을 관리하여 롤백Rollback하거나 특정 버전revision으로 돌아갈 수 있습니다.&lt;/p&gt;

&lt;p&gt;Deployment 오브젝트가 Pod의 버전을 관리하는 과정은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_17.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deployment Controller가 Deploy 조건을 체크하면서 원하는 버전에 맞게 Pod의 버전을 맞춥니다. 이 때 ReplicaSet에 있는 Pod들을 보통 한 번에 바꾸지 않고 조건에 맞게(예를 들어, 25%씩) 바꿔나감으로써 버전을 바꾸더라도 중간에 서비스가 중단되지 않도록 합니다. (무중단배포)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_19.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo-deploy&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;strategy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;RollingUpdate&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;rollingUpdate&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxSurge&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;maxUnavailable&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ghcr.io/subicura/echo:v2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;spec에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;replicas&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;selector&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;template&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;strategy&lt;/code&gt;  등이 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/#DeploymentSpec&quot; target=&quot;_blank&quot;&gt;(Deployment 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;service관련-리소스&quot;&gt;Service관련 리소스&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;In many use cases, a workload has to be accessed by other workloads in the cluster or exposed to the outside world.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;service&quot;&gt;Service&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_20.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Service는 네트워크와 관련된 오브젝트입니다. Pod은 자체 IP를 가지고 다른 Pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징 때문에 직접 통신하는 방법은 권장하지 않습니다. 쿠버네티스는 Pod와 직접 통신하는 방법 대신, 별도의 고정된 IP를 가진 서비스를 만들고 그 서비스를 통해 Pod에 접근하는 방식을 사용합니다.&lt;br /&gt;
Pod을 외부 네트워크와 연결해주고 여러 개의 Pod을 바라보는 내부 로드 밸런서를 생성할 때 사용합니다. 내부 DNS에 서비스 이름을 도메인으로 등록하기 때문에 서비스 디스커버리 역할도 합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ClusterIP: Pod가 동적으로 소멸/생성 되더라도 IP는 고정될 수 있도록 하는 역할&lt;/li&gt;
  &lt;li&gt;NodePort: 외부에서 접근가능하도록 하는 포트 역할&lt;/li&gt;
  &lt;li&gt;LoadBalancer: 살아있는 노드로 자동으로 연결해주는 역할&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;NodePort는 기본적으로 ClusterIP의 기능을 포함하고 있고, LoadBalancer는 NodePort의 기능을 포함하고 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ClusterIP&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# redis라는 Deployment 오브젝트에 IP할당&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;6379&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# clusterIP의 포트 (targetPort따로 없으면 targetPort(pod의 포트)도 6379가 됨)&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 어떤pod로 트래픽을 전달할지 결정&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;counter&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;db&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# NodePort&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;counter-np&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;NodePort&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# ClusterIP, Pod IP의 포트&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;nodePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;31000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Node IP의 포트&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;counter&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# LoadBalancer&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;counter-lb&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LoadBalancer&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;30000&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3000&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;counter&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;tier&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;ingress&quot;&gt;Ingress&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_21.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ingress는 경로 기반 라우팅 서비스를 제공해주는 오브젝트입니다.&lt;/p&gt;

&lt;p&gt;LoadBalancer는 단점이 있습니다. LoadBalancer는 한 개의 IP주소로 한 개의 서비스만 핸들링할 수 있습니다. 그래서 만약 N개의 서비스를 실행 중이라면 N개의 LoadBalancer가 필요합니다. 또한 보통 클라우드 프로바이더(AWS, GCP 등)의 로드밸런서를 생성해 사용하기 때문에 로컬서버에서는 사용이 어렵습니다.&lt;/p&gt;

&lt;p&gt;Ingress는 경로 기반 라우팅 서비스를 통해 N개의 service를 하나의 IP주소를 이용하더라도 경로를 통해 분기할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Ingress는 Pod, ReplicaSet, Deployment, Service와 달리 별도의 컨트롤러를 설치해야 합니다. 컨트롤러에는 대표적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nginx&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;haproxy&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;traefik&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;alb&lt;/code&gt;등이 있습니다.&lt;/p&gt;

&lt;p&gt;minikube를 이용할 경우 다음 명령어로 설치할 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# nginx ingress controller&lt;/span&gt;
minikube addons &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo-v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1.echo.192.168.64.5.sslip.io&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;pathType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Prefix&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;service&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;echo-v1&lt;/span&gt;
                &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;number&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3000&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 들어오는 요청의 host가 v1.echo.192.168.64.5.sslip.io이면 host echo-v1이라는 서비스가 가지는 IP 주소의 3000번 포트로 보내라&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;spec에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rules&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;defaultBackend&lt;/code&gt;(어느 rule에도 속하지 않을 경우) 등이 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/service-resources/ingress-v1/&quot; target=&quot;_blank&quot;&gt;(Ingress 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;config-and-storage관련-리소스&quot;&gt;Config and Storage관련 리소스&lt;/h2&gt;

&lt;h3 id=&quot;configmap&quot;&gt;ConfigMap&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_22.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ConfigMap은 설정, 환경 변수들을 담는 오브젝트입니다. 예를 들어 개발/운영에 따라 환경 변수값이 다른 경우, ConfigMap 을 활용해 Pod 생성시 넣어줄 수 있습니다.&lt;/p&gt;

&lt;p&gt;ConfigMap을 다양한 방법으로 만들 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ConfigMap yaml 파일로 오브젝트 생성&lt;/li&gt;
  &lt;li&gt;환경 변수 설정을 담고 있는 yaml파일을 ConfigMap 오브젝트로 생성&lt;/li&gt;
  &lt;li&gt;그냥 환경 변수를 담고 있는 임의의 파일을 ConfigMap 오브젝트로 생성&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ConfigMap yaml파일&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 참고로 v1이면 core API group&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ConfigMap&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-config&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;hello&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;world&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;kuber&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;netes&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; config-map.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# 환경 변수 설정을 담고 있는 yaml파일&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;global&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;scrape_interval&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;15s&lt;/span&gt;

&lt;span class=&quot;na&quot;&gt;scrape_configs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;job_name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;prometheus&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metrics_path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/prometheus/metrics&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;static_configs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;localhost:9090&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# yaml 파일로 ConfigMap 파일 생성&lt;/span&gt;
kubectl create cm my-config &lt;span class=&quot;nt&quot;&gt;--from-file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;config-file.yml

&lt;span class=&quot;c&quot;&gt;# ConfigMap 적용&lt;/span&gt;
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; my-config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# config-env.yml파일 (yml파일 아니지만 그냥 확장자 yml로 해놓아도됨)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;hello&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;world
&lt;span class=&quot;nv&quot;&gt;haha&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hoho
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 임의의 파일로 ConfigMap 파일 생성&lt;/span&gt;
kubectl create cm env-config &lt;span class=&quot;nt&quot;&gt;--from-env-file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;config-env.yml

&lt;span class=&quot;c&quot;&gt;# ConfigMap 적용&lt;/span&gt;
kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; env-config.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;여러 가지 방법으로 ConfigMap을 Pod에 적용할 수 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;디스크 볼륨 마운트&lt;/li&gt;
  &lt;li&gt;환경변수로 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ConfigMap yaml파일이 있는 볼륨 마운트&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;100000&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;config-vol&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/etc/config&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;config-vol&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;configMap&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-config&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# ConfigMap yaml파일 직접 환경변수로 설정&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine-env&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;alpine&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;100000&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hello&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;valueFrom&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;na&quot;&gt;configMapKeyRef&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-config&lt;/span&gt;
              &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;hello&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;volume&quot;&gt;Volume&lt;/h3&gt;
&lt;p&gt;Volume은 저장소와 관련된 오브젝트입니다. 지금까지 만들었던 컨테이너는 Pod을 제거하면 컨테이너 내부에 저장했던 데이터도 모두 사라집니다. MySQL과 같은 데이터베이스는 데이터가 유실되지 않도록 반드시 별도의 저장소에 데이터를 저장하고 컨테이너를 새로 만들 때 이전 데이터를 가져와야 합니다.&lt;br /&gt;
저장소를 호스트 디렉토리를 사용할 수도 있고 EBS 같은 스토리지를 동적으로 생성하여 사용할 수도 있습니다. 사실상 인기 있는 대부분의 저장 방식을 지원합니다.&lt;/p&gt;

&lt;p&gt;저장소의 종류에는 다음과 같은 것들이 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;임시 디스크
    &lt;ul&gt;
      &lt;li&gt;emptyDir
        &lt;ul&gt;
          &lt;li&gt;Pod 이 생성되고 삭제될 때, 같이 생성되고 삭제되는 임시 디스크&lt;/li&gt;
          &lt;li&gt;생성 당시에는 아무 것도 없는 빈 상태&lt;/li&gt;
          &lt;li&gt;물리 디스크(노드), 메모리에 저장&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;로컬 디스크
    &lt;ul&gt;
      &lt;li&gt;hostpath
        &lt;ul&gt;
          &lt;li&gt;노드가 생성될 때 이미 존재하고 있는 디렉토리&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;네트워크 디스크
    &lt;ul&gt;
      &lt;li&gt;awsElasticBlockStore, azureDisk 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# emptydir&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;shared-volumes&lt;/span&gt; 
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;redis&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;shared-storage&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/data/shared&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;shared-storage&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/data/shared&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name &lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;shared-storage&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;emptyDir&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;{}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# hostpath&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;host-log&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;log&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;busybox&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;/bin/sh&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;-c&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;sleep&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s&quot;&gt;infinity&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;varlog&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/host/var/log&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;varlog&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/var/log&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/2019/05/19/kubernetes-basic-1.html&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dailyheumsi.tistory.com/208#7.-configmap&quot; target=&quot;_blank&quot;&gt;하나씩 점을 찍어나가며 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kubernetes.io/docs/reference/kubernetes-api/&quot; target=&quot;_blank&quot;&gt;Kubernetes 공식문서&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rancher.com/docs/rancher/v2.5/en/k8s-in-rancher/&quot; target=&quot;_blank&quot;&gt;Rancher 공식문서&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 09 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series2</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series2</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part1]: Kubernetes Intro</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#들어가기-전에&quot; id=&quot;markdown-toc-들어가기-전에&quot;&gt;들어가기 전에&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#도커의-등장&quot; id=&quot;markdown-toc-도커의-등장&quot;&gt;도커의 등장&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes&quot; id=&quot;markdown-toc-kubernetes&quot;&gt;Kubernetes&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#쿠버네티스-소개&quot; id=&quot;markdown-toc-쿠버네티스-소개&quot;&gt;쿠버네티스 소개&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#쿠버네티스-아키텍쳐&quot; id=&quot;markdown-toc-쿠버네티스-아키텍쳐&quot;&gt;쿠버네티스 아키텍쳐&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#desired-state&quot; id=&quot;markdown-toc-desired-state&quot;&gt;Desired State&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#마치며&quot; id=&quot;markdown-toc-마치며&quot;&gt;마치며&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;들어가기-전에&quot;&gt;들어가기 전에&lt;/h1&gt;

&lt;h2 id=&quot;도커의-등장&quot;&gt;도커의 등장&lt;/h2&gt;
&lt;p&gt;2013년 도커가 등장하기 전까지 서버 관리는 굉장히 어렵고 컨트롤하기 어려운 것으로 여겨졌습니다. 하나의 서비스를 제공하기 위해서는 보통 수십에서 수백개의 애플리케이션이 서로 연결되어 동작하는데, 이 때 오류가 나게 되면 어디서 문제가 생긴건지 파악하기가 쉽지 않았습니다.&lt;/p&gt;

&lt;p&gt;이러한 문제를 해결하기 위해 사람들은 &lt;strong&gt;가상화 기술&lt;/strong&gt; 을 이용해 서버를 애플리케이션별로 격리시키고자 하였습니다. 이 때 크게 두가지 방법으로 접근할 수 있는데, 하나는 &lt;strong&gt;가상머신&lt;/strong&gt; 을 이용해 컴퓨팅 리소스를 따로 분리하여 사용하도록 하는 것이었습니다. 하지만 이 방법은 컴퓨팅 성능을 떨어트립니다.&lt;br /&gt;
두 번째 방법은 LXC(LinuX Containers)라는 &lt;strong&gt;리눅스 커널 기술&lt;/strong&gt; 로 기존의 하드웨어 레벨에서 하던 방식을 운영체제 레벨에서 해결하도록 했습니다. 이렇게 하면 컴퓨팅 성능도 떨어트리지 않으면서, 파일시스템, 리소스(CPU, 메모리, 네트워크)를 분리할 수 있습니다. 하지만 이 방법은 사용하기에는 운영체제에 대한 깊은 이해를 필요로 해서 많은 개발자들이 쉽게 쓰기는 힘들었습니다.&lt;/p&gt;

&lt;p&gt;이 때 등장한 것이 바로 도커입니다. 도커가 등장하게 되면서 &lt;strong&gt;컨테이너 기술&lt;/strong&gt; 에 대한 접근성이 훨씬 좋아지게 되자, 개발자들은 이제 모든 애플리케이션을 컨테이너화하여 사용하기 시작했습니다. 이렇게 도커는 인프라 세계를 컨테이너 세상으로 바꿔버렸습니다. 수많은 애플리케이션이 컨테이너로 배포되고 도커파일을 만들어 이미지를 빌드하고 컨테이너를 배포하는 게 흔한 개발 프로세스가 되었습니다.&lt;/p&gt;

&lt;p&gt;(도커에 관한 더 자세한 내용은 &lt;a href=&quot;&quot;&gt;여기&lt;/a&gt;를 참고하시기 바랍니다)&lt;/p&gt;

&lt;p&gt;이제 모든 것들을 컨테이너화하기 시작하면서 우리의 서비스는 다음과 같은 모습을 가지게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 서비스 하나를 배포하기 위해 수많은 컨테이너를 띄우고, 연결하고, 버전업을 해야하는 상황이 생긴겁니다. 그래서 개발자들은 이제 컨테이너들을 동시에 띄우고 관리까지 해주는 &lt;strong&gt;컨테이너 오케스트레이션&lt;/strong&gt; 기술이 필요해지게 되었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;kubernetes&quot;&gt;Kubernetes&lt;/h1&gt;

&lt;h2 id=&quot;쿠버네티스-소개&quot;&gt;쿠버네티스 소개&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;쿠버네티스는 컨테이너를 쉽고 빠르게 배포/확장하고 관리를 자동화해주는 오픈소스 플랫폼입니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;쿠버네티스는 단순한 컨테이너 플랫폼을 넘어 마이크로서비스, 클라우드 플랫폼을 지향하고 컨테이너로 이루어진 것들을 손쉽게 담고 관리할 수 있는 그릇 역할을 합니다. 또한 CI/CD, 머신러닝 등 다양한 기능이 쿠버네티스 플랫폼 위에서 동작합니다.&lt;/p&gt;

&lt;p&gt;쿠버네티스는 컨테이너 규모, 컨테이너의 상태, 네트워크, 스토리지, 버전과 같은 것들을 관리하며 이를 자동화합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_3.png&quot; alt=&quot;&quot; width=&quot;50%&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;쿠버네티스-아키텍쳐&quot;&gt;쿠버네티스 아키텍쳐&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_23.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;마스터: 전체 클러스터를 관리하는 서버&lt;/li&gt;
  &lt;li&gt;노드: 컨테이너가 배포되는 서버&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;쿠버네티스에서 모든 명령은 마스터의 API 서버를 호출하고 노드는 마스터와 통신하면서 필요한 작업을 수행합니다. 특정 노드의 컨테이너에 명령하거나 로그를 조회할 때도 노드에 직접 명령하는 게 아니라 마스터에 명령을 내리고 마스터가 노드에 접속하여 대신 결과를 응답합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마스터의 API 서버는 할일이 굉장히 많기 때문에, 함께 도와줄 일꾼들이 필요합니다. 이들을 스케줄러와 컨트롤러라고 합니다. 보통 하나의 스케줄러와 역할별로 다양한 컨트롤러가 존재합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;컨트롤러: 자신이 맡은 오브젝트의 상태를 계속 체크하고 Desired 상태를 유지, API서버 요청 처리&lt;/li&gt;
  &lt;li&gt;스케줄러: 새로 생성되는 Pod(컨테이너와 비슷)가 있는지 계속 체크, 생성되면 가장 적절한 노드 선택&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;컨트롤러는 자신이 맡고 있는 오브젝트의 상태를 계속 체크하고 상태를 유지합니다. 또한 API 서버에서 어떤 새로운 상태를 요구할 경우, 맞춰서 또 상태를 바꿔서 유지하고 이 때 새롭게 Pod가 생성되거나 삭제되면 스케줄러가 그에 맞춰서 노드에서 삭제, 할당합니다.&lt;/p&gt;

&lt;h2 id=&quot;desired-state&quot;&gt;Desired State&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;쿠버네티스에서 가장 중요한 것은 &lt;strong&gt;desired state(원하는 상태)&lt;/strong&gt; 라는 개념입니다. 원하는 상태라 함은 관리자가 바라는 환경을 의미하고 좀 더 구체적으로는 얼마나 많은 웹서버가 떠 있으면 좋은지, 몇 번 포트로 서비스하기를 원하는지 등을 말합니다.&lt;br /&gt;
쿠버네티스는 복잡하고 다양한 작업을 하지만 자세히 들여다보면 현재 상태current state를 모니터링하면서 관리자가 설정한 원하는 상태를 유지하려고 내부적으로 이런저런 작업을 하는 로직을 가지고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 상태가 바뀌게 되면 API서버는 차이점을 발견하고 컨트롤러에게 보내 desired state로 유지할 것을 요청합니다. 그리고 컨트롤러가 변경한 후 결과를 다시 API서버에 보내고 API서버는 다시 이 결과를 etcd(상태를 저장하고 있는 곳)에 저장하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;마치며&quot;&gt;마치며&lt;/h1&gt;

&lt;p&gt;쿠버네티스는 여러 컨테이너를 자동으로 배포해주고 관리해준다는 점에서 정말 좋은 기술입니다. 그리고 마이크로서비스, 클라우드 환경과도 정말 잘 어울리기 때문에 배워두면 정말 쓸모가 많을 것 같습니다. 하지만 쿠버네티스는 많은 영역을 커버하다보니 배워야할 것들이 굉장히 많습니다. 그리고 컨테이너들을 띄우는 서버를 관리하기 위한 서버를 더 사용하게 되는 것이기 때문에, 컴퓨팅 자원이 충분하지 않다면 사용하는 것이 적절하지 않을 수도 있습니다. (쿠버네티스를 운영환경에 설치하기 위해선 최소 3대의 마스터 서버와 컨테이너 배포를 위한 n개의 노드 서버가 필요)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 포스트에서는 서버가 넉넉하지 않은 상황에서 사용할 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minikube&lt;/code&gt;를 설치, 그리고 쿠버네티스에 명령어를 전달할 때 사용하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; 설치해보겠습니다.&lt;br /&gt;
그리고 도커에서는 컨테이너를 띄우지만 쿠버네티스에서는 컨테이너를 관리할 수 있도록 조금 더 패키징한 다양한 오브젝트를 띄우게 되는데 이 때 어떠한 오브젝트들이 있는지도 배워보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/2019/05/19/kubernetes-basic-1.html&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sat, 08 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series1</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series1</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Docker Series [Part1]: Docker Intro</title>
                <description>&lt;h2 id=&quot;파이썬-데이터는-객체다&quot;&gt;파이썬 데이터는 객체다&lt;/h2&gt;
&lt;p&gt;컴퓨터 메모리를 일련의 긴 선반으로 생각할 수 있습니다. 해 당 메모리 선반 중 각 슬롯은 폭이 1바이트 입니다. 파이썬 프로그램은 운영체제에서 컴퓨터의 일부 메모리에 접근할 수 있습니다. 이 메모리는 프로그램 자체의 코드와 데이터를 위해 사용될 수 있습니다. 파이썬은 값을 직접 처리하는 대신, 메모리에 객체로 래핑합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/object.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;타입&quot;&gt;타입&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;이름&lt;/th&gt;
      &lt;th&gt;타입&lt;/th&gt;
      &lt;th&gt;가변&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;불리언&lt;/td&gt;
      &lt;td&gt;bool&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;정수&lt;/td&gt;
      &lt;td&gt;int&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;부동소수점&lt;/td&gt;
      &lt;td&gt;float&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;복소수&lt;/td&gt;
      &lt;td&gt;complex&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;문자열&lt;/td&gt;
      &lt;td&gt;str&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;튜플&lt;/td&gt;
      &lt;td&gt;tuple&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;바이트&lt;/td&gt;
      &lt;td&gt;bytes&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;프로즌 셋&lt;/td&gt;
      &lt;td&gt;frozenset&lt;/td&gt;
      &lt;td&gt;불변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;리스트&lt;/td&gt;
      &lt;td&gt;list&lt;/td&gt;
      &lt;td&gt;가변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;바이트 배열&lt;/td&gt;
      &lt;td&gt;bytearray&lt;/td&gt;
      &lt;td&gt;가변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;셋&lt;/td&gt;
      &lt;td&gt;set&lt;/td&gt;
      &lt;td&gt;가변&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;딕셔너리&lt;/td&gt;
      &lt;td&gt;dict&lt;/td&gt;
      &lt;td&gt;가변&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;가변성&quot;&gt;가변성&lt;/h2&gt;
&lt;p&gt;값을 변경할 수 있는 경우를 가변성이라고 합니다. 그러나 파이썬은 강타입 언어이기 때문에 타입을 변경할 수는 없습니다. 즉 객체가 가변성인 경우 값은 변경 가능하지만, 타입은 변경할 수 없습니다. (타입 변경을 하면 무조건 새로운 메모리에 객체가 새로 생성된다는 얘기입니다)&lt;/p&gt;

&lt;h2 id=&quot;참조&quot;&gt;참조&lt;/h2&gt;
&lt;p&gt;변수에 값을 할당할 때 알아야 할 중요한 사실은 할당은 값을 복사하는 것이 아니라, 단지 객체에 이름을 붙이는 것입니다. 이를 변수를 통해 객체를 참조한다라고 합니다. 또 한가지 중요한 사실은 왼쪽 그림에서 b가 참조하고 있던 값을 변경하면 정수는 불변 객체이기 때문에 새로운 값이 메모리에 생성되고 b는 새로운 값을 참조하지만,오른쪽 그림과 같이 가변 객체는 말 그대로 값을 변경할 수 있기 때문에 자신이 참조하고 있던 값을 변경해도 새로운 메모리에 값이 생성되는 것이 아니라 데이터 값을 그 자리에서 바꾸게 됩니다.&lt;br /&gt;
&lt;img src=&quot;/images/참조.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그럼 만약 불변 객체는 값을 바꿀 때 마다 메모리에 새로운 데이터를 생성하게 되는데 그러면 메모리가 엄청 낭비되지 않을까 라는 생각을 할 수 있습니다. 이를 해결해 주기 위해 파이썬에는 가비지 컬렉터가 있고 이는 더 이상 참조되지 않는 객체를 메모리에서 삭제될 수 있도록 도와줍니다.&lt;/p&gt;

&lt;h2 id=&quot;복사&quot;&gt;복사&lt;/h2&gt;

&lt;h3 id=&quot;변수의-대입을-통한-복사&quot;&gt;변수의 대입을 통한 복사&lt;/h3&gt;
&lt;p&gt;immutable한 객체인 숫자, 부울, 문자열, 튜플 등의 경우에는 변수의 대입을 통해 복사가 가능합니다.&lt;br /&gt;
&lt;img src=&quot;/images/복사.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하지만 다음과 같이 mutable한 객체의 경우에는 복사가 안되고 값이 같이 변경되게 됩니다.&lt;br /&gt;
&lt;img src=&quot;/images/복사_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;얕은-복사&quot;&gt;얕은 복사&lt;/h3&gt;
&lt;p&gt;또 하나의 가능한 복사 방법은 얕은 복사입니다. 얕은 복사는 mmutable한 객체에 대해서도 복사가 됩니다. 하지만 이 또한 문제가 발생하는 경우가 있습니다.  &lt;br /&gt;
&lt;img src=&quot;/images/shallow.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예를 들어 리스트와 같은 가변 객체 안에 또 가변 객체가 있게 되고 그 원소를 수정하려고 하면 진짜 복사가 아니었던 것이 드러나게 됩니다. 
밑에 그림을 보게 되면  가변 객체 안에 있는 가변 객체 원소를 수정하게 되면 새로운 메모리에 할당하지 않고 그냥 바꾸기 때문에 가만히 있던 변수까지 참조하고 있는 객체의 값이 덩달아 바뀌게 됩니다.&lt;br /&gt;
&lt;img src=&quot;/images/shallow_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;깊은-복사&quot;&gt;깊은 복사&lt;/h3&gt;
&lt;p&gt;따라서 이러한 경우에 필요한 것이 바로 깊은 복사입니다.&lt;br /&gt;
&lt;img src=&quot;/images/deepcopy.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Sat, 08 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/docker-intro</link>
                <guid isPermaLink="true">http://localhost:4000/docker-intro</guid>
                
                <category>Docker</category>
                
                
                <category>devops</category>
                
            </item>
        
    </channel>
</rss>
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Code Museum</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Sun, 31 Jul 2022 17:55:58 +0900</pubDate>
        <lastBuildDate>Sun, 31 Jul 2022 17:55:58 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>MySQL Series [Part14] MySQL Optimizing SELECT Statements</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#optimization-overview&quot; id=&quot;markdown-toc-optimization-overview&quot;&gt;Optimization Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#things-to-consider-for-optimization&quot; id=&quot;markdown-toc-things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#optimizing-select-statements&quot; id=&quot;markdown-toc-optimizing-select-statements&quot;&gt;Optimizing SELECT Statements&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#where-clause-optimization&quot; id=&quot;markdown-toc-where-clause-optimization&quot;&gt;WHERE Clause Optimization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;optimization-overview&quot;&gt;Optimization Overview&lt;/h1&gt;

&lt;p&gt;Database performance depends on several factors at the database level, such as tables, queries, and configuration settings. These software constructs result in CPU and I/O operations at the hardware level, which you must minimize and make as efficient as possible.&lt;/p&gt;

&lt;h1 id=&quot;things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Are the tables structured properly? In particular, do the columns have the right data types, and does each table have the appropriate columns for the type of work? For example, applications that perform frequent updates often have many tables with few columns, while applications that analyze large amounts of data often have few tables with many columns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are the right indexes in place to make queries efficient?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are you using the appropriate storage engine for each table, and taking advantage of the strengths and features of each storage engine you use? In particular, the choice of a transactional storage engine such as InnoDB or a nontransactional one such as MyISAM can be very important for performance and scalability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does each table use an appropriate row format? This choice also depends on the storage engine used for the table. In particular, compressed tables use less disk space and so require less disk I/O to read and write the data. Compression is available for all kinds of workloads with InnoDB tables, and for read-only MyISAM tables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does the application use an appropriate locking strategy? For example, by allowing shared access when possible so that database operations can run concurrently, and requesting exclusive access when appropriate so that critical operations get top priority. Again, the choice of storage engine is significant. The InnoDB storage engine handles most locking issues without involvement from you, allowing for better concurrency in the database and reducing the amount of experimentation and tuning for your code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are all memory areas used for caching sized correctly? That is, large enough to hold frequently accessed data, but not so large that they overload physical memory and cause paging. The main memory areas to configure are the InnoDB buffer pool and the MyISAM key cache.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;optimizing-select-statements&quot;&gt;Optimizing SELECT Statements&lt;/h1&gt;

&lt;h2 id=&quot;where-clause-optimization&quot;&gt;WHERE Clause Optimization&lt;/h2&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html&quot; target=&quot;_blank&quot;&gt;MySQL 공식문서: Optimizing SELECT Statements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://phoenixnap.com/kb/improve-mysql-performance-tuning-optimization&quot; target=&quot;_blank&quot;&gt;MySQL Performance Tuning and Optimization Tips&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series14</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series14</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part19]: 로그 구조 스토리지</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;
</description>
                <pubDate>Mon, 25 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series19</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series19</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part18]: APIs</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#event-driven-api&quot; id=&quot;markdown-toc-event-driven-api&quot;&gt;Event Driven API&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#web-api&quot; id=&quot;markdown-toc-web-api&quot;&gt;Web API&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;event-driven-api&quot;&gt;Event Driven API&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Webhooks&lt;/li&gt;
  &lt;li&gt;Websockets&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;web-api&quot;&gt;Web API&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;REST&lt;/li&gt;
  &lt;li&gt;RPC&lt;/li&gt;
  &lt;li&gt;GraphQL&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 24 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series18</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series18</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part17]: 분산 시스템(Distributed Systems)의 핵심</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#장애-대응fault-tolerance&quot; id=&quot;markdown-toc-장애-대응fault-tolerance&quot;&gt;장애 대응(Fault Tolerance)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#장애-감지와-회복&quot; id=&quot;markdown-toc-장애-감지와-회복&quot;&gt;장애 감지와 회복&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#리더와-팔로워&quot; id=&quot;markdown-toc-리더와-팔로워&quot;&gt;리더와 팔로워&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#복제&quot; id=&quot;markdown-toc-복제&quot;&gt;복제&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#일관성-보장&quot; id=&quot;markdown-toc-일관성-보장&quot;&gt;일관성 보장&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#합의-알고리즘&quot; id=&quot;markdown-toc-합의-알고리즘&quot;&gt;합의 알고리즘&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-처리distributed-computing&quot; id=&quot;markdown-toc-분산-처리distributed-computing&quot;&gt;분산 처리(Distributed Computing)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#시스템-확장scale-out&quot; id=&quot;markdown-toc-시스템-확장scale-out&quot;&gt;시스템 확장(Scale-out)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;distributed system은 fault tolerance 하기 위해 여러 서버에 replication. replication할 때 유의할 점이 consistency (어떤 서버가 선택되어도 클라이언트에게 일관된 결과를 돌려 주는가). consistency를 제공하기 위한 것이 합의 알고리즘?&lt;/p&gt;

&lt;h1 id=&quot;장애-대응fault-tolerance&quot;&gt;장애 대응(Fault Tolerance)&lt;/h1&gt;

&lt;h2 id=&quot;장애-감지와-회복&quot;&gt;장애 감지와 회복&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Heartbeats and Ping&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;리더와-팔로워&quot;&gt;리더와 팔로워&lt;/h2&gt;

&lt;p&gt;동기화는 비용이 많이 들 수 있다. 동기화 오버헤드를 줄이기 위해 일부 알고리즘에는 분산 알고리즘의 단계를 수행 및 조정하는 리더 프로세스가 있다. 일반적으로 분산 시스템의 프로세스는 균일하고 모든 프로세스가 리더 역할을 맡을 수 있다. 장애가 발생하면 어떤 프로세스라도 리더 선출 과정을 시작할 수 있고 선출된 프로세스는 이전 리더의 작업을 이어서 수행한다.&lt;/p&gt;

&lt;p&gt;이상적으로는 한 번에 하나의 리더만 존재하고, 여러 리더가 선출되고 서로 존재를 모르는 상황(split brain 현상)은 절대로 발생하지 않아야 한다. 하지만 많은 리더 선출 알고리즘이 이 조건을 위반한다. 이 문제를 해결하기 위해서는 클러스터 구성원 과반수의 동의가 필요하다. 멀티 팍소스와 래프트를 비롯한 여러 합의 알고리즘은 리더 프로세스가 조율을 담당한다.&lt;/p&gt;

&lt;p&gt;선출 작업은 결정론적이어야 한다. 정확히 하나의 리더를 선출하고 모든 참가자가 결과를 인정해야 한다. 선출된 새로운 리더는 모든 구성원에게 자신의 존재를 알려야 한다.&lt;/p&gt;

&lt;p&gt;리더 프로세스가 있는 시스템의 가장 큰 문제는 리더가 병목 지점이 될 수 있다는 점이다. 이 문제를 해결하기 위해 많은 시스템이 데이터를 독립적인 파티션으로 나누고 각 파티션별로 리더를 선출한다. 스패너(Spanner)가 이 방식을 사용한다.&lt;/p&gt;

&lt;p&gt;선출 작업은 비용이 높은 작업이지만 자주 수행되지 않기 때문에 시스템 성능에 큰 영향을 주지 않는다.&lt;/p&gt;

&lt;p&gt;리더의 정체는 프로세스가 모르는 사이에 바뀔 수 있다. 따라서 각 프로세스가 개별적으로 알고 있는 리더에 대한 정보가 유효한지 확인해야 한다. 이 문제는 리더 선출 알고리즘과 장애 감지 알고리즘을 같이 사용해 해결할 수 있다. 예를 들어 안정적인 리더 선출 알고리즘은 안정적인 프로세스에 리더의 기회를 주고 타임아웃 기반의 장애 감지 알고리즘을 사용해 해당 프로세스에 장애가 발생하지 않고 접근이 가능한 동안 계속해서 리더 역할을 유지할 수 있도록 보장한다.&lt;/p&gt;

&lt;p&gt;리더에 의존하는 대부분의 알고리즘은 여러 리더가 존재하는 것을 허용하고 리더 사이의 충돌을 최대한 빠르게 해결한다.&lt;/p&gt;

&lt;h2 id=&quot;복제&quot;&gt;복제&lt;/h2&gt;

&lt;p&gt;In a distributed system data is stored is over different computers in a network. Therefore, we need to make sure that data is readily available for the users. Availability of the data is an important factor often accomplished by data replication. Replication is the practice of keeping several copies of data in different places.&lt;/p&gt;

&lt;p&gt;Why do we require replication?&lt;br /&gt;
The first and foremost thing is that it makes our system more stable because of node replication. It is good to have replicas of a node in a network due to following reasons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If a node stops working, the distributed network will still work fine due to its replicas which will be there. Thus it increases the fault tolerance of the system.&lt;/li&gt;
  &lt;li&gt;It also helps in load sharing where loads on a server are shared among different replicas.&lt;/li&gt;
  &lt;li&gt;It enhances the availability of the data. If the replicas are created and data is stored near to the consumers, it would be easier and faster to fetch data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Types of Replication&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Active Replication&lt;/li&gt;
  &lt;li&gt;Passive Replication&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Active Replication&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The request of the client goes to all the replicas.&lt;/li&gt;
  &lt;li&gt;It is to be made sure that every replica receives the client request in the same order else the system will get inconsistent.&lt;/li&gt;
  &lt;li&gt;There is no need for coordination because each copy processes the same request in the same sequence.&lt;/li&gt;
  &lt;li&gt;All replicas respond to the client’s request.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is really simple. The codes in active replication are the same throughout.&lt;/li&gt;
  &lt;li&gt;It is transparent.&lt;/li&gt;
  &lt;li&gt;Even if a node fails, it will be easily handled by replicas of that node.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It increases resource consumption. The greater the number of replicas, the greater the memory needed.&lt;/li&gt;
  &lt;li&gt;It increases the time complexity. If some change is done on one replica it should also be done in all others.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Passive Replication&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The client request goes to the primary replica, also called the main replica.&lt;/li&gt;
  &lt;li&gt;There are more replicas that act as backup for the primary replica.&lt;/li&gt;
  &lt;li&gt;Primary replica informs all other backup replicas about any modification done.&lt;/li&gt;
  &lt;li&gt;The response is returned to the client by a primary replica.&lt;/li&gt;
  &lt;li&gt;Periodically primary replica sends some signal to backup replicas to let them know that it is working perfectly fine.&lt;/li&gt;
  &lt;li&gt;In case of failure of a primary replica, a backup replica becomes the primary replica.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The resource consumption is less as backup servers only come into play when the primary server fails.&lt;/li&gt;
  &lt;li&gt;The time complexity of this is also less as there’s no need for updating in all the nodes replicas, unlike active replication.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If some failure occurs, the response time is delayed.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;일관성-보장&quot;&gt;일관성 보장&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;CAP Theorem&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Eventual Consistency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Eventual consistency is a consistency model that enables the data store to be highly available. It is also known as optimistic replication &amp;amp; is key to distributed systems. So, how exactly does it work? Let’s Understand this with the help of a use case.&lt;/p&gt;

&lt;p&gt;Real World Use Case :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Think of a popular microblogging site deployed across the world in different geographical regions like Asia, America, and Europe. Moreover, each geographical region has multiple data center zones: North, East, West, and South.&lt;/li&gt;
  &lt;li&gt;Furthermore, each zone has multiple clusters which have multiple server nodes running. So, we have many datastore nodes spread across the world that micro-blogging site uses for persisting data. Since there are so many nodes running, there is no single point of failure.&lt;/li&gt;
  &lt;li&gt;The data store service is highly available. Even if a few nodes go down persistence service is still up. Let’s say a celebrity makes a post on the website that everybody starts liking around the world.&lt;/li&gt;
  &lt;li&gt;At a point in time, a user in Japan likes a post which increases the “Like” count of the post from say 100 to 101. At the same point in time, a user in America, in a different geographical zone, clicks on the post, and he sees “Like” count as 100, not 101.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Reason for the above Use case :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simply, because the new updated value of the Post “Like” counter needs some time to move from Japan to America and update server nodes running there. Though the value of the counter at that point in time was 101, the user in America sees old inconsistent values.&lt;/li&gt;
  &lt;li&gt;But when he refreshes his web page after a few seconds “Like” counter value shows as 101. So, data was initially inconsistent but eventually got consistent across server nodes deployed around the world. This is what eventual consistency is.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Strong Consistency&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Strong Consistency simply means the data must be strongly consistent at all times. All the server nodes across the world should contain the same value as an entity at any point in time. And the only way to implement this behavior is by locking down the nodes when being updated.&lt;/p&gt;

&lt;p&gt;Real World Use Case :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Let’s continue the same Eventual Consistency example from the previous lesson. To ensure Strong Consistency in the system, when a user in Japan likes posts, all nodes across different geographical zones must be locked down to prevent any concurrent updates.&lt;/li&gt;
  &lt;li&gt;This means at one point in time, only one user can update the post “Like” counter value. So, once a user in Japan updates the “Like” counter from 100 to 101. The value gets replicated globally across all nodes. Once all nodes reach consensus, locks get lifted. Now, other users can Like posts.&lt;/li&gt;
  &lt;li&gt;If the nodes take a while to reach a consensus, they must wait until then. Well, this is surely not desired in the case of social applications. But think of a stock market application where the users are seeing different prices of the same stock at one point in time and updating it concurrently. This would create chaos. Therefore, to avoid this confusion we need our systems to be Strongly Consistent.&lt;/li&gt;
  &lt;li&gt;The nodes must be locked down for updates. Queuing all requests is one good way of making a system Strongly Consistent. The strong Consistency model hits the capability of the system to be Highly Available &amp;amp; perform concurrent updates. This is how strongly consistent ACID transactions are implemented.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ACID Transaction Support&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Distributed systems like NoSQL databases which scale horizontally on the fly don’t support ACID transactions globally &amp;amp; this is due to their design. The whole reason for the development of NoSQL tech is the ability to be Highly Available and Scalable. If we must lock down nodes every time, it becomes just like SQL. So, NoSQL databases don’t support ACID transactions and those that claim to, have terms and conditions applied to them. Generally, transaction support is limited to a geographic zone or an entity hierarchy. Developers of tech make sure that all the Strongly consistent entity nodes reside in the same geographic zone to make ACID transactions possible.&lt;/p&gt;

&lt;p&gt;Conclusion: For transactional things go for MySQL because it provides a lock-in feature and supports ACID transactions.&lt;/p&gt;

&lt;h2 id=&quot;합의-알고리즘&quot;&gt;합의 알고리즘&lt;/h2&gt;

&lt;p&gt;Consensus is a general agreement on a decision made by the majority of those involved. For example, the problem may be as simple as friends trying to decide which restaurant has multiple options to choose from or complex as decisions on distributed systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Need of consensus in a distributed system&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In a distributed system, nodes are distributed across the network. Some of these nodes might get failed(crash fault) or starts behaving abnormally (Byzantine Fault). In such a scenario, it becomes difficult to come to a common decision. More concisely,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There are n processes, m of which may be faulty.&lt;/li&gt;
  &lt;li&gt;The task is to make all the Nonfaulty processes agree on some value(s) even in the presence of the faulty processes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So we can remove these problems by given below solutions:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Consensus Without Any Fault&lt;/li&gt;
  &lt;li&gt;Consensus With at most m Crash Faults&lt;/li&gt;
  &lt;li&gt;Consensus With at most m Byzantine Faults&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;멀티 팍소스&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;래프트&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;분산-처리distributed-computing&quot;&gt;분산 처리(Distributed Computing)&lt;/h1&gt;

&lt;h1 id=&quot;시스템-확장scale-out&quot;&gt;시스템 확장(Scale-out)&lt;/h1&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/consensus-problem-of-distributed-systems/?ref=gcse&quot;&gt;GeeksforGeeks: Consensus Problem of Distributed Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sat, 23 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series17</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series17</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part16]: 분산 시스템(Distributed Systems)에서의 네트워크와 운영체제</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#운영체제-관점&quot; id=&quot;markdown-toc-운영체제-관점&quot;&gt;운영체제 관점&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#네트워크-inter-process-communication&quot; id=&quot;markdown-toc-네트워크-inter-process-communication&quot;&gt;네트워크: Inter Process Communication&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#remote-procedure-callrpc&quot; id=&quot;markdown-toc-remote-procedure-callrpc&quot;&gt;Remote Procedure Call(RPC)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#message-based-communication&quot; id=&quot;markdown-toc-message-based-communication&quot;&gt;Message based Communication&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sockets&quot; id=&quot;markdown-toc-sockets&quot;&gt;Sockets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;운영체제-관점&quot;&gt;운영체제 관점&lt;/h1&gt;

&lt;h1 id=&quot;네트워크-inter-process-communication&quot;&gt;네트워크: Inter Process Communication&lt;/h1&gt;

&lt;p&gt;Inter-process communication (IPC) is set of interfaces, which is usually programmed in order for the programs to communicate between series of processes. This allows running programs concurrently in an Operating System.&lt;/p&gt;

&lt;p&gt;Interprocess communication is at the heart of all distributed systems. It makes no sense to study distributed systems without carefully examining the ways that processes on different machines can exchange information. Communication in distributed systems is always based on low-level message passing as offered by the underlying network. Expressing communication through message passing is harder than using primitives based on shared memory, as available for nondistrib- uted platforms. Modem distributed systems often consist of thousands or even millions of processes scattered across a network with unreliable communication such as the Internet. Unless the primitive communication facilities of computer networks are replaced by something else, development of large-scale distributed applications is extremely difficult.&lt;/p&gt;

&lt;p&gt;In this chapter, we start by discussing the rules that communicating processes must adhere to, known as protocols, and concentrate on structuring those proto- cols in the form of layers. We then look at three widely-used models for commu- nication: Remote Procedure Call (RPC), Message-Oriented Middleware (MOM), and data streaming. We also discuss the general problem of sending data to multi- ple receivers, called multicasting.&lt;/p&gt;

&lt;p&gt;Our first model for communication in distributed systems is the remote proce- dure call (RPC). An RPC aims at hiding most of the intricacies of message pass- ing, and is ideal for client-server applications.&lt;/p&gt;

&lt;p&gt;In many distributed applications, communication does not follow the rather strict pattern of client-server interaction. In those cases, it turns out that thinking in terms of messages is more appropriate. However, the low-level communication facilities of computer networks are in many ways not suitable due to their lack of distribution transparency.&lt;/p&gt;

&lt;p&gt;An alternative is to use a high-level message-queuing model, in which communication proceeds much the same as in electronic maiI systems. Message-oriented middleware (MOM) is a subject important enough to warrant a section of its own.&lt;/p&gt;

&lt;p&gt;With the advent of multimedia distributed systems, it became apparent that
many systems were lacking support for communication of continuous media, such as audio and video. What is needed is the notion of a stream that can support the continuous flow of messages, subject to various timing constraints. Streams are discussed in a separate section.&lt;/p&gt;

&lt;p&gt;Finally, since our understanding of setting up multicast facilities has im- proved, novel and elegant solutions for data dissemination have emerged. We pay separate attention to this subject in the last section of this chapter.&lt;/p&gt;

&lt;h2 id=&quot;remote-procedure-callrpc&quot;&gt;Remote Procedure Call(RPC)&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Remote Procedure Call (RPC) is a communication technology that is used by one program to make a request to another program for utilizing its service on a network without even knowing the network’s details. A function call or a subroutine call are other terms for a procedure call.&lt;/p&gt;

&lt;p&gt;It is based on the client-server concept. The client is the program that makes the request, and the server is the program that gives the service. An RPC, like a local procedure call, is based on the synchronous operation that requires the requesting application to be stopped until the remote process returns its results. Multiple RPCs can be executed concurrently by utilizing lightweight processes or threads that share the same address space.&lt;/p&gt;

&lt;p&gt;Remote Procedure Call program as often as possible utilizes the Interface Definition Language (IDL), a determination language for describing a computer program component’s Application Programming Interface (API). In this circumstance, IDL acts as an interface between machines at either end of the connection, which may be running different operating systems and programming languages.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Elements of RPC&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/rpc_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Client&lt;/strong&gt;: The client process initiates RPC. The client makes a standard call, which triggers a correlated procedure in the client stub.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Client Stub&lt;/strong&gt;: Stubs are used by RPC to achieve semantic transparency. The client calls the client stub. Client stub does the following tasks:
    &lt;ul&gt;
      &lt;li&gt;The first task performed by client stub is when it receives a request from a client, it packs(marshalls) the parameters and required specifications of remote/target procedure in a message.&lt;/li&gt;
      &lt;li&gt;The second task performed by the client stub is upon receiving the result values after execution, it unpacks (unmarshalled) those results and sends them to the Client.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;RPC Runtime&lt;/strong&gt;: The RPC runtime is in charge of message transmission between client and server via the network. Retransmission, acknowledgment, routing, and encryption are all tasks performed by it. On the client-side, it receives the result values in a message from the server-side, and then it further sends it to the client stub whereas, on the server-side, RPC Runtime got the same message from the server stub when then it forwards to the client machine. It also accepts and forwards client machine call request messages to the server stub.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Server Stub&lt;/strong&gt;: Server stub does the following tasks:
    &lt;ul&gt;
      &lt;li&gt;The first task performed by server stub is that it unpacks(unmarshalled) the call request message which is received from the local RPC Runtime and makes a regular call to invoke the required procedure in the server.&lt;/li&gt;
      &lt;li&gt;The second task performed by server stub is that when it receives the server’s procedure execution result, it packs it into a message and asks the local RPC Runtime to transmit it to the client stub where it is unpacked.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Server&lt;/strong&gt;: After receiving a call request from the client machine, the server stub passes it to the server. The execution of the required procedure is made by the server and finally, it returns the result to the server stub so that it can be passed to the client machine using the local RPC Runtime.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Working Procedure for RPC Model&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The process arguments are placed in a precise location by the caller when the procedure needs to be called.&lt;/li&gt;
  &lt;li&gt;Control at that point passed to the body of the method, which is having a series of instructions.&lt;/li&gt;
  &lt;li&gt;The procedure body is run in a recently created execution environment that has duplicates of the calling instruction’s arguments.&lt;/li&gt;
  &lt;li&gt;At the end, after the completion of the operation, the calling point gets back the control, which returns a result.
    &lt;ul&gt;
      &lt;li&gt;The call to a procedure is possible only for those procedures that are not within the caller’s address space because both processes (caller and callee) have distinct address space and the access is restricted to the caller’s environment’s data and variables from the remote procedure.&lt;/li&gt;
      &lt;li&gt;The caller and callee processes in the RPC communicate to exchange information via the message-passing scheme.&lt;/li&gt;
      &lt;li&gt;The first task from the server-side is to extract the procedure’s parameters when a request message arrives, then the result, send a reply message, and finally wait for the next call message.&lt;/li&gt;
      &lt;li&gt;Only one process is enabled at a certain point in time.&lt;/li&gt;
      &lt;li&gt;The caller is not always required to be blocked.&lt;/li&gt;
      &lt;li&gt;The asynchronous mechanism could be employed in the RPC that permits the client to work even if the server has not responded yet.&lt;/li&gt;
      &lt;li&gt;In order to handle incoming requests, the server might create a thread that frees the server for handling consequent requests.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages of Remote Procedure Calls&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The technique of using procedure calls in RPC permits high-level languages to provide communication between clients and servers.&lt;/li&gt;
  &lt;li&gt;This method is like a local procedure call but with the difference that the called procedure is executed on another process and a different computer.&lt;/li&gt;
  &lt;li&gt;The thread-oriented model is also supported by RPC in addition to the process model.&lt;/li&gt;
  &lt;li&gt;The RPC mechanism is employed to conceal the core message passing method.&lt;/li&gt;
  &lt;li&gt;The amount of time and effort required to rewrite and develop the code is minimal.&lt;/li&gt;
  &lt;li&gt;The distributed and local environments can both benefit from remote procedure calls.&lt;/li&gt;
  &lt;li&gt;To increase performance, it omits several of the protocol layers.&lt;/li&gt;
  &lt;li&gt;Abstraction is provided via RPC.  To exemplify, the user is not known about the nature of message-passing in network communication.&lt;/li&gt;
  &lt;li&gt;RPC empowers the utilization of applications in a distributed environment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Disadvantages of Remote Procedure Calls&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In Remote Procedure Calls parameters are only passed by values as pointer values are not allowed.&lt;/li&gt;
  &lt;li&gt;It involves a communication system with another machine and another process, so this mechanism is extremely prone to failure.&lt;/li&gt;
  &lt;li&gt;The RPC concept can be implemented in a variety of ways, hence there is no standard.&lt;/li&gt;
  &lt;li&gt;Due to the interaction-based nature, there is no flexibility for hardware architecture in RPC.&lt;/li&gt;
  &lt;li&gt;Due to a remote procedure call, the process’s cost has increased.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;message-based-communication&quot;&gt;Message based Communication&lt;/h2&gt;

&lt;p&gt;In the development of models and technologies, message abstraction is a necessary aspect that enables distributed computing. Distributed system is defined as a system in which components reside at networked communication and synchronise its functions only by movement of messages. In this, message recognizes any discrete data that is moved from one entity to another. It includes any kind of data representation having restriction of size and time, whereas it invokes a remote procedure or a sequence of object instance or a common message. This is the reason that “message-based communication model” can be beneficial to refer various model for inter-process communication, which is based on the data streaming abstraction.&lt;/p&gt;

&lt;p&gt;Various distributed programming model use this type of communication despite of the abstraction which is shown to developers for programming the co-ordination of shared components. Below are some major distributed programming models that uses “message-based communication model”&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Message Passing
    &lt;ul&gt;
      &lt;li&gt;In this model, the concept of message as the major abstraction of model is introduced. The units which inter-change the data and information that is explicitly encode, in the form of message. According to then model, the schema and content of message changes or varies. Message Passing Interface and OpenMP are major example of this type of model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Remote Procedure Call
    &lt;ul&gt;
      &lt;li&gt;This model explores the keys of procedure call beyond the restrictions of a single process, thus pointing the execution of program in remote processes. In this, primary client-server is implied. A remote process maintains a server component, thus enabling client processes to invoke the approaches and returns the output of the execution. Messages, created by the Remote Procedure Call (RPC) implementation, retrieve the information of the procedure itself and that procedure is to execute having necessary arguments and also returns the values. The use of messages regarding this referred as marshal-ling of the arguments and return values.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sockets&quot;&gt;Sockets&lt;/h2&gt;

&lt;p&gt;This method is mostly used to communicate over a network between a client and a server. It allows for a standard connection which is computer and OS independent.&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/interprocess-communication-in-distributed-systems/?ref=gcse&quot; target=&quot;_blank&quot;&gt;GeeksforGeeks: Interprocess Communication in Distributed Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&amp;amp;blogId=horajjan&amp;amp;logNo=220956169499&quot; target=&quot;_blank&quot;&gt;조원호의 행복공간: [네트워크] IPC와 RPC의 차이점&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 22 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series16</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series16</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part15]: 분산 시스템(Distributed Systems) 개요</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-시스템&quot; id=&quot;markdown-toc-분산-시스템&quot;&gt;분산 시스템&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#개요&quot; id=&quot;markdown-toc-개요&quot;&gt;개요&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#특징&quot; id=&quot;markdown-toc-특징&quot;&gt;특징&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#장점&quot; id=&quot;markdown-toc-장점&quot;&gt;장점&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#단점&quot; id=&quot;markdown-toc-단점&quot;&gt;단점&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#아키텍처&quot; id=&quot;markdown-toc-아키텍처&quot;&gt;아키텍처&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#어려운-점&quot; id=&quot;markdown-toc-어려운-점&quot;&gt;어려운 점&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#synchronization&quot; id=&quot;markdown-toc-synchronization&quot;&gt;Synchronization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#network-partition&quot; id=&quot;markdown-toc-network-partition&quot;&gt;Network Partition&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-파일-시스템&quot; id=&quot;markdown-toc-분산-파일-시스템&quot;&gt;분산 파일 시스템&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-데이터베이스&quot; id=&quot;markdown-toc-분산-데이터베이스&quot;&gt;분산 데이터베이스&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-처리&quot; id=&quot;markdown-toc-분산-처리&quot;&gt;분산 처리&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#분산-시스템과-쿠버네티스&quot; id=&quot;markdown-toc-분산-시스템과-쿠버네티스&quot;&gt;분산 시스템과 쿠버네티스&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;분산-시스템&quot;&gt;분산 시스템&lt;/h1&gt;

&lt;h2 id=&quot;개요&quot;&gt;개요&lt;/h2&gt;

&lt;p&gt;Distributed System is a collection of autonomous computer systems that are physically separated but are connected by a centralized computer network that is equipped with distributed system software. The autonomous computers will communicate among each system by sharing resources and files and performing the tasks assigned to them.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;특징&quot;&gt;특징&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Resource Sharing&lt;/strong&gt;: It is the ability to use any Hardware, Software, or Data anywhere in the System.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Openness&lt;/strong&gt;: It is concerned with Extensions and improvements in the system (i.e., How openly the software is developed and shared with                                others)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Concurrency&lt;/strong&gt;: It is naturally present in the Distributed Systems, that deal with the same activity or functionality that can be performed by separate users who are in remote locations. Every local system has its independent Operating Systems and Resources.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Scalability&lt;/strong&gt;: It increases the scale of the system as a number of processors communicate with more users by accommodating to improve the responsiveness of the system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fault tolerance&lt;/strong&gt;: It cares about the reliability of the system if there is a failure in Hardware or Software, the system continues to operate properly without degrading the performance the system.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Transparency&lt;/strong&gt;: It hides the complexity of the Distributed Systems to the Users and Application programs as there should be privacy in every system.(유저는 자신이 사용하는 어플리케이션이 분산시스템인지 인지하지 못함 -&amp;gt; 단일 시스템을 사용하는 것처럼 사용할 수 있음)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;장점&quot;&gt;장점&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Applications in Distributed Systems are Inherently Distributed Applications.&lt;/li&gt;
  &lt;li&gt;Information in Distributed Systems is shared among geographically distributed users.&lt;/li&gt;
  &lt;li&gt;Resource Sharing (Autonomous systems can share resources from remote locations).&lt;/li&gt;
  &lt;li&gt;It has a better price performance ratio and flexibility.&lt;/li&gt;
  &lt;li&gt;It has shorter response time and higher throughput.&lt;/li&gt;
  &lt;li&gt;It has higher reliability and availability against component failure.&lt;/li&gt;
  &lt;li&gt;It has extensibility so that systems can be extended in more remote locations and also incremental growth.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;단점&quot;&gt;단점&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Relevant Software for Distributed systems does not exist currently.&lt;/li&gt;
  &lt;li&gt;Security possess a problem due to easy access to data as the resources are shared to multiple systems.&lt;/li&gt;
  &lt;li&gt;Networking Saturation may cause a hurdle in data transfer i.e., if there is a lag in the network then the user will face a problem accessing data.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;아키텍처&quot;&gt;아키텍처&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Centralized Architecture&lt;/li&gt;
  &lt;li&gt;Decentralized Architecture&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;어려운-점&quot;&gt;어려운 점&lt;/h2&gt;

&lt;h3 id=&quot;synchronization&quot;&gt;Synchronization&lt;/h3&gt;

&lt;p&gt;Distributed System is a collection of computers connected via the high speed communication network. In the distributed system, the hardware and software components communicate and coordinate their actions by message passing. Each node in distributed systems can share their resources with other nodes. So, there is need of proper allocation of resources to preserve the state of resources and help coordinate between the several processes. To resolve such conflicts, synchronization is used. Synchronization in distributed systems is achieved via clocks.&lt;/p&gt;

&lt;p&gt;The physical clocks are used to adjust the time of nodes.Each node in the system can share its local time with other nodes in the system. The time is set based on UTC (Universal Time Coordination). UTC is used as a reference time clock for the nodes in the system.&lt;/p&gt;

&lt;p&gt;The clock synchronization can be achieved by 2 ways: External and Internal Clock Synchronization.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;External clock synchronization is the one in which an external reference clock is present. It is used as a reference and the nodes in the system can set and adjust their time accordingly.&lt;/li&gt;
  &lt;li&gt;Internal clock synchronization is the one in which each node shares its time with other nodes and all the nodes set and adjust their times accordingly.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are 2 types of clock synchronization algorithms: Centralized and Distributed.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Centralized is the one in which a time server is used as a reference. The single time server propagates its time to the nodes and all the nodes adjust the time accordingly. It is dependent on single time server so if that node fails, the whole system will lose synchronization. Examples of centralized are- Berkeley Algorithm, Passive Time Server, Active Time Server etc.&lt;/li&gt;
  &lt;li&gt;Distributed is the one in which there is no centralized time server present. Instead the nodes adjust their time by using their local time and then, taking the average of the differences of time with other nodes. Distributed algorithms overcome the issue of centralized algorithms like the scalability and single point failure. Examples of Distributed algorithms are – Global Averaging Algorithm, Localized Averaging Algorithm, NTP (Network time protocol) etc.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;network-partition&quot;&gt;Network Partition&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;분산 시스템을 이용한 인터넷 서비스에서 사용하는 어플리케이션의 아키텍처는 대부분 비공유 아키텍처(Shared-nothing)
    &lt;ul&gt;
      &lt;li&gt;각 노드는 CPU, RAM, 디스크를 독립적으로 사용(디스크도 독립된다는 점에서 Shared Disk Architecture와 다름)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;비공유 시스템에서 장비들이 통신하는 유일한 수단은 네트워크&lt;/li&gt;
  &lt;li&gt;전송 측은 패킷이 전송 된건지, 네트워크 문제인지, 수신 측 서버 문제인지 알 수 없으며 심지어 전달이 실패한건지 아니면 지연된건지 조차 알 수 없음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;분산-파일-시스템&quot;&gt;분산 파일 시스템&lt;/h1&gt;

&lt;p&gt;A Distributed File System (DFS) as the name suggests, is a file system that is distributed on multiple file servers or multiple locations. It allows programs to access or store isolated files as they do with the local ones, allowing programmers to access files from any network or computer.&lt;/p&gt;

&lt;p&gt;The main purpose of the Distributed File System (DFS) is to allows users of physically distributed systems to share their data and resources by using a Common File System. A collection of workstations and mainframes connected by a Local Area Network (LAN) is a configuration on Distributed File System. A DFS is executed as a part of the operating system. In DFS, a namespace is created and this process is transparent for the clients.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Working of DFS&lt;/strong&gt;&lt;br /&gt;
There are two ways in which DFS can be implemented:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Standalone DFS namespace
    &lt;ul&gt;
      &lt;li&gt;It allows only for those DFS roots that exist on the local computer and are not using Active Directory. A Standalone DFS can only be acquired on those computers on which it is created. It does not provide any fault liberation and cannot be linked to any other DFS. Standalone DFS roots are rarely come across because of their limited advantage.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Domain-based DFS namespace
    &lt;ul&gt;
      &lt;li&gt;It stores the configuration of DFS in Active Directory, creating the DFS namespace root accessible at \&lt;domainname&gt;\&lt;dfsroot&gt; or \\&lt;FQDN&gt;\&lt;dfsroot&gt;&lt;/dfsroot&gt;&lt;/FQDN&gt;&lt;/dfsroot&gt;&lt;/domainname&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/dist_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Advantages&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DFS allows multiple user to access or store the data.&lt;/li&gt;
  &lt;li&gt;It allows the data to be share remotely.&lt;/li&gt;
  &lt;li&gt;It improved the availability of file, access time, and network efficiency.&lt;/li&gt;
  &lt;li&gt;Improved the capacity to change the size of the data and also improves the ability to exchange the data.&lt;/li&gt;
  &lt;li&gt;Distributed File System provides transparency of data even if server or disk fails.
&lt;strong&gt;Disadvantages&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;In Distributed File System nodes and connections needs to be secured therefore we can say that security is at stake.&lt;/li&gt;
  &lt;li&gt;There is a possibility of lose of messages and data in the network while movement from one node to another.&lt;/li&gt;
  &lt;li&gt;Database connection in case of Distributed File System is complicated.&lt;/li&gt;
  &lt;li&gt;Also handling of the database is not easy in Distributed File System as compared to a single user system.&lt;/li&gt;
  &lt;li&gt;There are chances that overloading will take place if all nodes tries to send data at once.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;분산-데이터베이스&quot;&gt;분산 데이터베이스&lt;/h1&gt;

&lt;h1 id=&quot;분산-처리&quot;&gt;분산 처리&lt;/h1&gt;

&lt;h1 id=&quot;분산-시스템과-쿠버네티스&quot;&gt;분산 시스템과 쿠버네티스&lt;/h1&gt;

&lt;p&gt;쿠버네티스는 ‘A라는 컨테이너화된 프로세스를 어느 서버에 띄울까? 프로세스를 몇 대의 서버에 복제해두고 일부 서버가 장애가 생기면 다른 서버에 있는 프로세스로 대체’&lt;/p&gt;

&lt;p&gt;분산 시스템은 ‘A라는 어플리케이션을 여러 서버에서 동작하도록 하여 동시처리(병렬적으로)하도록 하고, 일부 서버에 장애가 발생하면 해당 서버에 있는 어플리케이션을 클러스터에서 제외, 복구 되었는지 주기적으로 헬스체크해서 복구 되면 다시 클러스터에 추가’&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791161754963&amp;amp;orderClick=LEa&amp;amp;Kc=&quot; target=&quot;_blank&quot;&gt;책, Database Internals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.slideshare.net/rubentan/basic-distributed-systems-principles&quot; target=&quot;_blank&quot;&gt;slideshare: Basic distributed systems principles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://vowi.fsinf.at/images/b/bc/TU_Wien-Verteilte_Systeme_VO_%28G%C3%B6schka%29_-_Tannenbaum-distributed_systems_principles_and_paradigms_2nd_edition.pdf&quot; target=&quot;_blank&quot;&gt;pdf: Distributed Systems: Principles and Paradigms&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/what-is-a-distributed-system/?ref=gcse&quot; target=&quot;_blank&quot;&gt;GeeksforGeeks: What is a Distributed System?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/comparison-centralized-decentralized-and-distributed-systems/?ref=gcse&quot; target=&quot;_blank&quot;&gt;GeeksforGeeks: Comparison – Centralized, Decentralized and Distributed Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/synchronization-in-distributed-systems/?ref=gcse&quot; target=&quot;_blank&quot;&gt;GeeksforGeeks: Synchronization in Distributed Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.geeksforgeeks.org/what-is-dfsdistributed-file-system/?ref=gcse&quot; target=&quot;_blank&quot;&gt;GeeksforGeeks: What is DFS (Distributed File System)?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 21 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series15</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series15</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part14]: Database Internals 트랜잭션 처리와 복구</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#트랜잭션-처리와-복구&quot; id=&quot;markdown-toc-트랜잭션-처리와-복구&quot;&gt;트랜잭션 처리와 복구&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#버퍼-관리&quot; id=&quot;markdown-toc-버퍼-관리&quot;&gt;버퍼 관리&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#캐싱&quot; id=&quot;markdown-toc-캐싱&quot;&gt;캐싱&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#캐시-만료&quot; id=&quot;markdown-toc-캐시-만료&quot;&gt;캐시 만료&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#페이지-고정&quot; id=&quot;markdown-toc-페이지-고정&quot;&gt;페이지 고정&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#페이지-교체&quot; id=&quot;markdown-toc-페이지-교체&quot;&gt;페이지 교체&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#복구&quot; id=&quot;markdown-toc-복구&quot;&gt;복구&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#리두로그-언두로그&quot; id=&quot;markdown-toc-리두로그-언두로그&quot;&gt;리두로그 언두로그&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;트랜잭션-처리와-복구&quot;&gt;트랜잭션 처리와 복구&lt;/h1&gt;

&lt;p&gt;트랜잭션을 수행하기 위해서는 데이터를 디스크에 저장하고 유지하는 자료구조 외에도 여러 컴포넌트가 필요하다.&lt;/p&gt;

&lt;p&gt;트랜잭션 매니저는 트랜잭션의 세부 단계를 제어, 관리 및 스케줄링하는 컴포넌트다.&lt;/p&gt;

&lt;p&gt;잠금 매니저는 리소스에 대한 동시 접근을 제어하고 데이터 무결성을 보장한다. 잠금이 해제되거나 트랜잭션이 완료되면 잠금매니저는 대기 중인 트랜잭션에 이 사실을 알린다&lt;/p&gt;

&lt;p&gt;페이지 캐시는 디스크와 스토리지 엔진 사이에서 중개자 역할을 한다. 메인 메모리의 변경 사항을 저장하고 영구 저장소와 동기화되지 않은 페이지를 캐시한다. 모든 데이터베이스 상태에 대한 변경 사항은 우선 페이지 캐시에 저장된다&lt;/p&gt;

&lt;p&gt;로그 매니저는 영구 저장소와 동기화되지 않은 페이지 캐시의 내용이 손실되지 않도록 로그(작업 히스토리)를 저장한다.&lt;/p&gt;

&lt;h2 id=&quot;버퍼-관리&quot;&gt;버퍼 관리&lt;/h2&gt;

&lt;p&gt;대부분의 데이터베이스는 상대적으로 속도가 느린 디스크와 빠른 메인 메모리로 구성된 계층 구조이다. 따라서 영구 저장소 접근 횟수를 줄이기 위해 페이지를 메모리에 캐시하고 요청한 페이지가 캐시되어 있다면 디스크가 아닌 캐시에서 반환한다.&lt;/p&gt;

&lt;p&gt;다른 프로세스가 캐시된 페이지와 일치하는 디스크에 저장된 데이터를 변경하지 않았다면 메모리에 캐시된 페이지를 재사용할 수 있다. 이와 같은 방식을 페이지 캐시 또는 버퍼풀이라고 부른다. 이러한 방식은 요청된 페이지가 메모리에 없을 경우에만 물리적 저장소에 접근한다. 페이지 캐시는 디스크에서 읽은 페이지를 메모리에 캐시한다. 시스템 장애가 발생하거나 시스템이 비정상적으로 종료되면 캐시된 데이터는 사라진다.&lt;/p&gt;

&lt;p&gt;디스크에서 메모리로 복사하는 작업을 페이징이라고 한다. 그리고 디스크로 플러시되지 않은 변경된 페이지는 더티페이지라고 한다. 일반적으로 페이지 캐시의 메모리 영역은 전체 데이터셋보다 작기 때문에 결국 새로운 페이지를 추가하기 위해 기존 페이지를 만료시켜야 한다.&lt;/p&gt;

&lt;p&gt;페이지 캐시의 주요 기능은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;페이지 내용을 메모리에 캐시한다&lt;/li&gt;
  &lt;li&gt;디스크에 저장된 페이지에 대한 변경 사항을 버퍼링하고 캐시된 페이지에 반영한다&lt;/li&gt;
  &lt;li&gt;캐시되지 않은 데이터가 요청될 경우 추가할 공간이 있으면 페이징하고 캐시된 버전을 반환한다&lt;/li&gt;
  &lt;li&gt;추가할 공간이 없다면 일부 페이지를 만료시키고 디스크로 플러시한다&lt;/li&gt;
  &lt;li&gt;캐시된 데이터가 요청된 경우에는 그냥 메모리에서 반환하면 된다&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;캐싱&quot;&gt;캐싱&lt;/h3&gt;

&lt;p&gt;버퍼에 대한 변경 사항은 디스크에 쓰기 전까지 메모리에 남겨둔다. 어떤 프로세스도 원본 파일을 수정할 수 없기 때문에 동기화는 메모리에서 디스크로 플러시하는 단방향 작업이다. 데이터베이스는 페이지 캐시를 사용해 메모리를 관리하고 디스크 접근을 제어한다.&lt;/p&gt;

&lt;p&gt;스토리지 엔진이 특정 페이지를 요청하면 우선 캐시된 버전이 있는지 확인하고 있을 경우 반환한다. 없다면 논리적 페이지 주소 또는 페이지 번호를 물리적 주소로 변환해 해당 페이지를 메모리로 복사하고 반환한다. 이 떄 해당 페이지가 저장된 버퍼는 참조 상태라고 한다.&lt;/p&gt;

&lt;p&gt;페이지가 변경된 경우에는 페이지에 더티 플래그를 설정한다. 더티 플르개는 해당 페이지가 디스크와 동기화되지 않았고, 지속성을 위해 디스크로 플러시돼야 한다는 것을 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;캐시-만료&quot;&gt;캐시 만료&lt;/h3&gt;

&lt;p&gt;캐시된 데이터가 많을수록 더 많은 읽기 요청을 디스크에 접근하지 않고 처리할 수 있다. 또한 같은 페이지에 대한 변경사항을 더 많이 같이 버퍼할 수 있다. 하지만 페이지 캐시의 크기는 한정적이기 때문에, &lt;strong&gt;새로운 페이지를 저장하기 위해 오래된 페이지는 제거해야 한다.&lt;/strong&gt; 페이지가 동기화됐고 고정 또는 참조 상태가 아니라면 바로 제거 될 수 있다. 더티 페이지는 제거되기 전에 먼저 플러시해야 한다. 참조 상태의 페이지는 사용이 끝나기 전까지는 제거될 수 없다.&lt;/p&gt;

&lt;p&gt;페이지를 제거할 때마다 디스크로 플러시한다면 성능을 저하시킬 수 있다. 따라서 별도의 백그라운드 프로세스가 제거될 가능성이 높은 &lt;strong&gt;더티 페이지를 주기적으로 디스크로 플러시&lt;/strong&gt;한다. 또한 데이터베이스에 갑작스런 장애가 발생한 경우 플러시되지 않은 데이터는 사라질 수 있다. 이러한 데이터 손실을 방지하기 위해 체크포인트 프로세스가 플러시 시점을 제어한다. 체크포인트 프로세스는 선행 기록 로그(Write Ahead Log)와 페이지 캐시의 싱크가 맞도록 조정한다. (이 말은 선행 기록 로그가 디스크에 있는 데이터라는 말인가? 동기화는 메모리에서 디스크로 플러시하는 단방향 작업이라 했으니까 선행 기록 로그보다 페이지의 데이터가 더 최신? -&amp;gt; 그렇지. 캐시된 페이지에 제일 먼저 변화 생기고 -&amp;gt; 로그 버퍼에 리두 로그(WAL) 버퍼링 되고 버퍼된 리두 로그 디스크에 플러시 되고 -&amp;gt; 캐시된 페이지 플러시) 오직 플러시가 완료된 캐시된 페이지와 관련된 로그만 WAL에서 삭제될 수 있다. 이 과정이 완료될 때 끼지 더티 페이지는 제거될 수 없다.&lt;/p&gt;

&lt;h3 id=&quot;페이지-고정&quot;&gt;페이지 고정&lt;/h3&gt;

&lt;p&gt;B-트리의 상위 레벨 노드는 대부분의 읽기 작업에서 접근한다. 이러한 트리의 일부를 캐시하면 상당한 도움이 될 수 있다. 따라서 우선 상위 레벨 노드를 메모리에 고정시키고 나머지 노드는 요청 시 페이징해도 된다.&lt;/p&gt;

&lt;h3 id=&quot;페이지-교체&quot;&gt;페이지 교체&lt;/h3&gt;

&lt;p&gt;저장 공간이 부족한 캐시에 새로운 페이지를 추가하려면 일부 페이지를 만료시켜야 한다. 캐시된 페이지는 만료 정책(페이지 교체 알고리즘)에 따라 캐시에서 제거된다. 만료 정책은 다시 요청될 확률이 낮은 페이지를 만료시키고 해당 위치에 새로운 페이지를 페이징한다. 이렇게 페이지를 만료시키고 새로운 페이지로 교체하는 알고리즘은 대표적으로 FIFO, LRU 등이 있다.&lt;/p&gt;

&lt;p&gt;FIFO는 First In First Out의 약자로 가장 먼저 큐에 들어온 페이지 ID가 가리키는 페이지를 만료시킨다. 이 방식은 간단하지만 페이지 접근 순서를 전혀 고려하지 않기 때문에 실용적이지 않다.&lt;/p&gt;

&lt;p&gt;LRU는 FIFO를 확장한 방식이다. FIFO와 마찬가지로 요청 순서대로 큐에 추가되다가 재요청 되면 다시 큐의 끝에 추가한다.&lt;/p&gt;

&lt;h2 id=&quot;복구&quot;&gt;복구&lt;/h2&gt;

&lt;p&gt;데이터베이스 개발자는 여러 장애 시나리오를 고려하고 약속된 데이터가 실제로 저장되게 해야 한다. 선행 기록 로그(WAL 또는 커밋 로그)는 장애 및 트랜잭션 복구를 위해 디스크에 저장하는 추가 전용(Append-only) 보조 자료 구조다. 페이지 캐시는 페이지에 대한 변경 사항을 메모리에 버퍼링한다. &lt;strong&gt;캐시된 내용이 디스크로 플러시될 때 까지 관련 작업 이력의 유일한 디스크 복사본은 WAL&lt;/strong&gt;이다. MySQL, PostreSQL과 같은 많은 데이터베이스가 추가 전용 WAL을 사용한다.&lt;/p&gt;

&lt;p&gt;WAL의 주요 기능은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;캐시된 페이지가 디스크와 동기화 될 때 까지 작업 이력을 디스크에 저장한다. 데이터베이스의 상태를 변경하는 작업을 실제 페이지에 적용하기 전에 먼저 디스크에 로깅한다&lt;/li&gt;
  &lt;li&gt;장애 발생 시 로그를 기반으로 마지막 메모리 상태를 재구성한다&lt;/li&gt;
  &lt;li&gt;WAL은 데이터가 디스크에 저장되도록 보장하고, 장애 발생시 데이터 상태를 되돌리기 위해 필요한 로그를 저장한다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;WAL은 추가 전용 자료구조이며, 작성된 데이터는 불변하기 때문에 모든 쓰기 작업은 순차적이다.&lt;/p&gt;

&lt;p&gt;WAL은 여러 로그 레코드로 구성된다. 모든 레코드에는 단조 증가하는 고유 로그 시퀀스 번호(LSN, Log Sequence Number)가 있다. 로그 레코드의 크기는 디스크 블록의 크기보다 작을 수 있기 때문에, 로그 버퍼에 임시 저장하고 포스 작업시 디스크로 플러시한다. 포스 작업은 로그 버퍼가 가득 차면 수행되거나 트랜잭션 매니저 또는 페이지 캐시가 직접 요청할 수도 있다. 모든 로그 레코드는 LSN과 동일한 순서로 플러시돼야 한다.&lt;/p&gt;

&lt;p&gt;WAL는 작업 로그 레코드 외에도 트랜잭션 완료 여부를 나타내는 레코드를 저장한다. 트랜잭션의 커밋 레코드의 LSN까지 플러시되기 전까지는 해당 트랜잭션은 커밋된 것으로 간주 할 수 없다. 일부 시스템은 트랜잭션 롤백 또는 복구 중 장애가 발생해도 시스템이 계속해서 정상 작동할 수 있도록 보상 로그 레코드를 로그에 저장하고 언두 작업 시 사용한다.&lt;/p&gt;

&lt;p&gt;일반적으로 WAL은 체크포인트에 도달하면 이전 로그를 정리하는 인터페이스를 통해 기본 저장소와 동기화한다. 로깅은 데이터베이스의 정확성 측면에서 매우 중요한 작업이다. 로그 정리 작업과 데이터를 기본 스토리지에 저장하는 작업이 조금이라도 어긋나면 데이터는 손실될 수 있다.&lt;/p&gt;

&lt;p&gt;모든 데이터는 한 번에 디스크로 플러시하면 체크포인트 작업이 완료될 때 까지 다른 작업을 모두 중지해야 하기 떄문에 비효율적이다. 이 문제를 해결하기 위해 대부분의 데이터베이스 시스템은 퍼지(fuzzy) 체크포인트를 사용한다.&lt;/p&gt;

&lt;p&gt;마지막으로 성공한 체크포인트 작업에 대한 정보는 로그 헤더에 저장된 last_checkpoint 포인터에 저장한다. 퍼지 체크포인트는 begin_checkpoint라는 특별한 로그 레코드로 시작해 더티 페이지에 대한 정보와 트랜잭션 테이블의 내용을 저장한 end_checkpoint라는 로그 레코드로 끝난다. 이 로그 레코드에 명시된 모든 페이지가 플러시될 때 까지 해당 체크포인트는 미완료 상태다. 페이지는 비동기적으로 플러시되며, 이 작업이 끝나면 last_checkpoint 레코드를 begin_checkpoint의 LSN으로 업데이트 한다. 장애가 발생할 경우, 복구 프로세스는 해당 LSN에서부터 시작한다.&lt;/p&gt;

&lt;h3 id=&quot;리두로그-언두로그&quot;&gt;리두로그 언두로그&lt;/h3&gt;

&lt;p&gt;데이터베이스 시스템은 데이터의 지속성과 트랜잭션의 원자성을 보장하는 쓰기 시 복사(Copy-on-write) 방식의 섀도 페이징 기법을 사용한다. 새로운 데이터를 우선 내부 섀도 페이지에 쓴 다음, 이전 버전의 페이지를 가리키는 포인터를 섀도 페이지를 가리키도록 변경해 업데이트된 내용을 반영한다.&lt;/p&gt;

&lt;p&gt;모든 상태 변화는 이전 상태와 이후 상태의 조합으로 나타낼 수 있다. 또는 그에 대응되는 리두 작업과 언두 작업으로 나타낼 수 있다. 이전 상태에 리두 작업을 수행하면 이후 상태가 된다. 반대로 이후 상태에 언두 작업을 수행하면 이전 상태가 된다.&lt;/p&gt;

&lt;p&gt;장애 발생 후 데이터베이스 시스템을 재시작하면 복구는 다음 3단계로 진행된다&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;분석 단계: 페이지 캐시에 저장된 더티 페이지와 장애 발생 당시 수행 중이던 트랜잭션을 파악한다. 더티 페이지에 대한 정보를 기반으로 리두 단계의 시작 지점을 결정한다. 트랜잭션 목록은 언두 단계에서 미완료된 트랜잭션을 롤백하는 데 사용한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;리두 단계: 장애가 발생하기 전까지의 작업을 재수행하고 데이터베이스를 이전 상태로 복원한다. 불완전한 트랜잭션, 커밋됐지만 디스크로 플러시되지 않은 트랜잭션을 롤백하기 위한 준비 단계다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;언두 단계: 불완전한 트랜잭션을 롤백하고 데이터베이스를 마지막 일관된 상태로 복원한다. 모든 작업은 실제 수행 순어의 역순으로 롤백된다. 복구 중에도 장애가 발생할 수 있기 때문에 언두 작업도 로그에 기록해야 된다&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
                <pubDate>Wed, 20 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series14</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series14</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part13]: Database Internals 개요</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#스토리지-엔진&quot; id=&quot;markdown-toc-스토리지-엔진&quot;&gt;스토리지 엔진&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#소개-및-개요&quot; id=&quot;markdown-toc-소개-및-개요&quot;&gt;소개 및 개요&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dbms-구조&quot; id=&quot;markdown-toc-dbms-구조&quot;&gt;DBMS 구조&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#인메모리-dbms-디스크-기반-dbms&quot; id=&quot;markdown-toc-인메모리-dbms-디스크-기반-dbms&quot;&gt;인메모리 DBMS, 디스크 기반 DBMS&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#로우형-dbms-컬럼형-dbms&quot; id=&quot;markdown-toc-로우형-dbms-컬럼형-dbms&quot;&gt;로우형 DBMS, 컬럼형 DBMS&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;데이터베이스에서 중요한 개념: B-트리 알고리즘, 캐시 정책, 상태 복구, 동시성 제어&lt;br /&gt;
(실용적인 측면에서는, 데이터베이스별 기능, 얼마나 빠른지, 어떤 유형의 어플리케이션에 이용되는지, SQL, 쿼리 플랜)&lt;/p&gt;

&lt;p&gt;분산 데이터베이스에서 중요한 개념: 장애 감지, 리더 선출, 분산 트랜잭션, 합의 알고리즘&lt;/p&gt;

&lt;h1 id=&quot;스토리지-엔진&quot;&gt;스토리지 엔진&lt;/h1&gt;

&lt;p&gt;데이터베이스 관리 시스템의 주 목적은 데이터를 안정적으로 저장하고 사용자에게 제공하는 것이다. 일반적으로 데이터베이스는 기본 데이터 스토어로 사용되며 애플리케이션의 여러 구성 요소가 공유한다.&lt;/p&gt;

&lt;p&gt;데이터베이스는 모듈식 시스템이다. 요청을 전달하는 전송 계층, 가장 효율적인 쿼리 실행 계획을 결정하는 쿼리 프로세서, 실제 작업을 수행하는 실행 엔진 그리고 스토리지 엔진으로 구성된다.&lt;/p&gt;

&lt;p&gt;스토리지 엔진은 DBMS에서 데이터를 메모리와 디스크에 저장하고 검색 및 관리하는 소프트웨어 컴포넌트로써 각 노드에 데이터를 영구 저장한다. 데이터베이스가 복잡한 쿼리를 수행할 수 있도록 스토리지 엔진은 데이터를 세밀하게 조작할 수 있는 간단한 API를 제공한다. 사용자는 이를 사용해 레코드를 CRUD할 수 있다. 따라서 데이터베이스는 &lt;strong&gt;스토리지 엔진 위에서 스키마와 쿼리 언어, 인덱싱, 트랜잭션 등의 유용한 기능을 제공하는 애플리케이션&lt;/strong&gt;이라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;스토리지 엔진은 DBMS와 독립적으로 개발됐다. 데이터베이스 개발자는 플러그형 스토리지 엔진을 사용해 데이터베이스 시스템을 구현할 수 있다. 예를 들어 DBMS 중 MySQL은 InnoDB, MyISAM, RocksDB라는 스토리지 엔진을 가질 수 있고, 몽고DB는 WiredTiger, In-Memory 스토리지 엔진이 있다.&lt;/p&gt;

&lt;h2 id=&quot;소개-및-개요&quot;&gt;소개 및 개요&lt;/h2&gt;

&lt;p&gt;데이터베이스 관리 시스템의 용도는 다양하다. 일시적인 핫 데이터를 주로 저장하는 데 쓰이기도 하고, 장기 보관용 콜드 데이터 스토리지로 쓰이기도 한다. 복잡한 쿼리 분석을 지원하는 시스템도 있는가 하면, 키로만 값을 액세스할 수 있는 스토어도 있다.&lt;/p&gt;

&lt;p&gt;저장 매체에 따라 인메모리 DBMS와 디스크 기반 DBMS로 구분할 수도 있다. 레이아웃에 따라 로우형, 컬럼형, 와이드 컬럼형 DBMS로 구분할 수도 있다.&lt;/p&gt;

&lt;p&gt;또 사용 용도에 따라 온라인 트랜잭션 처리(OLTP), 온라인 분석 처리(OLAP) 데이터베이스로 구분할 수도 있다.&lt;/p&gt;

&lt;p&gt;저장 형태에 따라 로우형 레코드, 키-값 쌍, 다큐멘트형 데이터베이스가 있다.&lt;/p&gt;

&lt;h3 id=&quot;dbms-구조&quot;&gt;DBMS 구조&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/images/db_internals_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;클라이언트의 요청은 트랜스포트 서브시스템을 통해 전달된다. 요청은 쿼리 형태이고 주로 특정 쿼리 언어로 표현된다. 트랜스포트 서브시스템은 쿼리를 쿼리 프로세서에 전달한다. 쿼리 프로세서는 쿼리를 해석, 분석 및 검증한다. 분석된 쿼리는 쿼리 옵티마이저에 전달된다.&lt;/p&gt;

&lt;p&gt;쿼리는 일반적으로 실행 계획 형태로 표현한다. 실행 계획은 쿼리가 요구하는 결과를 도출하는 데 수행해야 하는 일련의 작업이다.&lt;/p&gt;

&lt;p&gt;스토리지 엔진은 다음과 같은 역할을 담당하는 컴포넌트로 구성된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;트랜잭션 매니저: 트랜잭션을 스케줄링하고 데이터베이스 상태의 논리적 일관성을 보장한다&lt;/li&gt;
  &lt;li&gt;잠금 매니저: 트랜잭셔넹서 접근하는 데이터베이스 객체에 대한 잠금을 제어한다. 동시 수행 작업이 데이터 무결성을 침해하지 않도록 제어한다.&lt;/li&gt;
  &lt;li&gt;버퍼 매니저: 데이터 페이지를 메모리에 캐시한다&lt;/li&gt;
  &lt;li&gt;복구 매니저: 로그를 유지 관리하고 장애 발생 시 시스템을 복구한다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;스토리지 엔진은 버퍼링, 가변성, 순서화와 같은 요소에 따라 구분된다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;버퍼링
    &lt;ul&gt;
      &lt;li&gt;데이터를 디스크에 쓰기 전에 일부를 메모리에 저장할지 여부&lt;/li&gt;
      &lt;li&gt;물론 모든 디스크 기반도 어느 정도 버퍼를 사용한다
(디스크의 가장 작은 전송 단위가 블록이므로 블록을 채워서 쓰기 위해 버퍼링 사용)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;가변성
    &lt;ul&gt;
      &lt;li&gt;데이터를 갱신할 때 in-place vs (append-only, copy-on-write)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;순서화
    &lt;ul&gt;
      &lt;li&gt;디스크 페이지에 레코드를 키 순서로 저장할지에 대한 여부&lt;/li&gt;
      &lt;li&gt;순서화 -&amp;gt; 빠른 읽기, 비순서화 -&amp;gt; 빠른 쓰기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;인메모리-dbms-디스크-기반-dbms&quot;&gt;인메모리 DBMS, 디스크 기반 DBMS&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;인메모리
    &lt;ul&gt;
      &lt;li&gt;메모리에 데이터 저장&lt;/li&gt;
      &lt;li&gt;디스크는 복구와 로그 저장 용도&lt;/li&gt;
      &lt;li&gt;단점: RAM의 휘발성(디스크로 보완, 배터리 장착된 RAM 사용), 가격&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;디스크 기반
    &lt;ul&gt;
      &lt;li&gt;디스크에 데이터 저장&lt;/li&gt;
      &lt;li&gt;메모리는 캐시 또는 임시 저장 용도&lt;/li&gt;
      &lt;li&gt;단점: 랜덤 디스크 접근 속도 « 랜덤 메모리 접근 속도&lt;/li&gt;
      &lt;li&gt;디스크 기반은 아무리 큰 페이지를 캐시해도 직렬화와 데이터 레이아웃을 유지하는 오버헤드 때문에 인메모리 보다 느리다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;인메모리 데이터베이스의 지속성&lt;/p&gt;

&lt;p&gt;인메모리 DBMS는 데이터의 지속성을 보장하고 데잍어 손실을 방지하기 위해 데이터를 디스크에 백업한다. 모든 작업은 로그 파일에 작업 내용을 순차적으로 기록해야 완료된다. 시스템 시작과 복구 시 모든 로그를 재수행하지 않기 위해 디스크에 백업본을 유지한다. 그렇게 함으로써 복구시에는 백업본과 로그를 기반으로 데이터를 재구성한다. (백업본만 있으면 이전 로그는 삭제해도 된다)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/db_internals_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;로우형-dbms-컬럼형-dbms&quot;&gt;로우형 DBMS, 컬럼형 DBMS&lt;/h3&gt;

&lt;p&gt;대부분의 데이터베이스는 열과 행으로 구성된 테이블에 레코드를 저장한다. 데이터를 디스크에 저장하는 방식에 따라 데이터베이스를 분류할 수 있다. 컬럼 저장 방식은 테이블을 수직 분할하고, 로우 저장 방식은 수평 분할한다.&lt;/p&gt;

&lt;p&gt;MySQL, PostreSQL 등 대부분의 전통적인 관계형 데이터베이스는 로우형 DBMS이다. MonetDB, C-Store는 컬럼형 오픈소스 데이터베이스이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로우형 데이터 레이아웃
    &lt;ul&gt;
      &lt;li&gt;이 방식은 여러 필드의 값을 고유 식별 키로 구분할 수 있는 레코드 형식에 적합하다&lt;/li&gt;
      &lt;li&gt;일반적으로 특정 사용자의 필드는 여러 개가 함꼐 요청되는 경우가 많다&lt;/li&gt;
      &lt;li&gt;로우형 DBMS는 한 개의 로우씩 접근하는 경우 적합하다&lt;/li&gt;
      &lt;li&gt;한 블록에 모든 컬럼의 값이 들어가게 된다&lt;/li&gt;
      &lt;li&gt;특정 사용자의 모든 정보를 읽어올 때는 효율적, 모든 사용자의 특정 필드를 읽어올 때는 비효율적이다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컬럼형 데이터 레이아웃
    &lt;ul&gt;
      &lt;li&gt;컬럼끼리 디스크에 연속해 저장하는 방식이다&lt;/li&gt;
      &lt;li&gt;컬럼형 DBMS는 데이터의 추세와 평균 등을 계산하는 집계 분석 작업에 적합하다&lt;/li&gt;
      &lt;li&gt;다시 레코드 형태로 재구성하기 위해 컬럼 사이의 관계를 정의하는 메타데이터가 필요하다 (각 값마다 키값을 중복저장해야 한다)&lt;/li&gt;
      &lt;li&gt;최근 몇 년 동안 대용량 데이터에 대한 복잡한 분석 쿼리 사용이 늘어나고 있다&lt;/li&gt;
      &lt;li&gt;결과적으로 파케이(Parquet), ORC와 같은 컬럼 기반 파일 포맷과 쿠두(Kude)와 같은 컬럼형 DBMS가 각광받고 있다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;같은 컬럼의 여러 값을 한 번에 읽으면 캐시 활용도와 처리 효율성이 높아진다. 같은 자료형끼리 저장하게 되면 압축률도 증가한다. 컬럼형과 로우형 DBMS 중 어떤 것을 사용할지 선택하려면 액세스 패턴을 파악해야 한다. 데이터를 레코드 단위로 접근하고 일반 쿼리와 범위 스캔 요청이 많다면 로우형 DBMS, 여러 로우를 스캔하거나 일부 컬럼에 대한 집계 작업이 많다면 컬럼형 DBMS가 더 적합할 수 있다.&lt;/p&gt;

&lt;p&gt;HBase와 같은 와이드 컬럼 스토어는 같은 자료형을 가지는 컬럼을 단위로 로우 형식으로 저장한다. 이 방식은 키 단위 액세스 패턴에 적합하다.&lt;/p&gt;
</description>
                <pubDate>Tue, 19 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series13</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series13</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Network Series [Part11]: 네트워크 활용편(5) 도커 네트워크와 AWS 네트워크</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
</description>
                <pubDate>Sun, 17 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/network-series11</link>
                <guid isPermaLink="true">http://localhost:4000/network-series11</guid>
                
                <category>Network</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>Network Series [Part10]: 네트워크 활용편(4) 로드 밸런서(Load Balancer)</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런싱의-정의&quot; id=&quot;markdown-toc-로드-밸런싱의-정의&quot;&gt;로드 밸런싱의 정의&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런서의-특징&quot; id=&quot;markdown-toc-로드-밸런서의-특징&quot;&gt;로드 밸런서의 특징&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런서가-제공하는-이점&quot; id=&quot;markdown-toc-로드-밸런서가-제공하는-이점&quot;&gt;로드 밸런서가 제공하는 이점&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런싱-방법&quot; id=&quot;markdown-toc-로드-밸런싱-방법&quot;&gt;로드 밸런싱 방법&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#round-robin-algorithm&quot; id=&quot;markdown-toc-round-robin-algorithm&quot;&gt;Round Robin Algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#least-connections&quot; id=&quot;markdown-toc-least-connections&quot;&gt;Least connections&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#source-ip-hash&quot; id=&quot;markdown-toc-source-ip-hash&quot;&gt;Source IP hash&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-least-bandwidth-method&quot; id=&quot;markdown-toc-the-least-bandwidth-method&quot;&gt;The least bandwidth method&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런싱의-이점&quot; id=&quot;markdown-toc-로드-밸런싱의-이점&quot;&gt;로드 밸런싱의 이점&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#software-load-balancers-vs-hardware-load-balancers&quot; id=&quot;markdown-toc-software-load-balancers-vs-hardware-load-balancers&quot;&gt;Software Load Balancers vs. Hardware Load Balancers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#로드-밸런서-종류&quot; id=&quot;markdown-toc-로드-밸런서-종류&quot;&gt;로드 밸런서 종류&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;

&lt;h1 id=&quot;로드-밸런싱의-정의&quot;&gt;로드 밸런싱의 정의&lt;/h1&gt;

&lt;blockquote&gt;
  &lt;p&gt;Load balancing is the process of distributing network traffic across multiple servers. This ensures no single server bears too much demand. By spreading the work evenly, load balancing improves application responsiveness. It also increases availability of applications and websites for users. Modern applications cannot run without load balancers. Over time, software load balancers have added additional capabilities including application security.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/images/network_37.svg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;로드-밸런서의-특징&quot;&gt;로드 밸런서의 특징&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;the load balancer decides which servers can handle that traffic. This maintains a good UX&lt;/li&gt;
  &lt;li&gt;Load balancers manage the flow of information between the server and an endpoint device&lt;/li&gt;
  &lt;li&gt;Load balancers conduct continuous health checks on servers to ensure they can handle requests&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;로드-밸런서가-제공하는-이점&quot;&gt;로드 밸런서가 제공하는 이점&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Automatically detect server failures and redirect client traffic&lt;/li&gt;
  &lt;li&gt;Allow for server maintenance without any impact&lt;/li&gt;
  &lt;li&gt;Provide automated disaster recovery to backup sites&lt;/li&gt;
  &lt;li&gt;Add and remove application servers without disruption&lt;/li&gt;
  &lt;li&gt;Monitor and block malicious content&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;로드-밸런싱-방법&quot;&gt;로드 밸런싱 방법&lt;/h1&gt;

&lt;h2 id=&quot;round-robin-algorithm&quot;&gt;Round Robin Algorithm&lt;/h2&gt;

&lt;p&gt;Round robin (RR) algorithm is a circular distribution of requests to enterprise servers in a sequence. There are two types of Round Robin – Weighted Round Robin and Dynamic Round Robin. Used mainly for a cluster of different servers, in weighted round robin each server is assigned a weight depending on its composition. Based on the preassigned efficiency the load is distributed in a cyclical procedure. Dynamic round robin are used to forward requests to associated servers based on real time calculation of assigned server weights.&lt;/p&gt;

&lt;h2 id=&quot;least-connections&quot;&gt;Least connections&lt;/h2&gt;

&lt;p&gt;By taking into consideration the number of active and current connections to each application instance, ‘Least Connections’ load balancing algorithm distributes the load by choosing the server with the least number of active transactions (connections).&lt;/p&gt;

&lt;h2 id=&quot;source-ip-hash&quot;&gt;Source IP hash&lt;/h2&gt;

&lt;p&gt;In a source IP Hash, load balancing a server is selected based on a unique hash key. The Hash key is generated by taking the source and destination of the request. Based on the generated hash key, servers are assigned to clients.&lt;/p&gt;

&lt;h2 id=&quot;the-least-bandwidth-method&quot;&gt;The least bandwidth method&lt;/h2&gt;

&lt;p&gt;In the least bandwidth method backend servers are selected based on the server’s bandwidth consumption i.e the server consuming least bandwidth is selected (measured in Mbps). Similar to least bandwidth method is the least packets method. Here the server transmitting least packets are selected by the load balancer.&lt;/p&gt;

&lt;h1 id=&quot;로드-밸런싱의-이점&quot;&gt;로드 밸런싱의 이점&lt;/h1&gt;

&lt;p&gt;Load balancing can do more than just act as a network traffic cop. Software load balancers provide benefits like predictive analytics that determine traffic bottlenecks before they happen. As a result, the software load balancer gives an organization actionable insights. These are key to automation and can help drive business decisions.&lt;/p&gt;

&lt;p&gt;In the seven-layer Open System Interconnection (OSI) model, network firewalls are at levels one to three (L1-Physical Wiring, L2-Data Link and L3-Network). Meanwhile, load balancing happens between layers four to seven (L4-Transport, L5-Session, L6-Presentation and L7-Application).&lt;/p&gt;

&lt;p&gt;Load balancers have different capabilities, which include:&lt;/p&gt;

&lt;p&gt;L4 — directs traffic based on data from network and transport layer protocols, such as IP address and TCP port.&lt;/p&gt;

&lt;p&gt;L7 — adds content switching to load balancing. This allows routing decisions based on attributes like HTTP header, uniform resource identifier, SSL session ID and HTML form data.
GSLB — Global Server Load Balancing extends L4 and L7 capabilities to servers in different geographic locations.&lt;/p&gt;

&lt;p&gt;More enterprises are seeking to deploy cloud-native applications in data centers and public clouds. This is leading to significant changes in the capability of load balancers. In turn, this creates both challenges and opportunities for infrastructure and operations leaders.&lt;/p&gt;

&lt;h1 id=&quot;software-load-balancers-vs-hardware-load-balancers&quot;&gt;Software Load Balancers vs. Hardware Load Balancers&lt;/h1&gt;

&lt;p&gt;Load balancers run as hardware appliances or are software-defined. Hardware appliances often run proprietary software optimized to run on custom processors. As traffic increases, the vendor simply adds more load balancing appliances to handle the volume. Software defined load balancers usually run on less-expensive, standard Intel x86 hardware. Installing the software in cloud environments like AWS EC2 eliminates the need for a physical appliance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/network_38.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;로드-밸런서-종류&quot;&gt;로드 밸런서 종류&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Network Server Load Balancers(NLB)&lt;/strong&gt;: Load balancers entered the market in the mid-1990s to support the surge of traffic on the internet. Load balancers had basic functionality designed to pool server resources to meet this demand. The load balancer managed connections based on the packet header. Specifically, they looked at the 5-tuple – source IP, destination IP, source port, destination port, and IP protocol. This is the entry of the network server load balancer or Layer 4 load balancer.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Application Load Balancers(ALB)&lt;/strong&gt;: As technology evolved, so did the load balancers. They became more advanced and started providing content awareness and content switching. These load balancers looked beyond the packet header and into the content payload. These load balancers look at the content such as the URL, HTTP header, and other things to make load balancing decisions. These are the application load balancers or Layer 7 load balancers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Elastic Load Balancers(ELB)&lt;/strong&gt;: Elastic Load Balancing scales traffic to an application as demand changes over time. It uses system health checks to learn the status of application pool members (application servers) and routes traffic appropriately to available servers, manages fail-over to high availability targets, or automatically spins-up additional capacity.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://avinetworks.com/what-is-load-balancing/&quot; target=&quot;_blank&quot;&gt;AVI Networks: What Is Load Balancing?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://kemptechnologies.com/what-is-load-balancing&quot; target=&quot;_blank&quot;&gt;Progress Kemp: What is load balancing?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jaehoney.tistory.com/73&quot; target=&quot;_blank&quot;&gt;Jae Honey, L4 / L7 로드밸런서 차이 (Load balancer)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sat, 16 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/network-series10</link>
                <guid isPermaLink="true">http://localhost:4000/network-series10</guid>
                
                <category>Network</category>
                
                
                <category>CS</category>
                
            </item>
        
    </channel>
</rss>
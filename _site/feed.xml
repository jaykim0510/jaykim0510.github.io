<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Code Museum</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Tue, 24 May 2022 16:45:20 +0900</pubDate>
        <lastBuildDate>Tue, 24 May 2022 16:45:20 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>Data Engineering Series [Part8]: 정규표현식</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://greeksharifa.github.io/정규표현식(re)/2018/07/20/regex-usage-01-basic/&quot; target=&quot;_blank&quot;&gt;greeksharifa, 파이썬 정규표현식(re) 사용법&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://regexone.com&quot; target=&quot;_blank&quot;&gt;regexone: 정규표현식 문제&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://regexr.com&quot; target=&quot;_blank&quot;&gt;regexr: 정규표현식 테스트&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://regexper.com&quot; target=&quot;_blank&quot;&gt;regexper: 정규표현식 시각화&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://programmers.co.kr/learn/courses/11&quot; target=&quot;_blank&quot;&gt;프로그래머스: 정규표현식 문제&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.acmicpc.net/workbook/view/6082&quot; target=&quot;_blank&quot;&gt;백준: 정규표현식 문제&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Tue, 17 May 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series8</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series8</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part7]: CDC(Change Data Capture)</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#cdc&quot; id=&quot;markdown-toc-cdc&quot;&gt;CDC&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#debezium&quot; id=&quot;markdown-toc-debezium&quot;&gt;Debezium&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;cdc&quot;&gt;CDC&lt;/h1&gt;

&lt;h1 id=&quot;debezium&quot;&gt;Debezium&lt;/h1&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.striim.com/blog/change-data-capture-cdc-what-it-is-and-how-it-works/&quot; target=&quot;_blank&quot;&gt;striim: Change Data Capture (CDC): What it is and How it Works&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/change-data-capture-cdc-for-data-ingestion-ca81ff5934d2&quot; target=&quot;_blank&quot;&gt;Farhan Siddiqui, Change Data Capture(CDC) for Data Lake Data Ingestion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rockset.com/blog/cdc-mysql-postgres/&quot; target=&quot;_blank&quot;&gt;How to Implement CDC for MySQL and Postgres&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.hvr-software.com/blog/change-data-capture/&quot; target=&quot;_blank&quot;&gt;/blog What are the Different Methods of Change Data Capture (CDC)?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Tue, 17 May 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series7</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series7</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Shopper Intent Prediction from Clickstream E‑Commerce Data</title>
                <description>&lt;p&gt;아직 글을 작성 중입니다.&lt;/p&gt;

&lt;p&gt;Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace &lt;a href=&quot;https://unsplash.com/photos/ag51iExrb0U&quot;&gt;Sam Bark&lt;/a&gt; diversity and empowerment.&lt;/p&gt;

&lt;p&gt;Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.&lt;/p&gt;

&lt;h3 id=&quot;synergistically-evolve&quot;&gt;Synergistically evolve&lt;/h3&gt;

&lt;p&gt;Podcasting operational change management inside of workflows to establish a framework. Taking seamless key performance indicators offline to maximise the long tail. Keeping your eye on the ball while performing a deep dive on the start-up mentality to derive convergence on cross-platform integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/08-1.jpg#wide&quot; alt=&quot;Winter&quot; /&gt;
&lt;em&gt;Photo by &lt;a href=&quot;https://unsplash.com/photos/ag51iExrb0U&quot;&gt;Sam Bark&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/&quot;&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Leverage agile frameworks to provide a robust synopsis for high level overviews. Iterative approaches to corporate strategy foster collaborative thinking to further the overall value proposition. Organically grow the holistic world view of disruptive innovation via workplace diversity and empowerment.&lt;/p&gt;

&lt;p&gt;Capitalize on low hanging fruit to identify a ballpark value added activity to beta test. Override the digital divide with additional clickthroughs from DevOps. Nanotechnology immersion along the information highway will close the loop on focusing solely on the bottom line.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The longer I live, the more I realize that I am never wrong about anything, and that all the pains I have so humbly taken to verify my notions have only wasted my time!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Bring to the table win-win survival strategies to ensure proactive domination. At the end of the day, going forward, a new normal that has evolved from generation X is on the runway heading towards a streamlined cloud solution. User generated content in real-time will have multiple touchpoints for offshoring.&lt;/p&gt;

&lt;p&gt;Phosfluorescently engage worldwide methodologies with web-enabled technology. Interactively coordinate proactive e-commerce via process-centric “outside the box” thinking. Completely pursue scalable customer service through sustainable &lt;a href=&quot;https://unsplash.com/@samuelbarkos&quot;&gt;Sam Bark&lt;/a&gt; potentialities.&lt;/p&gt;

&lt;h3 id=&quot;podcasting&quot;&gt;Podcasting&lt;/h3&gt;

&lt;p&gt;Collaboratively administrate turnkey channels whereas virtual e-tailers. Objectively seize scalable metrics whereas proactive e-services. Seamlessly empower fully researched growth strategies and interoperable internal or “organic” sources.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/08-2.jpg&quot; alt=&quot;Trees&quot; /&gt;
&lt;em&gt;Photo by &lt;a href=&quot;https://unsplash.com/@samuelbarkos&quot;&gt;Sam Bark&lt;/a&gt; on &lt;a href=&quot;https://unsplash.com/&quot;&gt;Unsplash&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Completely synergize resource taxing relationships via premier niche markets. Professionally cultivate one-to-one customer service with robust ideas. Dynamically innovate resource-leveling customer service for state of the art customer service.&lt;/p&gt;

&lt;p&gt;Globally incubate standards compliant channels before scalable benefits. Quickly disseminate superior deliverables whereas web-enabled applications. Quickly drive clicks-and-mortar catalysts for change before vertical architectures.&lt;/p&gt;

&lt;p&gt;Credibly reintermediate backend ideas for cross-platform models. Continually reintermediate integrated processes through technically sound intellectual capital. Holistically foster superior methodologies without market-driven best practices.&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.coveo.com/blog/dataset-release-intent-prediction-ecommerce/&quot; target=&quot;_blank&quot;&gt;Sharing Is Caring: Our First Dataset Release for The AI Community&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 15 May 2022 00:00:00 +0900</pubDate>
                <link>http://localhost:4000/project-clickstream-analysis</link>
                <guid isPermaLink="true">http://localhost:4000/project-clickstream-analysis</guid>
                
                
            </item>
        
            <item>
                <title>Kafka Series [Part10]: 카프카의 데이터 저장</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.cloudera.com/csa/1.2.0/flink-sql-table-api/topics/csa-kafka-sql-datatypes.html&quot; target=&quot;_blank&quot;&gt;Data types for Kafka connector&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/&quot; target=&quot;_blank&quot;&gt;Kafka Connect Deep Dive – Converters and Serialization Explained&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dol9.tistory.com/274&quot; target=&quot;_blank&quot;&gt;dol9, Kafka 스키마 관리, Schema Registry&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.freblogg.com/kafka-storage-internals&quot; target=&quot;_blank&quot;&gt;A Practical Introduction to Kafka Storage Internals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.freecodecamp.org/news/what-makes-apache-kafka-so-fast-a8d4f94ab145/&quot; target=&quot;_blank&quot;&gt;Here’s what makes Apache Kafka so fast&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/40369238/which-directory-does-apache-kafka-store-the-data-in-broker-nodes#&quot; target=&quot;_blank&quot;&gt;stackoverflow: Which directory does apache kafka store the data in broker nodes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@abhisheksharma_59226/how-kafka-stores-data-37ee611c89a2&quot; target=&quot;_blank&quot;&gt;Abhishek Sharma, How kafka stores data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rohithsankepally.github.io/Kafka-Storage-Internals/&quot; target=&quot;_blank&quot;&gt;Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sat, 14 May 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series10</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series10</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part11] MySQL 서버 설치, 설정, 사용자 및 권한 관리</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791158392703&amp;amp;orderClick=LAG&amp;amp;Kc=&quot; target=&quot;_blank&quot;&gt;Real MySQL 8.0 (1권) 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://skyvia.com/blog/how-to-import-csv-file-into-mysql-table-in-4-different-ways&quot; target=&quot;_blank&quot;&gt;How to Import CSV File into MySQL Table in 4 Different Ways&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://2oneweek.dev/others/ubuntu/install-mysql/&quot; target=&quot;_blank&quot;&gt;Ubuntu 18.04에 MySQL 설치하기 (feat. docker)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://seulcode.tistory.com/396&quot; target=&quot;_blank&quot;&gt;Docker : run ufw, iptables command in docker container&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.jiniworld.me/64&quot; target=&quot;_blank&quot;&gt;[MySQL] sql_mode로 알아보는 시스템 변수 permanent, runtime설정&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://zetawiki.com/wiki/리눅스_MySQL_시작,_정지,_재시작,_상태확인&quot; target=&quot;_blank&quot;&gt;리눅스 MySQL 시작, 정지, 재시작, 상태확인&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://serverfault.com/questions/586651/mysql-refuses-to-accept-remote-connections&quot; target=&quot;_blank&quot;&gt;MySQL refuses to accept remote connections&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://myblog.opendocs.co.kr/archives/479&quot; target=&quot;_blank&quot;&gt;[문제해결] mysql 접속에러&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://mytory.net/archives/2372&quot; target=&quot;_blank&quot;&gt;[우분투] MySql 원격 접속 허용하기 – PhpMyAdmin 사용&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot; target=&quot;_blank&quot;&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 13 May 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series11</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series11</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part9]: 카프카를 공부하면서 궁금한 점들</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#connect&quot; id=&quot;markdown-toc-connect&quot;&gt;Connect&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#프로듀서컨슈머의-설정값과-커넥터의-관계&quot; id=&quot;markdown-toc-프로듀서컨슈머의-설정값과-커넥터의-관계&quot;&gt;프로듀서/컨슈머의 설정값과 커넥터의 관계&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;connect&quot;&gt;Connect&lt;/h1&gt;

&lt;h2 id=&quot;프로듀서컨슈머의-설정값과-커넥터의-관계&quot;&gt;프로듀서/컨슈머의 설정값과 커넥터의 관계&lt;/h2&gt;

&lt;p&gt;프로듀서와 컨슈머를 커넥터로 연결하고 나면 딱히 건드릴게 없다. 근데 생각해보면 프로듀서와 컨슈머 각각 설정할 configuration들이 굉장히 많다. 그러면 그런것들은 각각의 커넥터를 REST API로 등록할 때 바꿀 수 있는 것인가? &lt;a href=&quot;https://devidea.tistory.com/96&quot;&gt;devidea: [Kafka] Connector-level producer/consumer configuration&lt;/a&gt; 글을 보면 그런 것 같은데 아직 Confuent 쪽에서는 커넥터에 관해 이런 Configuration을 커스텀하도록 fully 지원하지는 않는 것 같기도 하다. 아마 예를 들어 MongoDB, MySQL 등 각각의 프로듀서/컨슈머별로 지원해야 하는 특성들을 자기들 생각에는 잘 설정해놓았기 때문에 사용자들이 직접 건드릴 필요가 없다고 생각해서 그런 건가?&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.confluent.io/kafka-connect-jdbc/current/sink-connector/sink_config_options.html&quot; target=&quot;_blank&quot;&gt;JDBC Sink Connector Configuration Properties&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://presentlee.tistory.com/6&quot; target=&quot;_blank&quot;&gt;[Kafka] Sink Connector 생성&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.confluent.io/5.5.1/connect/kafka-connect-jdbc/index.html#mysql-server&quot; target=&quot;_blank&quot;&gt;JDBC Connector (Source and Sink) for Confluent Platform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cjw-awdsd.tistory.com/53&quot; target=&quot;_blank&quot;&gt;[Kafka] Kafka Connect 개념/예제&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://wecandev.tistory.com/110&quot; target=&quot;_blank&quot;&gt;[Kafka] Kafka Connect - JDBC Connector 예제&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.projectpro.io/recipes/save-dataframe-mysql-pyspark&quot; target=&quot;_blank&quot;&gt;How to save a DataFrame to MySQL in PySpark&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://devidea.tistory.com/96&quot;&gt;devidea: [Kafka] Connector-level producer/consumer configuration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Wed, 11 May 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series9</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series9</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part6]: 데이터 파이프라인 구축하기(1)</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#데이터-파이프라인-설계&quot; id=&quot;markdown-toc-데이터-파이프라인-설계&quot;&gt;데이터 파이프라인 설계&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;데이터-파이프라인-설계&quot;&gt;데이터 파이프라인 설계&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/data_engineering_0.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;https://hevodata.com/learn/connect-kafka-to-s3/&lt;/p&gt;

&lt;p&gt;https://swalloow.github.io/kafka-connect/&lt;/p&gt;

&lt;p&gt;https://data-engineer-tech.tistory.com/34&lt;/p&gt;

&lt;p&gt;https://www.confluent.io/ko-kr/blog/apache-kafka-to-amazon-s3-exactly-once/&lt;/p&gt;

&lt;p&gt;https://stackoverflow.com/questions/46731746/move-data-from-postgres-mysql-to-s3-using-airflow&lt;/p&gt;

&lt;p&gt;https://reddit.fun/161619/import-data-from-bucket-mysql-database-using-connectivity&lt;/p&gt;
</description>
                <pubDate>Thu, 21 Apr 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series6</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series6</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part5]: 데이터 멱등성과 ACID Transaction</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/airbnb-engineering/avoiding-double-payments-in-a-distributed-payments-system-2981f6b070bb&quot; target=&quot;_blank&quot;&gt;Avoiding Double Payments in a Distributed Payments System&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://d2.naver.com/helloworld/407507&quot; target=&quot;_blank&quot;&gt;Naver D2: DBMS는 어떻게 트랜잭션을 관리할까?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.baeldung.com/cs/transactions-intro&quot; target=&quot;_blank&quot;&gt;Baeldung: Introduction to Transactions&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://betterprogramming.pub/a-deep-dive-into-idempotence-1a39393df7e6&quot; target=&quot;_blank&quot;&gt;Yuchen Z., A Deep Dive Into Idempotence&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=12f5wB2qHI8&quot; target=&quot;_blank&quot;&gt;Youtube: ACID 2.0: Designing Better API’s and Messages - Improving Talks Series&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Wed, 20 Apr 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series5</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series5</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Apache Spark Series [Part6]: 몽고DB에서 스파크(pyspark)로 데이터 읽어오기(feat.Docker)</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#도커-컴포즈로-컨테이너-띄우기&quot; id=&quot;markdown-toc-도커-컴포즈로-컨테이너-띄우기&quot;&gt;도커 컴포즈로 컨테이너 띄우기&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#spark-client-컨테이너에서-pyspark-셸을-실행&quot; id=&quot;markdown-toc-spark-client-컨테이너에서-pyspark-셸을-실행&quot;&gt;spark-client 컨테이너에서 pyspark 셸을 실행&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pyspark-셸에서-몽고db와-연결&quot; id=&quot;markdown-toc-pyspark-셸에서-몽고db와-연결&quot;&gt;pyspark 셸에서 몽고DB와 연결&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;도커-컴포즈로-컨테이너-띄우기&quot;&gt;도커 컴포즈로 컨테이너 띄우기&lt;/h1&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;3.2'&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;services&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spark-client&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kimziont/spark:1.0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spark-client&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spark-master&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bash&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-c&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;apt -y install python-is-python3&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;sleep infinity&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;spark-master&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kimziont/spark-master:1.0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spark-master&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;4041:8080&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bash&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-c&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;./spark/sbin/start-master.sh &amp;amp;&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;sleep infinity&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;spark-worker1&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kimziont/spark-worker:1.0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;worker1&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spark-master&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;4042:8081&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bash&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-c&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;sleep infinity&lt;/span&gt;

    &lt;span class=&quot;na&quot;&gt;spark-worker2&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kimziont/spark-worker:1.0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;worker2&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;depends_on&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;spark-master&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;4043:8081&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; 
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bash&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;-c&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;./spark/sbin/start-worker.sh spark://spark-master:7077 &amp;amp;&lt;/span&gt;
          &lt;span class=&quot;s&quot;&gt;sleep infinity&lt;/span&gt;
    
    &lt;span class=&quot;na&quot;&gt;mongodb&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo:latest&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;27017:27017&quot;&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;environment&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;MONGO_INITDB_ROOT_USERNAME&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;MONGO_INITDB_ROOT_PASSWORD&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;root&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;tty&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;마스터의 UI는 디폴트로 8080포트로 보여준다, 워커는 8081포트이다&lt;/li&gt;
  &lt;li&gt;워커들은 마스터의 7077포트로 연결될 수 있다&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker compose up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;spark-client-컨테이너에서-pyspark-셸을-실행&quot;&gt;spark-client 컨테이너에서 pyspark 셸을 실행&lt;/h1&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./bin/pyspark &lt;span class=&quot;nt&quot;&gt;--master&lt;/span&gt; spark://spark-master:7077 &lt;span class=&quot;nt&quot;&gt;--packages&lt;/span&gt; org.mongodb.spark:mongo-spark-connector_2.12:3.0.1 &lt;span class=&quot;nt&quot;&gt;--jars&lt;/span&gt; /spark/mysql-connector-java-8.0.29.jar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;pyspark-셸에서-몽고db와-연결&quot;&gt;pyspark 셸에서 몽고DB와 연결&lt;/h1&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from pyspark.sql import SparkSession

spark = SparkSession.builder.master('spark://spark-master:7077').config('spark.mongodb.input.uri', 'mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin').getOrCreate()

df = spark.read.format(&quot;mongo&quot;).option(&quot;uri&quot;, &quot;mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin&quot;).load()

df.show()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;spark = SparkSession.builder.master('spark://spark-master:7077').config('spark.mongodb.input.uri', 'mongodb://root:root@mongodb:27017/quickstart.topicData?authSource=admin').config(&quot;spark.jars&quot;, &quot;/spark/mysql-connector-java-8.0.29.jar&quot;).master(&quot;spark-master:7077&quot;).getOrCreate()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/reference/connection-string/&quot; target=&quot;_blank&quot;&gt;MongoDB 공식문서: Connection String URI Format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mongodb.com/docs/manual/reference/connection-string/#mongodb-urioption-urioption.authSource&quot; target=&quot;_blank&quot;&gt;MongoDB 공식문서: Connection String URI Format: authSource&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/58305720/error-connecting-from-pyspark-to-mongodb-with-password&quot; target=&quot;_blank&quot;&gt;Error connecting from pyspark to mongodb with password&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 17 Apr 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/spark-series6</link>
                <guid isPermaLink="true">http://localhost:4000/spark-series6</guid>
                
                <category>Spark</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part10] 옵티마이저를 이용한 실행 최적화</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791158392703&amp;amp;orderClick=LAG&amp;amp;Kc=&quot; target=&quot;_blank&quot;&gt;Real MySQL 8.0 (1권) 책&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 15 Apr 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series10</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series10</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
    </channel>
</rss>
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Code Museum</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Fri, 12 Aug 2022 23:41:32 +0900</pubDate>
        <lastBuildDate>Fri, 12 Aug 2022 23:41:32 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>Data Engineering Series [Part26]: I ğŸ¤ Logs(2) Data Integration</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#two-complications&quot; id=&quot;markdown-toc-two-complications&quot;&gt;Two Complications&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-is-more-diverse&quot; id=&quot;markdown-toc-data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-explosion-of-specialized-data-systems&quot; id=&quot;markdown-toc-the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#log-structured-data-flow&quot; id=&quot;markdown-toc-log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experience-at-linkedin&quot; id=&quot;markdown-toc-experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relationship-to-etl-and-data-warehouse&quot; id=&quot;markdown-toc-relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#etl-and-scalability&quot; id=&quot;markdown-toc-etl-and-scalability&quot;&gt;ETL and Scalability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#where-should-we-put-the-data-transformations&quot; id=&quot;markdown-toc-where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#decoupling-systems&quot; id=&quot;markdown-toc-decoupling-systems&quot;&gt;Decoupling Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scaling-a-log&quot; id=&quot;markdown-toc-scaling-a-log&quot;&gt;Scaling a Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Integration and Logs&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data Integration means making available all the data (that an organization has) to all the services and systems that need it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The more recognizable term ETL(populating a relational data warehouse) usually covers only a limited part of data integration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Capturing all the relevant data and being able to put it together in an applicable processing environment&lt;/li&gt;
  &lt;li&gt;This data has to be modeled in a uniform way to make it easy to read and process&lt;/li&gt;
  &lt;li&gt;Process this data in various ways: MapReduce, real-time query systems, ans so on&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Focus on step-by-step&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reliable and complete data flow&lt;/li&gt;
  &lt;li&gt;Refining data modeling and consistency&lt;/li&gt;
  &lt;li&gt;Better visualization, reporting, algorithmic processing and prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How can we build reliable data flow throughout all the data systems?&lt;/p&gt;

&lt;h1 id=&quot;two-complications&quot;&gt;Two Complications&lt;/h1&gt;

&lt;p&gt;Two things have made data integration an increasingly difficult proflem.&lt;/p&gt;

&lt;h2 id=&quot;data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/h2&gt;

&lt;p&gt;Transactional data - things that &lt;strong&gt;are&lt;/strong&gt;,  Event data - things that &lt;strong&gt;happen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Log = Data structure what event data is logged&lt;/p&gt;

&lt;p&gt;Event data is generated from Web service, Financial organization, IoT&lt;/p&gt;

&lt;p&gt;This type of event data shakes up traditional data integration approaches because it tends to be several orders of magnitude larger than transactional data.&lt;/p&gt;

&lt;h2 id=&quot;the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/h2&gt;

&lt;p&gt;ex. OLAP, Search service, Batch processing, Graph analysis&lt;/p&gt;

&lt;h1 id=&quot;log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/h1&gt;

&lt;p&gt;Log is the natural problem data structure for handling data flow between systems.&lt;/p&gt;

&lt;h1 id=&quot;experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/h1&gt;

&lt;h1 id=&quot;relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/h1&gt;

&lt;h1 id=&quot;etl-and-scalability&quot;&gt;ETL and Scalability&lt;/h1&gt;

&lt;h1 id=&quot;where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/h1&gt;

&lt;h1 id=&quot;decoupling-systems&quot;&gt;Decoupling Systems&lt;/h1&gt;

&lt;h1 id=&quot;scaling-a-log&quot;&gt;Scaling a Log&lt;/h1&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewEng.laf?barcode=9781491909386&amp;amp;ejkGb=BNT&amp;amp;mallGb=ENG&quot; target=&quot;_blank&quot;&gt;ì±… I Heart Logs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/logika-io/try-kill-batch-processing-with-unified-log-stream-processing-d92709117f74&quot; target=&quot;_blank&quot;&gt;Try Kill batch processing with unified log stream processingâ€¦&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series26</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series26</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part25]: I ğŸ¤ Logs(1) Introduction</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-a-log&quot; id=&quot;markdown-toc-what-is-a-log&quot;&gt;What Is a Log?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-database&quot; id=&quot;markdown-toc-logs-in-database&quot;&gt;Logs in Database&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-distributed-system&quot; id=&quot;markdown-toc-logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#log-centric-design-pattern&quot; id=&quot;markdown-toc-log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-a-log&quot;&gt;What Is a Log?&lt;/h1&gt;

&lt;p&gt;Yet other than perhaps occasionally tailing a log file, most engineers donâ€™t think much about logs. To help remedy that, Iâ€™ll give an overview of how logs work in distributed systems, and then give some practical applications of these concepts to a variety of common uses: data integration, enterprise architecture, real-time data processing, and data system design.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[2022-07-02 05:30:44] method=POST user=crazyboy0510 path=/movies/comment-create/ movie_id=16
[2022-07-02 05:30:57] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:15] method=GET user=crazyboy0510 path=/movies/movie-play/16 movie_id=16
[2022-07-02 05:31:18] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:19] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Every programmer is familiar with this kind of log - a series of loosely structured requests, errors, or other messages in a sequence of rotating text files.&lt;/p&gt;

&lt;p&gt;The purpose of logs quickly becomes an input to queries in order to understand behavior across many machines, something that English text in files is not nearly as appropriate for as the kind of structured log Iâ€™ll be talking about.&lt;/p&gt;

&lt;p&gt;The log Iâ€™ll be discussing is a little more general and closer to what in the database or systems called a â€˜commit logâ€™. It is append-only sequence of records ordered by time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each rectangle represents a record that was appended to the log. Records are stored in the order they were appended. The contents and format of the records arenâ€™t important for the purposes of this discussion. To be concrete, we can just imagine each record to be a JSON blob.&lt;/p&gt;

&lt;p&gt;The log entry number can be thought of as the â€˜timestampâ€™ of the entry. this is convenient property of being decoupled from any particular physical clock. This property is essential as we get to distributed systems.&lt;/p&gt;

&lt;p&gt;A log is just kind of table or file where the records are sorted by time&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;table: array of records  
file: array of bytes  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However it is important that we thing about the log as an &lt;strong&gt;abstract data structure, not a text file&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Logs have a specific purpose: &lt;strong&gt;they record what happened and when&lt;/strong&gt;. For distributed data systems, this is the heart of the problem.&lt;/p&gt;

&lt;h1 id=&quot;logs-in-database&quot;&gt;Logs in Database&lt;/h1&gt;

&lt;p&gt;The usage in databases has to do with keeping in sync a variety of data structures and indexes in the presence of crashes. To make this atomic and durable, a database uses a log to write out information about the records it will be modifying before applying the changes to all the various data structures that it maintains.&lt;/p&gt;

&lt;p&gt;The log is the record of what happened, and each table or index is a projection of this history into some useful data structure or index.&lt;/p&gt;

&lt;p&gt;Over time, the usage of the log grew from &lt;strong&gt;an implementation detail of the ACID database properties&lt;/strong&gt; to a &lt;strong&gt;method for replicating data between databases&lt;/strong&gt;. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a remote replica database in sync. Oracle, MySQL, PostreSQL, and MongoDB include log shipping protocols to transmit portions of a log to replica databases that act as slaves. The slaves can then apply the changes recorded in the log to their own local data structures to stay in sync with the master.&lt;/p&gt;

&lt;p&gt;In fact, the use of logs is variations on the two uses in database internals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The log is used as a publish/subscribe mechanism to transmit data to other replicas&lt;/li&gt;
  &lt;li&gt;The log is used as a consistency mechanism to order the updates that are applied to multiple replicas&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/h1&gt;

&lt;p&gt;The same problems that databases solve with logs (like distributing data to replicas and agreeing on update order) are among the most fundamental problems for all distributed systems.&lt;/p&gt;

&lt;p&gt;The log-centric approach to distributed systems arises from a simple observation&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Desterministic means that the processing isnâ€™t timing dependent)&lt;/p&gt;

&lt;p&gt;The application to distributed computing is pretty obvious. You can reduce the problem of making multiple machines all do the same thing to the problem of implementaing a consistent log to feed input to theses processes. The purpose of the log here is to squeeze all the nondeterminism out of the input stream to ensure that each replica stays in sync.&lt;/p&gt;

&lt;p&gt;Discrete log entry numbers act as a clock for the state of the replicas - you can describe the state of each replica by a single number: the timestamp for the maximum log entry that it has processed. Two replicas at the same time will be in the same state.&lt;/p&gt;

&lt;h2 id=&quot;log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/h2&gt;

&lt;p&gt;There are many variations on how this principle can be applied., depending on what is put in the log. For example, we can log the incoming requests to a service and have each replica process these independently. Or we can have one instance that processed requests and log the state changes that the service undergoes in response to a request.&lt;/p&gt;

&lt;p&gt;Database people generally differentiate between physical and logical logging. Physical or row-based logging means logging the contents of each row that is changed (ë¡œìš°ë³„ ì‹¤ì œ ë³€ê²½ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê²ƒ). Logical or statement logging  means logging the SQL commands that lead to the row changes (insert, update, and delete statements).&lt;/p&gt;

&lt;p&gt;The distributed systems distinguished two broad approaches to processing and replication. The &lt;strong&gt;state machine model&lt;/strong&gt; keep a log of the incoming requests and &lt;strong&gt;each replica processes each request&lt;/strong&gt; in log order. &lt;strong&gt;primary backup&lt;/strong&gt; elect one replica as the leader. This leader processes requests in the order they arrive and logs the changes to its state that occur as a result of processing the requests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series25</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series25</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Coding Test Series [Part22]: í–‰ë ¬(Matrix) - ë¬¸ì œ</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix&quot; id=&quot;markdown-toc-matrix&quot;&gt;Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;matrix&quot;&gt;Matrix&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://leetcode.com/tag/matrix/&quot; target=&quot;_blank&quot;&gt;Leetcode: Two-Pointers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ğŸ’Ÿ âœ… â&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
---------------------------------------------EASY 3ë¬¸ì œ
- Richest Customer Wealth
- Flipping an Image
- Matrix Diagonal Sum
- The K Weakest Rows in a Matrix
- Toeplitz Matrix
- Shift 2D Grid
- Transpose Matrix

---------------------------------------------MEDIUM 5ë¬¸ì œ
- Sort the Matrix Diagonally
- Remove All Ones With Row and Column Flips
- Candy Crush
- Max Area of Island
- Rotate Image
- Sparse Matrix Multiplication
- Game of Life
- Construct Quad Tree
- Spiral Matrix II
- Walls and Gates
- Number of Islands
- Rotting Oranges
- Shortest Path in Binary Matrix
- Word Search
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/matrix</link>
                <guid isPermaLink="true">http://localhost:4000/matrix</guid>
                
                <category>Coding_Test</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part15] IntelliJ IDEë¥¼ ì´ìš©í•œ ë°ì´í„°ë² ì´ìŠ¤ ì‹œê°í™”</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.jetbrains.com/help/idea/creating-diagrams.html&quot; target=&quot;_blank&quot;&gt;IntelliJ: Database diagrams&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series15</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series15</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part14] MySQL Optimizing SELECT Statements</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#optimization-overview&quot; id=&quot;markdown-toc-optimization-overview&quot;&gt;Optimization Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#things-to-consider-for-optimization&quot; id=&quot;markdown-toc-things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#select&quot; id=&quot;markdown-toc-select&quot;&gt;SELECT&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#where&quot; id=&quot;markdown-toc-where&quot;&gt;WHERE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#group-by&quot; id=&quot;markdown-toc-group-by&quot;&gt;GROUP BY&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#join&quot; id=&quot;markdown-toc-join&quot;&gt;JOIN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#subquery&quot; id=&quot;markdown-toc-subquery&quot;&gt;Subquery&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#temporary-table&quot; id=&quot;markdown-toc-temporary-table&quot;&gt;Temporary Table&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#order-by&quot; id=&quot;markdown-toc-order-by&quot;&gt;ORDER BY&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot; id=&quot;markdown-toc-ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot;&gt;ì •ë ¬ ì²˜ë¦¬ ë°©ë²•&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;optimization-overview&quot;&gt;Optimization Overview&lt;/h1&gt;

&lt;p&gt;Database performance depends on several factors at the database level, such as tables, queries, and configuration settings. These software constructs result in CPU and I/O operations at the hardware level, which you must minimize and make as efficient as possible.&lt;/p&gt;

&lt;h1 id=&quot;things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Are the tables structured properly? In particular, do the columns have the right data types, and does each table have the appropriate columns for the type of work? For example, applications that perform frequent updates often have many tables with few columns, while applications that analyze large amounts of data often have few tables with many columns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are the right indexes in place to make queries efficient?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are you using the appropriate storage engine for each table, and taking advantage of the strengths and features of each storage engine you use? In particular, the choice of a transactional storage engine such as InnoDB or a nontransactional one such as MyISAM can be very important for performance and scalability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does each table use an appropriate row format? This choice also depends on the storage engine used for the table. In particular, compressed tables use less disk space and so require less disk I/O to read and write the data. Compression is available for all kinds of workloads with InnoDB tables, and for read-only MyISAM tables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does the application use an appropriate locking strategy? For example, by allowing shared access when possible so that database operations can run concurrently, and requesting exclusive access when appropriate so that critical operations get top priority. Again, the choice of storage engine is significant. The InnoDB storage engine handles most locking issues without involvement from you, allowing for better concurrency in the database and reducing the amount of experimentation and tuning for your code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are all memory areas used for caching sized correctly? That is, large enough to hold frequently accessed data, but not so large that they overload physical memory and cause paging. The main memory areas to configure are the InnoDB buffer pool and the MyISAM key cache.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;select&quot;&gt;SELECT&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid using *&lt;/li&gt;
  &lt;li&gt;Avoid using DISTINCT -&amp;gt; ì¤‘ë³µ ë°ì´í„° ì œê±°ë¥¼ ìœ„í•´ í…Œì´ë¸” í’€ ìŠ¤ìº” í•´ì•¼í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where&quot;&gt;WHERE&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use Indexes Where Appropriate&lt;/li&gt;
  &lt;li&gt;Avoid % Wildcard in a Predicate&lt;/li&gt;
  &lt;li&gt;Avoid using a function in the predicate of a query&lt;/li&gt;
  &lt;li&gt;BETWEEN, IN, &amp;lt;, &amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;group-by&quot;&gt;GROUP BY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GROUP BY ì‘ì—…ì€ í¬ê²Œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì™€ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°(ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©)&lt;/li&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì „ì²´ í…Œì´ë¸”ì„ ìŠ¤ìº”í•˜ì—¬ ê° ê·¸ë£¹ì˜ ëª¨ë“  í–‰ì´ ì—°ì†ë˜ëŠ” ìƒˆ ì„ì‹œ í…Œì´ë¸”ì„ ë§Œë“  ë‹¤ìŒ ì´ ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ê·¸ë£¹ì„ ê²€ìƒ‰í•˜ê³  ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒ
    &lt;ul&gt;
      &lt;li&gt;ì´ë ‡ê²Œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ í•  ìˆ˜ ìˆëŠ” ìµœì„ ì˜ ë°©ë²•ì€ WHEREì ˆì„ ì´ìš©í•´ GROUP BY í•˜ê¸° ì „ì— ë°ì´í„°ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì˜ ì„¤ì •í•œë‹¤ë©´ ì„ì‹œ í…Œì´ë¸”ì„ ìƒì„±í•˜ì§€ ì•Šê³  ë¹ ë¥´ê²Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ìµœëŒ€ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GROUP BY ì»¬ëŸ¼ê³¼, ì¸ë±ìŠ¤ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ê°„ì˜ ìˆœì„œê°€ ì¤‘ìš”í•¨&lt;/li&gt;
      &lt;li&gt;SELECTì ˆì— ì‚¬ìš©ë˜ëŠ” ì§‘ê³„í•¨ìˆ˜ì˜ ê²½ìš° MIN(), MAX()ëŠ” ì¸ë±ìŠ¤ì˜ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì°¸ê³ ë¡œ MySQL 8.0ë¶€í„°ëŠ” GROUP BYë¥¼ í•œë‹¤ê³  í•´ì„œ ì•”ë¬µì ìœ¼ë¡œ ì •ë ¬ì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ -&amp;gt; ì •ë ¬ í•„ìš”í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ORDER BY ì¨ì•¼í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì€ ë ˆì½”ë“œë¥¼ ê±´ë„ˆë›°ë©´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ëŠ” ìŠ¤ìº” ë°©ì‹&lt;/li&gt;
      &lt;li&gt;EXPLAINì„ í†µí—¤ ì‹¤í–‰ ê³„íšì„ í™•ì¸í•´ë³´ë©´ Extra ì»¬ëŸ¼ì— â€˜Using index for group-byâ€™ ë¼ê³  í‘œê¸°ë¨&lt;/li&gt;
      &lt;li&gt;MIN(), MAX() ì´ì™¸ì˜ í•¨ìˆ˜ê°€ SELECT ì ˆì— ì‚¬ìš©ë˜ë©´ ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ&lt;/li&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ê°€ (col1 col2, col3) ì¼ ë•Œ , GROUP BY col1, col2 ê³¼ ê°™ì•„ì•¼ í•¨ (GROUP BY col2, col3ì€ ì•ˆë¨)&lt;/li&gt;
      &lt;li&gt;SELECT ì ˆê³¼ GROUP BY ì ˆì˜ ì»¬ëŸ¼ì´ ì¼ì¹˜í•´ì•¼ í•¨. SELECT col1, col2, MAX(col3) GROUP BY col1, col2 ê³¼ ê°™ì•„ì•¼ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;íƒ€ì´íŠ¸ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;SELECT ì ˆê³¼ GROUP BY ì ˆì˜ ì»¬ëŸ¼ì´ ì¼ì¹˜í•˜ì§€ ì•Šì§€ë§Œ, ì¡°ê±´ì ˆì„ ì´ìš©í•´ ë²”ìœ„ ìŠ¤ìº”ì´ ê°€ëŠ¥í•œ ê²½ìš°
        &lt;ul&gt;
          &lt;li&gt;SELECT c1, c2, c3 FROM t1 WHERE c2 = â€˜aâ€™ GROUP BY c1, c3;&lt;/li&gt;
          &lt;li&gt;SELECT c1, c2, c3 FROM t1 WHERE c1 = â€˜aâ€™ GROUP BY c2, c3;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join&quot;&gt;JOIN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;INNER joins, order doesnâ€™t matter&lt;/li&gt;
  &lt;li&gt;OUTER joins, the order matters&lt;/li&gt;
  &lt;li&gt;ì—¬ëŸ¬ ì¡°ì¸ì„ í¬í•¨í•˜ëŠ” LOOP JOIN ì—ì„œëŠ” ë“œë¼ì´ë¹™ í…Œì´ë¸”(Driving Table)ì´ í–‰ë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ë¦¬í„´í•˜ë„ë¡ í•´ì•¼ë¨&lt;/li&gt;
  &lt;li&gt;JOIN ë˜ëŠ” ì»¬ëŸ¼ì˜ í•œìª½ì—ë§Œ INDEXê°€ ìˆëŠ” ê²½ìš°ëŠ” INDEXê°€ ì§€ì •ëœ TABLEì´ DRIVING TABLEì´ ëœë‹¤&lt;/li&gt;
  &lt;li&gt;JOIN ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¹¼ëŸ¼ì€ ì¸ë±ìŠ¤ë¡œ ë“±ë¡í•œë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì¸ë±ìŠ¤ ë ˆì¸ì§€ ìŠ¤ìº”ì€ ì¸ë±ìŠ¤ë¥¼ íƒìƒ‰(Index Seek)í•˜ëŠ” ë‹¨ê³„ì™€ ì¸ë±ìŠ¤ë¥¼ ìŠ¤ìº”(Index Scan)í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ êµ¬ë¶„í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ì„œ ì¿¼ë¦¬í•˜ëŠ” ì‘ì—…ì—ì„œëŠ” ê°€ì ¸ì˜¤ëŠ” ë ˆì½”ë“œì˜ ê±´ìˆ˜ê°€ ì†ŒëŸ‰(ì „ì²´ ë°ì´í„° í¬ê¸°ì˜ 20% ì´ë‚´)ì´ê¸° ë•Œë¬¸ì— ì¸ë±ìŠ¤ ìŠ¤ìº” ì‘ì—…ì€ ë¶€í•˜ê°€ ì‘ê³ , íŠ¹ì • ì¸ë±ìŠ¤ í‚¤ë¥¼ ì°¾ëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ì´ ë¶€í•˜ê°€ ë†’ì€ í¸ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;JOIN ì‘ì—…ì—ì„œ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì„ ì½ì„ ë•ŒëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ì„ ë‹¨ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì´í›„ë¶€í„°ëŠ” ìŠ¤ìº”ë§Œ ì‹¤í–‰í•˜ë©´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ë“œë¦¬ë¸ í…Œì´ë¸”ì—ì„œëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ê³¼ ìŠ¤ìº” ì‘ì—…ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì—ì„œ ì½ì€ ë ˆì½”ë“œ ê±´ìˆ˜ë§Œí¼ ë°˜ë³µí•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë“œë¼ì´ë¹™ í…Œì´ë¸”ê³¼ ë“œë¦¬ë¸ í…Œì´ë¸”ì´ 1:1 ì¡°ì¸ë˜ë”ë¼ë„ &lt;strong&gt;ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ì½ëŠ” ê²ƒì´ í›¨ì”¬ ë” í° ë¶€í•˜ë¥¼ ì°¨ì§€&lt;/strong&gt;í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ ì˜µí‹°ë§ˆì´ì €ëŠ” í•­ìƒ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ì•„ë‹ˆë¼ ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ìµœì ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆê²Œ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•œë‹¤.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SELECT *
FROM employees e, dept_emp de
WHERE e.emp_no=de.emp_no
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ì—¬ê¸°ì„œ ê° í…Œì´ë¸”ì˜ emp_no ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ê°€ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œ ì¡°ì¸ ìˆœì„œê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ” í•œ ë²ˆ ì‚´í´ë³´ì.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‘ ì»¬ëŸ¼ ëª¨ë‘ ì¸ë±ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì–´ëŠ í…Œì´ë¸”ì„ ë“œë¼ì´ë¹™ìœ¼ë¡œ ì„ íƒí•˜ë“  ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ë“œë¦¬ë¸ í…Œì´ë¸”ì˜ ê²€ìƒ‰ ì‘ì—…ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤&lt;/li&gt;
      &lt;li&gt;ë³´í†µì˜ ê²½ìš° ì–´ëŠ ìª½ í…Œì´ë¸”ì´ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ë˜ë“  ì˜µí‹°ë§ˆì´ì €ê°€ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ ìµœì ì¼ ë•Œê°€ ë§ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;employees í…Œì´ë¸”ì—ë§Œ ì¸ë±ìŠ¤ê°€ ìˆëŠ”ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì´ ë•ŒëŠ” employees í…Œì´ë¸”ì„ ë“œë¦¬ë¸ í…Œì´ë¸”ë¡œ ì„ íƒí•œë‹¤&lt;/li&gt;
      &lt;li&gt;ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ì½ëŠ” ê²ƒì´ í›¨ì”¬ ë” í° ë¶€í•˜ë¥¼ ì°¨ì§€í•˜ê¸° ë•Œë¬¸ì— ë“œë¦¬ë¸ í…Œì´ë¸”ì—ì„œ ì¸ë±ìŠ¤ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;INNER JOINì€ ì¡°ì¸ ëŒ€ìƒ í…Œì´ë¸” ëª¨ë‘ì— í•´ë‹¹í•˜ëŠ” ë ˆì½”ë“œë§Œ ë°˜í™˜í•œë‹¤. ì´ê°™ì€ íŠ¹ì„± ë•Œë¬¸ì— OUTER JOINìœ¼ë¡œë§Œ ì¡°ì¸ì„ ì‹¤í–‰í•˜ëŠ” ì¿¼ë¦¬ë“¤ë„ ìì£¼ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ëŒ€ê°œì˜ ê²½ìš° OUTER JOINì€ ëŒ€ìƒ í…Œì´ë¸”ë“¤ì˜ ë°ì´í„°ê°€ ì¼ê´€ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„ìš”í•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;MySQL ì˜µí‹°ë§ˆì´ì €ëŠ” OUTER JOINì‹œ ì¡°ì¸ ë˜ëŠ” í…Œì´ë¸”(FROM A LEFT JOIN Bì—ì„œ B)ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ë¡œ ì„ íƒí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ë¬´ì¡°ê±´ ì•ì— ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ë¡œ ì„ íƒí•œë‹¤. ê·¸ ê²°ê³¼ ì¸ë±ìŠ¤ ìœ ë¬´ì— ë”°ë¼ ì¡°ì¸ ìˆœì„œë¥¼ ë³€ê²½í•¨ìœ¼ë¡œì¨ ì–»ê²Œ ë˜ëŠ” ìµœì í™”ì˜ ì´ì ì„ ì–»ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì¿¼ë¦¬ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ê¼­ í•„ìš”í•œ ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´ INNER JOINì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ì— ë„ì›€ì´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;JOINì˜ ìˆœì„œ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;INNER JOINì¸ ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì–´ì°¨í”¼ A and B and C ì´ê¸° ë•Œë¬¸ì— A JOIN B JOIN Cì´ë“  B JOIN A JOIN Cì´ë“  ê°™ë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LEFT JOINì˜ ê²½ìš° ê²°ê³¼ë„ ì„±ëŠ¥ë„ ë‹¬ë¼ì§„ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì¼ë‹¨ ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì´ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ëœë‹¤ -&amp;gt; ì´ ë§ì€ ë’¤ì— ë”°ë¼ì˜¤ëŠ” í…Œì´ë¸”ì€ ë“œë¦¬ë¸ í…Œì´ë¸”ì´ ëœë‹¤ëŠ” ë§ì´ë‹¤ -&amp;gt; ë“œë¦¬ë¸ í…Œì´ë¸”ì€ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ -&amp;gt; ë’¤ì— ì¡°ì¸ë˜ëŠ” í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ìœ ë¬´ì— ë”°ë¼ ì¿¼ë¦¬ ì„±ëŠ¥ì´ ë‹¬ë¼ì§„ë‹¤&lt;/li&gt;
      &lt;li&gt;ê²°ê³¼ ìì²´ë„ ë§¨ ì•ì— ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì˜ ëª¨ë“  ë ˆì½”ë“œê°€ ê¸°ì¤€ì´ ë˜ê¸° ë•Œë¬¸ì— ìˆœì„œì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;INNER JOINê³¼ OUTER JOINì´ ê²°í•©ë˜ëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ê°€ëŠ¥í•˜ë‹¤ë©´ INNER JOINì´ ì•ì— ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;subquery&quot;&gt;Subquery&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid correlated sub queries as it searches row by row, impacting the speed of SQL query processing&lt;/li&gt;
  &lt;li&gt;JOINìœ¼ë¡œ í•´ê²°ë˜ë©´ ì„œë¸Œì¿¼ë¦¬ ëŒ€ì‹  JOINì„ ì‚¬ìš©í•˜ì&lt;/li&gt;
  &lt;li&gt;ì„œë¸Œì¿¼ë¦¬ ì•ˆì— whereì ˆê³¼ group byë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¤ëŠ” ë°ì´í„°ì–‘ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤&lt;/li&gt;
  &lt;li&gt;ì„œë¸Œì¿¼ë¦¬ëŠ” ì¸ë±ìŠ¤ ë˜ëŠ” ì œì•½ ì •ë³´ë¥¼ ê°€ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— ìµœì í™”ë˜ì§€ ëª»í•œë‹¤&lt;/li&gt;
  &lt;li&gt;ìœˆë„ìš° í•¨ìˆ˜ë¥¼ ê³ ë ¤í•´ë³´ì&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;temporary-table&quot;&gt;Temporary Table&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use a temporary table to handle bulk data&lt;/li&gt;
  &lt;li&gt;Temporary table vs Using index access&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;order-by&quot;&gt;ORDER BY&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ëŒ€ë¶€ë¶„ì˜ SELECT ì¿¼ë¦¬ì—ì„œ ì •ë ¬ì€ í•„ìˆ˜ì &lt;/li&gt;
  &lt;li&gt;ì •ë ¬ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ &lt;strong&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•&lt;/strong&gt;ê³¼ &lt;strong&gt;Filesort&lt;/strong&gt;ë¼ëŠ” ë³„ë„ì˜ ì²˜ë¦¬ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ë°©ë²•&lt;/td&gt;
      &lt;td&gt;ì¥ì &lt;/td&gt;
      &lt;td&gt;ë‹¨ì &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ì¸ë±ìŠ¤ ì´ìš©&lt;/td&gt;
      &lt;td&gt;SELECT ë¬¸ì„ ì‹¤í–‰í•  ë•Œ ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ì •ë ¬ë¼ ìˆì–´ ìˆœì„œëŒ€ë¡œ ì½ê¸°ë§Œ í•˜ë©´ ë˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¥´ë‹¤&lt;/td&gt;
      &lt;td&gt;INSERT, UPDATE, DELETE ì‘ì—…ì‹œ ë¶€ê°€ì ì¸ ì¸ë±ìŠ¤ ì¶”ê°€/ì‚­ì œ ì‘ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ëŠë¦¬ë‹¤&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Filesort ì´ìš©&lt;/td&gt;
      &lt;td&gt;ì¸ë±ìŠ¤ ì´ìš©ê³¼ ë°˜ëŒ€ë¡œ INSERT, UPDATE, DELETE ì‘ì—…ì´ ë¹ ë¥´ë‹¤&lt;/td&gt;
      &lt;td&gt;ì •ë ¬ ì‘ì—…ì´ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œ ì²˜ë¦¬ë˜ì–´ ì¿¼ë¦¬ì˜ ì‘ë‹µ ì†ë„ê°€ ëŠë ¤ì§„ë‹¤&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Filesortë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì •ë ¬ ê¸°ì¤€ì´ ë„ˆë¬´ ë§ì•„ì„œ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°&lt;/li&gt;
  &lt;li&gt;ì–´ë–¤ ì²˜ë¦¬ì˜ ê²°ê³¼ë¥¼ ì •ë ¬í•´ì•¼ í•˜ëŠ” ê²½ìš°&lt;/li&gt;
  &lt;li&gt;ëœë¤í•˜ê²Œ ê²°ê³¼ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ëŠ” ê²½ìš°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ì†ŒíŠ¸ ë²„í¼&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MySQLì€ ì •ë ¬ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ë©”ëª¨ë¦¬ ê³µê°„ì„ í• ë‹¹ë°›ì•„ì„œ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì†ŒíŠ¸ ë²„í¼ë¼ê³  í•œë‹¤&lt;/li&gt;
  &lt;li&gt;ì •ë ¬í•´ì•¼ í•  ë ˆì½”ë“œì˜ ê±´ìˆ˜ê°€ ì†ŒíŠ¸ ë²„í¼ì˜ í¬ê¸°ë³´ë‹¤ í¬ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?
    &lt;ul&gt;
      &lt;li&gt;ì •ë ¬í•´ì•¼ í•  ë ˆì½”ë“œë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ê²Œ ë¨. ì´ ê³¼ì •ì—ì„œ ì„ì‹œ ì €ì¥ì„ ìœ„í•´ ë””ìŠ¤í¬ë¥¼ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;ì¼ë¶€ë¥¼ ì²˜ë¦¬í•˜ê³  ë””ìŠ¤í¬ì— ì €ì¥í•˜ê¸°ë¥¼ ë°˜ë³µ ìˆ˜í–‰í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ì •ë ¬ ì•Œê³ ë¦¬ì¦˜&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ë§Œ ê°€ì ¸ì™€ì„œ ì •ë ¬í•˜ëŠ” ë°©ì‹
    &lt;ul&gt;
      &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ ê°’ë§Œ ì†ŒíŠ¸ ë²„í¼ì— ë‹´ì•„ ì •ë ¬ì„ ìˆ˜í–‰&lt;/li&gt;
      &lt;li&gt;ê·¸ë¦¬ê³  ë‹¤ì‹œ ì •ë ¬ ìˆœì„œëŒ€ë¡œ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ë¡œ í…Œì´ë¸”ì„ ì½ì–´ì„œ SELECTí•  ì»¬ëŸ¼ì„ ê°€ì ¸ì˜´&lt;/li&gt;
      &lt;li&gt;ê°€ì ¸ì˜¤ëŠ” ì»¬ëŸ¼ì´ ë‘ ê°œ ë¿ì´ë¼ ì†ŒíŠ¸ ë²„í¼ì— ë§ì€ ë ˆì½”ë“œë¥¼ í•œ ë²ˆì— ì½ì–´ì˜¬ ìˆ˜ ìˆìŒ&lt;/li&gt;
      &lt;li&gt;ë‹¨ì ì€ í…Œì´ë¸”ì„ ë‘ ë²ˆ ì½ì–´ì•¼ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ SELECTë¬¸ìœ¼ë¡œ ìš”ì²­í•œ ì»¬ëŸ¼ì„ ëª¨ë‘ ê°€ì ¸ì™€ì„œ ì •ë ¬í•˜ëŠ” ë°©ì‹
    &lt;ul&gt;
      &lt;li&gt;ìµœì‹  ë²„ì „ì˜ MySQLì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹&lt;/li&gt;
      &lt;li&gt;SELECT ë¬¸ì—ì„œ ìš”ì²­í•œ ì»¬ëŸ¼ì˜ ê°œìˆ˜ê°€ ë§ì•„ì§€ë©´ ê³„ì† ë¶„í• í•´ì„œ ì†ŒíŠ¸ ë²„í¼ì— ì½ì–´ì™€ì•¼í•¨&lt;/li&gt;
      &lt;li&gt;ë ˆì½”ë“œì˜ í¬ê¸°ë‚˜ ê±´ìˆ˜ê°€ ì‘ì€ ê²½ìš° ì„±ëŠ¥ì´ ì¢‹ìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot;&gt;ì •ë ¬ ì²˜ë¦¬ ë°©ë²•&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ì •ë ¬
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ì •ë ¬ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ORDER BYì˜ ìˆœì„œëŒ€ë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ê°€ ìˆì–´ì•¼ í•¨&lt;/li&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ì •ë ¬ì´ ê°€ëŠ¥í•œ ì´ìœ ëŠ” B-Tree ì¸ë±ìŠ¤ê°€ í‚¤ ê°’ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆê¸° ë•Œë¬¸&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Filesortë¥¼ ì‚¬ìš©í•œ ì •ë ¬
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, WHERE ì¡°ê±´ì— ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œë¥¼ ê²€ìƒ‰í•´ ì •ë ¬ ë²„í¼ì— ì €ì¥í•˜ë©´ì„œ ì •ë ¬ì„ ì²˜ë¦¬(FIlesort)í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html&quot; target=&quot;_blank&quot;&gt;MySQL ê³µì‹ë¬¸ì„œ: Optimizing SELECT Statements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://phoenixnap.com/kb/improve-mysql-performance-tuning-optimization&quot; target=&quot;_blank&quot;&gt;MySQL Performance Tuning and Optimization Tips&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://danbi-ncsoft.github.io/works/2021/11/05/etl-performace-tips.html&quot; target=&quot;_blank&quot;&gt;ETL ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëª‡ ê°€ì§€ íŒë“¤&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://til.songyunseop.com/mysql/group-by-optimization.html&quot; target=&quot;_blank&quot;&gt;ì „ì§€ì  ì†¡ìœ¤ì„­ì‹œì  TIL, GROUP BY ìµœì í™”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://otsteam.tistory.com/136&quot; target=&quot;_blank&quot;&gt;SQL ì„±ëŠ¥ì„ ìœ„í•œ 25ê°€ì§€ ê·œì¹™&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jojoldu.tistory.com/173?category=761883&quot; target=&quot;_blank&quot;&gt;íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ SQLíŠœë‹ìº í”„ 4ì¼ì°¨ - ì¡°ì¸ì˜ ê¸°ë³¸ ì›ë¦¬ì™€ í™œìš©&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brightestbulb.tistory.com/147&quot; target=&quot;_blank&quot;&gt;ì·¨ë¯¸ëŠ” ê³µë¶€ íŠ¹ê¸°ëŠ” ê¸°ë¡, Nested Loop Join, Driving Table&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/9614922/does-the-join-order-matter-in-sql&quot; target=&quot;_blank&quot;&gt;stackoverflow, Does the join order matter in SQL?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://coding-factory.tistory.com/756&quot; target=&quot;_blank&quot;&gt;ì½”ë”©íŒ©í† ë¦¬, [DB] ë°ì´í„°ë² ì´ìŠ¤ NESTED LOOPS JOIN (ì¤‘ì²© ë£¨í”„ ì¡°ì¸)ì— ëŒ€í•˜ì—¬&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://schatz37.tistory.com/2&quot; target=&quot;_blank&quot;&gt;ê³ ë™ì˜ ë°ì´í„° ë¶„ì„, [SQL] â€œì„±ëŠ¥ ê´€ì â€ì—ì„œ ë³´ëŠ” ê²°í•©(Join)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://schatz37.tistory.com/3?category=878798&quot; target=&quot;_blank&quot;&gt;ê³ ë™ì˜ ë°ì´í„° ë¶„ì„, [SQL] ì„±ëŠ¥ ê´€ì ì—ì„œì˜ ì„œë¸Œì¿¼ë¦¬(Subquery)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series14</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series14</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Coding Test Series [Part21]: íŒŒì´ì¬ ë¬¸ë²•</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#in-python-and-operation-will-not-return-a-boolean-value&quot; id=&quot;markdown-toc-in-python-and-operation-will-not-return-a-boolean-value&quot;&gt;In python, â€˜andâ€™ operation will not return a boolean value&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;in-python-and-operation-will-not-return-a-boolean-value&quot;&gt;In python, â€˜andâ€™ operation will not return a boolean value&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;If itâ€™s true, it will return the last true value, remember is the value, not True. Otherwise, it will return the first false value.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'ban' and 'car' -&amp;gt; 'car'
0 and 'car' -&amp;gt; 0
'ban' and False -&amp;gt; False
'ban' or False -&amp;gt; 'ban'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/python-things</link>
                <guid isPermaLink="true">http://localhost:4000/python-things</guid>
                
                <category>Coding_Test</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part24]: ì‹œìŠ¤í…œ ë””ìì¸(4) Database</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#factors&quot; id=&quot;markdown-toc-factors&quot;&gt;Factors&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#caching-solution&quot; id=&quot;markdown-toc-caching-solution&quot;&gt;Caching Solution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#file-storage-solution&quot; id=&quot;markdown-toc-file-storage-solution&quot;&gt;File Storage Solution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storage-solutions-offering-text-search-capability&quot; id=&quot;markdown-toc-storage-solutions-offering-text-search-capability&quot;&gt;Storage Solutions Offering Text Search Capability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#time-series-database&quot; id=&quot;markdown-toc-time-series-database&quot;&gt;Time Series Database&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#data-warehousing-storage-solution&quot; id=&quot;markdown-toc-data-warehousing-storage-solution&quot;&gt;Data Warehousing Storage Solution&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#rdbms-vs-nosql&quot; id=&quot;markdown-toc-rdbms-vs-nosql&quot;&gt;RDBMS vs NoSQL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Databases will not impact your functional requirements. Whichever database you use, you can still achieve your functional requirements somehow, but at the cost of huge performance degradation. So when we say requirement, we usually mean non-functional requirements.&lt;/p&gt;

&lt;h1 id=&quot;factors&quot;&gt;Factors&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Structure of the data&lt;/li&gt;
  &lt;li&gt;Query pattern&lt;/li&gt;
  &lt;li&gt;Amount or scale that you need to handle&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These are the factors we need to consider when selecting which database to use. Now let us look at various types of storage solutions and some use cases where they will be suitable.&lt;/p&gt;

&lt;h1 id=&quot;caching-solution&quot;&gt;Caching Solution&lt;/h1&gt;
&lt;p&gt;If you are calling your database very frequently or making a remote call to independent services with high latency, you might want to cache some data locally at your end. Some of the most commonly used caching solutions are Memcached, Hazelcast, and Redis. You could also use some other solutions; this is not an exhaustive list. In the following articles, we will usually use Redis as it is one of the most widely used and stable solutions.&lt;/p&gt;

&lt;h1 id=&quot;file-storage-solution&quot;&gt;File Storage Solution&lt;/h1&gt;

&lt;p&gt;Assume you are working on something like Netflix and you need a data store for images, videos, etc. Now, in this case, a database is not very useful to us as we are storing files rather than information. &lt;strong&gt;Databases are meant to store information that can be queried&lt;/strong&gt;, whereas files you do not need to query. You just deliver them as they are.&lt;/p&gt;

&lt;p&gt;This is when we use something called Blob (Binary Large Object) storage. Amazon S3 is an example of blob storage. Usually, blob storage is used in combination with a Content delivery network or a CDN. A CDN is a network of servers around the world that delivers content in different geographical locations with reduced latency. If the server you are getting content from is closer to your geographic location, the content will take less time (reduced latency) to be delivered from the server to you.&lt;/p&gt;

&lt;h1 id=&quot;storage-solutions-offering-text-search-capability&quot;&gt;Storage Solutions Offering Text Search Capability&lt;/h1&gt;

&lt;p&gt;Letâ€™s again take the Netflix example. Suppose you want to build a search functionality where the user can search by movie, genre, actor, actress, director, etc. Here you use a search engine like Solr or Elasticsearch which can support fuzzy search.&lt;/p&gt;

&lt;p&gt;To understand fuzzy search, let us take an example of an Uber user searching for airprot. If you notice this is a typo, what the user means to search is airport. But if, because of this typo, we donâ€™t provide any search results, it will be a very poor user experience. So we search for terms similar to airport in the database. This is known as fuzzy search.&lt;/p&gt;

&lt;p&gt;Now a key point here is that these search engines are not databases. Databases provide a guarantee that once stored, our data will not be lost unless we delete it; search engines offer no such guarantee. This is why we should never use search engines like Elasticsearch as our primary data source. We can load the data to them from our primary database to reduce search latency and provide fuzzy and relevance-based text search.&lt;/p&gt;

&lt;h1 id=&quot;time-series-database&quot;&gt;Time Series Database&lt;/h1&gt;

&lt;p&gt;Suppose we are trying to build a metric tracking system. We will need something called a time-series database. Time-series databases are, in a way, an extension of relational databases, but unlike a standard relational DB, time-series databases will never be randomly updated. It will be &lt;strong&gt;updated sequentially in an append-only format&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Also, it will have more bulk reads for a certain time range as opposed to random reads. For example, how many people watched a video in the last 1 week, 10 days, 1 month, 1 year, and so on. Some examples of time series databases are OpenTSDB and InfluxDB.&lt;/p&gt;

&lt;h1 id=&quot;data-warehousing-storage-solution&quot;&gt;Data Warehousing Storage Solution&lt;/h1&gt;

&lt;p&gt;Sometimes we need a large database to dump all of the data available to us, to perform analytics. Eg. a company like Uber will store all of their data so they can perform analytics to identify where Uber is not used very much, where are the hotspots, what are the peak hours, etc. These systems are not used for regular transactions but offline reporting. Hadoop is a very commonly used Data warehouse.&lt;/p&gt;

&lt;h1 id=&quot;rdbms-vs-nosql&quot;&gt;RDBMS vs NoSQL&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_14.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If you need ACID properties, then you need to use a relational DBMS. Some examples are MySQL, Oracle, and Postgres. But what if you donâ€™t need ACID? Well, you can still use RDBMS, or you can use a Non-relational database.&lt;/p&gt;

&lt;p&gt;Letâ€™s consider an example. Suppose you are trying to build a catalog for something like Amazon, where you want to store information about different products that have various attributes. These attributes will normally not be the same for different products. In such a case, our data cannot be represented as a table. This means we need to use a NoSQL database.&lt;/p&gt;

&lt;p&gt;Also, we donâ€™t just need to store this data but also &lt;strong&gt;query on this data&lt;/strong&gt;. Here comes the factor of query pattern. Which type of database we use here will be decided based on what type of data we store and what types of queries will be run on it. If we have vast data - not just volume but also a vast variety of attributes - and we need to run a vast variety of queries, we need to use something called a Document DB. Couchbase and MongoDB are some commonly used document databases. (Elasticsearch is special cases of document DB)&lt;/p&gt;

&lt;p&gt;But what if &lt;strong&gt;you donâ€™t have a vast variety of attributes&lt;/strong&gt; i.e. very &lt;strong&gt;limited variety of queries&lt;/strong&gt;, but the &lt;strong&gt;size of the database increases very rapidly&lt;/strong&gt;? For example, data collected by Uber for their driversâ€™ location pings. Now the number of Uber drivers will keep increasing day by day, and therefore so will the data collected every day. This results in an ever-increasing amount of data. In such cases, we use &lt;strong&gt;Columnar DBs like Cassandra or HBase&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Using Cassandra is lighter to deploy, whereas HBase is built on top of Hadoop; we would need to first set up Hadoop and then setup HBase on top of it. This makes the setup of HBase a little lengthy, but performance-wise both are pretty much the same.&lt;/p&gt;

&lt;p&gt;Let us assume we have stored Uberâ€™s ride-related data in a Cassandra with driver id as a partition key. Now when we want to fetch a ride for a particular driver on a particular date, Cassandra can find it easily enough. But if we want to find a customerâ€™s ride on a particular date, Cassandra will have to fan out this query to all the partitions since customer id is not a partition key. So what is the point of using Cassandra if it is not going to scale well!&lt;/p&gt;

&lt;p&gt;Well, there is a simple enough fix. We can replicate the same data to another table or column family with a different partition key. Now when we receive the query for customer id and date, we can simply direct it to the table where the partition key is customer id. This is what we mean by a limited variety of queries but a huge scale. Cassandra (and HBase) can scale massively as long as the queries are of similar types.&lt;/p&gt;

&lt;p&gt;If the queries are more diverse, then we will have to replicate again and again for each partition key, which we can, but only to a certain limit. If we cannot control the types of queries, then something like MongoDB might be the way to go. But if we just need a huge scale for a few types of queries, the Cassandra is the perfect solution.&lt;/p&gt;

&lt;p&gt;Now letâ€™s shake things up a bit!&lt;/p&gt;

&lt;p&gt;Letâ€™s consider the Amazon example again. If for a product we have only one item in stock but multiple users are trying to buy it, it should only be sold to one user, which means we need ACID here. So we should choose a Relational DB like MySQL.&lt;/p&gt;

&lt;p&gt;But the orders-related data for Amazon will be ever-increasing and will have a variety of attributes. That means we should use a Columnar NoSQL database like Cassandra. So which one to go for? We decide to go with a combination of both. We can store the data of orders that are not yet delivered in a MySQL database, and once the order is completed, we can move it to Cassandra to be permanently stored.&lt;/p&gt;

&lt;p&gt;But again, our requirements might be a little more complex. Suppose you want to build a reporting system for how many people bought a particular item. Now, on Amazon, products are sold by various users of different brands and different variations. So the report can not target a single product, rather, it should target a subset of products, which can be in either Cassandra or MySQL.&lt;/p&gt;

&lt;p&gt;Such a requirement is an example of a situation where our best choice would be a document DB like Mongo DB. So we decide to keep a subset of this orders data in Mongo DB that tells us which users bought how much quantity of a certain product, at what time, on what date, etc. So suppose you want to check how many people bought sugar in the last month. You can get order ids from Mongo DB and use this order id to pick up the rest of the data from Cassandra or MySQL.&lt;/p&gt;

&lt;p&gt;That should be it for Storage Solutions in System Design!&lt;/p&gt;
</description>
                <pubDate>Tue, 26 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series24</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series24</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part23]: ì‹œìŠ¤í…œ ë””ìì¸(4) Caching</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-a-cache&quot; id=&quot;markdown-toc-what-is-a-cache&quot;&gt;What is a Cache?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#when-to-cache&quot; id=&quot;markdown-toc-when-to-cache&quot;&gt;When to Cache?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#how-does-caching-work&quot; id=&quot;markdown-toc-how-does-caching-work&quot;&gt;How Does Caching Work?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#when-not-to-cache&quot; id=&quot;markdown-toc-when-not-to-cache&quot;&gt;When Not to Cache?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#cache-eviction-strategies&quot; id=&quot;markdown-toc-cache-eviction-strategies&quot;&gt;Cache Eviction Strategies&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#time-based&quot; id=&quot;markdown-toc-time-based&quot;&gt;Time Based&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#size-based&quot; id=&quot;markdown-toc-size-based&quot;&gt;Size Based&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#metrics&quot; id=&quot;markdown-toc-metrics&quot;&gt;Metrics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#caching-products&quot; id=&quot;markdown-toc-caching-products&quot;&gt;Caching Products&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-a-cache&quot;&gt;What is a Cache?&lt;/h1&gt;
&lt;p&gt;A cache is high-speed data access and storage layer that helps us fetch data that we had previously retrieved or computed.&lt;/p&gt;

&lt;p&gt;If we need frequent or repeated access to certain information that we have already queried from a service or a database, then instead of repeatedly querying the service, itâ€™s better to cache that information for subsequent use, for it to be readily available.&lt;/p&gt;

&lt;p&gt;For Example, for Twitter users, we can cache all the information that is needed to load the homepage of a user (like followers, tweets, etc.). Caching will help avoid repeatedly querying the same service, caused by the refreshing of the browser by the user or other similar use cases. In this case, caching helps to reduce latency and improve resource utilization of servers.&lt;/p&gt;

&lt;h1 id=&quot;when-to-cache&quot;&gt;When to Cache?&lt;/h1&gt;
&lt;p&gt;A cache is primarily used in the following scenarios:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;When the data involves a lot of computation, then to reduce latency and CPU utilization, itâ€™s good to cache the pre-calculated information for fast retrieval later on.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;To reduce frequent database or network (API) calls, itâ€™s beneficial to cache the previously fetched data for fast retrieval. This helps reduce latency as well as bandwidth requirements.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;how-does-caching-work&quot;&gt;How Does Caching Work?&lt;/h1&gt;

&lt;p&gt;A cache is primarily used to store the most recently or frequently used data, in the hope that it will soon be fetched again. Caches are typically faster than databases and services when it comes to re-accessing this stored information. What makes them so fast is the fact that caches store data in SSDs and mostly RAMs which reduces the lookup time. However, this does not mean that we should cache everything.&lt;/p&gt;

&lt;h1 id=&quot;when-not-to-cache&quot;&gt;When Not to Cache?&lt;/h1&gt;
&lt;p&gt;There are some scenarios where the negative aspects of caching outweigh its benefits:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;High Consistency requirements: When we fetch the previously stored data from the cache, there is a possibility of stale data being displayed to the user. For example, for a social media app, then some stale data is probably fine. However, for a stock price display app, then the cache must be in sync with the primary data source.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Write heavy / Read Once: When write operations (updates to data) are more frequent than read operations (data retrieval). For example, caching the data of an analytics system would only increase the hardware maintenance cost.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Low repetition: When the action of retrieval of the same information is not frequently repeated by the user. For example, the cost calculated by the trip cost estimation module of a cab booking app between the exact two points need not be cached.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;cache-eviction-strategies&quot;&gt;Cache Eviction Strategies&lt;/h1&gt;
&lt;p&gt;We need to regularly expel data from the cache to limit the size of the cache and to maintain its speed while ensuring that the entries are up to date.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;time-based&quot;&gt;Time Based&lt;/h2&gt;

&lt;p&gt;We keep an entry in the cache for some amount of time. In this strategy, we set a TTL (Time To Live). We will evict the entry from the cache after a certain pre-determined time has elapsed. The time which an entry stays in the cache before being evicted is called TTL.&lt;/p&gt;

&lt;h2 id=&quot;size-based&quot;&gt;Size Based&lt;/h2&gt;

&lt;p&gt;We keep at most some number of entries in the cache.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;FIFO (First In First Out): We harness the FIFO property of the queue data structure to evict old entries.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LFU (Least Frequently Used): When we must evict an entry, we will evict the least frequently used entry.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LRU (Least Recently Used): When we must evict an entry, we will evict the least recently used entry.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;LFRU (Least Frequently and Recently Used): We evict the least valuable entry, the one thatâ€™s neither used frequently nor recently. This strategy gives the best results.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;metrics&quot;&gt;Metrics&lt;/h1&gt;
&lt;p&gt;We can use the following metrics to evaluate the performance of the cache:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Size: Increasing the size of the cache usually increases the response time and therefore reduces its performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Latency: The introduction of cache into the system must reduce latency from what it was earlier.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cache Hit Rate: It is the ratio of results found in the cache to the requests made to the cache. If the cache hit rate is low i.e., requests to cache are not returning the desired data, then it essentially slows down the system.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;caching-products&quot;&gt;Caching Products&lt;/h1&gt;

&lt;p&gt;Itâ€™s usually not recommended to write your own cache implementation, because there are a lot of really good cache implementations out there that you can use. Some of the most popular caching products that are available in the market are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ehcache&lt;/li&gt;
  &lt;li&gt;Hazelcast&lt;/li&gt;
  &lt;li&gt;Memcached&lt;/li&gt;
  &lt;li&gt;Redis&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Tue, 26 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series23</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series23</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part22]: ì‹œìŠ¤í…œ ë””ìì¸(3) Consistent Hashing</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#why-hashing&quot; id=&quot;markdown-toc-why-hashing&quot;&gt;Why hashing?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#consistent-hashing&quot; id=&quot;markdown-toc-consistent-hashing&quot;&gt;Consistent Hashing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#where-is-it-used&quot; id=&quot;markdown-toc-where-is-it-used&quot;&gt;Where is it Used&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;why-hashing&quot;&gt;Why hashing?&lt;/h1&gt;

&lt;p&gt;Hash is basically a function that takes a value and converts it into another value of a specific format. Hash functions that are commonly used are MD5, SHA1, SHA256, etc.&lt;/p&gt;

&lt;p&gt;Suppose you build a distributed cache, where the data is distributed over various nodes, sometimes spanning multiple data centers. When we want to store the data for a user, we need to decide which node will cache this data. And when we want to retrieve this cached value, we must query the same node. So let us use a simple hash function for this again.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hash(user) = Sum(ASCII value of all characters in name) % 5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where 5 is the number of nodes. This way, the hashes generated for users will be as follows:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since hash(Alice) is 0, Aliceâ€™s data will be stored in node 0. Similarly, Bobâ€™s data will be stored in node 1 and Eveâ€™s in 4. Now when you need to look up the information for Alice, you will use the same hash function to determine which node to query. Since hash(Alice) equals 0, you will query node 0 and fetch the information.&lt;/p&gt;

&lt;p&gt;But there is a problem with this solution. This model is not scalable. If the traffic increases and you want to add a new node, the formula to calculate the hash will get updated to&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hash(user) = Sum(ASCII value of all characters in name) % 6
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And similarly hash(Alice) will get updated to 2. So now you will search for Aliceâ€™s information in node 2, but the information is actually stored in node 0 so you wonâ€™t find it.&lt;/p&gt;

&lt;p&gt;To fix this, you will need to rehash all the data every time a node is added or removed, and once rehashed, you need to move the data to their respective new nodes, which could be across different data centers. This is not a very good solution as it uses up a lot of CPU resources and bandwidth.&lt;/p&gt;

&lt;p&gt;This is where Consistent Hashing comes in.&lt;/p&gt;

&lt;h1 id=&quot;consistent-hashing&quot;&gt;Consistent Hashing&lt;/h1&gt;

&lt;p&gt;Consistent Hashing tries to optimize the system in such a way that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You donâ€™t need to move around all the data while adding or removing nodes.&lt;/li&gt;
  &lt;li&gt;There will be minimal movement of data as if a node is removed, the data from that node will be supported by another node. Similarly, when a node is added, some data will be mapped to it as you donâ€™t want it to sit idle.&lt;/li&gt;
  &lt;li&gt;You can have a nearly even distribution of data across all machines.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The idea is &lt;strong&gt;removing the number of nodes in the system out of the equation&lt;/strong&gt; while calculating the hash. Now hashes for data and nodes will all be independently calculated and adding or removing nodes wonâ€™t change these hashes.&lt;/p&gt;

&lt;p&gt;Now instead of assigning the data to these nodes in a sequential manner, we will plot these nodes, or their hashes, on the number line, and each node will be responsible for the range between its position and the position of the first node to its right.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When the data comes in for storage, we will calculate its hash to determine its place on the number line. Based on which nodeâ€™s range it falls in, we will map the data to that node.&lt;/p&gt;

&lt;p&gt;If you want to remove a node from this system, the range of the previous machine on the number line will be extended, and all the data from the removed node will be mapped to the previous node in the number line. This resolves the issue of data transfer as only the data from one machine will need to be mapped to another machine.&lt;/p&gt;

&lt;p&gt;But there is still a problem with this system. &lt;strong&gt;After removing a node, the ranges of certain machines might not be balanced.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;An ideal solution to this would be &lt;strong&gt;assigning data between various nodes.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The idea is to assign the same machine to multiple hashes and map each of these hashes on the number line.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This can be achieved by either using multiple hash functions or by assigning multiple identifiers to the same node, and then calculating the hashes for all of the instanced. Then, We can map them on the number line, or what we can now refer to as a consistent hashing ring, essentially representing the same node on the ring multiple times.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now the data handled by a removed node will be distributed across the ring. So when a node needs to be removed, all its identifiers will be removed and the data will be mapped to the nodes falling to the right of each identifier, as shown in the following diagram, making the distribution much more even.&lt;/p&gt;

&lt;p&gt;(ë…¸ë“œê°€ ì‚­ì œë˜ë„, ê·¸ ë…¸ë“œì— ì €ì¥ë˜ì–´ ìˆë˜ ë°ì´í„°ê°€ ì—¬ëŸ¬ í•´ì‹œ í•¨ìˆ˜ì— ì˜í•´ ë¶„ë¦¬ëœ ì¸ìŠ¤í„´ìŠ¤ì— í©ì–´ì ¸ì„œ ì €ì¥ë˜ì–´ ìˆì—ˆìœ¼ë¯€ë¡œ, ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™í•  ë•Œë„ ê° ì¸ìŠ¤í„´ìŠ¤ì˜ ì˜¤ë¥¸ìª½ ì¸ìŠ¤í„´ìŠ¤ë¡œ ì €ì¥ë˜ë¯€ë¡œ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚° ì €ì¥ë˜ê²Œ ëœë‹¤. -&amp;gt; ë°ì´í„°ê°€ í›¨ì”¬ evení•´ì§„ë‹¤.)&lt;/p&gt;

&lt;p&gt;The reason why it gets even is because each identifier of machine is hashed separately, with the hash function, it is very likely that they would be arranged in a random fashion like Blue_4 - Red_1 - Blue_2 - Green_3 - Red_4 - Orange_1, which makes sure that when one machine is removed, itâ€™s data us spread across multiple other machines.&lt;/p&gt;

&lt;p&gt;Similarly, when a new node is added, its identifiers will also be mapped across the ring, picking up data from various nodes, maintaining even distribution across the ring.&lt;/p&gt;

&lt;h1 id=&quot;where-is-it-used&quot;&gt;Where is it Used&lt;/h1&gt;

&lt;p&gt;Now we just saw how we could use consistent hashing while building a caching system. There are a lot of systems out there that use consistent hashing for improving their performance. For example, Cassandra, a distributed NoSQL Columnar DB that deals with huge traffic uses consistent hashing to distribute its data. Amazonâ€™s Dynamo DB is another such example. It is a managed distributed DB which uses consistent hashing for distributing its data. Similarly, Couchbase is another NoSQL DB (a document DB) which uses consistent hashing to distribute its data across various instances.&lt;/p&gt;
</description>
                <pubDate>Tue, 26 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series22</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series22</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part21]: ì‹œìŠ¤í…œ ë””ìì¸(2) Inter-Service Communication</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#inter-service-communication&quot; id=&quot;markdown-toc-inter-service-communication&quot;&gt;Inter-Service Communication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#modes-of-communication&quot; id=&quot;markdown-toc-modes-of-communication&quot;&gt;Modes of communication&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#synchronous-approach&quot; id=&quot;markdown-toc-synchronous-approach&quot;&gt;Synchronous Approach&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#asynchronous-approach&quot; id=&quot;markdown-toc-asynchronous-approach&quot;&gt;Asynchronous Approach&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#best-of-both-worlds&quot; id=&quot;markdown-toc-best-of-both-worlds&quot;&gt;Best of Both Worlds&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#message-queues&quot; id=&quot;markdown-toc-message-queues&quot;&gt;Message Queues&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#protocols-for-communication&quot; id=&quot;markdown-toc-protocols-for-communication&quot;&gt;Protocols for communication&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#how-clients-and-servers-interact&quot; id=&quot;markdown-toc-how-clients-and-servers-interact&quot;&gt;How clients and servers interact&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#http&quot; id=&quot;markdown-toc-http&quot;&gt;HTTP&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#websocket&quot; id=&quot;markdown-toc-websocket&quot;&gt;WebSocket&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;inter-service-communication&quot;&gt;Inter-Service Communication&lt;/h1&gt;

&lt;p&gt;In this article, we will be looking at how services interact with each other. Why is this important? Well, when you have a huge system with a lot of microservices interacting with each other, their communication needs to be efficient to provide the best user experience and also to avoid any cascading effects across the system.&lt;/p&gt;

&lt;h1 id=&quot;modes-of-communication&quot;&gt;Modes of communication&lt;/h1&gt;
&lt;p&gt;There are primarily two modes of communication between services:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Synchronous&lt;/strong&gt;: When a service &lt;strong&gt;waits&lt;/strong&gt; for a downstream system to respond before responding back to the client with a success or failure response.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Asynchronous&lt;/strong&gt;: This is a more of a fire and forget approach. A service will fire a call to the downstream system and wonâ€™t track it further.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;synchronous-approach&quot;&gt;Synchronous Approach&lt;/h1&gt;
&lt;p&gt;Letâ€™s say you are building Amazon. You have a user U1 trying to place an order. U1 will reach out to the Order Service. Order Service will now talk to the Inventory Service to find out if a sufficient quantity of the product is available. If that is the case, Inventory Service will send a success response. Otherwise, it will respond with an error, and Order Service will respond to the user saying the order could not be placed.&lt;/p&gt;

&lt;p&gt;Now if the inventory response was a success, the Order Service will talk to the Payment Service to process the payment. Once the payment is successful, the Order Service will now talk to the Warehouse Service asking it to start packing and prepare for shipping the product to the user. Once Warehouse Service responds with a success, the Order Service will talk to a Notification Service to send an email to the user saying their order has been placed, with so and so payment details and sharing an ETA for the delivery of the product.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now, this is a happy scenario. What happens when one of the calls fails? Well, it depends on which call fails. If the call to Notification Service fails, does it make sense to cancel the order? No. We shouldnâ€™t cancel an order just because the Notification Service failed. However, what if payment fails? Now we definitely need to cancel the order. But now we need to update the Inventory again to undo the change to the product quantity. What if the call to Inventory Service fails?&lt;/p&gt;

&lt;p&gt;So as you can see, there are some loopholes in a purely synchronous approach.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It has very high latency as the user does not get notified until all the calls have come back with a success or failure response.&lt;/li&gt;
  &lt;li&gt;The system is tightly coupled, and any failure will have cascading effects across the board.&lt;/li&gt;
  &lt;li&gt;The code becomes very complex since we need to handle all the cascading error scenarios.&lt;/li&gt;
  &lt;li&gt;Due to complexity, it requires extremely high maintenance.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;asynchronous-approach&quot;&gt;Asynchronous Approach&lt;/h1&gt;
&lt;p&gt;Let us see what happens in a purely asynchronous approach.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;U1 sends a call to the Order Service which makes asynchronous calls to all the downstream systems. In such a case, even if Inventory Service responds with an error code, or even if the payment fails, the order would get placed. Which is an even bigger mess! So how do we go about this?&lt;/p&gt;

&lt;p&gt;Well, as we can see, some parts of this process must be mandatory, and some can be done on a best-effort basis. If the Inventory Service or Payment Service responds with an error, we cannot place the order. But if the notification does not go through or the Warehouse Service is temporarily down, we donâ€™t need to cancel our order. So we can follow a hybrid approach here; &lt;strong&gt;use a synchronous approach for the mandatory steps and an asynchronous approach for the rest.&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;best-of-both-worlds&quot;&gt;Best of Both Worlds&lt;/h2&gt;
&lt;p&gt;The Hybrid approach suggests that the mandatory tasks need to be performed in a synchronous manner and everything else can be done asynchronously.&lt;/p&gt;

&lt;p&gt;So Order Service will send out a synchronous call to Inventory Service, and wait for a response. In case of success, it will call the Payment Service. If the Payment Service gives a successful response, Order Service will make parallel asynchronous calls to the Warehouse Service and Notification Service and, at the same time, respond to the user saying the order has been placed. If the Payment Service call had failed, Order Service would send an asynchronous call to the Inventory Service reverting the quantity change.&lt;/p&gt;

&lt;p&gt;So this looks like a much better solution. There are still some misses here though. What if the asynchronous call to Warehouse Service failed? It would lose the details for that order. This is where we would use &lt;strong&gt;Message Queues&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;message-queues&quot;&gt;Message Queues&lt;/h2&gt;
&lt;p&gt;Message Queues(Kafka, RabbitMQ, ActiveMQ ë“±) are highly fault-tolerant and persist messages for some time. How a message Queue works is, it has some Publishers adding messages to it, and some Subscribers listening to it and picking up the events meant for them at their own pace. Since these queues store messages for some time, if a subscriber is temporarily down, the messages will remain in the queue and will be picked up when the subscriber is running again.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;So now, when Order Service wants to make asynchronous calls to Warehouse and Notification services, it will instead put an event in the Message Queue. Warehouse Service and Notification Service, which will be listening to the queue, will pick up the events meant for them. If one of the systems is down, the messages will remain in the queue until the service is back up and ready to receive messages again. This way, none of the data gets lost.&lt;/p&gt;

&lt;h1 id=&quot;protocols-for-communication&quot;&gt;Protocols for communication&lt;/h1&gt;

&lt;p&gt;In this article, we will look at the protocols we can use to interact with clients.&lt;/p&gt;

&lt;h2 id=&quot;how-clients-and-servers-interact&quot;&gt;How clients and servers interact&lt;/h2&gt;

&lt;p&gt;In a real-world scenario, rather than talking to a specific server, the clientâ€™s request will instead be sent to a data center, where it could be picked up by any of the servers. However, irrespective of which server receives the request, the response will be the same. Based on this flow, we can draw the following conclusions about this architecture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is client-driven. Only on the userâ€™s button click will the client send the requests to the server, and the server will only respond to these requests.&lt;/li&gt;
  &lt;li&gt;It is a simple request-response model. For every request from the client, the server will respond with some information or a simple confirmation.&lt;/li&gt;
  &lt;li&gt;There are occasional requests from clients, only one request every few seconds based on the userâ€™s actions i.e. from the client-side it is a low throughput system.&lt;/li&gt;
  &lt;li&gt;It is a stateless system i.e. irrespective of which server is responding, the response remains the same.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;http&quot;&gt;HTTP&lt;/h2&gt;

&lt;p&gt;These requirements make this a perfect use case for HTTP(s) protocol. Although these days, most architectures on HTTP have moved to HTTPS, which is a more secure version of HTTP as it prevents man-in-the-middle attacks.&lt;/p&gt;

&lt;p&gt;Now, when we are using HTTP, REST is usually the best API standard to follow as it is very widely used and very user friendly.&lt;/p&gt;

&lt;p&gt;Let us look at an example for a REST request and response:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Request:
Method: GET
URL: https://www.twitter.com/user/{id}

Response:
Status: 200 OK
Headers: &amp;lt;...&amp;gt;
Body: {
    â€œuserIdâ€: 1,
    â€œEmailâ€: â€œsomeone@example.comâ€
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The client makes a request to twitter.com over HTTPS to get information about a user with an id. In response, the server sends a success status code along with the userâ€™s user id and email. As you can see, REST API standard is pretty much self-documenting, which adds to its user friendliness.&lt;/p&gt;

&lt;p&gt;Now let us look at an example of a chat application.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We know that HTTP is a client-driven protocol, so the server cannot initiate any contact with the client. It can only respond to the client upon receiving a request. So when U1 sends a message to U2 via chat server, U2 doesnâ€™t receive the message until it asks the server to share any pending messages. This leads to a delay when receiving messages.&lt;/p&gt;

&lt;p&gt;A solution to this would be that U2 sends frequent requests to the chat server in the hopes of receiving a message. But this puts a huge load on the chat server as it will receive a huge number of requests from all its clients.&lt;/p&gt;

&lt;p&gt;The best approach would be &lt;strong&gt;if the server could send a notification to the user every time there is a message&lt;/strong&gt;. For this, we use a protocol called &lt;strong&gt;WebSocket&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;(ì„œë²„ê°€ U2ì—ê²Œ ë³´ë‚¼ ë©”ì‹œì§€ë¥¼ ê°€ì§€ê³  ìˆìŒì—ë„ ë¶ˆêµ¬í•˜ê³ , ëŠ¥ë™ì ìœ¼ë¡œ U2ì—ê²Œ ë³´ë‚´ì§€ ì•ŠëŠ”ë‹¤. U2ë¡œë¶€í„° requestë¥¼ ë°›ì„ë•Œê¹Œì§€ ê¸°ë‹¤ë¦°ë‹¤.)&lt;br /&gt;
(U2ëŠ” ì–¸ì œ ìì‹ ì´ ë°›ì•„ì•¼í•  ë©”ì‹œì§€ê°€ ì„œë²„ì— ë„ì°©í–ˆëŠ”ì§€ ëª¨ë¥´ë¯€ë¡œ, ê³„ì† ì„œë²„ì— requestë¥¼ ë³´ë‚´ì•¼ í•œë‹¤.)&lt;br /&gt;
(WebSocketì„ ì‚¬ìš©í•˜ë©´ ì„œë²„ëŠ” U2ì™€ connectionë˜ì–´ ìˆìœ¼ë©´ requestë¥¼ ë°›ì§€ ì•Šì•„ë„ ì•Œì•„ì„œ U2ì— ë©”ì‹œì§€ë¥¼ ë³´ë‚¸ë‹¤)&lt;br /&gt;
(connectionë˜ì–´ ìˆì§€ ì•Šìœ¼ë©´, ê°€ë§Œíˆ ìˆë‹¤ê°€, connectionë˜ê³  U2ê°€ requestë³´ë‚´ë©´ ë©”ì‹œì§€ ë³´ë‚¸ë‹¤)&lt;br /&gt;
(WebSocketì—ë„ ë‹¨ì ì€ ìˆë‹¤. cost of maintaining a persistent connection with millions of users.)&lt;/p&gt;

&lt;h2 id=&quot;websocket&quot;&gt;WebSocket&lt;/h2&gt;
&lt;p&gt;A WebSocket connection is a persistent connection. It is also a bidirectional protocol, where communication can be initiated by the client or the server as long as there is an open connection. It is &lt;strong&gt;optimized for high-frequency communication&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Letâ€™s look at how our chat application would work in the case of WebSocket protocol.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/system_design_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;First, U1 and U2 will establish HTTP connections with the chat server, which are then upgraded to a WebSocket connection. When U1 sends a message for U2 via the chat server, it will store the message along with its status, RECEIVED, letâ€™s say.&lt;/p&gt;

&lt;p&gt;The chat server, if it has an open connection with U2, will then send the message to U2 and update the status to SENT. If U2 was not online and there was no open connection between U2 and the server, the messages will be saved until U2 comes online and requests the server to send all pending messages. The server will send all messages with the status RECEIVED and update the status to SENT.&lt;/p&gt;

&lt;p&gt;As you can see, with this approach we have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Reduced the latency, since the server can simply send the messages over an open connection&lt;/li&gt;
  &lt;li&gt;Saved on CPU and bandwidth, as the client doesnâ€™t need to unnecessarily send requests to the server and the server is not under unnecessary load&lt;/li&gt;
  &lt;li&gt;Provided better user experience&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Even with the benefits, there is a high cost to using WebSockets; that is the cost of maintaining a persistent connection with millions of users.&lt;/p&gt;

&lt;p&gt;So how do we decide whether to use HTTP or WebSocket? Do we always go for Websocket then? Well, not really, as WebSocket is much more expensive than HTTP. We can safely say, if the communication between client and server is at a &lt;strong&gt;lower throughput on the client-side&lt;/strong&gt;, HTTP is the way to go. &lt;strong&gt;If the communication is always client-driven&lt;/strong&gt;, WebSocket is not needed. Also, if you are on a &lt;strong&gt;tight budget&lt;/strong&gt;, HTTP may be the better choice.&lt;/p&gt;

&lt;p&gt;On the other hand, if the communication from the &lt;strong&gt;client is at a higher throughput&lt;/strong&gt;, WebSocket may be a better option. If the &lt;strong&gt;communication can be driven by both client and server&lt;/strong&gt;, WebSocket is the way to go. Although here comes the tradeoff between cost and performance. We must decide if the optimization is really worth the huge cost of maintaining persistent connections with so many users.&lt;/p&gt;
</description>
                <pubDate>Tue, 26 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series21</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series21</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
    </channel>
</rss>
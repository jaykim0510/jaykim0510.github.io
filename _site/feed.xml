<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Jay Tech</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Mon, 31 Jan 2022 02:38:01 +0900</pubDate>
        <lastBuildDate>Mon, 31 Jan 2022 02:38:01 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>Kubernetes Series [Part7]: StatefulSet</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes-basics&quot; id=&quot;markdown-toc-kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#localhost-ip-address-127001&quot; id=&quot;markdown-toc-localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pod-network&quot; id=&quot;markdown-toc-pod-network&quot;&gt;Pod network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#service-network&quot; id=&quot;markdown-toc-service-network&quot;&gt;Service network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.ibm.com/articles/kubernetes-networking-what-you-need-to-know/&quot; target=&quot;_blank&quot;&gt;원문: Kubernetes networking for developers - IBM developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.&lt;/p&gt;

&lt;p&gt;그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/h1&gt;

&lt;p&gt;컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.&lt;/p&gt;

&lt;p&gt;쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 &lt;strong&gt;파드(Pod)&lt;/strong&gt;로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/h1&gt;
&lt;p&gt;같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 &lt;strong&gt;공유되는 네트워크 네임스페이스&lt;/strong&gt;를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.&lt;/p&gt;

&lt;h1 id=&quot;pod-network&quot;&gt;Pod network&lt;/h1&gt;
&lt;p&gt;파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;사용되는 IP 주소는 &lt;strong&gt;파드 네트워크&lt;/strong&gt;라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;service-network&quot;&gt;Service network&lt;/h1&gt;
&lt;p&gt;쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 &lt;strong&gt;Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해&lt;/strong&gt; 트래픽을 전송하도록 해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-app&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web-server&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_33.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ClusterIP&lt;/li&gt;
  &lt;li&gt;NodePort&lt;/li&gt;
  &lt;li&gt;LoadBalancer&lt;/li&gt;
  &lt;li&gt;ExternalName&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Sat, 29 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series7</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series7</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Network Series [Part2]: IP주소와 DNS 서버</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#ipinternet-protocol-주소&quot; id=&quot;markdown-toc-ipinternet-protocol-주소&quot;&gt;IP(Internet Protocol) 주소&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#ip-주소&quot; id=&quot;markdown-toc-ip-주소&quot;&gt;IP 주소&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#서브넷-마스크subnet-mask&quot; id=&quot;markdown-toc-서브넷-마스크subnet-mask&quot;&gt;서브넷 마스크(Subnet Mask)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dnsdomain-name-system-서버&quot; id=&quot;markdown-toc-dnsdomain-name-system-서버&quot;&gt;DNS(Domain Name System) 서버&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;ipinternet-protocol-주소&quot;&gt;IP(Internet Protocol) 주소&lt;/h1&gt;

&lt;h2 id=&quot;ip-주소&quot;&gt;IP 주소&lt;/h2&gt;
&lt;p&gt;IP 주소는 인터넷에 연결하고자 하는 &lt;strong&gt;디바이스가 가지고 있는 NIC(Network Interface Controller)의 고유한 주소&lt;/strong&gt;를 뜻합니다. 편지를 주고 받기 위해서는 서로의 주소가 필요한 것처럼 디바이스간 통신을 위해서는 IP주소가 필요합니다. IP주소는 &lt;strong&gt;네트워크 번호와 호스트 번호로 이루어진 32비트 숫자&lt;/strong&gt;입니다.(IPv4 기준)&lt;/p&gt;
&lt;h2 id=&quot;서브넷-마스크subnet-mask&quot;&gt;서브넷 마스크(Subnet Mask)&lt;/h2&gt;

&lt;h1 id=&quot;dnsdomain-name-system-서버&quot;&gt;DNS(Domain Name System) 서버&lt;/h1&gt;
&lt;p&gt;DNS 서버는 도메인 네임을 IP주소로 매핑하여 보관하고 있는 서버입니다. 하지만 모든 도메인 정보를 저장할 수는 없고 저장한다고 해도 IP주소를 가지고 오는데 많은 시간이 소요됩니다. 이를 해결하기 위해 DNS 서버를 계층적으로 구성해 IP 주소를 가져오도록 했으며 한 번 가져온 정보는 캐시에 저장해둡니다. 하지만 캐시에 저장된 후 정보가 변경될 수 있기 때문에 캐시에 저장된 정보는 유효기간이 지나면 캐시에서 삭제됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/network_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Fri, 28 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/network-series2</link>
                <guid isPermaLink="true">http://localhost:4000/network-series2</guid>
                
                <category>Network</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part6]: ConfigMap과 Secret</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes-basics&quot; id=&quot;markdown-toc-kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#localhost-ip-address-127001&quot; id=&quot;markdown-toc-localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pod-network&quot; id=&quot;markdown-toc-pod-network&quot;&gt;Pod network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#service-network&quot; id=&quot;markdown-toc-service-network&quot;&gt;Service network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.ibm.com/articles/kubernetes-networking-what-you-need-to-know/&quot; target=&quot;_blank&quot;&gt;원문: Kubernetes networking for developers - IBM developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.&lt;/p&gt;

&lt;p&gt;그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/h1&gt;

&lt;p&gt;컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.&lt;/p&gt;

&lt;p&gt;쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 &lt;strong&gt;파드(Pod)&lt;/strong&gt;로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/h1&gt;
&lt;p&gt;같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 &lt;strong&gt;공유되는 네트워크 네임스페이스&lt;/strong&gt;를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.&lt;/p&gt;

&lt;h1 id=&quot;pod-network&quot;&gt;Pod network&lt;/h1&gt;
&lt;p&gt;파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;사용되는 IP 주소는 &lt;strong&gt;파드 네트워크&lt;/strong&gt;라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;service-network&quot;&gt;Service network&lt;/h1&gt;
&lt;p&gt;쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 &lt;strong&gt;Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해&lt;/strong&gt; 트래픽을 전송하도록 해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-app&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web-server&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_33.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ClusterIP&lt;/li&gt;
  &lt;li&gt;NodePort&lt;/li&gt;
  &lt;li&gt;LoadBalancer&lt;/li&gt;
  &lt;li&gt;ExternalName&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Fri, 28 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series6</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series6</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Network Series [Part1]: 웹브라우저의 동작(Application Layer)</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#어플리케이션-계층application-layer&quot; id=&quot;markdown-toc-어플리케이션-계층application-layer&quot;&gt;어플리케이션 계층(Application Layer)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#웹-브라우저의-동작원리&quot; id=&quot;markdown-toc-웹-브라우저의-동작원리&quot;&gt;웹 브라우저의 동작원리&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#http-리퀘스트-작성&quot; id=&quot;markdown-toc-http-리퀘스트-작성&quot;&gt;HTTP 리퀘스트 작성&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#url-입력&quot; id=&quot;markdown-toc-url-입력&quot;&gt;URL 입력&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#url에-관한-문법&quot; id=&quot;markdown-toc-url에-관한-문법&quot;&gt;URL에 관한 문법&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#http-리퀘스트-작성-1&quot; id=&quot;markdown-toc-http-리퀘스트-작성-1&quot;&gt;HTTP 리퀘스트 작성&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dns-서버에-웹-서버의-ip주소-조회&quot; id=&quot;markdown-toc-dns-서버에-웹-서버의-ip주소-조회&quot;&gt;DNS 서버에 웹 서버의 IP주소 조회&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#dns-resolver를-이용해-dns-서버-조회&quot; id=&quot;markdown-toc-dns-resolver를-이용해-dns-서버-조회&quot;&gt;DNS Resolver를 이용해 DNS 서버 조회&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#프로토콜-스택에-메시지-송신-요청&quot; id=&quot;markdown-toc-프로토콜-스택에-메시지-송신-요청&quot;&gt;프로토콜 스택에 메시지 송신 요청&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고&quot; id=&quot;markdown-toc-참고&quot;&gt;참고&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr /&gt;
&lt;h1 id=&quot;어플리케이션-계층application-layer&quot;&gt;어플리케이션 계층(Application Layer)&lt;/h1&gt;

&lt;p&gt;어플리케이션 계층은 인터넷으로 연결 가능한 두 디바이스의 OSI layer 가장 끝단에 있는 계층으로, &lt;strong&gt;웹 브라우저, 게임, 메일&lt;/strong&gt;과 같은 것들이 있습니다. 그 중에서도 웹 브라우저(사파리, 크롬 등)는 웹 서버로의 접근, 파일 업로드/다운로드, 메일 전송과 같은 다양한 클라이언트 기능을 겸비한 복합적인 클라이언트 소프트웨어입니다. 그래서 저는 이번 포스트에서 웹 브라우저의 동작원리에 대해 집중적으로 알아보도록 하겠습니다.&lt;/p&gt;

&lt;p&gt;디바이스간 연결을 위해서는 계층별로 지켜야 규약들이 있습니다. 만약 이러한 규약들이 없다면 세상에 존재하는 다양한 디바이스들을 연결시키기 어렵습니다. 어플리케이션에서도 이러한 규약들이 있는데 대표적으로 다음과 같은 것들이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Application Layer Protocols&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;HTTP&lt;/strong&gt;: HyperText Transfer Protocol의 약자. 하이퍼링크로 연결된 html 문서(사실상 거의 모든 데이터)를 전송할 때의 규약&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;FTP&lt;/strong&gt;: File Transfer Protocol의 약자. 파일을 업로드/다운로드 할 때 사용되는 규약&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;SMTP&lt;/strong&gt;: Simple Mail Transfer Protocol의 약자. 메일을 전송할 때 사용되는 규약&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;웹-브라우저의-동작원리&quot;&gt;웹 브라우저의 동작원리&lt;/h1&gt;
&lt;p&gt;우리가 웹 브라우저(크롬, 사파리 등)에서 뉴스 보기를 클릭하거나 유튜브 비디오를 시청할 때 내부적으로 어떤 일들이 일어나는지 한 번 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;http-리퀘스트-작성&quot;&gt;HTTP 리퀘스트 작성&lt;/h2&gt;
&lt;p&gt;우리는 보통 웹 브라우저에서 URL을 입력하거나 어떤 버튼을 클릭하는 식으로 웹 서버와 상호작용 하게 되는데 이 때 &lt;strong&gt;웹 브라우저는 내부에서 HTTP 리퀘스트라는 것을 웹 서버에 전송&lt;/strong&gt;합니다.&lt;/p&gt;

&lt;h3 id=&quot;url-입력&quot;&gt;URL 입력&lt;/h3&gt;

&lt;h3 id=&quot;url에-관한-문법&quot;&gt;URL에 관한 문법&lt;/h3&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# URL 문법&lt;/span&gt;
scheme://[userinfo@]host[:port][/path][?query][#fragment]

예: https://www.google.com/search?q&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hello&amp;amp;hl&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ko


&lt;span class=&quot;c&quot;&gt;# scheme&lt;/span&gt;
예: https
- 주로 프로토콜이 사용됩니다.
- 프로토콜: 어떤 방식으로 자원에 접근할 것인가 하는 약속 규칙 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https, http, ftp&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
- 포트가 생략되어 있을 때 https가 사용되면 443포트, http가 사용되면 80포트가 디폴트
- https는 http에 보안 추가 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;HTTP Secure&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# host&lt;/span&gt;
예: www.google.com
- 도메인명 또는 IP주소

&lt;span class=&quot;c&quot;&gt;# port&lt;/span&gt;
예: 8888
- 접속 포트

&lt;span class=&quot;c&quot;&gt;# path&lt;/span&gt;
예: /search
- 리소스 경로 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;계층적 구조&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
- 디렉토리명/파일명

&lt;span class=&quot;c&quot;&gt;# query&lt;/span&gt;
예: ?q&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hello&amp;amp;hl&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ko
- &lt;span class=&quot;nv&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;value 형태
- ?로 시작, &amp;amp;로 추가 가능
- query parameter 또는 query string으로 보통 불림

&lt;span class=&quot;c&quot;&gt;# fragment&lt;/span&gt;
예: &lt;span class=&quot;c&quot;&gt;#getting-started-introducing-spring-boot&lt;/span&gt;
- html 내부 북마크 등에 사용
- 서버에 전송하는 정보는 아님
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;http-리퀘스트-작성-1&quot;&gt;HTTP 리퀘스트 작성&lt;/h3&gt;
&lt;p&gt;URL을 입력하고 나면 웹 브라우저는 URL을 바탕으로 HTTP 리퀘스트 메시지를 만듭니다.&lt;br /&gt;
HTTP 리퀘스트 메시지의 형태는 다음과 같습니다.&lt;br /&gt;
&lt;img src=&quot;../../images/network_1.jpeg&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://joie-kim.github.io/HTTP/&quot; target=&quot;_blank&quot;&gt;(joie-kim님 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;dns-서버에-웹-서버의-ip주소-조회&quot;&gt;DNS 서버에 웹 서버의 IP주소 조회&lt;/h2&gt;
&lt;p&gt;HTTP 리퀘스트를 작성하고 나면 이제 OS에게 이것을 웹 서버로 전송해달라고 요청합니다. (웹 브라우저가 직접 전송하지 않는 이유는 메시지를 송신하는 기능은 하나의 애플리케이션에만 종속되는 기능이 아니므로 OS에서 전송 기능을 담당하는 것이 더 좋다고 합니다.)&lt;/p&gt;

&lt;p&gt;OS에서는 리퀘스트 메시지를 전송하기 전에 먼저 &lt;strong&gt;도메인 네임을 IP 주소로 변환&lt;/strong&gt;하는 과정을 거칩니다. 이를 &lt;strong&gt;네임 레졸루션(name resolution)&lt;/strong&gt;이라고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;dns-resolver를-이용해-dns-서버-조회&quot;&gt;DNS Resolver를 이용해 DNS 서버 조회&lt;/h3&gt;
&lt;p&gt;네임 레졸루션을 시행하는 것이 &lt;strong&gt;DNS 리졸버(DNS Resolver)&lt;/strong&gt;입니다. 리졸버는 Socket 라이브러리에 들어있는 부품화된 프로그램입니다. Socket 라이브러리는 네트워크 관련 기능을 하는 프로그램을 모아놓은 라이브러리입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/network_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;프로토콜-스택에-메시지-송신-요청&quot;&gt;프로토콜 스택에 메시지 송신 요청&lt;/h2&gt;
&lt;p&gt;DNS Resolver가 IP주소를 찾아오면 이제 진짜 웹 서버로 보낼 준비가 완료되었습니다. 이렇게 준비된 HTTP Request 메시지는 OS의 내부에 포함된 프로토콜 스택을 호출하여 실행을 요청합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/network_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고&quot;&gt;참고&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;amp;ejkGb=KOR&amp;amp;barcode=9788931556742&quot; target=&quot;_blank&quot;&gt;성공과 실패를 결정하는 1%의 네트워크 원리 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.imperva.com/learn/application-security/osi-model/&quot; target=&quot;_blank&quot;&gt;imperva 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 27 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/network-series1</link>
                <guid isPermaLink="true">http://localhost:4000/network-series1</guid>
                
                <category>Network</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part4]: Kafka on Kubernetes</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kafka-on-kubernetes&quot; id=&quot;markdown-toc-kafka-on-kubernetes&quot;&gt;Kafka on Kubernetes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#쿠버네티스-클러스터-구축&quot; id=&quot;markdown-toc-쿠버네티스-클러스터-구축&quot;&gt;쿠버네티스 클러스터 구축&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#단일-노드-클러스터&quot; id=&quot;markdown-toc-단일-노드-클러스터&quot;&gt;단일 노드 클러스터&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#minikube&quot; id=&quot;markdown-toc-minikube&quot;&gt;minikube&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#docker-desktop&quot; id=&quot;markdown-toc-docker-desktop&quot;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#멀티-노드-클러스터&quot; id=&quot;markdown-toc-멀티-노드-클러스터&quot;&gt;멀티 노드 클러스터&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#kind&quot; id=&quot;markdown-toc-kind&quot;&gt;kind&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#클라우드-환경gke-eks&quot; id=&quot;markdown-toc-클라우드-환경gke-eks&quot;&gt;클라우드 환경(GKE, EKS)&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#쿠버네티스-gui-도구-lens&quot; id=&quot;markdown-toc-쿠버네티스-gui-도구-lens&quot;&gt;쿠버네티스 GUI 도구: Lens&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#카프카-메니페스트-작성&quot; id=&quot;markdown-toc-카프카-메니페스트-작성&quot;&gt;카프카 메니페스트 작성&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#loadbalancer-생성&quot; id=&quot;markdown-toc-loadbalancer-생성&quot;&gt;LoadBalancer 생성&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#zookeeper-설치&quot; id=&quot;markdown-toc-zookeeper-설치&quot;&gt;Zookeeper 설치&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kafka-broker-설치&quot; id=&quot;markdown-toc-kafka-broker-설치&quot;&gt;Kafka Broker 설치&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#카프카-클라이언트&quot; id=&quot;markdown-toc-카프카-클라이언트&quot;&gt;카프카 클라이언트&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#카프카-모니터링&quot; id=&quot;markdown-toc-카프카-모니터링&quot;&gt;카프카 모니터링&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kafka-on-kubernetes&quot;&gt;Kafka on Kubernetes&lt;/h1&gt;

&lt;h1 id=&quot;쿠버네티스-클러스터-구축&quot;&gt;쿠버네티스 클러스터 구축&lt;/h1&gt;
&lt;p&gt;쿠버네티스 클러스터를 구축하는 방법에 대해서는 &lt;a href=&quot;http://jaykim0510.github.io/kubernetes-series4&quot;&gt;Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기&lt;/a&gt;를 참고하시면 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;단일-노드-클러스터&quot;&gt;단일 노드 클러스터&lt;/h2&gt;
&lt;h3 id=&quot;minikube&quot;&gt;minikube&lt;/h3&gt;
&lt;p&gt;미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 &lt;strong&gt;단일 노드 구성&lt;/strong&gt;이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 &lt;strong&gt;하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요&lt;/strong&gt;합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;minikube

minikube version
&lt;span class=&quot;c&quot;&gt;# minikube version: v1.25.1&lt;/span&gt;

minikube start &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker &lt;span class=&quot;c&quot;&gt;# --kubernetes-version 옵션으로 버전 선택 가능&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------&lt;/span&gt;
😄  Darwin 12.1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;arm64&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 의 minikube v1.25.1
✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중
👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중
🚜  베이스 이미지를 다운받는 중 ...
💾  쿠버네티스 v1.23.1 을 다운로드 중 ...
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/
🔥  Creating docker container &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CPUs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2, &lt;span class=&quot;nv&quot;&gt;Memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;7903MB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중
    ▪ kubelet.housekeeping-interval&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5m
    ▪ 인증서 및 키를 생성하는 중 ...
    ▪ 컨트롤 플레인이 부팅...
    ▪ RBAC 규칙을 구성하는 중 ...
🔎  Kubernetes 구성 요소를 확인...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  애드온 활성화 : storage-provisioner, default-storageclass
🏄  끝났습니다! kubectl이 &lt;span class=&quot;s2&quot;&gt;&quot;minikube&quot;&lt;/span&gt; 클러스터와 &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube status
&lt;span class=&quot;nt&quot;&gt;--------------------&lt;/span&gt;
minikube
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

minikube ip
&lt;span class=&quot;c&quot;&gt;# 192.168.49.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;정지하고 삭제하는 명령어도 간단합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube stop

minikube delete
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;docker-desktop&quot;&gt;Docker Desktop&lt;/h3&gt;
&lt;p&gt;Docker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/kube_24.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)&lt;/p&gt;

&lt;h2 id=&quot;멀티-노드-클러스터&quot;&gt;멀티 노드 클러스터&lt;/h2&gt;

&lt;h3 id=&quot;kind&quot;&gt;kind&lt;/h3&gt;
&lt;p&gt;kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 &lt;strong&gt;멀티 노드 클러스터&lt;/strong&gt;를 구축할 수 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kind.sigs.k8s.io&quot; target=&quot;_blank&quot;&gt;(kind 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
- role: control-plane
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Creating cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
 ✓ Ensuring node image &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;kindest/node:v1.23.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 🖼
 ✓ Preparing nodes 📦 📦 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
 ✓ Joining worker nodes 🚜
Set kubectl context to &lt;span class=&quot;s2&quot;&gt;&quot;kind-kindcluster&quot;&lt;/span&gt;
You can now use your cluster with:

kubectl cluster-info &lt;span class=&quot;nt&quot;&gt;--context&lt;/span&gt; kind-kindcluster

Have a &lt;span class=&quot;nb&quot;&gt;nice &lt;/span&gt;day! 👋
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;클러스터가 성공적으로 구축되었습니다.&lt;br /&gt;
쿠버네티스에서 실행중인 노드를 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
----------------------------------------------------------------------------
NAME                        STATUS   ROLES                  AGE   VERSION
kindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1
kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;클러스터는 다음 명령어로 삭제하시면 됩니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind delete cluster &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;------------------------------------------&lt;/span&gt;
Deleting cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;클라우드-환경gke-eks&quot;&gt;클라우드 환경(GKE, EKS)&lt;/h3&gt;

&lt;h2 id=&quot;쿠버네티스-gui-도구-lens&quot;&gt;쿠버네티스 GUI 도구: Lens&lt;/h2&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;lens
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;카프카-메니페스트-작성&quot;&gt;카프카 메니페스트 작성&lt;/h1&gt;

&lt;h2 id=&quot;loadbalancer-생성&quot;&gt;LoadBalancer 생성&lt;/h2&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml

kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml

kubectl create -f ./metallb/configmap.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# configmap.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ConfigMap&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;metallb-system&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;config&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;config&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;|&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;address-pools:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;- name: default&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;protocol: layer2&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;addresses:&lt;/span&gt;
      &lt;span class=&quot;s&quot;&gt;- 192.168.72.102&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;zookeeper-설치&quot;&gt;Zookeeper 설치&lt;/h2&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# deployment.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-deploy&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-1&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zoo1&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper:latest&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2181&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ZOOKEEPER_ID&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;1&quot;&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# zooservice.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-service&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;client&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2181&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;follower&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;2888&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;leader&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3888&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;kafka-broker-설치&quot;&gt;Kafka Broker 설치&lt;/h2&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# kafkaservice.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-service&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;annotations&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metallb.universe.tf/address-pool&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;default&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metallb.universe.tf/allow-shared-ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;shared&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;LoadBalancer&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-port&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9092&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9092&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&quot;&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# deployment.yaml&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps/v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Deployment&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-broker0&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;replicas&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;matchLabels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&quot;&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&quot;&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;hostname&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka-host0&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;kafka&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;wurstmeister/kafka&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;9092&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_LISTENERS&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_ADVERTISED_LISTENERS&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INTERNAL_LISTENER://kafka-host0:19092, EXTERNAL_LISTENER://localhost:9092&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_INTER_BROKER_LISTENER_NAME&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INTERNAL_LISTENER&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_LISTENER_SECURITY_PROTOCOL_MAP&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;INTERNAL_LISTENER:PLAINTEXT, EXTERNAL_LISTENER:PLAINTEXT&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_ZOOKEEPER_CONNECT&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;zookeeper-service:2181&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_BROKER_ID&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;0&quot;&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;KAFKA_CREATE_TOPICS&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;admintome-test:1:1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 위치: opt/kafka_버전&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 주키퍼 실행&lt;/span&gt;
bin/zookeeper-server-start.sh ./config/zookeeper.properties

&lt;span class=&quot;c&quot;&gt;# 카프카 실행&lt;/span&gt;
bin/kafka-server-start.sh ./config/server.properties

&lt;span class=&quot;c&quot;&gt;# 토픽 생성&lt;/span&gt;
bin/kafka-topics.sh &lt;span class=&quot;nt&quot;&gt;--create&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--zookeeper&lt;/span&gt; zookeeper-service:2181 &lt;span class=&quot;nt&quot;&gt;--replication-factors&lt;/span&gt; 1 &lt;span class=&quot;nt&quot;&gt;--partitions&lt;/span&gt; 1 &lt;span class=&quot;nt&quot;&gt;--topic&lt;/span&gt; test-topic
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;카프카-클라이언트&quot;&gt;카프카 클라이언트&lt;/h1&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 카프카 클라이언트 파이썬 버전 설치&lt;/span&gt;
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kafka-python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# producer.py
&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;kafka&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KafkaProducer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KafkaProducer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;security_protocol&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;PLAINTEXT&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bootstrap_servers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'192.168.111.2:9092'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;api_version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;send&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'test'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;sa&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'finally working kafka'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 현재 이부분에서 안넘어감 (bootstrap_servers의 host에 어떤거 넣어야 할지 모르겠음)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;producer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flush&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h1 id=&quot;카프카-모니터링&quot;&gt;카프카 모니터링&lt;/h1&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://js94.tistory.com/entry/kafka-에러-해결&quot; target=&quot;_blank&quot;&gt;옥탑방의 일상로그 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/how-to-install-apache-kafka-using-docker-the-easy-way-4ceb00817d8b&quot; target=&quot;_blank&quot;&gt;Towards Data Science 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gist.github.com/pcjayasinghe/d1319f0135d197a42d770480e0a5701b#file-zookeeper-kafka-cluster-yml&quot; target=&quot;_blank&quot;&gt;pcjayasinghe 깃허브&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/PharosProduction/tutorial-apache-kafka-cluster&quot; target=&quot;_blank&quot;&gt;PharosProduction 깃허브&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@pcjayasinghe/deploy-apache-kafka-and-zookeeper-cluster-on-kubernetes-df9f0757b608&quot; target=&quot;_blank&quot;&gt;Deploy Apache Kafka and Zookeeper Cluster on Kubernetes 블로그 글&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 27 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series4</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series4</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part3]: Fault tolerance in Kafka</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#fault-tolerance-in-kafka&quot; id=&quot;markdown-toc-fault-tolerance-in-kafka&quot;&gt;Fault tolerance in Kafka&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#카프카-리플리케이션replication&quot; id=&quot;markdown-toc-카프카-리플리케이션replication&quot;&gt;카프카 리플리케이션(Replication)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#리더leader와-팔로워follower&quot; id=&quot;markdown-toc-리더leader와-팔로워follower&quot;&gt;리더(Leader)와 팔로워(Follower)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#컨트롤러controller&quot; id=&quot;markdown-toc-컨트롤러controller&quot;&gt;컨트롤러(Controller)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#리플리케이션-과정&quot; id=&quot;markdown-toc-리플리케이션-과정&quot;&gt;리플리케이션 과정&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;fault-tolerance-in-kafka&quot;&gt;Fault tolerance in Kafka&lt;/h1&gt;
&lt;p&gt;카프카는 데이터 파이프라인의 중앙에 위치하는 메인 허브 역할을 합니다. 그래서 만약 하드웨어의 문제나 네트워크의 장애로 인해 정상적으로 동작하지 못한다면, 카프카에 연결된 모든 파이프라인에 심각한 영향을 미치게 됩니다. 이러한 이유로 카프카는 초기 설계 단계에서부터 장애가 발생하더라도 안정적인 서비스를 제공할 수 있도록 구상됐습니다.&lt;/p&gt;

&lt;h2 id=&quot;카프카-리플리케이션replication&quot;&gt;카프카 리플리케이션(Replication)&lt;/h2&gt;
&lt;p&gt;카프카는 데이터를 저장할 때 하나의 브로커에만 저장하지 않고, &lt;strong&gt;다른 브로커에 파티션을 복제&lt;/strong&gt;해놓음으로써 임의의 브로커 장애에 대비할 수 있습니다. 만약 N개의 리플리케이션이 있을 경우, N-1개의 브로커에 장애가 발생하더라도 손실되지 않고 데이터를 주고 받을 수 있습니다.&lt;/p&gt;

&lt;p&gt;그런데 만약 같은 데이터를 여러 브로커에서 읽게되면 어떻게 될까요? 아마 불필요한 데이터 전송으로 처리량이 낮아지고, 중복 처리를 해야하는 불필요한 오버헤드가 생길 것입니다. 이런 문제를 해결하고자 카프카에는 &lt;strong&gt;리더와 팔로워&lt;/strong&gt;가 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_14.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;(shwitha B G 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;리더leader와-팔로워follower&quot;&gt;리더(Leader)와 팔로워(Follower)&lt;/h2&gt;
&lt;p&gt;카프카는 내부적으로 리플리케이션들을 리더와 팔로워로 구분하고, 파티션에 대한 쓰기와 읽기는 모두 리더 파티션을 통해서만 가능합니다. 다시 말해, 프로듀서는 리더 파티션에만 메시지를 전송하고, 컨슈머도 리더를 통해서만 메시지를 가져옵니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_15.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 팔로워는 어떤 역할을 할까요? 팔로워는 리더에 문제가 발생할 경우를 대비해 언제든지 새로운 리더가 될 수 있도록 준비를 하고 있어야합니다. 그러기 위해 팔로워들은 리더에게 새로운 메시지가 있는지 요청하고 있다면 메시지를 리더로부터 복제합니다.&lt;/p&gt;

&lt;h2 id=&quot;컨트롤러controller&quot;&gt;컨트롤러(Controller)&lt;/h2&gt;
&lt;p&gt;리더를 뽑기 위해서는 리더 선정을 담당하는 무엇인가가 카프카 클러스터에 있어야 합니다. 여기서 &lt;strong&gt;컨트롤러&lt;/strong&gt;라는 개념이 등장합니다. 컨트롤러는 카프카 클러스터 중 하나의 브로커가 컨트롤러 역할을 하게됩니다. 그래서 이러한 역할을 하는 브로커를 컨트롤러 브로커라고도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_16.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;(shwitha B G 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;컨트롤러가 새로운 리더를 임명하는 과정을 살펴보겠습니다. &lt;strong&gt;주키퍼(Zookeeper)&lt;/strong&gt; 개념이 잠깐 등장합니다.&lt;br /&gt;
(&lt;strong&gt;Zookeeper&lt;/strong&gt; is the centralized service for storing metadata of topic, partition, and broker)&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;주키퍼는 카프카의 모든 브로커들과 하트비트(Heartbeat)를 주고 받으며 브로커가 살아있는지 체크합니다.&lt;/li&gt;
  &lt;li&gt;브로커와 관련하여 어떤 이벤트가 발생하면 주키퍼는 이를 감지하고 자신을 subscribe하고 있는 브로커들에게 알립니다&lt;/li&gt;
  &lt;li&gt;컨트롤러는 알림을 받고 어떤 파티션을 새로운 리더로 임명할지 결정합니다.&lt;/li&gt;
  &lt;li&gt;컨트롤러는 어떤 브로커가 새로운 리더를 할당받을지 결정하고, 파티션을 리밸런싱합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;리플리케이션-과정&quot;&gt;리플리케이션 과정&lt;/h2&gt;
&lt;p&gt;마지막으로 리더와 팔로워간의 리플리케이션 과정을 살펴보고 포스트를 마치도록 하겠습니다.&lt;br /&gt;
먼저 리더와 팔로워에 대해 조금 더 알아보겠습니다. 리더와 몇몇의 팔로워는 &lt;strong&gt;ISR(InSyncReplica)&lt;/strong&gt;이라는 논리적 그룹으로 묶여 있습니다. 이렇게 ISR 그룹안에 속하는 팔로워만이 리더가 될 수 있는 후보입니다.&lt;br /&gt;
ISR 내의 팔로워들은 리더와의 데이터를 일치시키기 위해 지속적으로 리더의 데이터를 따라가게 되고, 리더는 ISR내의 팔로워가 모두 메세지를 받을 때까지 기다립니다.&lt;/p&gt;

&lt;p&gt;그러나 만약 팔로워를 가지는 브로커가 장애로 데이터를 리플리케이션하지 못하게 되면 더이상 리더와의 데이터가 일치하지 않게되므로 해당 파티션은 ISR 그룹에서 제외되게 됩니다. (리더 파티션을 가지는 브로커에 장애가 발생하면 리더 재선출 및 파티션 재할당, 팔로워의 경우 ISR그룹에서 제외)&lt;/p&gt;

&lt;p&gt;ISR 내에서 모든 팔로워의 복제가 완료되면, 리더는 내부적으로 커밋되었다는 표시를 하게됩니다. 이 때 마지막 커밋의 오프셋 위치를 &lt;strong&gt;하이워터마크(high water mark)&lt;/strong&gt;라고 부릅니다. 즉 커밋되었다는 것은 모든 팔로워가 리더의 데이터를 저장했음을 의미합니다. 그리고 이렇게 커밋된 메시지만 컨슈머가 읽어갈 수 있도록 함으로써 메시지의 일관성을 유지하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_17.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 커밋되지 않은 메시지를 컨슈머가 읽어갈 수 있게 되면 어떻게 될까요? 위의 그림으로 설명을 해보겠습니다. 어떤 컨슈머가 Leader가 가지고 있던 아직 커밋되지 않은 Message 3을 읽어갔습니다. 그런데 갑자기 Leader 파티션을 가지고 있던 브로커에 장애가 발생해 Follower가 새로운 Leader가 되었습니다. 이렇게 되면 아까 컨슈머는 message 3을 읽어갔지만, 이제는 더이상 message 3을 읽어갈 수 없게 됩니다. 이러한 메세지 불일치 현상을 막고자 카프카는 커밋된 메세지만 읽어갈 수 있도록 한 것입니다.&lt;/p&gt;
&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://hackernoon.com/apache-kafkas-distributed-system-firefighter-the-controller-broker-1afca1eae302&quot; target=&quot;_blank&quot;&gt;Hackernoon 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@anchan.ashwithabg95/fault-tolerance-in-apache-kafka-d1f0444260cf&quot; target=&quot;_blank&quot;&gt;Ashwitha B G 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 24 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series3</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series3</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part5]: Kubernetes networking for developers [번역]</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes-basics&quot; id=&quot;markdown-toc-kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#localhost-ip-address-127001&quot; id=&quot;markdown-toc-localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pod-network&quot; id=&quot;markdown-toc-pod-network&quot;&gt;Pod network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#service-network&quot; id=&quot;markdown-toc-service-network&quot;&gt;Service network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;a href=&quot;https://developer.ibm.com/articles/kubernetes-networking-what-you-need-to-know/&quot; target=&quot;_blank&quot;&gt;원문: Kubernetes networking for developers - IBM developers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;불과 몇 년 전까지만 하더라도 개발자들이 네트워크에 대해 알아야 할 지식은 그렇게 많지 않았습니다. 그들은 그들의 코드를 작성하고 로컬에서 동작하는지 확인한 후 테스트나 서비스를 위한 서버에 배포할 수 있었으면 됐었습니다.&lt;/p&gt;

&lt;p&gt;그러나 시간이 지나면서 컨테이너 기술이 점점 발전되어가며 이런 추세가 약간 변하기 시작했습니다. 이번 글을 통해 컨테이너 기반의 배포환경에서 개발자들이 알아두면 좋은 네트워크 지식에 대해 알아보도록 하겠습니다.&lt;/p&gt;

&lt;h1 id=&quot;kubernetes-basics&quot;&gt;Kubernetes Basics&lt;/h1&gt;

&lt;p&gt;컨테이너는 한 개의 어플리케이션 또는 프로세스로 이들을 실행시키기 위해 필요한 모든 것들이 포함되어 있습니다. 어플리케이션 관점에서 컨테이너는 마치 서버위에 동작중인 하나의 운영체제처럼 보입니다. 컨테이너는 컨테이너만의 네트워크 인터페이스, 파일 시스템 등 필요한 모든 것들을 갖추고 있습니다.&lt;/p&gt;

&lt;p&gt;쿠버네티스는 이러한 컨테이너의 집합으로 컨테이너는 쿠버네티스의 워커 노드 안에서 돌아갑니다. 만약 항상 함께 설치되어야 하는 두 개의 어플리케이션이 있다면 이 두 개의 어플리케이션을 가진 컨테이너 두 개를 하나의 &lt;strong&gt;파드(Pod)&lt;/strong&gt;로 정의할 수 있습니다. (파드는 확장가능한 단위로 보통 특정 노드에 장애가 발생할 경우 파드에 문제가 생기는 경우를 대비해, 여러 워커 노드에 파드를 분산 배포합니다.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_31.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;localhost-ip-address-127001&quot;&gt;Localhost (IP address 127.0.0.1)&lt;/h1&gt;
&lt;p&gt;같은 파드내에 있는 컨테이너들은 같은 노드 안에서 실행됩니다. 쿠버네티스는 이러한 구조의 장점을 이용해 파드에 &lt;strong&gt;공유되는 네트워크 네임스페이스&lt;/strong&gt;를 각각 제공합니다. 네트워크 네임스페이스는 리눅스 커널의 특징 중 하나로 네트워크 자원들을 그룹으로 만들 수 있습니다.&lt;/p&gt;

&lt;p&gt;이러한 그룹내에서 실행되는 프로세스들은 그룹 안에 정의된 네트워크 리소스만 볼 수 있습니다. 예를 들어, 파드 A는 네트워크 네임스페이스가 A인 네트워크 리소스만 볼 수 있습니다. 이것이 바로 쿠버네티스가 파드들을 각각 독립시킬 수 있는 방법입니다. 예를 들어, 파드 A가 포트 80번으로 리스닝하는 것과 파드 B가 포트 80번으로 리스닝하는 것은 서로 독립되어 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드 안의 컨테이너들은 같은 네트워크 네임스페이스를 공유하기 때문에 localhost를 통해 서로 통신할 수 있습니다. 그러므로 컨테이너들이 같은 포트 번호로 리스닝하면 트래픽이 컨테이너들에 모두 전송됩니다. 따라서 같은 트래픽을 받기를 원치 않는 다른 역할의 컨테이너들이라면 포트 번호를 서로 다르게 지정해야 합니다.&lt;/p&gt;

&lt;h1 id=&quot;pod-network&quot;&gt;Pod network&lt;/h1&gt;
&lt;p&gt;파드안의 컨테이너들은 파드 IP를 통해 묶여지고 포트를 통해 구분됩니다. 이러한 점에서 파드는 마치 하나의 가상 서버와 같다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;사용되는 IP 주소는 &lt;strong&gt;파드 네트워크&lt;/strong&gt;라고 알려진 주소 블록에서 가져온 것입니다. 파드가 새로 기동될 때 마다 쿠버네티스는 파드 네트워크에서 사용 가능한 IP 주소를 가져와 파드에 할당한 후 실행합니다. 파드는 모두 동일한 네트워크에 있으며 이 네트워크를 통해 서로 통신할 수 있습니다. 파드가 통신할 수 있는 대상을 제한할 수 있는 네트워크 정책이 있지만 기본적으로 서로 자유롭게 통신할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;파드는 새로 기동될 때마다 파드 네트워크에서 IP주소를 새로 할당받습니다. 반면에 파드안의 컨테이너는 컨테이너가 파드 안에서 다시 기동되더라도 같은 IP주소를 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;만약 특정 노드에 장애가 발생해 노드 안에 있던 파드들이 다른 노드 안에서 다시 기동되었다면 파드는 모두 새로운 IP주소를 받게 될겁니다. 만약 우리가 개발하는 상황이 IP 주소에 의존적이라면 이러한 쿠버네티스의 특징은 큰 단점이 될 수 있습니다.&lt;/p&gt;

&lt;h1 id=&quot;service-network&quot;&gt;Service network&lt;/h1&gt;
&lt;p&gt;쿠버네티스에서는 이러한 단점을 해결해주는 리소스를 제공합니다. 쿠버네티스의 &lt;strong&gt;Service 리소스는 파드의 IP주소에 상관 없이 도메인 네임을 통해&lt;/strong&gt; 트래픽을 전송하도록 해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;namespace&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;my-app&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web-server&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;위의 서비스 리소스는 서비스 네트워크의 IP 주소로 확인되는 web.my-app.svc.cluster.local DNS 항목을 생성합니다. 할당된 서비스 IP는 서비스 spec.selector와 일치하는 모든 파드에 대한 트래픽 로드 밸런싱을 수행합니다. 위의 리소스 서비스는 metadata.labels.app가 web-server인 모든 파드가 포트 80으로 트래픽을 전송받도록 해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_33.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Service 리소스의 기본 타입은 ClusterIP이며 다음과 같은 종류의 타입들을 제공합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ClusterIP&lt;/li&gt;
  &lt;li&gt;NodePort&lt;/li&gt;
  &lt;li&gt;LoadBalancer&lt;/li&gt;
  &lt;li&gt;ExternalName&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series5</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series5</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part4]: Kubernetes 실습환경 구축하기</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#로컬환경&quot; id=&quot;markdown-toc-로컬환경&quot;&gt;로컬환경&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#미니큐브minikube&quot; id=&quot;markdown-toc-미니큐브minikube&quot;&gt;미니큐브(minikube)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#docker-desktop&quot; id=&quot;markdown-toc-docker-desktop&quot;&gt;Docker Desktop&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#kindkubernetes-in-docker&quot; id=&quot;markdown-toc-kindkubernetes-in-docker&quot;&gt;kind(Kubernetes in Docker)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#클라우드환경&quot; id=&quot;markdown-toc-클라우드환경&quot;&gt;클라우드환경&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#gkegoogle-kubernetes-engine&quot; id=&quot;markdown-toc-gkegoogle-kubernetes-engine&quot;&gt;GKE(Google Kubernetes Engine)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ekselastic-kubernetes-service&quot; id=&quot;markdown-toc-ekselastic-kubernetes-service&quot;&gt;EKS(Elastic Kubernetes Service)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;로컬환경&quot;&gt;로컬환경&lt;/h1&gt;
&lt;p&gt;쿠버네티스는 여러 플랫폼 환경에서 클러스터를 구성하여 사용할 수 있습니다. 로컬 쿠버네티스는 별다른 비용 발생 없이 간단하게 클러스터를 구축해 테스트해 볼 수 있어서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;테스트, 개발 환경에 적합&lt;/code&gt;합니다.&lt;/p&gt;

&lt;h2 id=&quot;미니큐브minikube&quot;&gt;미니큐브(minikube)&lt;/h2&gt;
&lt;p&gt;미니큐브는 물리 머신에 로컬 쿠버네티스를 쉽게 구축하고 실행할 수 있는 도구입니다. 실행되는 쿠버네티스는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;단일 노드 구성&lt;/code&gt;이기 때문에 여러 대의 구성이 필요한 쿠버네티스 기능은 사용할 수 없습니다. 또한 미니큐브는 로컬 가상 머신 위에 쿠버네티스를 설치하기 때문에 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;하이퍼바이저(Docer, Hyperkit, VirtualBox, ..)가 필요&lt;/code&gt;합니다. 제가 현재 사용하고 있는 맥 환경에서는 기본적으로 하이퍼킷이 설치되어 있습니다. 하지만 m1칩의 경우에는 아직 하이퍼킷을 지원하지 않기 때문에 먼저 도커를 설치, 실행한 후 미니큐브를 실행하셔야 합니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;minikube

minikube version
&lt;span class=&quot;c&quot;&gt;# minikube version: v1.25.1&lt;/span&gt;

minikube start &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;docker &lt;span class=&quot;c&quot;&gt;# --kubernetes-version 옵션으로 버전 선택 가능&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------&lt;/span&gt;
😄  Darwin 12.1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;arm64&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 의 minikube v1.25.1
✨  유저 환경 설정 정보에 기반하여 docker 드라이버를 사용하는 중
👍  minikube 클러스터의 minikube 컨트롤 플레인 노드를 시작하는 중
🚜  베이스 이미지를 다운받는 중 ...
💾  쿠버네티스 v1.23.1 을 다운로드 중 ...
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; preloaded-images-k8s-v16-v1...: 417.88 MiB / 417.88 MiB  100.00% 9.58 MiB
    &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; gcr.io/k8s-minikube/kicbase: 343.02 MiB / 343.02 MiB  100.00% 3.90 MiB p/
🔥  Creating docker container &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CPUs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2, &lt;span class=&quot;nv&quot;&gt;Memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;7903MB&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; ...
🐳  쿠버네티스 v1.23.1 을 Docker 20.10.12 런타임으로 설치하는 중
    ▪ kubelet.housekeeping-interval&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5m
    ▪ 인증서 및 키를 생성하는 중 ...
    ▪ 컨트롤 플레인이 부팅...
    ▪ RBAC 규칙을 구성하는 중 ...
🔎  Kubernetes 구성 요소를 확인...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
🌟  애드온 활성화 : storage-provisioner, default-storageclass
🏄  끝났습니다! kubectl이 &lt;span class=&quot;s2&quot;&gt;&quot;minikube&quot;&lt;/span&gt; 클러스터와 &lt;span class=&quot;s2&quot;&gt;&quot;default&quot;&lt;/span&gt; 네임스페이스를 기본적으로 사용하도록 구성되었습니다.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;이제 도커로 띄운 가상머신 위에서 쿠버네티스가 돌아가고 있습니다. 한 번 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube status
&lt;span class=&quot;nt&quot;&gt;--------------------&lt;/span&gt;
minikube
&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured

minikube ip
&lt;span class=&quot;c&quot;&gt;# 192.168.49.2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;정지하고 삭제하는 명령어도 간단합니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;minikube stop

minikube delete
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;docker-desktop&quot;&gt;Docker Desktop&lt;/h2&gt;
&lt;p&gt;Docker Desktop은 도커를 맥/윈도우에서 사용하기 위한 목적으로 만들어졌습니다. 그리고 Docker Desktop 버전 18.06.0부터는 쿠버네티스도 사용할 수 있도록 지원하고 있습니다. 사용 방법은 간단합니다. Docker Desktop을 설치, 실행한 뒤 Enable Kubernetes 목록을 클릭해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../images/../../images/kube_24.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(쿠버네티스를 Docker Desktop으로 실행할 때는 도커에서 제공하는 가상 머신위에 쿠버네티스 클러스터를 구성하는 것 같다. 그래서 클러스터 외부에서 쿠버네티스에 접속하려 할 때, 먼저 도커의 가상 머신 안으로 엔드포인트로 접근해야 하는데 이를 도커에서 localhost로 접근하도록 해준다. 그래서 별도로 도커 가상머신의 IP주소를 알려고 할 필요가 없다. 뇌피셜)&lt;/p&gt;

&lt;h2 id=&quot;kindkubernetes-in-docker&quot;&gt;kind(Kubernetes in Docker)&lt;/h2&gt;
&lt;p&gt;minikube와 Docker Desktop은 단일 노드로 구성된 쿠버네티스였다면, kind는 도커 컨테이너를 여러 개 띄워서 컨테이너 각각을 노드로 사용함으로써 &lt;strong&gt;멀티 노드 클러스터&lt;/strong&gt;를 구축할 수 있습니다.&lt;br /&gt;
&lt;a href=&quot;https://kind.sigs.k8s.io&quot; target=&quot;_blank&quot;&gt;(kind 공식문서 참고)&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;brew &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;kind

kind version
&lt;span class=&quot;nt&quot;&gt;--------------------&lt;/span&gt;
kind v0.11.1 go1.17.2 darwin/arm64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;잘 설치가 되었습니다. 이제 kind를 이용해 쿠버네티스에서 마스터와 워커 노드 역할을 하는 노드를 각각 3개씩 띄워 다음과 같이 멀티 노드 클러스터를 구축해보겠습니다.&lt;/p&gt;

&lt;p&gt;(실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kube_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# kind로 클러스터 구축을 위한 kind.yaml&lt;/span&gt;
apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
- role: control-plane
  image: kindest/node:v1.23.1
- role: control-plane
  image: kindest/node:v1.23.1
- role: control-plane
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Creating cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
 ✓ Ensuring node image &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;kindest/node:v1.23.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 🖼
 ✓ Preparing nodes 📦 📦 📦 📦 📦 📦
 ✓ Configuring the external load balancer ⚖️
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
 ✗ Joining worker nodes 🚜 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;실행 결과 리소스 부족으로 kindcluster-worker2를 만들다가 오류가 발생하여 마스터의 서버는 1개, 워커는 2개로 다시 구성해 실행해 보았습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;apiVersion: kind.x-k8s.io/v1alpha4
kind: Cluster
nodes:
- role: control-plane
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
- role: worker
  image: kindest/node:v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind create cluster &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt; kind.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;----------------------------------------------------------------------&lt;/span&gt;
Creating cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
 ✓ Ensuring node image &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;kindest/node:v1.23.1&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; 🖼
 ✓ Preparing nodes 📦 📦 📦
 ✓ Writing configuration 📜
 ✓ Starting control-plane 🕹️
 ✓ Installing CNI 🔌
 ✓ Installing StorageClass 💾
 ✓ Joining worker nodes 🚜
Set kubectl context to &lt;span class=&quot;s2&quot;&gt;&quot;kind-kindcluster&quot;&lt;/span&gt;
You can now use your cluster with:

kubectl cluster-info &lt;span class=&quot;nt&quot;&gt;--context&lt;/span&gt; kind-kindcluster

Have a &lt;span class=&quot;nb&quot;&gt;nice &lt;/span&gt;day! 👋
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;클러스터가 성공적으로 구축되었습니다.&lt;br /&gt;
쿠버네티스에서 실행중인 노드를 확인해보겠습니다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl get nodes
----------------------------------------------------------------------------
NAME                        STATUS   ROLES                  AGE   VERSION
kindcluster-control-plane   Ready    control-plane,master   58s   v1.23.1
kindcluster-worker          Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
kindcluster-worker2         Ready    &amp;lt;none&amp;gt;                 25s   v1.23.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;클러스터는 다음 명령어로 삭제하시면 됩니다.&lt;/p&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kind delete cluster &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; kindcluster
&lt;span class=&quot;nt&quot;&gt;------------------------------------------&lt;/span&gt;
Deleting cluster &lt;span class=&quot;s2&quot;&gt;&quot;kindcluster&quot;&lt;/span&gt; ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;클라우드환경&quot;&gt;클라우드환경&lt;/h1&gt;

&lt;h2 id=&quot;gkegoogle-kubernetes-engine&quot;&gt;GKE(Google Kubernetes Engine)&lt;/h2&gt;

&lt;h2 id=&quot;ekselastic-kubernetes-service&quot;&gt;EKS(Elastic Kubernetes Service)&lt;/h2&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791165216283&quot; target=&quot;_blank&quot;&gt;쿠버네티스 완벽 가이드 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/k8s/guide/&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series4</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series4</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kubernetes Series [Part3]: Kubernetes Service</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kubernetes-network&quot; id=&quot;markdown-toc-kubernetes-network&quot;&gt;Kubernetes Network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#clusterip&quot; id=&quot;markdown-toc-clusterip&quot;&gt;ClusterIP&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#nodeport&quot; id=&quot;markdown-toc-nodeport&quot;&gt;NodePort&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#loacbalancer&quot; id=&quot;markdown-toc-loacbalancer&quot;&gt;LoacBalancer&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ingress&quot; id=&quot;markdown-toc-ingress&quot;&gt;Ingress&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-외부의-로드-밸런서만을-이용한-ingress&quot; id=&quot;markdown-toc-클러스터-외부의-로드-밸런서만을-이용한-ingress&quot;&gt;클러스터 외부의 로드 밸런서만을 이용한 Ingress&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#클러스터-내부의-ingress-파드를-곁들인-ingress&quot; id=&quot;markdown-toc-클러스터-내부의-ingress-파드를-곁들인-ingress&quot;&gt;클러스터 내부의 Ingress 파드를 곁들인 Ingress&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kubernetes-network&quot;&gt;Kubernetes Network&lt;/h1&gt;
&lt;p&gt;쿠버네티스에서 파드 내부에는 여러 컨테이너가 존재할 수 있는데, 같은 파드 내에 있는 컨테이너는 동일한 IP 주소를 할당받게 됩니다. 따라서 같은 파드의 컨테이너로 통신하려면 &lt;strong&gt;localhost&lt;/strong&gt;로 통신할 수 있고, 다른 파드의 컨테이너와 통신하려면 &lt;strong&gt;파드의 IP 주소&lt;/strong&gt;로 통신하면 됩니다. 또한 노드 간의 통신은 &lt;strong&gt;VXLAN이나 L2 Routing&lt;/strong&gt;을 이용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_25.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 쿠버네티스에서는 클러스터 내부에서는 네트워크가 자동으로 구성되어 Service 리소스를 이용하지 않고도 파드 간 통신이 가능합니다. 그러나 Service 리소스를 이용하면 다음과 같은 장점을 얻을 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;로드 밸런싱&lt;/li&gt;
  &lt;li&gt;서비스 디스커버리&lt;/li&gt;
  &lt;li&gt;클러스터 내부 DNS&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;clusterip&quot;&gt;ClusterIP&lt;/h1&gt;
&lt;p&gt;ClusterIP는 서비스의 기본 타입입니다. ClusterIP 서비스를 생성하면 &lt;strong&gt;클러스터 내부에서만 통신 가능한 가상 IP&lt;/strong&gt;가 할당됩니다. kube-proxy는 노드 안에서 ClusterIP에서 들어온 트래픽을 원하는 파드로 전송합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_26.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;nodeport&quot;&gt;NodePort&lt;/h1&gt;
&lt;p&gt;NodePort는 모든 노드의 IP주소:포트에서 수신한 트래픽을 컨테이너에 전송하는 형태로 외부와 통신할 수 있습니다. NodePort는 전체 노드 N개 중 임의의 노드의 IP주소를 외부에 노출합니다. 그럼에도 ClusterIP를 통해 다른 노드의 파드로 통신하는데에는 문제 없습니다. 그러나 &lt;strong&gt;노출된 IP주소의 노드는 단일 장애점(Single Point of Failure)&lt;/strong&gt;이 되기 때문에 NodePort만을 이용해 외부와 통신하는 것은 분명한 한계점이 있습니다. 또한 NodePort는 쿠버네티스에서 &lt;strong&gt;지정한 범위(30000~32767)&lt;/strong&gt; 안에서만 지정할 수 있기 때문에 서비스로 활용하기에는 포트 번호가 예쁘지는 않습니다. 노드 포트 번호는 범위 안에서 직접 지정 가능하지만 쿠버네티스에서는 노드 포트 번호를 직접 지정하는 것을 지양합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_27.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h1 id=&quot;loacbalancer&quot;&gt;LoacBalancer&lt;/h1&gt;
&lt;p&gt;LoadBalancer에서는 NodePort와 다르게 별도로 외부 로드 밸런서를 사용하기 때문에 노드 장애가 발생해도 크게 문제가 되지 않습니다. 노드에 장애가 발생한 경우 해당 노드를 목적지에서 제외 처리하고 트래픽을 전송하지 않게됩니다. LoadBalancer서비스를 생성하면 컨테이너 내부에서의 통신을 위해 ClusterIP도 자동 할당됩니다. 실제 서비스 운영 환경에서는 외부로부터 요청을 수신하는 External IP 주소를 DNS 설정 등의 이유로 고정하는 것을 선호하고 LoadBalancer 서비스는 이를 지원합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../../images/kube_28.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ingress&quot;&gt;Ingress&lt;/h1&gt;
&lt;p&gt;인그레스는 L7(application layer) 로드 밸런싱을 제공하는 리소스입니다. 인그레스는 서비스들을 묶는 상위 객체로, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kind: Ingress&lt;/code&gt;타입 리소스를 지정합니다. 인그레스를 이용하면 하나의 IP주소로 N개의 애플리케이션을 로드 밸런싱할 수 있습니다.&lt;/p&gt;
&lt;h2 id=&quot;클러스터-외부의-로드-밸런서만을-이용한-ingress&quot;&gt;클러스터 외부의 로드 밸런서만을 이용한 Ingress&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GKE 인그레스&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;외부 로드 밸런서로 인그레스를 사용한다면, 인그레스 리소스 생성만으로 충분합니다.&lt;br /&gt;
&lt;img src=&quot;../../images/kube_29.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h2 id=&quot;클러스터-내부의-ingress-파드를-곁들인-ingress&quot;&gt;클러스터 내부의 Ingress 파드를 곁들인 Ingress&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Nginx 인그레스&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;클러스터 내부에서 인그레스를 이용해 로드 밸런싱을 할 경우 인그레스용 파드를 클러스터 내부에 생성해야 합니다. 또 내부의 인그레스용 파드를 외부에서 접속할 수 있도록 하기 위해 별도의 LoadBalancer 서비스를 생성해야 합니다.&lt;/p&gt;

&lt;p&gt;Nginx 인그레스 컨트롤러는 이름은 컨트롤러이지만 L7 수준의 로드 밸런싱을 직접 처리하기도 합니다.&lt;br /&gt;
&lt;img src=&quot;../../images/kube_30.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;amp;mallGb=KOR&amp;amp;barcode=9791165216283&quot; target=&quot;_blank&quot;&gt;쿠버네티스 완벽 가이드 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://subicura.com/k8s/guide/&quot; target=&quot;_blank&quot;&gt;subicura님의 kubenetes안내서&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/65887993/when-to-choose-loadbalancer-over-nodeport-service-typeor-vice-versa-in-kub&quot; target=&quot;_blank&quot;&gt;NodePort vs LoadBalancer stackoverflow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.google.com/kubernetes-engine/docs/concepts/network-overview&quot; target=&quot;_blank&quot;&gt;Google Kubernetes Engine 가이드&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kubernetes-series3</link>
                <guid isPermaLink="true">http://localhost:4000/kubernetes-series3</guid>
                
                <category>Kubernetes</category>
                
                
                <category>devops</category>
                
            </item>
        
            <item>
                <title>Kafka Series [Part2]: Main elements of Kafka</title>
                <description>&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kafka의-주요-구성요소&quot; id=&quot;markdown-toc-kafka의-주요-구성요소&quot;&gt;Kafka의 주요 구성요소&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#topic-partition-segment&quot; id=&quot;markdown-toc-topic-partition-segment&quot;&gt;Topic, Partition, Segment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#producer&quot; id=&quot;markdown-toc-producer&quot;&gt;Producer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#메세지-전송과정&quot; id=&quot;markdown-toc-메세지-전송과정&quot;&gt;메세지 전송과정&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#라운드-로빈round-robbin-방식&quot; id=&quot;markdown-toc-라운드-로빈round-robbin-방식&quot;&gt;라운드 로빈(Round-Robbin) 방식&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#스티키-파티셔닝sticky-partitioning-방식&quot; id=&quot;markdown-toc-스티키-파티셔닝sticky-partitioning-방식&quot;&gt;스티키 파티셔닝(Sticky Partitioning) 방식&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#중복-없는-전송&quot; id=&quot;markdown-toc-중복-없는-전송&quot;&gt;중복 없는 전송&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#정확히-한-번-전송&quot; id=&quot;markdown-toc-정확히-한-번-전송&quot;&gt;정확히 한 번 전송&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#broker&quot; id=&quot;markdown-toc-broker&quot;&gt;Broker&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#consumer&quot; id=&quot;markdown-toc-consumer&quot;&gt;Consumer&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#컨슈머-오프셋-관리&quot; id=&quot;markdown-toc-컨슈머-오프셋-관리&quot;&gt;컨슈머 오프셋 관리&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#그룹-코디네이터&quot; id=&quot;markdown-toc-그룹-코디네이터&quot;&gt;그룹 코디네이터&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#파티션-할당-전략&quot; id=&quot;markdown-toc-파티션-할당-전략&quot;&gt;파티션 할당 전략&lt;/a&gt;            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#라운드-로빈-파티션-할당-전략&quot; id=&quot;markdown-toc-라운드-로빈-파티션-할당-전략&quot;&gt;라운드 로빈 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#스티키-파티션-할당-전략&quot; id=&quot;markdown-toc-스티키-파티션-할당-전략&quot;&gt;스티키 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#협력적-스티키-파티션-할당-전략&quot; id=&quot;markdown-toc-협력적-스티키-파티션-할당-전략&quot;&gt;협력적 스티키 파티션 할당 전략&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#마치며&quot; id=&quot;markdown-toc-마치며&quot;&gt;마치며&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#참고자료&quot; id=&quot;markdown-toc-참고자료&quot;&gt;참고자료&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kafka의-주요-구성요소&quot;&gt;Kafka의 주요 구성요소&lt;/h1&gt;
&lt;p&gt;Kafka는 크게 3가지로 이루어 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Producer: Kafka로 메시지를 보내는 모든 클라이언트&lt;/li&gt;
  &lt;li&gt;Broker: 메시지를 분산 저장 및 관리하는 Kafka 애플리케이션이 설치된 서버&lt;/li&gt;
  &lt;li&gt;Consumer: Kafka에서 메시지를 꺼내서 사용하는 모든 클라이언트&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_7.png&quot; alt=&quot;&quot; /&gt;
&lt;a href=&quot;https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html&quot; target=&quot;_blank&quot;&gt;(참고: cloudkarafka)&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;topic-partition-segment&quot;&gt;Topic, Partition, Segment&lt;/h2&gt;
&lt;p&gt;Kafka의 구성요소에 대해 알아보기 전에 메시지가 어떤 식으로 구성, 저장되는지에 대해 짚고 넘어가려고 합니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Topic: 메시지가 저장될 카테고리 이름 (논리적인 저장소)&lt;/li&gt;
  &lt;li&gt;Partition: 병렬 처리를 위해 Topic을 여러 개로 나눈 것 (Server 디스크에 저장된 디렉토리)&lt;/li&gt;
  &lt;li&gt;Segment: 메시지가 실제로 저장되는 파일&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_6.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.cloudkarafka.com/blog/part1-kafka-for-beginners-what-is-apache-kafka.html&quot; target=&quot;_blank&quot;&gt;(참고: cloudkarafka)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;카프카를 실행하게 되면 보통 토픽을 가장 먼저 생성합니다. 그리고 토픽은 병렬 처리를 통한 성능 향상을 위해 파티션으로 나뉘어 구성됩니다. 그리고 프로듀서가 카프카로 전송한 메시지는 해당 토픽 내 각 파티션의 로그 세그먼트에 저장됩니다. 따라서 프로듀서는 토픽으로 메시지를 보낼 때 해당 토픽의 어느 파티션으로 메시지를 보낼지를 결정해야 합니다.&lt;/p&gt;

&lt;h2 id=&quot;producer&quot;&gt;Producer&lt;/h2&gt;
&lt;p&gt;프로듀서는 카프카의 토픽으로 메시지를 전송하는 역할을 합니다. 프로듀서가 동작하는 방식은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_13.webp&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://dzone.com/articles/take-a-deep-dive-into-kafka-producer-api&quot; target=&quot;_blank&quot;&gt;(Dzone 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;메세지-전송과정&quot;&gt;메세지 전송과정&lt;/h3&gt;
&lt;p&gt;프로듀서가 카프카의 브로커로 데이터를 전송할 때에는 &lt;strong&gt;ProducerRecord&lt;/strong&gt;라고 하는 형태로 전송되며, &lt;strong&gt;Topic&lt;/strong&gt;과 &lt;strong&gt;Value&lt;/strong&gt;는 필수값이며, &lt;strong&gt;Partition&lt;/strong&gt;과 &lt;strong&gt;Key&lt;/strong&gt;는 선택값입니다. 프로듀서는 카프카로 레코드를 전송할 때, 카프카의 특정 토픽으로 메세지를 전송합니다. 전송 과정은&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;프로듀서에서 send() 메소드 호출&lt;/li&gt;
  &lt;li&gt;Serializer는 JSON, String, Avro 등의 object를 bytes로 변환&lt;/li&gt;
  &lt;li&gt;ProducerRecord에 target Partition이 있으면 해당 파티션으로 레코드 전달&lt;/li&gt;
  &lt;li&gt;Partition이 지정되지 않았을 때, Key값이 지정되었다면 &lt;strong&gt;Partitioner가 Key값을 바탕으로&lt;/strong&gt; 해당 파티션에 전달&lt;/li&gt;
  &lt;li&gt;Partition, Key값이 모두 없으면 &lt;strong&gt;라운드 로빈(Round-Robbin)&lt;/strong&gt;방식 또는 &lt;strong&gt;스티키 파티셔닝(Sticky Partitioning)&lt;/strong&gt; 방식으로 메세지를 파티션에 할당&lt;/li&gt;
  &lt;li&gt;파티션에 세그먼트 파일 형태로 저장된 레코드는 바로 전송할 수도 있고, 프로듀서의 버퍼 메모리 영역에 잠시 저장해두고 &lt;strong&gt;배치로 전송할 수도 있음&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;라운드-로빈round-robbin-방식&quot;&gt;라운드 로빈(Round-Robbin) 방식&lt;/h3&gt;
&lt;p&gt;프로듀서의 메시지에서 키값은 필수값이 아니므로, 값이 null일 수도 있습니다. 그럴 경우 기본적인 메세지 할당 방식은 라운드 로빈 방식 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_30.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;메시지를 위 그림과 같이 순차적으로 파티션에 할당합니다. 하지만 이 방법은 배치 전송을 할 경우 배치 사이즈가 3일 때, 메시지를 5개 보내는 동안에도 카프카로 전송되지 못한채 프로듀서의 버퍼 메모리 영역에서 대기하고 있습니다. 이러한 비효율적인 전송을 보완하기 위해 카프카에서는 스티키 파티셔닝 방식을 공개했습니다.&lt;/p&gt;

&lt;h3 id=&quot;스티키-파티셔닝sticky-partitioning-방식&quot;&gt;스티키 파티셔닝(Sticky Partitioning) 방식&lt;/h3&gt;
&lt;p&gt;라운드 로빈 방식의 비효율적인 전송을 개선하기 위해 아파치 카프카 2.4버전부터는 스티키 파티셔닝 방식을 사용하고 있습니다. 스키티 파티셔닝이란 하나의 파티션에 레코드를 먼저 채워 카프카로 빠르게 배치 전송하는 방식을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_29.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
이렇게 파티셔너는 배치를 위한 레코드 수에 도달할 때까지 파티션 한 곳에만 메시지를 담아놓습니다. 이러한 미묘한 변화가 프로듀서 성능을 높일 수 있는지 의구심이 들지만 컨플루언트에서는 블로그에서 약 30% 이상 지연시간이 감소되었다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_31.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;a href=&quot;https://www.confluent.io/en-gb/blog/apache-kafka-producer-improvements-sticky-partitioner/&quot; target=&quot;_blank&quot;&gt;(Confluent 블로그 참고, linger.ms는 배치 전송을 위해 버퍼 메모리에서 메시지가 대기하는 최대시간입니다.)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&quot;중복-없는-전송&quot;&gt;중복 없는 전송&lt;/h3&gt;

&lt;h3 id=&quot;정확히-한-번-전송&quot;&gt;정확히 한 번 전송&lt;/h3&gt;

&lt;h2 id=&quot;broker&quot;&gt;Broker&lt;/h2&gt;
&lt;p&gt;브로커는 Topic내의 Partition들을 분산 저장, 관리해줍니다. 하나의 브로커에는 Topic의 모든 데이터를 가지고 있지 않고, 일부분(Partition)만 가지게 됩니다. 보통 Broker를 최소 3대 이상으로 구성해 Kafka cluster를 형성합니다.&lt;/p&gt;

&lt;h2 id=&quot;consumer&quot;&gt;Consumer&lt;/h2&gt;
&lt;p&gt;컨슈머는 카프카에 저장되어 있는 메시지를 가져오는 역할을 합니다. 그러나 단순히 가져오는 역할만 하지는 않고, 조금 더 자세히 들여다 보면 컨슈머 그룹을 만들고, 그룹 내 모든 컨슈머가 파티션을 골고루 가져오도록 하는 리밸런싱과 같은 역할도 합니다. 컨슈머 수는 파티션 수보다 작거나 같도록 하는 것이 바람직합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;컨슈머 그룹 내에 있는 컨슈머들은 서로 협력하여 메시지를 처리합니다. 이 때 Partition은 같은 그룹에 있는 컨슈머 중 한 개의 컨슈머에 의해서만 소비됩니다. (같은 그룹에 있는 여러 컨슈머가 한 개의 Partition을 소비하면 메시지 중복 문제를 해결하는데 또 비용이 든다) 컨슈머에서 고려해야 할 사항에는 다음과 같은 것들이 있습니다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;파티션 할당 전략&lt;/li&gt;
  &lt;li&gt;프로듀서가 카프카에 메세지를 저장하는 속도와 컨슈머가 읽어가는 속도가 비슷한가&lt;/li&gt;
  &lt;li&gt;컨슈머의 개수가 파티션보다 많지는 않은가&lt;/li&gt;
  &lt;li&gt;컨슈머 그룹 내에 장애가 발생한 컨슈머가 생기면 어떻게 처리할 것인가&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;컨슈머-오프셋-관리&quot;&gt;컨슈머 오프셋 관리&lt;/h3&gt;
&lt;p&gt;컨슈머의 동작 중 가장 핵심은 바로 오프셋 관리입니다. 이를 통해 마지막 고려사항인 컨슈머 장애 발생에 대응할 수 있습니다. 오프셋 관리는 &lt;strong&gt;컨슈머가 메시지를 어디까지 가져왔는지를 표시하는 것&lt;/strong&gt;이라고 할 수 있습니다. 예를 들어 컨슈머가 일시적으로 동작을 멈추고 재시작하거나, 컨슈머 서버에 문제가 발생해 새로운 컨슈머가 생성된 경우 새로운 컨슈머는 기존 컨슈머의 마지막 위치에서 메시지를 가져올 수 있어야 장애를 복구할 수 있습니다. 카프카에서는 메시지의 위치를 나타내는 숫자를 오프셋이라고 하고 이러한 오프셋 정보는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;__consumer_offsets&lt;/code&gt;라는 별도의 토픽에 저장합니다. 이러한 정보는 컨슈머 그룹별로 기록됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 __consumer_offsets 토픽에 정보를 기록해 두면 컨슈머의 변경이 발생했을 때 해당 컨슈머가 어디까지 읽었는지 추적할 수 있습니다. 여기서 주의할 점은 저장되는 오프셋값은 컨슈머가 마지막으로 읽은 위치가 아니라, &lt;strong&gt;컨슈머가 다음으로 읽어야 할 위치&lt;/strong&gt;를 말합니다.&lt;/p&gt;

&lt;p&gt;참고로 __consumer_offsets 또한 하나의 토픽이기 때문에 파티션 수와 리플리케이션 팩터 수를 설정할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;그룹-코디네이터&quot;&gt;그룹 코디네이터&lt;/h3&gt;
&lt;p&gt;컨슈머 그룹 내의 각 컨슈머들은 서로 정보를 공유하며 하나의 공동체로 동작합니다. 컨슈머 그룹에는 컨슈머가 떠나거나 새로 합류하는 등 변화가 일어나기 때문에 이러한 변화가 일어날 때마다 컨슈머 리밸런싱을 통해 작업을 새로 균등하게 분배해야 합니다.&lt;/p&gt;

&lt;p&gt;이렇게 컨슈머 그룹내의 변화를 감지하기 위해 &lt;strong&gt;트래킹하는 것이 바로 그룹 코디네이터&lt;/strong&gt;입니다. 그룹 코디네이터는 컨슈머 그룹 내의 컨슈머 리더와 통신을 하고, 실제로 파티션 할당 전략에 따라 &lt;strong&gt;컨슈머들에게 파티션을 할당하는 것은 컨슈머 리더&lt;/strong&gt;입니다. 리더 컨슈머가 작업을 마친 뒤 그룹 코디네이터에게 전달하면 그룹 코디네이터는 해당 정보를 캐시하고 그룹 내의 컨슈머들에게 성공을 알립니다. 할당을 마치고 나면 각 &lt;strong&gt;컨슈머들은 각자 할당받은 파티션으로부터 메시지를 가져옵니다.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;그룹 코디네이터는 그룹 별로 하나씩 존재하며 브로커 중 하나에 위치합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_28.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그룹 코디네이터는 컨슈머와 주기적으로 하트비트를 주고받으며 컨슈머가 잘 동작하는지 확인합니다. 컨슈머는 그룹에서 빠져나가거나 새로 합류하게 되면 그룹 코디네이터에게 join, leave 요청을 보내고 그룹 코디네이터는 이러한 정보를 컨슈머 리더에게 전달해 새로 파티션을 할당하도록 합니다. 이 밖에도 컨슈머가 일정 시간(session.timeout.ms)이 지나도록 하트비트를 보내지 않으면 컨슈머에 문제가 발생한 것으로 간주하고 다시 컨슈머 리더에게 이러한 정보를 알려줍니다.&lt;/p&gt;

&lt;p&gt;이렇게 컨슈머에 변화가 생길 때마다 파티션 리밸런싱이 일어나게 되는데 파티션 리밸런싱은 파티션을 골고루 분배해 성능을 향상시키기도 하지만 너무 자주 일어나게 되면 오히려 배보다 배꼽이 더 커지는 상황이 발생할 수 있습니다. 이러한 문제를 해결하기 위해 아파치 카프카에서는 몇가지의 파티션 할당 전략을 제공하고 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;파티션-할당-전략&quot;&gt;파티션 할당 전략&lt;/h3&gt;

&lt;h4 id=&quot;라운드-로빈-파티션-할당-전략&quot;&gt;라운드 로빈 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;라운드 로빈 방식은 파티션 할당 방법 중 가장 간단한 방법입니다. 할당해야할 모든 파티션과 컨슈머들을 나열한 후 하나씩 파티션과 컨슈머를 할당하는 방식입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_32.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 하면 파티션을 균등하게 분배할 수 있지만 컨슈머 리밸런싱이 일어날 때 마다 컨슈머가 작업하던 파티션이 계속 바뀌게 되는 문제점이 생깁니다. 예를 들어 컨슈머 1이 처음에는 파티션 0을 작업하고 있었으나 컨슈머 리밸런싱이 일어난 후 파티션 0은 컨슈머 2에게 가고 컨슈머 1은 다른 파티션을 작업해야 합니다. 이런 현상을 최대한 줄이고자 나오게 된 것이 바로 스티키 파티션 할당 전략입니다.&lt;/p&gt;

&lt;h4 id=&quot;스티키-파티션-할당-전략&quot;&gt;스티키 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;스티키 파티션 할당 전략의 첫 번째 목적은 파티션을 균등하게 분배하는 것이고, 두 번째 목적은 재할당이 일어날 때 최대한 파티션의 이동이 적게 발생하도록 하는 것입니다. 우선순위는 첫 번째가 더 높습니다.&lt;/p&gt;

&lt;p&gt;동작 방식은 먼저 문제가 없는 컨슈머에 연결된 파티션은 그대로 둡니다. 그리고 문제가 생긴 컨슈머에 할당된 파티션들만 다시 라운드 로빈 방식으로 재할당합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_33.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;마지막 할당 전략으로 넘어가기 전에 짚고 넘어갈 점이 있습니다. 위에서 배웠던 재할당 방식은 모두 EAGER라는 리밸런스 프로토콜을 사용했고, EAGER 프로토콜은 리밸런싱할 때 컨슈머에게 할당되었던 모든 파티션들을 할당 취소합니다. 스티키 파티션 할당 전략은 문제가 없는 컨슈머의 파티션은 그렇지 않을 것 같지만 스티키 파티션 할당 전략도 마찬가지로 모든 파티션을 할당 취소합니다. 이렇게 구현한 이유는 먼저 파티션은 그룹 내의 컨슈머에게 중복 할당 되어서는 안되기 때문에 이러한 로직을 쉽게 구현하고자 하였던 것입니다. 그러나 이렇게 모든 파티션을 할당 취소하게 되면 일시적으로 컨슈머가 일을 할 수 없게 됩니다. 이 때 소요되는 시간을 다운타임이라고 합니다. 즉 컨슈머의 다운타임 동안 LAG가 급격하게 증가합니다.&lt;/p&gt;

&lt;h4 id=&quot;협력적-스티키-파티션-할당-전략&quot;&gt;협력적 스티키 파티션 할당 전략&lt;/h4&gt;
&lt;p&gt;이러한 이슈를 개선하고자 아파치 카프카 2.3 버전부터는 새로운 리밸런싱 프로토콜인 COOPERATIVE 프로토콜을 적용하기 시작했고, 이 프로토콜은 리밸런싱이 동작하기 전의 컨슈머 상태를 유지할 수 있게 했습니다.&lt;/p&gt;

&lt;p&gt;이 방식은 컨슈머 리밸런싱이 트리거 될 때(컨슈머의 이탈 또는 합류) 모든 컨슈머들은 자신의 정보를 그룹 코디네이터에게 전송하고 그룹 코디네이터는 이를 조합해 컨슈머 리더에게 전달합니다. 리더는 이를 바탕으로 새로 파티션 할당 전략을 세우고 이를 컨슈머들에게 전달합니다. 컨슈머들은 이를 통해 기존의 할당 전략과 차이를 비교해보고 차이가 생긴 파티션만 따로 제외시킵니다. 그리고 제외된 파티션만을 이용해 다시 리밸런싱을 진행합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_35.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이런식으로 스티키 파티션 할당 전략은 리밸런싱이 여러번 일어나게 됩니다. 이 협력적 스티키 파티션 할당 전략은 아파치 카프카 2.5 버전에서 서비스가 안정화되어 본격적으로 이용되기 시작하면서 컨슈머 리밸런싱으로 인한 다운타임을 최소화 할 수 있게 되었습니다.&lt;/p&gt;

&lt;p&gt;컨플루언트 블로그에서는 기존의 EAGER 방식과 COOPERATIVE 프로토콜 방식의 성능을 비교한 결과를 공개하였는데 COOPERATIE 방식이 더 빠른 시간 안에 짧은 다운타임을 가지고 리밸런싱을 할 수 있었습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/kafka_34.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.confluent.io/blog/incremental-cooperative-rebalancing-in-kafka/&quot; target=&quot;_blank&quot;&gt;(컨플루언트 블로그 참고)&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;마치며&quot;&gt;마치며&lt;/h1&gt;
&lt;p&gt;이번 포스트에서는 카프카에서 중요한 개념들에 대해 간단히 살펴보았습니다. 프로듀서는 메세지의 전송, 브로커는 저장, 컨슈머는 읽어가는 역할을 담당합니다. 또한 카프카에서 주고 받는 데이터는 토픽, 파티션, 세그먼트라는 단위로 나뉘어 처리, 저장됩니다.&lt;/p&gt;

&lt;p&gt;카프카는 데이터 파이프라인의 중심에 위치하는 허브 역할을 합니다. 그렇기 때문에 카프카는 &lt;strong&gt;장애 발생에 대처 가능한 안정적인 서비스를 제공&lt;/strong&gt;해 줄 수 있어야 하고, 각 서비스들의 원활한 이용을 위한 &lt;strong&gt;높은 처리량&lt;/strong&gt;, &lt;strong&gt;데이터 유실, 중복을 해결함으로써 각 서비스에서의 이용을 원활&lt;/strong&gt;하게 해주는 것이 좋습니다.&lt;/p&gt;

&lt;h1 id=&quot;참고자료&quot;&gt;참고자료&lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewKor.laf?mallGb=KOR&amp;amp;ejkGb=KOR&amp;amp;barcode=9791189909345&quot; target=&quot;_blank&quot;&gt;실전 카프카 개발부터 운영까지 책&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dzone.com/articles/take-a-deep-dive-into-kafka-producer-api&quot; target=&quot;_blank&quot;&gt;Dzone 블로그&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/codex/apache-kafka-series-part-1-introduction-to-apache-kafka-9b890832002&quot; target=&quot;_blank&quot;&gt;CodeX 블로그&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Sun, 23 Jan 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/kafka-series2</link>
                <guid isPermaLink="true">http://localhost:4000/kafka-series2</guid>
                
                <category>Kafka</category>
                
                
                <category>DE</category>
                
            </item>
        
    </channel>
</rss>
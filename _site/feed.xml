<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Code Museum</title>
        <description>Jay Tech personal blogging theme for Jekyll</description>
        <link>http://localhost:4000/</link>
        <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Sat, 27 Aug 2022 10:33:49 +0900</pubDate>
        <lastBuildDate>Sat, 27 Aug 2022 10:33:49 +0900</lastBuildDate>
        <generator>Jekyll v4.2.1</generator>
        
            <item>
                <title>AWS Series [Part9]: AWS Analytics Service: Kinesis</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis&quot; id=&quot;markdown-toc-kinesis&quot;&gt;Kinesis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis-data-streams&quot; id=&quot;markdown-toc-kinesis-data-streams&quot;&gt;Kinesis Data Streams&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kinesis-data-firehose&quot; id=&quot;markdown-toc-kinesis-data-firehose&quot;&gt;Kinesis Data Firehose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#youtube&quot; id=&quot;markdown-toc-youtube&quot;&gt;Youtube&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;kinesis&quot;&gt;Kinesis&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real time&lt;/li&gt;
  &lt;li&gt;Apache Kafkaì™€ ê°™ì€ ìš©ë„ë¡œ ì‚¬ìš©í•œë‹¤&lt;/li&gt;
  &lt;li&gt;Apache Kafkaì™€ ì•„í‚¤í…ì²˜ê¹Œì§€ ë¹„ìŠ·í•œ ì„œë¹„ìŠ¤ë¡œëŠ”  Managed Streaming for Apache Kafka(MSK)ê°€ ìˆë‹¤&lt;/li&gt;
  &lt;li&gt;Kinesisê°€ MSKë³´ë‹¤ ì¡°ê¸ˆ ë” ì¼ì° ì„œë¹„ìŠ¤ë¡œ ì œê³µë˜ì—ˆê¸° ë•Œë¬¸ì— ì•„ì§ê¹Œì§€ëŠ” MSK ë³´ë‹¤ í†µí•©ì„±ì´ ì¢‹ì§€ë§Œ ì ì°¨ MSKë„ ë‚˜ì•„ì§€ëŠ” ì¤‘&lt;/li&gt;
  &lt;li&gt;Kinesisê°€ ë” ì¼ì° ì„œë¹„ìŠ¤ë˜ì—ˆê¸° ë•Œë¬¸ì— ì‹¤ë¬´ì—ì„œëŠ” MSKë³´ë‹¤ Kinesisë¥¼ ë” ë§ì´ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì•„ë˜ ì˜ìƒì€ Kinesisì™€ MSKì˜ íŠ¹ì§•ê³¼ ì°¨ì´ë¥¼ ìì„¸íˆ ì„¤ëª…í•´ì¤€ë‹¤.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/9y-aCX5O3Ms&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;kinesis-data-streams&quot;&gt;Kinesis Data Streams&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Kafkaì˜ Brokerì™€ ë¹„ìŠ·í•œ ìš©ë„&lt;/li&gt;
  &lt;li&gt;ì•„ë˜ ê·¸ë¦¼ì€ ì„¤ì • í™”ë©´ì„ ìº¡ì²˜í•œ ê²ƒì¸ë° í¬ê²Œ ì„¤ì •í• ê²Œ ì—†ëŠ” ê²ƒ ê°™ë‹¤.
&lt;img src=&quot;/images/kinesis_1.png&quot; alt=&quot;&quot; /&gt;
    &lt;ul&gt;
      &lt;li&gt;ì¤‘ë³µì—†ëŠ” ì „ì†¡ê³¼ ê°™ì€ ì„¤ì •ì€ script(pythonì˜ boto3 ë¼ì´ë¸ŒëŸ¬ë¦¬)ë¡œ í•´ê²°í•˜ê±°ë‚˜ Sourceì—ì„œ í•´ê²°í•´ì•¼ í•˜ëŠ” ê²ƒ ê°™ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Kinesis Data Stream ì„œë¹„ìŠ¤ ìì²´ì— Auto-scaling ì„¤ì • ê¸°ëŠ¥ì´ ì—†ë‹¤
    &lt;ul&gt;
      &lt;li&gt;Unlike some other AWS services, Kinesis does not provide a native auto-scaling solution like DynamoDB On-Demand or EC2 Auto Scaling. Therefore, there is a need for the right number of shards to be calculated for every stream based on the expected number of records and/or the size of the records&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì•„ë˜ì™€ ê°™ì€ ë°©ë²•ì´ í•˜ë‚˜ì˜ ì†”ë£¨ì…˜ì´ ëœë‹¤ 
&lt;img src=&quot;/images/kinesis_3.png&quot; alt=&quot;&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;kinesis-data-firehose&quot;&gt;Kinesis Data Firehose&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Kafkaì˜ Connectorì™€ ë¹„ìŠ·í•œ ìš©ë„&lt;/li&gt;
  &lt;li&gt;Sourceë¡œ ì„¤ì •ê°€ëŠ¥í•œ ê°’ì€ Kinesis Data Stream ë˜ëŠ” Direct PUT ë¿ì´ë‹¤&lt;/li&gt;
  &lt;li&gt;Targetìœ¼ë¡œ ê°€ëŠ¥í•œ ê²ƒì€ S3, Redshiftì™€ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤&lt;/li&gt;
  &lt;li&gt;Sourceë¡œ Direct PUTì„ ì‚¬ìš©í•˜ë©´ Apache Kafka + Kinesis Firehose ì¡°í•©ë„ ê°€ëŠ¥í•  ê²ƒ ê°™ë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/kinesis_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;youtube&quot;&gt;Youtube&lt;/h1&gt;

&lt;p&gt;Kinesis Data Streamê³¼ Kinesis Data Firehoseì˜ Use caseë¥¼ í¬í•¨í•´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì— ê´€í•œ ì¢‹ì€ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•´ì¤€ë‹¤.&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/S3vdTBbQ2YM&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/&quot; target=&quot;_blank&quot;&gt;AWS docs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.whizlabs.com/blog/aws-kinesis-data-streams-vs-aws-kinesis-data-firehose/&quot; target=&quot;_blank&quot;&gt;whizlabs, AWS Kinesis Data Streams vs AWS Kinesis Data Firehose&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=S3vdTBbQ2YM&amp;amp;list=LL&amp;amp;index=8&quot; target=&quot;_blank&quot;&gt;Youtube AWS Korea: AWSì—ì„œ ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•˜ê¸° ìœ„í•œ ì‹¤ì‹œê°„, ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘ ë°©ë²• ì•Œì•„ë³´ê¸°&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/slalom-data-ai/amazon-kinesis-data-streams-auto-scaling-the-number-of-shards-105dc967bed5&quot; target=&quot;_blank&quot;&gt;Brandon Stanley, Amazon Kinesis Data Streams: Auto-scaling the number of shards&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series9</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series9</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part8]: AWS [Database, Analytics] Service: Redshift</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#use-case&quot; id=&quot;markdown-toc-use-case&quot;&gt;Use Case&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples&quot; id=&quot;markdown-toc-examples&quot;&gt;Examples&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#í´ëŸ¬ìŠ¤í„°-ìƒì„±&quot; id=&quot;markdown-toc-í´ëŸ¬ìŠ¤í„°-ìƒì„±&quot;&gt;í´ëŸ¬ìŠ¤í„° ìƒì„±&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ìŠ¤í‚¤ë§ˆ-ìƒì„±&quot; id=&quot;markdown-toc-ìŠ¤í‚¤ë§ˆ-ìƒì„±&quot;&gt;ìŠ¤í‚¤ë§ˆ ìƒì„±&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#ì¿¼ë¦¬-ë°-ë¶„ì„&quot; id=&quot;markdown-toc-ì¿¼ë¦¬-ë°-ë¶„ì„&quot;&gt;ì¿¼ë¦¬ ë° ë¶„ì„&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Amazon Redshift is a fully managed(setting up, operating, and scaling a data warehouse, provisioning capacity, monitoring and backing up the cluster), petabyte-scale data warehouse service in the cloud. You can start with just a few hundred gigabytes of data and scale to a petabyte or more. This enables you to use your data to acquire new insights for your business and customers.&lt;/p&gt;

&lt;p&gt;The first step to create a data warehouse is to launch a set of nodes, called an Amazon Redshift cluster. After you provision your cluster, you can upload your data set and then perform data analysis queries. Regardless of the size of the data set, Amazon Redshift offers fast query performance using the same SQL-based tools and business intelligence applications that you use today.&lt;/p&gt;

&lt;p&gt;Redshift is an OLAP-style (Online Analytical Processing) column-oriented database. It is based on PostgreSQL version 8.0.2. This means regular SQL queries can be used with Redshift. But this is not what separates it from other services. The fast delivery to queries made on a large database with exabytes of data is what helps Redshift stand out.&lt;/p&gt;

&lt;p&gt;Fast querying is made possible by Massively Parallel Processing design or MPP. The technology was developed by ParAccel. With MPP, a large number of computer processors work in parallel to deliver the required computations. Sometimes processors situated across multiple servers can be used to deliver a process.&lt;/p&gt;

&lt;h1 id=&quot;use-case&quot;&gt;Use Case&lt;/h1&gt;

&lt;p&gt;Amazon Redshift is used when the data to be analyzed is humongous. The data has to be at least of a petabyte-scale (1015 bytes) for Redshift to be a viable solution. The MPP technology used by Redshift can be leveraged only at that scale. Beyond the size of data, there are some specific use cases that warrant its use.&lt;/p&gt;

&lt;p&gt;(ì¿¼ë¦¬ì˜ ì„±ëŠ¥ì´ ê·¹ëŒ€í™”ë¨ -&amp;gt; ëŒ€ìš©ëŸ‰ ë°ì´í„° or ì‹¤ì‹œê°„ ë¶„ì„ì— ì í•© -&amp;gt; ê·¸ ì™¸ì˜ ê²½ìš° ìš”êµ¬ì‚¬í•­ ëŒ€ë¹„ ì§€ë‚˜ì¹œ ì„±ëŠ¥ìœ¼ë¡œ ë‚­ë¹„ê°€ ë  ìˆ˜ ìˆìŒ)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;more than petabyte-scale&lt;/li&gt;
  &lt;li&gt;processing real-time analytics&lt;/li&gt;
  &lt;li&gt;combining multiple data sources&lt;/li&gt;
  &lt;li&gt;business intelligence&lt;/li&gt;
  &lt;li&gt;log analysis&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;examples&quot;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&quot;í´ëŸ¬ìŠ¤í„°-ìƒì„±&quot;&gt;í´ëŸ¬ìŠ¤í„° ìƒì„±&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ë…¸ë“œ ìœ í˜•, ê°œìˆ˜ëŠ” ì‘ê²Œ ì‹œì‘í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ (RedshiftëŠ” ë¹„ì‹¸ë‹ˆê¹Œ)&lt;/li&gt;
  &lt;li&gt;IAM ì—­í• ì„ ì œëŒ€ë¡œ ì§€ì •ì•ˆí•˜ë©´ ì•ˆë¨ -&amp;gt; ë‚˜ì˜ ê²½ìš° Athena, Glue, S3ì˜ FullAccessë¥¼ ì´ìš©
    &lt;ul&gt;
      &lt;li&gt;ì²˜ìŒì— RedshiftFullAccessë„ ì¶”ê°€í•´ì¤¬ì—ˆëŠ”ë° ì™œì¸ì§€ ëª¨ë¥´ê² ì§€ë§Œ ì—ëŸ¬ë‚¨&lt;/li&gt;
      &lt;li&gt;(ì™œ Redshiftë¥¼ ì´ìš©í•  ë•Œ RedshiftFullAccessë¥¼ ì¶”ê°€í•˜ë©´ ì—ëŸ¬ê°€ ë‚ ê¹Œ)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ìŠ¤í‚¤ë§ˆ-ìƒì„±&quot;&gt;ìŠ¤í‚¤ë§ˆ ìƒì„±&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Glueì˜ Catalogê°€ ìˆìœ¼ë©´ Redshiftë¥¼ ì‚¬ìš©í•  ë•Œë„ ì •ë§ í¸í•˜ë‹¤&lt;/li&gt;
  &lt;li&gt;Catalog ì—†ìœ¼ë©´ [Create Schema] -&amp;gt; [Create Table] -&amp;gt; [Load Data] í•´ì¤˜ì•¼ë¨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;ì¿¼ë¦¬-ë°-ë¶„ì„&quot;&gt;ì¿¼ë¦¬ ë° ë¶„ì„&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;MongoDBì—ì„œ ì €ì¥í•  ë•Œ requirementsë¼ëŠ” ì†ì„±ì„ arrayí˜•íƒœë¡œ ì €ì¥í–ˆì—ˆë‹¤&lt;/li&gt;
  &lt;li&gt;arrayê°€ ìˆìœ¼ë©´ ì¿¼ë¦¬ì‹œ ì—ëŸ¬ê°€ ë‚œë‹¤ -&amp;gt; unnestingì„ ì§„í–‰í–ˆë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/redshift_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/redshift/latest/mgmt/welcome.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Redshift&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/redshift/latest/dg/query-super.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Querying semistructured data&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cloudzero.com/blog/aws-redshift&quot; target=&quot;_blank&quot;&gt;CLOUDZERO, AWS Redshift 101: What Is It and When Should You Use It?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series8</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series8</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part7]: AWS Database Service: DynamoDB</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-dynamodb&quot; id=&quot;markdown-toc-what-is-dynamodb&quot;&gt;What is DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#features-of-dynamodb&quot; id=&quot;markdown-toc-features-of-dynamodb&quot;&gt;Features of DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#components-of-dynamodb&quot; id=&quot;markdown-toc-components-of-dynamodb&quot;&gt;Components of DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#storing-data-in-dynamodb&quot; id=&quot;markdown-toc-storing-data-in-dynamodb&quot;&gt;Storing Data in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#querying-data-in-dynamodb&quot; id=&quot;markdown-toc-querying-data-in-dynamodb&quot;&gt;Querying Data in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#configuration-in-dynamodb&quot; id=&quot;markdown-toc-configuration-in-dynamodb&quot;&gt;Configuration in DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#use-case&quot; id=&quot;markdown-toc-use-case&quot;&gt;Use Case&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pricing&quot; id=&quot;markdown-toc-pricing&quot;&gt;Pricing&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#dynamodb-provisioned-capacity&quot; id=&quot;markdown-toc-dynamodb-provisioned-capacity&quot;&gt;DynamoDB Provisioned Capacity&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dynamodb-on-demand-pricing&quot; id=&quot;markdown-toc-dynamodb-on-demand-pricing&quot;&gt;DynamoDB On-demand Pricing&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-dynamodb&quot;&gt;What is DynamoDB&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_00.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Amazon DynamoDB is a cloud-native fully managed NoSQL primarily key-value database.&lt;/p&gt;

&lt;p&gt;DynamoDBâ€™s NoSQL design is oriented towards simplicity and scalability, which appeal to developers and devops teams respectively. It can be used for a wide variety of semistructured data-driven applications prevalent in modern and emerging use cases beyond traditional databases, from the Internet of Things (IoT) to social apps or massive multiplayer games. With its broad programming language support, it is easy for developers to get started and to create very sophisticated applications using DynamoDB.&lt;/p&gt;

&lt;p&gt;While we cannot describe exactly what DynamoDB is, we can describe how you interact with it. When you set up DynamoDB on AWS, you do not provision specific servers or allocate set amounts of disk. Instead, you provision throughput â€” you define the database based on provisioned capacity â€” how many transactions and how many kilobytes of traffic you wish to support per second. Users specify a service level of read capacity units (RCUs) and write capacity units (WCUs).&lt;/p&gt;

&lt;p&gt;DynamoDB needed to â€œprovide fast performance at any scale,â€ allowing developers to â€œstart small with just the capacity they need and then increase the request capacity of a given table as their app grows in popularity.â€ Predictable performance was ensured by provisioning the database with guarantees of throughput, measured in â€œcapacity unitsâ€ of reads and writes. â€œFastâ€ was defined as single-digit milliseconds, based on data stored in Solid State Drives (SSDs).&lt;/p&gt;

&lt;p&gt;DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you donâ€™t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling. DynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data.&lt;/p&gt;

&lt;p&gt;You can scale up or scale down your tablesâ€™ throughput capacity without downtime or performance degradation. You can use the AWS Management Console to monitor resource utilization and performance metrics.&lt;/p&gt;

&lt;p&gt;DynamoDB allows you to delete expired items from tables automatically to help you reduce storage usage and the cost of storing data that is no longer relevant. For more information, see Expiring items by using DynamoDB Time to Live (TTL).&lt;/p&gt;

&lt;h1 id=&quot;features-of-dynamodb&quot;&gt;Features of DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;NoSQL primarily key-value (and document using JSON) database service&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Fully Managed Distributed Systems -&amp;gt; Stable Performance&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Managed&lt;/strong&gt; â€” provided â€˜as-a-Serviceâ€™ so users would not need to maintain the database&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Scalable&lt;/strong&gt; â€” automatically provision hardware on the backend, invisible to the user&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Durable&lt;/strong&gt; and highly available â€” multiple availability zones for failures/disaster recovery&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Integrates well with other AWS services&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Built-in support for ACID transactions&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Encryption at rest&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;On-demand backup&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Point-in-time recovery&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;components-of-dynamodb&quot;&gt;Components of DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Attribute&lt;/strong&gt;: &lt;strong&gt;single field&lt;/strong&gt; that is attached to an item. Key-value pair&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Item&lt;/strong&gt;: &lt;strong&gt;unique set of attributes&lt;/strong&gt; in a table. set of key-value pair&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Table&lt;/strong&gt;: &lt;strong&gt;group of items&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Primary Key&lt;/strong&gt;: The primary key &lt;strong&gt;uniquely identifies each item in the table&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;therefore, no two items can have the same key&lt;/li&gt;
      &lt;li&gt;primary key could be just &lt;strong&gt;partition key&lt;/strong&gt; or &lt;strong&gt;partition key + sort key&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Partiton Key&lt;/strong&gt;: key for determining physical storage in which the item will be stored
    &lt;ul&gt;
      &lt;li&gt;input to an internal hash function. the output from the hash function determines the partition&lt;/li&gt;
      &lt;li&gt;í•„ìˆ˜ ì§€ì •ê°’. primary keyë¡œ partition keyë§Œ ì§€ì •ë˜ë©´ partition keyëŠ” ê³ ìœ í•œ ê°’ì„ ê°€ì ¸ì•¼í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sort Key&lt;/strong&gt;: ë™ì¼í•œ íŒŒí‹°ì…˜ í‚¤ë¥¼ ê³µìœ í•˜ëŠ” ëª¨ë“  í•­ëª©ì„ ì •ë ¬í•˜ê±°ë‚˜ ê²€ìƒ‰í•˜ëŠ”ë° ì´ìš© (ì„ íƒ ì‚¬í•­)
    &lt;ul&gt;
      &lt;li&gt;partition key + sort key -&amp;gt; Referred to as a composite primary key&lt;/li&gt;
      &lt;li&gt;All items with the same partition key value are stored together, in sorted order by sort key value.&lt;/li&gt;
      &lt;li&gt;In a table that has a partition key and a sort key, itâ€™s possible for multiple items to have the same partition key value. However, those items must have different sort key values.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Secondary Indexes&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;You can create one or more secondary indexes on a table. A secondary index lets you query the data in the table using an alternate key, in addition to queries against the primary key. DynamoDB doesnâ€™t require that you use indexes, but they give your applications more flexibility when querying your data. After you create a secondary index on a table, you can read data from the index in much the same way as you do from the table.&lt;/li&gt;
      &lt;li&gt;Global secondary index â€“ An index with a partition key and sort key that can be different from those on the table.&lt;/li&gt;
      &lt;li&gt;Local secondary index â€“ An index that has the same partition key as the table, but a different sort key.&lt;/li&gt;
      &lt;li&gt;In the example Music table shown previously, you can query data items by Artist (partition key) or by Artist and SongTitle (partition key and sort key). What if you also wanted to query the data by Genre and AlbumTitle? To do this, you could create an index on Genre and AlbumTitle, and then query the index in much the same way as youâ€™d query the Music table.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;storing-data-in-dynamodb&quot;&gt;Storing Data in DynamoDB&lt;/h1&gt;

&lt;p&gt;A key-value store holds for each key a single value. Arguably, if the value can be an entire document, you can call this database a â€œdocument storeâ€. In this sense, DynamoDB is a document store. The DynamoDB API lets you conveniently store a JSON document as the value, and also read or writes part of this document directly instead of reading or writing the entire document (although, you actually pay for reading and writing the entire document).&lt;/p&gt;

&lt;p&gt;ë³´í†µì€ Key-value storeë¡œ ì“´ë‹¤. Document storeë¡œ ì‚¬ìš©í•˜ê³  ì‹¶ì€ ê²½ìš° MongoDBì˜ AWS ë²„ì „ì¸ DocumentDBë¥¼ ì“°ëŠ” ê²ƒì„ ì¶”ì²œí•œë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DynamoDB is a â€œwide columnâ€ style of NoSQL database. While the schema isnâ€™t defined beyond the primary key at table construction time, the querying abilities are limited to primary keys or secondary indexes. Creating Global Secondary Indexes allows you to query against other attribute values. Local Secondary Indexes can be queried too, but theyâ€™re a bit of an odd duck. See here for a good comparison of the two secondary index types.&lt;/p&gt;

&lt;p&gt;If your needs do include querying inside the attributes, check out some of the â€œdocument-orientedâ€ style of NoSQL databases, of which MongoDB is the one most people think of. If youâ€™re already embedded in the AWS ecosystem and donâ€™t want to break out of it, AWS offers DocumentDB as a MongoDB-compatible service managed by AWS.&lt;/p&gt;

&lt;p&gt;Wide-column and document-style data stores have different proâ€™s &amp;amp; cons. Generally-speaking, the wide-column approach is better for extreme scalability at consistent cost &amp;amp; speed, whereas the document-oriented approach gives more flexibility as your data access patterns evolve over time. Choose the one that suits your needs the best.&lt;/p&gt;

&lt;h1 id=&quot;querying-data-in-dynamodb&quot;&gt;Querying Data in DynamoDB&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;PartiQL (SQL like)&lt;/li&gt;
  &lt;li&gt;Primary Key ë˜ëŠ” Global Secondary Indexes(GSI)ì— ëŒ€í•´ì„œë§Œ ì¿¼ë¦¬ ê°€ëŠ¥&lt;/li&gt;
  &lt;li&gt;FilterëŠ” ì¿¼ë¦¬ ì´í›„ ê²°ê³¼ë¥¼ ì œí•œí•˜ëŠ” ìš©ë„&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì¼ë°˜ Attributeì— ëŒ€í•´ì„œëŠ” ì¿¼ë¦¬ê°€ ì•ˆëœë‹¤.&lt;/p&gt;

&lt;p&gt;ì•„ë˜ì™€ ê°™ì´ ì›í•˜ëŠ” Attributeë¥¼ GSIë¡œ ë§Œë“¤ê³  ë‚˜ë©´ ì¿¼ë¦¬ê°€ ê°€ëŠ¥í•´ì§„ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;configuration-in-dynamodb&quot;&gt;Configuration in DynamoDB&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ìœ„ì˜ ì„¤ì •ì€ ìš°ì„  ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ í…Œì´ë¸”ì„ ë§Œë“ ë’¤ ì´í›„ ì„¤ì •ê°’ìœ¼ë¥´ ìˆ˜ì •í•  ìˆ˜ ë„ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ ë°–ì— í…Œì´ë¸” ìƒì„± ì´í›„ ì„¤ì •í•  ìˆ˜ ìˆëŠ” ì„¤ì •ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/dynamo_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;use-case&quot;&gt;Use Case&lt;/h1&gt;

&lt;p&gt;In this tutorial, you will create a bookstore application that showcases a product catalog. Products typically contain unique identifiers and attributes such as descriptions, quantities, locations, and prices. The method for retrieving these types of attributes (specifically, the access pattern) is often a key-value lookup based on the productâ€™s unique identifier. This means that an application can retrieve these other attributes when a productâ€™s unique identifier is provided.&lt;/p&gt;

&lt;p&gt;While the product catalog can start off with a few products, it should have the ability to scale to billions if needed without having to be re-architected or requiring a different database. It should also maintain fast, predictable performance at any scale for these key-value lookups. With these requirements in mind, Amazon DynamoDB is a good fit as the durable system of record for the bookstore because it offers low latency performance and scales as your application grows. Another benefit is that you do not need to manage any servers or clusters.&lt;/p&gt;

&lt;h1 id=&quot;pricing&quot;&gt;Pricing&lt;/h1&gt;
&lt;p&gt;DynamoDB can be extremely expensive to use. There are two pricing structures to choose from: provisioned capacity and on-demand capacity.&lt;/p&gt;

&lt;h2 id=&quot;dynamodb-provisioned-capacity&quot;&gt;DynamoDB Provisioned Capacity&lt;/h2&gt;
&lt;p&gt;In this Amazon DynamoDB Pricing Plan, youâ€™re billed hourly per the use of operational capacity units (or read and write capacity units). You can control costs by specifying the maximum amount of resources needed by each database table being managed. The provisioned capacity provides autoscaling and dynamically adapts to an increase in traffic. However, it does not implement autoscaling for sudden changes in data traffic unless thatâ€™s enabled.&lt;/p&gt;

&lt;p&gt;You should use provisioned capacity when:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You have an idea of the maximum workload your application will have&lt;/li&gt;
  &lt;li&gt;Your applicationâ€™s traffic is consistent and does not require scaling (unless you enable the autoscaling feature, which costs more)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;dynamodb-on-demand-pricing&quot;&gt;DynamoDB On-demand Pricing&lt;/h2&gt;
&lt;p&gt;This plan is billed per request units (or read and write request units). Youâ€™re only charged for the requests you make, making this a truly serverless choice. This choice can become expensive when handling large production workloads, though. The on-demand capacity method is perfect for autoscaling if youâ€™re not sure how much traffic to expect.&lt;/p&gt;

&lt;p&gt;Knowing which capacity best suits your requirements is the first step in optimizing your costs with DynamoDB. Here are some factors to consider before making your choice.&lt;/p&gt;

&lt;p&gt;You should use on-demand capacity when:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Youâ€™re not sure about the workload your application will have&lt;/li&gt;
  &lt;li&gt;You donâ€™t know how consistent your applicationâ€™s data traffic will be&lt;/li&gt;
  &lt;li&gt;You only want to pay for what you use&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.scylladb.com/learn/dynamodb/introduction-to-dynamodb/&quot; target=&quot;_blank&quot;&gt;scylladb, Introduction to DynamoDB [ì¶”ì²œ]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=2k2GINpO308&amp;amp;list=LL&amp;amp;index=14&quot; target=&quot;_blank&quot;&gt;Youtube, Be A Better Dev: AWS DynamoDB Tutorial For Beginners&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/HowItWorks.CoreComponents.html&quot; target=&quot;_blank&quot;&gt;AWS docs, Core components of Amazon DynamoDB&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/67646412/how-is-it-possible-for-dynamodb-to-support-both-key-value-and-document-database&quot; target=&quot;_blank&quot;&gt;stackoverflow: How is it possible for DynamoDB to support both Key-Value and Document database properties at the same time&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/47521164/aws-dynamodb-query-based-on-non-primary-keys&quot; target=&quot;_blank&quot;&gt;stackoverflow: AWS DynamoDB Query based on non-primary keys&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cloudforecast.io/blog/dynamodb-pricing/&quot; target=&quot;_blank&quot;&gt;cloudforecast: DynamoDB Pricing and Cost Optimization Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series7</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series7</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>AWS Series [Part10]: AWS Analytics Service: Glue, Athena</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aws-glue&quot; id=&quot;markdown-toc-aws-glue&quot;&gt;AWS Glue&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#aws-athena&quot; id=&quot;markdown-toc-aws-athena&quot;&gt;AWS Athena&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#s3--glue-catalog--athena--glue-etl-í™˜ìƒì˜-ì¡°í•©&quot; id=&quot;markdown-toc-s3--glue-catalog--athena--glue-etl-í™˜ìƒì˜-ì¡°í•©&quot;&gt;S3 + Glue Catalog + Athena + Glue ETL: í™˜ìƒì˜ ì¡°í•©&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#glue-catalog&quot; id=&quot;markdown-toc-glue-catalog&quot;&gt;Glue Catalog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#glue-etl&quot; id=&quot;markdown-toc-glue-etl&quot;&gt;Glue ETL&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;ë°ì´í„°ì™€ ê´€ë ¨ëœ ì—…ë¬´ë¥¼ í•˜ë‹¤ë³´ë©´ ì‹¤ì œë¡œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³ , ë¨¸ì‹ ëŸ¬ë‹ê³¼ ê°™ì€ ë¶„ì•¼ì— í™œìš©í•˜ëŠ” ì‹œê°„ì€ ì•½ 30%, ë‚˜ë¨¸ì§€ ì‹œê°„ì€ ìˆ˜ì§‘, ì ì¬, ë³€í™˜ê³¼ ê°™ì€ ETL ì‘ì—…ì— ëŒ€ë¶€ë¶„ì˜ ì‹œê°„ì„ í• ì• í•˜ê²Œ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ì²« ë²ˆì§¸ ë¬¸ì œ(ë‚®ì€ ë°ì´í„° í’ˆì§ˆ)ë¥¼ í•´ê²°í•˜ë„ë¡ ë„ì™€ì£¼ëŠ” AWS ì„œë¹„ìŠ¤ì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;aws-glue&quot;&gt;AWS Glue&lt;/h1&gt;

&lt;p&gt;AWS Glue is a fully managed ETL service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores and data streams.&lt;/p&gt;

&lt;p&gt;AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog, an ETL engine that automatically generates Python or Scala code, and a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so thereâ€™s no infrastructure to set up or manage.&lt;/p&gt;

&lt;p&gt;No schema is required initially. With dynamic frames, you get schema flexibility and a set of advanced transformations specifically designed for dynamic frames. You can convert between dynamic frames and Spark dataframes, so that you can take advantage of both AWS Glue and Spark transformations to do the kinds of analysis that you want&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;aws-athena&quot;&gt;AWS Athena&lt;/h1&gt;

&lt;p&gt;Amazon Athena is an interactive query service that makes it easy to analyze data directly in Amazon Simple Storage Service (Amazon S3) using standard SQL. With a few actions in the AWS Management Console, you can point Athena at your data stored in Amazon S3 and begin using standard SQL to run ad-hoc queries and get results in seconds.&lt;/p&gt;

&lt;p&gt;Athena is serverless, so there is no infrastructure to set up or manage, and you pay only for the queries you run. Athena scales automaticallyâ€”running queries in parallelâ€”so results are fast, even with large datasets and complex queries.&lt;/p&gt;

&lt;h1 id=&quot;s3--glue-catalog--athena--glue-etl-í™˜ìƒì˜-ì¡°í•©&quot;&gt;S3 + Glue Catalog + Athena + Glue ETL: í™˜ìƒì˜ ì¡°í•©&lt;/h1&gt;

&lt;p&gt;ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¡œ ê°€ê¸° ì „ ë‹¨ê³„ì— í™œìš©í•˜ê¸° ì¢‹ì€ ê²ƒ ê°™ë‹¤. ë³´í†µ S3ì™€ ê°™ì€ ë°ì´í„° ë ˆì´í¬ì—ëŠ” raw-dataê°€ ë§ê¸° ë•Œë¬¸ì— ì´ëŸ¬í•œ ë°ì´í„°ë¥¼ í´ë Œì§•í•˜ê³ , ê°€ê³µí•˜ëŠ” ê³¼ì •ì´ ìˆ˜ë°˜ë˜ì–´ì•¼ í•˜ëŠ”ë° ì´ëŸ¬í•œ ì‘ì—…ë“¤ì„ AWS Glueê°€ í•´ì¤€ë‹¤.&lt;/p&gt;

&lt;p&gt;ë˜í•œ S3ì˜ ë°ì´í„°ë¥¼ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì˜®ê¸°ê¸° ì „ì— ë¨¼ì € ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ì€ ê²½ìš°ê°€ ë§ë‹¤. ì–´ë–¤ ë°ì´í„°ê°€ ìˆê³ , ìŠ¤í‚¤ë§ˆê°€ ì–´ë–»ê³ , ì–´ë–¤ ë°ì´í„°ë¥¼ ì˜®ê¸°ë©´ ì¢‹ì„ì§€, ì–´ë–¤ ë°ì´í„°ê°€ ê°€ì¹˜ê°€ ìˆì„ì§€ë¥¼ ë¨¼ì € S3ì—ì„œ ì¶©ë¶„íˆ íƒìƒ‰í•´ì•¼ í•œë‹¤. ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ í•˜ëŠ” ê²ƒì´ ë°”ë¡œ AWS Athenaì´ë‹¤. Athenaë¥¼ ì´ìš©í•˜ë©´ ì¿¼ë¦¬ë¥¼ í†µí•´ S3ì˜ ë°ì´í„°ë¥¼ íƒìƒ‰/ë¶„ì„í•  ìˆ˜ ìˆë‹¤. ê·¼ë° AthenaëŠ” ë°˜ë“œì‹œ Data Catalogì—ì„œ ì¿¼ë¦¬ë¥¼ ì§„í–‰í•œë‹¤. (ì¹´íƒˆë¡œê·¸(Catalog): ë°ì´í„°ì— ëŒ€í•œ í•˜ë‚˜ì˜ ë‹¨ì¼í™”ëœ ë·°)&lt;/p&gt;

&lt;p&gt;ë”°ë¼ì„œ S3ë¥¼ ë‹¤ë¥¸ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¡œ ì˜®ê¸°ê¸° ì „ì— ë¨¼ì € Glueë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì¹´íƒˆë¡œê·¸í™” í•˜ê³ , ê·¸ ì¹´íƒˆë¡œê·¸ë¥¼ Athenaë¥¼ ì´ìš©í•´ ë¶„ì„í•˜ê³  ë‹¤ì‹œ Glueì˜ ETL ì‘ì—…ì„ í†µí•´ í´ë Œì§•, ê°€ê³µí•´ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¡œ ì˜®ê²¨ì£¼ëŠ” ê²ƒì´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì˜ ì¢‹ì€ ì˜ˆì´ë‹¤.&lt;/p&gt;

&lt;p&gt;ì°¸ê³ ë¡œ í¬ë¡¤ëŸ¬ë¥¼ í†µí•´ ì¹´íƒˆë¡œê·¸í™” ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°ì´í„° ì†ŒìŠ¤ëŠ” S3ë¿ë§Œ ì•„ë‹ˆë¼ DynamoDB, DocumentDB, DataLake ë“±ì´ ìˆë‹¤.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;glue-catalog&quot;&gt;Glue Catalog&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;í¬ë¡¤ëŸ¬ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ì‹œí‚´ìœ¼ë¡œì¨ ìŠ¤í‚¤ë§ˆ ë³€ê²½ì„ ê°ì§€í•˜ê³  ê´€ë¦¬í•´ì¤Œ&lt;/li&gt;
  &lt;li&gt;ìŠ¤í‚¤ë§ˆì˜ ë²„ì „ì„ ê´€ë¦¬í•˜ê³  í•´ë‹¹ ìŠ¤í‚¤ë§ˆì— ë§ëŠ” ì¼ê´€ëœ ë°ì´í„° ë·° ì œê³µ(ëŒ€í‘œì ìœ¼ë¡œ Athena, EMR, Redshiftì— ì œê³µ)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;glue-etl&quot;&gt;Glue ETL&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;ETL ì‘ì—…ì„ ê·¸ë˜í”„ë¡œ ì‹œê°í™”í•˜ì—¬ ì‰½ê²Œ íŒŒì´í”„ë¼ì¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ&lt;/li&gt;
  &lt;li&gt;ìŠ¤ì¼€ì¤„ë§ ê¸°ëŠ¥ë„ ìˆìŒ&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/images/glue_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(ì´ìƒí•˜ê²Œ S3ì— ì €ì¥í•˜ëŠ” ë¶€ë¶„ì—ì„œ ë°ì´í„°ê°€ ì‚¬ë¼ì§.. ì‹¤ì œë¡œ S3ì— ì €ì¥ì€ ë˜ì§€ë§Œ í¬ê¸°ê°€ 0Byte..)&lt;/p&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.aws.amazon.com/glue/latest/dg/what-is-glue.html&quot; target=&quot;_blank&quot;&gt;AWS docs, What is AWS Glue?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=LkkgtNtuEoU&amp;amp;list=LL&amp;amp;index=24&quot; target=&quot;_blank&quot;&gt;Youtube, AWS Korea: AWS Glueë¥¼ í†µí•œ ì†ì‰¬ìš´ ë°ì´í„° ì „ì²˜ë¦¬ ì‘ì—…í•˜ê¸°&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=dQnRP6X8QAU&amp;amp;list=LL&amp;amp;index=17&quot; target=&quot;_blank&quot;&gt;Youtube, Johny Chivers: AWS Glue Tutorial for Beginners&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Thu, 11 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/aws-series10</link>
                <guid isPermaLink="true">http://localhost:4000/aws-series10</guid>
                
                <category>AWS</category>
                
                
                <category>Cloud</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part27]: Redis</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Redis: an open source, in-memory key-value database that supports abstract data structures like lists, hashes, strings, and sets&lt;/p&gt;

&lt;p&gt;in this article iâ€™m going to be covering everything you need to know about redis. starting with what redis is moving on to how to install it all of the commands you need to know and then finally ending up with a real world example of how you would implement redis yourself. this is a really important video to watch because redis can be included in every single production level application and itâ€™s going to make your application more performant. so letâ€™s get started now.&lt;/p&gt;

&lt;p&gt;redis is essentially a type of database and more specifically it is a nosql database. but itâ€™s not very similar to any other nosql databases. itâ€™s definitely not similar to mongodb and itâ€™s obviously very different than sql databases like postgres and mysql and this is because redis doesnâ€™t really have any idea of tables or documents and instead all of the data in redis is
stored inside of key value pairs. so think about a json object you have the key name and you have the value kyle. this is essentially a key value pair and redis is just like one giant json object that has key value pairs and thatâ€™s all that you have inside of redis. so itâ€™s not very good at storing a bunch of structured data like you have in sql .but itâ€™s really good for storing you know individual key value pairs that you need to access or get data from another.&lt;/p&gt;

&lt;p&gt;important thing to note about redis is that unlike a normal database that runs on your disk and stores all your information on disk. redis actually runs inside of your working memory your ram on your computer and this means that redis is incredibly fast because itâ€™s all working inside of ram. but itâ€™s much more unstable because if all of a sudden your system crashes youâ€™re going to lose everything thatâ€™s in redis unless youâ€™re backing it up consistently, which is why redis is generally not used as like an actual persistent database store like you have with mongodb and postgres and instead itâ€™s used more &lt;strong&gt;for caching&lt;/strong&gt; where you take things that are really you know things that you access a lot or things that take a long time to compute and you store those values inside of redis that way when you need to access them in the future. itâ€™s incredibly quick since redis is in the memory already loaded. itâ€™s milliseconds to get data as opposed to hundreds of milliseconds or even seconds of time to get data from a traditional database.&lt;/p&gt;

&lt;p&gt;really the important thing to realize about redis is that itâ€™s going to be built on top of a traditional database. almost always youâ€™re going to have your mongodb or postgres database in the background and youâ€™re going to have redis sitting in front of that database and any time that you have a really long or slow query to your database or you have data you access all the time but doesnâ€™t change that much what youâ€™re going to do is youâ€™re going to store that data inside of redis as well as inside your database. and then when you go to get that information if itâ€™s already in redis you can access that data in milliseconds as opposed to going all the way to the database. computing the data and then coming all the way back which is going to take you hundreds to even thousands of milliseconds depending on how complex your data is. so redis is going to take your app and make it hundreds to even thousands of times faster when it comes to querying these pieces of information.&lt;/p&gt;

&lt;p&gt;letâ€™s actually talk about how we can install redis. installing redis on your computer is really simple if you have a mac or linux computer. if you use mac just use homebrew to do the install and if youâ€™re on linux just use your package manager of choice to install it. itâ€™s just called redis itâ€™s that simple. but if youâ€™re on windows itâ€™s a bit more complex. because there is no way to install redis on windows. instead you need to go through the windows subsystem for linux which is pretty simple to install.&lt;/p&gt;
</description>
                <pubDate>Tue, 02 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series27</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series27</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part26]: I ğŸ¤ Logs(2) Data Integration</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#two-complications&quot; id=&quot;markdown-toc-two-complications&quot;&gt;Two Complications&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-is-more-diverse&quot; id=&quot;markdown-toc-data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#the-explosion-of-specialized-data-systems&quot; id=&quot;markdown-toc-the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#log-structured-data-flow&quot; id=&quot;markdown-toc-log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#experience-at-linkedin&quot; id=&quot;markdown-toc-experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#relationship-to-etl-and-data-warehouse&quot; id=&quot;markdown-toc-relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#etl-and-scalability&quot; id=&quot;markdown-toc-etl-and-scalability&quot;&gt;ETL and Scalability&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#where-should-we-put-the-data-transformations&quot; id=&quot;markdown-toc-where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#decoupling-systems&quot; id=&quot;markdown-toc-decoupling-systems&quot;&gt;Decoupling Systems&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#scaling-a-log&quot; id=&quot;markdown-toc-scaling-a-log&quot;&gt;Scaling a Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data Integration and Logs&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Data Integration means making available all the data (that an organization has) to all the services and systems that need it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The more recognizable term ETL(populating a relational data warehouse) usually covers only a limited part of data integration.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Capturing all the relevant data and being able to put it together in an applicable processing environment&lt;/li&gt;
  &lt;li&gt;This data has to be modeled in a uniform way to make it easy to read and process&lt;/li&gt;
  &lt;li&gt;Process this data in various ways: MapReduce, real-time query systems, ans so on&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;Focus on step-by-step&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Reliable and complete data flow&lt;/li&gt;
  &lt;li&gt;Refining data modeling and consistency&lt;/li&gt;
  &lt;li&gt;Better visualization, reporting, algorithmic processing and prediction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How can we build reliable data flow throughout all the data systems?&lt;/p&gt;

&lt;h1 id=&quot;two-complications&quot;&gt;Two Complications&lt;/h1&gt;

&lt;p&gt;Two things have made data integration an increasingly difficult proflem.&lt;/p&gt;

&lt;h2 id=&quot;data-is-more-diverse&quot;&gt;Data is More Diverse&lt;/h2&gt;

&lt;p&gt;Transactional data - things that &lt;strong&gt;are&lt;/strong&gt;,  Event data - things that &lt;strong&gt;happen&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Log = Data structure what event data is logged&lt;/p&gt;

&lt;p&gt;Event data is generated from Web service, Financial organization, IoT&lt;/p&gt;

&lt;p&gt;This type of event data shakes up traditional data integration approaches because it tends to be several orders of magnitude larger than transactional data.&lt;/p&gt;

&lt;h2 id=&quot;the-explosion-of-specialized-data-systems&quot;&gt;The Explosion of Specialized Data Systems&lt;/h2&gt;

&lt;p&gt;ex. OLAP, Search service, Batch processing, Graph analysis&lt;/p&gt;

&lt;h1 id=&quot;log-structured-data-flow&quot;&gt;Log-Structured Data Flow&lt;/h1&gt;

&lt;p&gt;Log is the natural problem data structure for handling data flow between systems.&lt;/p&gt;

&lt;h1 id=&quot;experience-at-linkedin&quot;&gt;Experience at LinkedIn&lt;/h1&gt;

&lt;h1 id=&quot;relationship-to-etl-and-data-warehouse&quot;&gt;Relationship to ETL and Data Warehouse&lt;/h1&gt;

&lt;h1 id=&quot;etl-and-scalability&quot;&gt;ETL and Scalability&lt;/h1&gt;

&lt;h1 id=&quot;where-should-we-put-the-data-transformations&quot;&gt;Where Should We Put the Data Transformations?&lt;/h1&gt;

&lt;h1 id=&quot;decoupling-systems&quot;&gt;Decoupling Systems&lt;/h1&gt;

&lt;h1 id=&quot;scaling-a-log&quot;&gt;Scaling a Log&lt;/h1&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.kyobobook.co.kr/product/detailViewEng.laf?barcode=9781491909386&amp;amp;ejkGb=BNT&amp;amp;mallGb=ENG&quot; target=&quot;_blank&quot;&gt;ì±… I Heart Logs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/logika-io/try-kill-batch-processing-with-unified-log-stream-processing-d92709117f74&quot; target=&quot;_blank&quot;&gt;Try Kill batch processing with unified log stream processingâ€¦&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series26</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series26</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Data Engineering Series [Part25]: I ğŸ¤ Logs(1) Introduction</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#what-is-a-log&quot; id=&quot;markdown-toc-what-is-a-log&quot;&gt;What Is a Log?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-database&quot; id=&quot;markdown-toc-logs-in-database&quot;&gt;Logs in Database&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#logs-in-distributed-system&quot; id=&quot;markdown-toc-logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#log-centric-design-pattern&quot; id=&quot;markdown-toc-log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;what-is-a-log&quot;&gt;What Is a Log?&lt;/h1&gt;

&lt;p&gt;Yet other than perhaps occasionally tailing a log file, most engineers donâ€™t think much about logs. To help remedy that, Iâ€™ll give an overview of how logs work in distributed systems, and then give some practical applications of these concepts to a variety of common uses: data integration, enterprise architecture, real-time data processing, and data system design.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[2022-07-02 05:30:44] method=POST user=crazyboy0510 path=/movies/comment-create/ movie_id=16
[2022-07-02 05:30:57] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:15] method=GET user=crazyboy0510 path=/movies/movie-play/16 movie_id=16
[2022-07-02 05:31:18] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
[2022-07-02 05:31:19] method=GET user=crazyboy0510 path=/movies/movie-play/7 movie_id=7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Every programmer is familiar with this kind of log - a series of loosely structured requests, errors, or other messages in a sequence of rotating text files.&lt;/p&gt;

&lt;p&gt;The purpose of logs quickly becomes an input to queries in order to understand behavior across many machines, something that English text in files is not nearly as appropriate for as the kind of structured log Iâ€™ll be talking about.&lt;/p&gt;

&lt;p&gt;The log Iâ€™ll be discussing is a little more general and closer to what in the database or systems called a â€˜commit logâ€™. It is append-only sequence of records ordered by time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Each rectangle represents a record that was appended to the log. Records are stored in the order they were appended. The contents and format of the records arenâ€™t important for the purposes of this discussion. To be concrete, we can just imagine each record to be a JSON blob.&lt;/p&gt;

&lt;p&gt;The log entry number can be thought of as the â€˜timestampâ€™ of the entry. this is convenient property of being decoupled from any particular physical clock. This property is essential as we get to distributed systems.&lt;/p&gt;

&lt;p&gt;A log is just kind of table or file where the records are sorted by time&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;table: array of records  
file: array of bytes  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However it is important that we thing about the log as an &lt;strong&gt;abstract data structure, not a text file&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Logs have a specific purpose: &lt;strong&gt;they record what happened and when&lt;/strong&gt;. For distributed data systems, this is the heart of the problem.&lt;/p&gt;

&lt;h1 id=&quot;logs-in-database&quot;&gt;Logs in Database&lt;/h1&gt;

&lt;p&gt;The usage in databases has to do with keeping in sync a variety of data structures and indexes in the presence of crashes. To make this atomic and durable, a database uses a log to write out information about the records it will be modifying before applying the changes to all the various data structures that it maintains.&lt;/p&gt;

&lt;p&gt;The log is the record of what happened, and each table or index is a projection of this history into some useful data structure or index.&lt;/p&gt;

&lt;p&gt;Over time, the usage of the log grew from &lt;strong&gt;an implementation detail of the ACID database properties&lt;/strong&gt; to a &lt;strong&gt;method for replicating data between databases&lt;/strong&gt;. It turns out that the sequence of changes that happened on the database is exactly what is needed to keep a remote replica database in sync. Oracle, MySQL, PostreSQL, and MongoDB include log shipping protocols to transmit portions of a log to replica databases that act as slaves. The slaves can then apply the changes recorded in the log to their own local data structures to stay in sync with the master.&lt;/p&gt;

&lt;p&gt;In fact, the use of logs is variations on the two uses in database internals:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The log is used as a publish/subscribe mechanism to transmit data to other replicas&lt;/li&gt;
  &lt;li&gt;The log is used as a consistency mechanism to order the updates that are applied to multiple replicas&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;logs-in-distributed-system&quot;&gt;Logs in Distributed System&lt;/h1&gt;

&lt;p&gt;The same problems that databases solve with logs (like distributing data to replicas and agreeing on update order) are among the most fundamental problems for all distributed systems.&lt;/p&gt;

&lt;p&gt;The log-centric approach to distributed systems arises from a simple observation&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;If two identical, deterministic processes begin in the same state and get the same inputs in the same order, they will produce the same output and end in the same state&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;(Desterministic means that the processing isnâ€™t timing dependent)&lt;/p&gt;

&lt;p&gt;The application to distributed computing is pretty obvious. You can reduce the problem of making multiple machines all do the same thing to the problem of implementaing a consistent log to feed input to theses processes. The purpose of the log here is to squeeze all the nondeterminism out of the input stream to ensure that each replica stays in sync.&lt;/p&gt;

&lt;p&gt;Discrete log entry numbers act as a clock for the state of the replicas - you can describe the state of each replica by a single number: the timestamp for the maximum log entry that it has processed. Two replicas at the same time will be in the same state.&lt;/p&gt;

&lt;h2 id=&quot;log-centric-design-pattern&quot;&gt;Log-Centric Design Pattern&lt;/h2&gt;

&lt;p&gt;There are many variations on how this principle can be applied., depending on what is put in the log. For example, we can log the incoming requests to a service and have each replica process these independently. Or we can have one instance that processed requests and log the state changes that the service undergoes in response to a request.&lt;/p&gt;

&lt;p&gt;Database people generally differentiate between physical and logical logging. Physical or row-based logging means logging the contents of each row that is changed (ë¡œìš°ë³„ ì‹¤ì œ ë³€ê²½ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê²ƒ). Logical or statement logging  means logging the SQL commands that lead to the row changes (insert, update, and delete statements).&lt;/p&gt;

&lt;p&gt;The distributed systems distinguished two broad approaches to processing and replication. The &lt;strong&gt;state machine model&lt;/strong&gt; keep a log of the incoming requests and &lt;strong&gt;each replica processes each request&lt;/strong&gt; in log order. &lt;strong&gt;primary backup&lt;/strong&gt; elect one replica as the leader. This leader processes requests in the order they arrive and logs the changes to its state that occur as a result of processing the requests.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/logs_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/data-engineering-series25</link>
                <guid isPermaLink="true">http://localhost:4000/data-engineering-series25</guid>
                
                <category>Data_Engineering</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>Coding Test Series [Part22]: í–‰ë ¬(Matrix) - ë¬¸ì œ</title>
                <description>&lt;hr /&gt;

&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#matrix&quot; id=&quot;markdown-toc-matrix&quot;&gt;Matrix&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;matrix&quot;&gt;Matrix&lt;/h1&gt;

&lt;p&gt;&lt;a href=&quot;https://leetcode.com/tag/matrix/&quot; target=&quot;_blank&quot;&gt;Leetcode: Two-Pointers&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;ğŸ’Ÿ âœ… â&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
---------------------------------------------EASY 3ë¬¸ì œ
- Richest Customer Wealth
- Flipping an Image
- Matrix Diagonal Sum
- The K Weakest Rows in a Matrix
- Toeplitz Matrix
- Shift 2D Grid
- Transpose Matrix

---------------------------------------------MEDIUM 5ë¬¸ì œ
- Sort the Matrix Diagonally
- Remove All Ones With Row and Column Flips
- Candy Crush
- Max Area of Island
- Rotate Image
- Sparse Matrix Multiplication
- Game of Life
- Construct Quad Tree
- Spiral Matrix II
- Walls and Gates
- Number of Islands
- Rotting Oranges
- Shortest Path in Binary Matrix
- Word Search
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

</description>
                <pubDate>Mon, 01 Aug 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/matrix</link>
                <guid isPermaLink="true">http://localhost:4000/matrix</guid>
                
                <category>Coding_Test</category>
                
                
                <category>CS</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part15] MySQLì˜ ë¡œê·¸</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#logì˜-ì¤‘ìš”ì„±&quot; id=&quot;markdown-toc-logì˜-ì¤‘ìš”ì„±&quot;&gt;Logì˜ ì¤‘ìš”ì„±&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mysqlì˜-ë¡œê·¸&quot; id=&quot;markdown-toc-mysqlì˜-ë¡œê·¸&quot;&gt;MySQLì˜ ë¡œê·¸&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#mysql-ëª¨ë‹ˆí„°ë§ì„-ìœ„í•œ-ë¡œê·¸&quot; id=&quot;markdown-toc-mysql-ëª¨ë‹ˆí„°ë§ì„-ìœ„í•œ-ë¡œê·¸&quot;&gt;MySQL ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë¡œê·¸&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#general-query-log&quot; id=&quot;markdown-toc-general-query-log&quot;&gt;General Query Log&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#slow-query-log&quot; id=&quot;markdown-toc-slow-query-log&quot;&gt;Slow Query Log&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#error-log&quot; id=&quot;markdown-toc-error-log&quot;&gt;Error Log&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#binary-log&quot; id=&quot;markdown-toc-binary-log&quot;&gt;Binary Log&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#íŠ¸ëœì­ì…˜-ì²˜ë¦¬ë¥¼-ìœ„í•œ-ë¡œê·¸&quot; id=&quot;markdown-toc-íŠ¸ëœì­ì…˜-ì²˜ë¦¬ë¥¼-ìœ„í•œ-ë¡œê·¸&quot;&gt;íŠ¸ëœì­ì…˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¡œê·¸&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#undo-log&quot; id=&quot;markdown-toc-undo-log&quot;&gt;Undo Log&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#redo-log&quot; id=&quot;markdown-toc-redo-log&quot;&gt;Redo Log&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#binary-log-1&quot; id=&quot;markdown-toc-binary-log-1&quot;&gt;Binary Log&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;logì˜-ì¤‘ìš”ì„±&quot;&gt;Logì˜ ì¤‘ìš”ì„±&lt;/h1&gt;

&lt;p&gt;MySQL is open-source relational databases, and you should learn how to use MySQL database logs to improve efficiency and security. It is crucial to understand how to diagnose and monitor the performance of a MySQL instance in the long run.&lt;/p&gt;

&lt;p&gt;While using a MySQL instance in production, you will come across issues like &lt;strong&gt;slow queries, deadlocks, and aborted connections&lt;/strong&gt;. &lt;strong&gt;Logging is essential to diagnosing these issues&lt;/strong&gt;. A good understanding of your MySQL database logs will help you improve operations by reducing the mean time to recovery and the mean time between failures. Logs are also key to detecting and diagnosing &lt;strong&gt;security issues&lt;/strong&gt; within your MySQL instance.&lt;/p&gt;

&lt;h1 id=&quot;mysqlì˜-ë¡œê·¸&quot;&gt;MySQLì˜ ë¡œê·¸&lt;/h1&gt;

&lt;p&gt;There are six types of log files in MySQL: redo log(WAL), undo log, binlog, error log, slow query log, general log, relay log. Redo logs and undo logs are closely related to transaction operations. Binlogs are also related to transaction operations. These three types of logs are important for understanding transaction operations in MySQL.&lt;/p&gt;

&lt;h1 id=&quot;mysql-ëª¨ë‹ˆí„°ë§ì„-ìœ„í•œ-ë¡œê·¸&quot;&gt;MySQL ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë¡œê·¸&lt;/h1&gt;

&lt;p&gt;While using a MySQL instance in production, you will come across issues like &lt;strong&gt;slow queries, deadlocks, and aborted connections&lt;/strong&gt;. &lt;strong&gt;Logging is essential to diagnosing these issues&lt;/strong&gt;. A good understanding of your MySQL database logs will help you improve operations by reducing the mean time to recovery and the mean time between failures.&lt;/p&gt;

&lt;p&gt;Logs are also key to detecting and diagnosing security issues within your MySQL instance. In the event of a compromise, logs track the details of an attack and the actions taken by the attackers. This information provides context to your data and helps you take remedial action.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- General Query Log
- Slow Query Log
- Error Log
- Binary Log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; show variables;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Enable Logging on MySQL&lt;/span&gt;
mysql&amp;gt;SET GLOBAL general_log &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; â€˜ONâ€™&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Logê°€ ì €ì¥ë˜ëŠ” íŒŒì¼ ê²½ë¡œ&lt;/span&gt;
mysql&amp;gt;SET GLOBAL general_log_file &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; â€˜path_on_your_systemâ€™&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-sh highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Enable Logging on MySQL&lt;/span&gt;
mysql&amp;gt;SET GLOBAL slow_query_log &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; â€˜ONâ€™&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Logê°€ ì €ì¥ë˜ëŠ” íŒŒì¼ ê²½ë¡œ&lt;/span&gt;
mysql&amp;gt;SET GLOBAL slow_query_log_file &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; â€˜path_on_your_systemâ€™&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;general-query-log&quot;&gt;General Query Log&lt;/h2&gt;

&lt;p&gt;As the name implies, the general query log is a general record of what MySQL is doing. Information is written to this log when clients connect or disconnect to the server. The server also logs each SQL statement it receives from clients. If you suspect an error in a client, you can know exactly what the client sent to the MySQL instance by looking at the general query log.&lt;/p&gt;

&lt;p&gt;You should be aware that MySQL writes statements to the general query log in the order in which it receives them. The order might differ from the order in which the queries are executed because, unlike other log formats, the query is written to this log file before MySQL even attempts to execute the query. MySQL database logs are therefore perfect for debugging MySQL crashes.&lt;/p&gt;

&lt;p&gt;Since the general query log is a record of every query received by the server, it can grow large quite quickly. If you only want a record of queries that change data, it might be better to use the binary log instead (more on that later).&lt;/p&gt;

&lt;h2 id=&quot;slow-query-log&quot;&gt;Slow Query Log&lt;/h2&gt;

&lt;p&gt;As applications scale in size, queries that were once extremely fast can become quite slow. When youâ€™re debugging a MySQL instance for performance issues, the slow query log is a good starting place to see which queries are the slowest and how often they are slow.&lt;/p&gt;

&lt;p&gt;The slow query log is the MySQL database log queries that exceed a given threshold of execution time. By default, all queries taking longer than 10 seconds are logged.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Configuration options&lt;/strong&gt;&lt;br /&gt;
You can change the threshold query execution time by setting the value of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;long_query_time&lt;/code&gt; system variable. It uses a unit of seconds, with an optional milliseconds component.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;long_query_time&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;To verify if the slow query log is working properly, you can execute the following query with a time greater than the value of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;long_query_time&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SLEEP&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Queries not using indexes are often good candidates for optimization. The &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;log_queries_not_using_indexes&lt;/code&gt; system variable can be switched on to make MySQL log all queries that do not use an index to limit the number of rows scanned. In this case, logging occurs regardless of execution time of the query.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SHOW&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variables&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;LIKE&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'slow%'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ìŠ¬ë¡œìš°&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ë¡œê·¸&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ì¡°íšŒí•˜ê¸°&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;ë˜ëŠ”&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;start_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_host&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;query_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lock_time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows_sent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows_examined&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;db&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;CONVERT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sql_text&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;USING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;utf8&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sql_text&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mysql&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;slow_log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;error-log&quot;&gt;Error Log&lt;/h2&gt;

&lt;p&gt;MySQL uses the error log to record diagnostic messages, warnings, and notes that occur during server startup and shutdown, and while the server is running. The error log also records MySQL startup and shutdown times.&lt;/p&gt;

&lt;p&gt;Error logging is always enabled. On Linux, if the destination is not specified, the server writes the error log to the console and sets the log_error system variable to stderr. On Windows, by default, the server writes the error log to the host_name.err file in the data directory. You can customize the path and file name of the error log by setting the value of the log_error system variable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Commonly logged errors&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Permission errors&lt;/li&gt;
  &lt;li&gt;Configuration errors&lt;/li&gt;
  &lt;li&gt;Out of memory errors&lt;/li&gt;
  &lt;li&gt;Errors with initiation or shutdown of plugins and InnoDB&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;binary-log&quot;&gt;Binary Log&lt;/h2&gt;

&lt;p&gt;The binary log is used by MySQL to record events that change the data within the tables or change the table schema itself. For example, binary logs record &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;INSERT&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DELETE&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UPDATE&lt;/code&gt; statements but not &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SELECT&lt;/code&gt; or &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SHOW&lt;/code&gt; statements that do not modify data. Binary logs also contain information about how long each statement took to execute.&lt;/p&gt;

&lt;p&gt;The logging order of a binary login is in contrast with that of the general query log. Events are logged only after the transaction is committed by the server.&lt;/p&gt;

&lt;p&gt;MySQL writes binary log files in binary format. To read their contents in text format, you need to use the mysqlbinlog utility. For example, you can use the code below to convert the contents of the binary log file named binlog.000001 to text.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mysql&amp;gt; mysqlbinlog binlog.0000001
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;MySQL database logs offer three formats for binary logging.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Statement-based logging: In this format, MySQL records the SQL statements that produce data changes. Statement-based logging is useful when many rows are affected by an event because it is more efficient to log a few statements than many rows.&lt;/li&gt;
  &lt;li&gt;Row-based logging: In this format, changes to individual rows are recorded instead of the SQL statements. This is useful for queries that require a lot of execution time on the source but result in just a few rows being modified.&lt;/li&gt;
  &lt;li&gt;Mixed logging: This is the recommended logging format. It uses statement-based logging by default but switches to row-based logging when required.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The binary logging format can be changed using the code below. However, you should note that it is not recommended to do so at runtime or while replication is ongoing.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binlog_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'STATEMENT'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binlog_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'ROW'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;SET&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;GLOBAL&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;binlog_format&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'MIXED'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Enabling binary logging on your MySQL instance will lower the performance slightly. However, the advantages discussed above generally outweigh this minor dip in performance.&lt;/p&gt;

&lt;h1 id=&quot;íŠ¸ëœì­ì…˜-ì²˜ë¦¬ë¥¼-ìœ„í•œ-ë¡œê·¸&quot;&gt;íŠ¸ëœì­ì…˜ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë¡œê·¸&lt;/h1&gt;

&lt;p&gt;ë°ì´í„°ë² ì´ìŠ¤ëŠ” ë³´í†µ íŠ¸ëœì­ì…˜ ìˆ˜í–‰ì„ ìœ„í•´ ë¡œê·¸ë¥¼ ì‚¬ìš©í•œë‹¤. ë³´í†µ ì´ëŸ¬í•œ ì—­í• ì„ ìˆ˜í–‰í•˜ëŠ” ë¡œê·¸ë¥¼ íŠ¸ëœì­ì…˜ ë¡œê·¸ë¼ê³  í•˜ë©° MySQLì˜ ê²½ìš° Redo Log, Undo Log, Binary Logê°€ ì´ì— í•´ë‹¹í•œë‹¤. Undo LogëŠ” íŠ¸ëœì­ì…˜ì˜ Atomicity, Redo LogëŠ” Durabilityë¥¼ ì œê³µí•´ì¤€ë‹¤.&lt;/p&gt;

&lt;p&gt;MySQLì€ ë””ìŠ¤í¬ì˜ I/Oìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ë¥¼ ì¤„ì´ê¸° ìœ„í•´ ìºì‹± ë©”ì»¤ë‹ˆì¦˜ì„ ì‚¬ìš©í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìˆ˜ì •í•˜ëŠ” ì¿¼ë¦¬ê°€ ë“¤ì–´ì˜¤ë©´, InnoDBëŠ” ë¨¼ì € ë©”ëª¨ë¦¬ì— ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ ë””ìŠ¤í¬ì—ì„œ ë¶ˆëŸ¬ì™€ ë©”ëª¨ë¦¬ì— ì˜¬ë¦¬ê³  ë°ì´í„°ë¥¼ ìˆ˜ì •í•œë‹¤. ì´ë ‡ê²Œ ë©”ëª¨ë¦¬ì—ì„œë§Œ ê³„ì† ì½ê³  ì“°ê²Œë˜ë©´ ì¥ì• ë¡œ ì„œë²„ê°€ ì¢…ë£Œë  ë•Œ ë°ì´í„°ê°€ ë‚ ì•„ê°€ê²Œ ëœë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ MySQLì—ì„œëŠ” Redo Logë¥¼ ì‚¬ìš©í•œë‹¤. ë” ìì„¸í•œ ë‚´ìš©ì€ ë°‘ì—ì„œ ì‚´í´ë³¼ ê²ƒì´ë‹¤. ë˜í•œ ë°ì´í„° ìˆ˜ì • ì¿¼ë¦¬ê°€ ë“¤ì–´ì˜¬ ë•Œë§ˆë‹¤ ë©”ëª¨ë¦¬ì˜ ë°ì´í„°ë¥¼ ë°”ë¡œ ìˆ˜ì •í•˜ì§€ëŠ” ì•Šê³ , ì´ ì „ ê°’ì„ Undo Logì— ë³´ê´€í•´ë‘ê³  ìˆ˜ì •ì„ ê°€í•¨ìœ¼ë¡œì¨, íŠ¸ëœì­ì…˜ì´ ì‹¤íŒ¨í•  ê²½ìš° ë°ì´í„°ë¥¼ ë¡¤ë°±(Roll back)í•  ì¤€ë¹„ë¥¼ í•œë‹¤. ì´ ë‚´ìš©ë„ ë°‘ì—ì„œ ë” ìì„¸íˆ ì‚´í´ë³´ì.&lt;/p&gt;

&lt;p&gt;(ì°¸ê³ ë¡œ ë””ìŠ¤í¬ì— ìˆëŠ” íŒŒì¼ì¤‘ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” íŒŒì¼ì„ ë°ì´í„° íŒŒì¼, ë¡œê·¸ë¥¼ ì €ì¥í•˜ë¥¼ íŒŒì¼ì„ ë¡œê·¸ íŒŒì¼ì´ë¼ê³  í•¨)&lt;/p&gt;

&lt;p&gt;(ë©”ëª¨ë¦¬ì— ìˆëŠ” ë²„í¼ì¤‘ ë°ì´í„° í˜ì´ì§€ë¥¼ ìºì‹œí•´ë†“ëŠ” ìœ„ì¹˜ë¥¼ ë°ì´í„° ë²„í¼, ë¡œê·¸ë¥¼ ìºì‹œí•´ë†“ëŠ” ìœ„ì¹˜ë¥¼ ë¡œê·¸ ë²„í¼ë¼ê³  í•¨)&lt;/p&gt;

&lt;p&gt;(InnoDBì—”ì§„ì´ ë°ì´í„°ë¥¼ ì½ê³  ì“¸ ë•ŒëŠ” í•´ë‹¹í•˜ëŠ” ë°ì´í„°ì™€ ì¸ë±ìŠ¤ë¥¼ ë©”ëª¨ë¦¬ ë²„í¼ í’€ì— ì˜¬ë ¤ë‘”ë‹¤.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Memory Buffer pool&lt;/strong&gt;: occupies the largest block of memory. The cache used to store various data includes index pages, data pages, undo pages, insert buffers, adaptive hash indexes, lock information stored in innodb, data dictionary information, etc. The working method always reads the database file into the buffer pool by page (16k per page), and then retains the cached data in the buffer pool according to the least recently used (lru) algorithm. If the database file needs to be modified, always modify the page in the buffer pool first (dirty page after the modification occurs), and then flush the dirty page of the buffer pool to the file at a certain frequency.&lt;/p&gt;

&lt;h2 id=&quot;undo-log&quot;&gt;Undo Log&lt;/h2&gt;

&lt;p&gt;Undo log is to achieve atomicity of transactions. Undo Log is also used to implement multi-version concurrency control (referred to as: MVCC).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Undo logëŠ” íŠ¸ëœì­ì…˜ì˜ Atomicity íŠ¹ì„±ì„ ì§€í‚¤ê¸° ìœ„í•œ ë¡œê·¸&lt;/li&gt;
  &lt;li&gt;Undo logëŠ” MVCC(multi-version concurrency control)ë¥¼ ìœ„í•´ì„œë„ ì‚¬ìš©&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Internal mechanism of delete/update operation&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When the transaction is not committed, InnoDB will not delete the undo log immediately, because the undo log may be used later. For example, when the isolation level is repeatable read, the transaction reads the latest committed row version when the transaction is started, as long as the transaction is not over, the row version cannot be deleted, that is, the undo log cannot be deleted.&lt;/p&gt;

&lt;p&gt;But when the transaction is committed, the undo log corresponding to the transaction will be put into the delete list, and will be deleted by purge in the future.&lt;/p&gt;

&lt;p&gt;The principle of Undo Log is very simple. In order to satisfy the atomicity of transactions, before operating any data, first back up the data to a place (this place where data backup is stored is called Undo Log). Then modify the data. If an error occurs or the user executes a ROLLBACK statement, the system can use the backup in Undo Log to restore the data to the state before the transaction started.&lt;/p&gt;

&lt;p&gt;Suppose there are two data, A and B, with values 1, 2 respectively. Perform a +2 transaction operation. A. The transaction begins.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- Record A=1 to undo log. 
- Modify A=3. 
- Record B=2 to undo log. 
- Modify B=4. 
- Write undo log to disk. 
- Write data to disk. 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The reason why atomicity and persistence can be guaranteed at the same time is because of the following characteristics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Record Undo log before updating data.&lt;/li&gt;
  &lt;li&gt;In order to ensure durability, data must be written to disk before the transaction is committed. As long as the transaction is successfully submitted, the data must have been persisted.&lt;/li&gt;
  &lt;li&gt;Undo log must be persisted to disk before data. If the system crashes between transaction, the undo log is complete and can be used to roll back the transaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;redo-log&quot;&gt;Redo Log&lt;/h2&gt;

&lt;p&gt;Redo log is to save the executed SQL statement to a specified Log file. When mysql performs data recovery, you can re-execute the SQL operation recorded by the redo log. The introduction of the buffer pool will cause the updated data to not be persisted to the disk in real time. When the system crashes, although the data in the buffer pool is lost and the data is not persisted, the system can restore all data to the latest according to the content of the Redo Log status. The redo log exists as a separate file on the disk. There will be two files by default, named ib_logfile0 and ib_logfile1.&lt;/p&gt;

&lt;p&gt;The parameter innodb_log_file_size specifies the size of the redo log innodb_log_file_in_group specifies the number of the redo log, and the default is 2; innodb_log_group_home_dir specifies the path where the redo log is located.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;innodb_additional_mem_pool_size = 100M
innodb_buffer_pool_size = 128M
innodb_data_home_dir =/home/mysql/local/mysql/var
innodb_data_file_path = ibdata1:1G: autoextend
innodb_file_io_threads = 4
innodb_thread_concurrency = 16
innodb_flush_log_at_trx_commit = 1
innodb_log_buffer_size = 8M
innodb_log_file_size = 128M
innodb_log_file_in_group = 2
innodb_log_group_home_dir =/home/mysql/local/mysql/var
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In order to satisfy the atomicity of the transaction, before operating any data, first back up the data to Undo Log, and then modify the data. If an error occurs or the user executes the ROLLBACK statement, the system can use the backup in Undo Log to restore the data to the state before the transaction started. Unlike redo log, there is no separate undo log file on the disk, it is stored in a special segment (segment) inside the database, which is called the undo segment, and the undo segment is located in the shared table space.&lt;/p&gt;

&lt;p&gt;The undo log and redo log itself are separate. Innodbâ€™s undo log is recorded in the data file (ibd), and innodb regards the content of the undo log as data, so the operation of the undo log itself (such as inserting an undo record into the undo log, etc.) will record redo log. The undo log does not need to be persisted to disk immediately. Even if it is lost, it can be restored through redo log. So when inserting a record:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Insert an undo log record into the undo log.&lt;/li&gt;
  &lt;li&gt;Insert a redo log record of â€œinsert undo log recordâ€ into the redo log.&lt;/li&gt;
  &lt;li&gt;Insert data.&lt;/li&gt;
  &lt;li&gt;Insert an â€œinsertâ€ redo log record into the redo log.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Redo log io performance&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;In order to ensure that Redo Log has better IO performance, InnoDBâ€™s Redo Log is designed with the following features:&lt;/p&gt;

&lt;p&gt;Try to keep Redo Log stored in a continuous space. Therefore, the log file space is completely allocated when the system is first started. The Redo Log is recorded in sequential addition.&lt;br /&gt;
Write logs in batches. The log is not written directly to the file, but first written to the redo log buffer, and then the data in the buffer is written to the disk every second
Concurrent transactions share the storage space of Redo Log, and their Redo Logs are recorded together alternately according to the execution order of statements to reduce the space occupied by the log.&lt;br /&gt;
Redo Log only performs sequential append operations. When a transaction needs to be rolled back, its Redo Log records will not be deleted from Redo Log.&lt;/p&gt;

&lt;p&gt;Contrary to Undo Log, Redo Log records a backup of new data. Before the transaction is committed, only the Redo Log needs to be persisted, and the data does not need to be persisted. When the system crashes, although the data is not persisted, Redo Log has persisted. The system can restore all data to the latest state according to the content of Redo Log.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Simplified process of Undo + Redo transaction&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A. Transaction start. B. Record A=1 to undo log. 
C. Modify A=3. 
D. Record A=3 to redo log. 
E. Record B=2 to undo log. 
F. Modify B=4. 
G . Record B=4 to redo log. 
H. Write redo log to disk. 
I. Transaction commit
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The AG process is carried out in memory, and the corresponding operations are recorded in redo log buffer (B&amp;amp;E), redo log buffer (E&amp;amp;G), transaction execution results (not submitted at this time) are also stored in db buffer (C&amp;amp;F), and the buffer is full If the number of transactions stored in the buffer is all 1, it means that the log is flushed to the disk immediately, and the data consistency is well guaranteed. If there are multiple storages, the redo log will be synchronized to the disk after a transaction is completed and there will be a status bit to record whether it is committed, then the transaction is actually committed, and the data in the db buffer will be synchronized to the disk of the DB. . To ensure that the contents of the db buffer are written to the disk database file, the contents of the log buffer should be written to the disk log file. This approach can reduce disk IO and increase throughput. However, this method is suitable for scenarios where consistency is not high. Because if there is a system failure such as a power failure, the completed transactions in the log buffer and db buffer have not been synchronized to the disk will be lost. For banks such as banks that require higher transaction consistency, it is necessary to ensure that each transaction is recorded to the disk. If the server is down, go to the redo log to recover and redo the committed transaction.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;the role of redo &amp;amp; undo log&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data persistence
    &lt;ul&gt;
      &lt;li&gt;The buffer pool maintains a linked list in the order of dirty page modification, called flush_list. Flush data to persistent storage according to the order of pages in flush_list. The pages are arranged in the order of the earliest modification. Under normal circumstances, when is the dirty page flushed to the disk?
        &lt;ul&gt;
          &lt;li&gt;When the redo space is full, part of the dirty page will be flushed to the disk, and then part of the redo log will be released.&lt;/li&gt;
          &lt;li&gt;When you need to allocate a page in the Buffer pool, but it is full, you must flush dirty pages to disk. Generally, this situation can be controlled by the startup parameter innodb_max_dirty_pages_pct. When the dirty page in the buffer pool reaches this ratio, the dirty page is flushed to the disk.&lt;/li&gt;
          &lt;li&gt;When the system is detected to be idle, it will flush.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Data Recovery
    &lt;ul&gt;
      &lt;li&gt;Over time, Redo Log will become very large. If you start to recover from the first record every time, the recovery process will be very slow and cannot be tolerated. In order to reduce the recovery time, the Checkpoint mechanism is introduced. Suppose that at a certain point in time, all dirty pages have been flushed to disk. All Redo Logs before this time point do not need to be redone. The system records the end of the redo log at this point in time as the checkpoint. When restoring, just start from this checkpoint position. The log before the checkpoint point is no longer needed and can be deleted.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;binary-log-1&quot;&gt;Binary Log&lt;/h2&gt;

&lt;p&gt;The redo log is specific to the InnoDB engine and keeps data safe, but how do other engines log data?&lt;/p&gt;

&lt;p&gt;At the Server level, MySQL has its own log, that is bin-log (an archived log). You must look at MySQL in isolation. MySQL = Server + different data storage engines, not as a whole.&lt;/p&gt;

&lt;p&gt;Binlog records a binary log of all changes to the MySQL database table structure and table data, but not the select and show queries. bin-loglogs are logged as events and include the time consumed by the statements.&lt;/p&gt;

&lt;p&gt;The two most important scenarios for turning on Binlog logging are as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Parent-Child replication: enable the Binlog function in the primary library, so that the primary library can pass Binlog to the secondary library, and the secondary library can get Binlog and achieve data recovery to achieve primary-secondary data consistency.&lt;/li&gt;
  &lt;li&gt;Data recovery: recover data through tools such as mysqlbinlog.
Bin-log files are logged in three modes: statement, rowand mixed, with row mode usually being used.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;The difference between bin log and redo log.&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The content is different: redo log is a physical log and the content is based on the Page on disk, bin-log content is binary and depending on the binlog_format parameter, may be based on SQL statements, on the data itself, or a mixture of the two.&lt;/li&gt;
  &lt;li&gt;Different levels: redo log works with InnoDB and the engine, bin-log is located at the MySQL Server level and is available to all engines.&lt;/li&gt;
  &lt;li&gt;Different forms of disk storage: redo log writes cyclically, bin-log accumulates, so it can be used for data recovery or primary-secondary synchronization&lt;/li&gt;
  &lt;li&gt;The timing of writing is different: bin-logare written when a transaction usually commits or when N transactions commit once, redo log are written at a variety of times, either every time a transaction commits, by another threaded transaction, or every second when the disk is flushed. (Note: uncommitted transactions in redo log may also be flushed to disk)&lt;/li&gt;
  &lt;li&gt;Different roles: redo log is used for crash recovery to ensure that MySQL downtime does not affect persistence; bin-log is used for point-in-time recovery to ensure that the server can recover data based on the point in time, in addition bin-log is also used for primary-secondary replication.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Two-phase Commit&lt;/strong&gt;&lt;br /&gt;
Because redo-logis in the InnoDB tier and bin-logis in the Server tier, this introduces a new problem.&lt;/p&gt;

&lt;p&gt;If the redo log is written successfully and the bin-log crashes before it is written to disk, the transaction has not yet been committed, so the new data written to the redo-log is invalid.&lt;/p&gt;

&lt;p&gt;Restarting the database for data recovery restores the data in the redo-log to disk, which creates invalid data.&lt;/p&gt;

&lt;p&gt;In this case, as you wisely know, a two-phase commit is introduced.&lt;/p&gt;

&lt;p&gt;In the first stage, the redo-log is written and in the prepared state. After the Server layer saves the bin-log data and drops it to disk, the transaction commits the redo-log at the same time, so that the redo-log becomes committed, which ensures the consistency of the redo-log data and the bin-log data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The Execution of an Update Statement&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;With the previous knowledge, you can now explore how the update statement is executed in MySQL.&lt;/p&gt;

&lt;p&gt;Suppose we now execute the SQL : update table_test set a = a+1 where id = 2;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;First, the client connects via the connector and determines the permissions.&lt;/li&gt;
  &lt;li&gt;After verification, the SQL goes through the parser for lexical and syntax analysis (AST) and if it is an Update statement, MySQL will clear all the query cache for the query table table_test. (As you can see, it is not recommended to turn on the query cache)&lt;/li&gt;
  &lt;li&gt;The optimizer optimizes the validated SQL, plans to match the id index, and generates an execution plan.&lt;/li&gt;
  &lt;li&gt;The executor gets the final SQL and calls the interface of the corresponding storage engine to start executing the update SQL.&lt;/li&gt;
  &lt;li&gt;The InnoDB engine opens a transaction, the execution engine first queries from memory whether there is data with id=2, if it matches then the corresponding data with field+1, and then saves it to memory. If it does not query the data with id=2 then it will go to the disk, the query will read the data into memory in pages, then update it and save it to memory.&lt;/li&gt;
  &lt;li&gt;The InnoDB engine will then save the data rows to redo-log, which is pre-committed, notifying the Serverâ€™s executor that it is ready to commit the transaction.&lt;/li&gt;
  &lt;li&gt;The executor will generate the corresponding bin-log and write it to disk.&lt;/li&gt;
  &lt;li&gt;The transaction is committed and the redo-log is then committed.&lt;/li&gt;
  &lt;li&gt;This is where the execution of a transaction is complete.&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://bourbonkk.tistory.com/86&quot; target=&quot;_blank&quot;&gt;í•´ì»¤ì˜ ê°œë°œì¼ê¸°, ë°ì´í„°ë² ì´ìŠ¤ì˜ ë¬´ê²°ì„±ì„ ë³´ì¥í•´ì£¼ëŠ” Write-Ahead-Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.alibabacloud.com/blog/what-are-the-differences-and-functions-of-the-redo-log-undo-log-and-binlog-in-mysql_598035&quot; target=&quot;_blank&quot;&gt;alibabacloud, What are the Differences and Functions of the Redo Log, Undo Log, and Binlog in MySQL?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://coralogix.com/blog/5-essential-mysql-database-logs-to-keep-an-eye-on/&quot; target=&quot;_blank&quot;&gt;Coralogix, 5 Essential MySQL Database Logs To Keep an Eye On&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://scaling.dev/replication/log&quot; target=&quot;_blank&quot;&gt;scaling.dev, Transaction Log. Commit Log. WAL.&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.postgresql.org/docs/current/wal-intro.html&quot; target=&quot;_blank&quot;&gt;PostgreSQL ê³µì‹ë¬¸ì„œ, Write-Ahead Logging (WAL)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://betterprogramming.pub/mysqls-redolog-and-binlog-1a35bc052489&quot; target=&quot;_blank&quot;&gt;Dwen, MySQLâ€™s RedoLog and BinLog&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developpaper.com/you-must-understand-the-three-mysql-logs-binlog-redo-log-and-undo-log/&quot; target=&quot;_blank&quot;&gt;developPAPER, You must understand the three MySQL logs â€“ binlog, redo log and undo log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/5.7/en/innodb-redo-log.html&quot; target=&quot;_blank&quot;&gt;MySQL ê³µì‹ë¬¸ì„œ, 14.6.6 Redo Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blog.katastros.com/a?ID=01200-5fbbae7f-7eb9-4570-876c-23048d66fb82&quot; target=&quot;_blank&quot;&gt;Katastros, InnoDB transaction log (redo log and undo log) detailed&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/heesuk-ahn/today-I-learned/blob/master/database/binary-log.md&quot; target=&quot;_blank&quot;&gt;heesuk-ahn, [ë°ì´í„°ë² ì´ìŠ¤] binary log ë€?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.oracle.com/cd/E17952_01/mysql-5.7-en/binary-log.html&quot; target=&quot;_blank&quot;&gt;The Binary Log&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jvns.ca/blog/2017/06/11/log-structured-storage/&quot; target=&quot;_blank&quot;&gt;Log-structured storage&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dreamcoding.tistory.com/62&quot; target=&quot;_blank&quot;&gt;ghyeong, ìŠ¬ë¡œìš° ì¿¼ë¦¬(Slow Query) ì¡°íšŒ ì¿¼ë¦¬(Oracle, MS-SQL, Mysql, postgreSQL)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/8.0/en/slow-query-log.html&quot; target=&quot;_blank&quot;&gt;MySQL ê³µì‹ë¬¸ì„œ, The Slow Query Log&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series15</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series15</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
            <item>
                <title>MySQL Series [Part14] MySQL Optimizing SELECT Statements</title>
                <description>&lt;hr /&gt;
&lt;p id=&quot;toc&quot;&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;
&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#optimization-overview&quot; id=&quot;markdown-toc-optimization-overview&quot;&gt;Optimization Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#things-to-consider-for-optimization&quot; id=&quot;markdown-toc-things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#select&quot; id=&quot;markdown-toc-select&quot;&gt;SELECT&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#where&quot; id=&quot;markdown-toc-where&quot;&gt;WHERE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#group-by&quot; id=&quot;markdown-toc-group-by&quot;&gt;GROUP BY&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#join&quot; id=&quot;markdown-toc-join&quot;&gt;JOIN&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#subquery&quot; id=&quot;markdown-toc-subquery&quot;&gt;Subquery&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#temporary-table&quot; id=&quot;markdown-toc-temporary-table&quot;&gt;Temporary Table&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#order-by&quot; id=&quot;markdown-toc-order-by&quot;&gt;ORDER BY&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot; id=&quot;markdown-toc-ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot;&gt;ì •ë ¬ ì²˜ë¦¬ ë°©ë²•&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#ì°¸ê³ &quot; id=&quot;markdown-toc-ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h1 id=&quot;optimization-overview&quot;&gt;Optimization Overview&lt;/h1&gt;

&lt;p&gt;Database performance depends on several factors at the database level, such as tables, queries, and configuration settings. These software constructs result in CPU and I/O operations at the hardware level, which you must minimize and make as efficient as possible.&lt;/p&gt;

&lt;h1 id=&quot;things-to-consider-for-optimization&quot;&gt;Things to Consider for Optimization&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Are the tables structured properly? In particular, do the columns have the right data types, and does each table have the appropriate columns for the type of work? For example, applications that perform frequent updates often have many tables with few columns, while applications that analyze large amounts of data often have few tables with many columns.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are the right indexes in place to make queries efficient?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are you using the appropriate storage engine for each table, and taking advantage of the strengths and features of each storage engine you use? In particular, the choice of a transactional storage engine such as InnoDB or a nontransactional one such as MyISAM can be very important for performance and scalability.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does each table use an appropriate row format? This choice also depends on the storage engine used for the table. In particular, compressed tables use less disk space and so require less disk I/O to read and write the data. Compression is available for all kinds of workloads with InnoDB tables, and for read-only MyISAM tables.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Does the application use an appropriate locking strategy? For example, by allowing shared access when possible so that database operations can run concurrently, and requesting exclusive access when appropriate so that critical operations get top priority. Again, the choice of storage engine is significant. The InnoDB storage engine handles most locking issues without involvement from you, allowing for better concurrency in the database and reducing the amount of experimentation and tuning for your code.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Are all memory areas used for caching sized correctly? That is, large enough to hold frequently accessed data, but not so large that they overload physical memory and cause paging. The main memory areas to configure are the InnoDB buffer pool and the MyISAM key cache.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;select&quot;&gt;SELECT&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid using *&lt;/li&gt;
  &lt;li&gt;Avoid using DISTINCT -&amp;gt; ì¤‘ë³µ ë°ì´í„° ì œê±°ë¥¼ ìœ„í•´ í…Œì´ë¸” í’€ ìŠ¤ìº” í•´ì•¼í•¨&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;where&quot;&gt;WHERE&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use Indexes Where Appropriate&lt;/li&gt;
  &lt;li&gt;Avoid % Wildcard in a Predicate&lt;/li&gt;
  &lt;li&gt;Avoid using a function in the predicate of a query&lt;/li&gt;
  &lt;li&gt;BETWEEN, IN, &amp;lt;, &amp;gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;group-by&quot;&gt;GROUP BY&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;GROUP BY ì‘ì—…ì€ í¬ê²Œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš°ì™€ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°(ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©)&lt;/li&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, ì „ì²´ í…Œì´ë¸”ì„ ìŠ¤ìº”í•˜ì—¬ ê° ê·¸ë£¹ì˜ ëª¨ë“  í–‰ì´ ì—°ì†ë˜ëŠ” ìƒˆ ì„ì‹œ í…Œì´ë¸”ì„ ë§Œë“  ë‹¤ìŒ ì´ ì„ì‹œ í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ê·¸ë£¹ì„ ê²€ìƒ‰í•˜ê³  ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ê²ƒ
    &lt;ul&gt;
      &lt;li&gt;ì´ë ‡ê²Œ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ì„ ë•Œ í•  ìˆ˜ ìˆëŠ” ìµœì„ ì˜ ë°©ë²•ì€ WHEREì ˆì„ ì´ìš©í•´ GROUP BY í•˜ê¸° ì „ì— ë°ì´í„°ëŸ‰ì„ ì¤„ì´ëŠ” ê²ƒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì˜ ì„¤ì •í•œë‹¤ë©´ ì„ì‹œ í…Œì´ë¸”ì„ ìƒì„±í•˜ì§€ ì•Šê³  ë¹ ë¥´ê²Œ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆë‹¤
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ìµœëŒ€ë¡œ í™œìš©í•˜ê¸° ìœ„í•´ì„œëŠ” GROUP BY ì»¬ëŸ¼ê³¼, ì¸ë±ìŠ¤ë˜ì–´ ìˆëŠ” ì»¬ëŸ¼ê°„ì˜ ìˆœì„œê°€ ì¤‘ìš”í•¨&lt;/li&gt;
      &lt;li&gt;SELECTì ˆì— ì‚¬ìš©ë˜ëŠ” ì§‘ê³„í•¨ìˆ˜ì˜ ê²½ìš° MIN(), MAX()ëŠ” ì¸ë±ìŠ¤ì˜ ì„±ëŠ¥ì„ ìµœëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆë„ë¡ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ì°¸ê³ ë¡œ MySQL 8.0ë¶€í„°ëŠ” GROUP BYë¥¼ í•œë‹¤ê³  í•´ì„œ ì•”ë¬µì ìœ¼ë¡œ ì •ë ¬ì´ ì´ë£¨ì–´ì§€ì§€ ì•ŠìŒ -&amp;gt; ì •ë ¬ í•„ìš”í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ORDER BY ì¨ì•¼í•¨&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì€ ë ˆì½”ë“œë¥¼ ê±´ë„ˆë›°ë©´ì„œ í•„ìš”í•œ ë¶€ë¶„ë§Œ ê°€ì ¸ì˜¤ëŠ” ìŠ¤ìº” ë°©ì‹&lt;/li&gt;
      &lt;li&gt;EXPLAINì„ í†µí—¤ ì‹¤í–‰ ê³„íšì„ í™•ì¸í•´ë³´ë©´ Extra ì»¬ëŸ¼ì— â€˜Using index for group-byâ€™ ë¼ê³  í‘œê¸°ë¨&lt;/li&gt;
      &lt;li&gt;MIN(), MAX() ì´ì™¸ì˜ í•¨ìˆ˜ê°€ SELECT ì ˆì— ì‚¬ìš©ë˜ë©´ ë£¨ìŠ¤ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ&lt;/li&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ê°€ (col1 col2, col3) ì¼ ë•Œ , GROUP BY col1, col2 ê³¼ ê°™ì•„ì•¼ í•¨ (GROUP BY col2, col3ì€ ì•ˆë¨)&lt;/li&gt;
      &lt;li&gt;SELECT ì ˆê³¼ GROUP BY ì ˆì˜ ì»¬ëŸ¼ì´ ì¼ì¹˜í•´ì•¼ í•¨. SELECT col1, col2, MAX(col3) GROUP BY col1, col2 ê³¼ ê°™ì•„ì•¼ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;íƒ€ì´íŠ¸ ì¸ë±ìŠ¤ ìŠ¤ìº”ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;SELECT ì ˆê³¼ GROUP BY ì ˆì˜ ì»¬ëŸ¼ì´ ì¼ì¹˜í•˜ì§€ ì•Šì§€ë§Œ, ì¡°ê±´ì ˆì„ ì´ìš©í•´ ë²”ìœ„ ìŠ¤ìº”ì´ ê°€ëŠ¥í•œ ê²½ìš°
        &lt;ul&gt;
          &lt;li&gt;SELECT c1, c2, c3 FROM t1 WHERE c2 = â€˜aâ€™ GROUP BY c1, c3;&lt;/li&gt;
          &lt;li&gt;SELECT c1, c2, c3 FROM t1 WHERE c1 = â€˜aâ€™ GROUP BY c2, c3;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;join&quot;&gt;JOIN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;INNER joins, order doesnâ€™t matter&lt;/li&gt;
  &lt;li&gt;OUTER joins, the order matters&lt;/li&gt;
  &lt;li&gt;ì—¬ëŸ¬ ì¡°ì¸ì„ í¬í•¨í•˜ëŠ” LOOP JOIN ì—ì„œëŠ” ë“œë¼ì´ë¹™ í…Œì´ë¸”(Driving Table)ì´ í–‰ë“¤ì„ ìµœì†Œí•œìœ¼ë¡œ ë¦¬í„´í•˜ë„ë¡ í•´ì•¼ë¨&lt;/li&gt;
  &lt;li&gt;JOIN ë˜ëŠ” ì»¬ëŸ¼ì˜ í•œìª½ì—ë§Œ INDEXê°€ ìˆëŠ” ê²½ìš°ëŠ” INDEXê°€ ì§€ì •ëœ TABLEì´ DRIVING TABLEì´ ëœë‹¤&lt;/li&gt;
  &lt;li&gt;JOIN ì‹œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ì¹¼ëŸ¼ì€ ì¸ë±ìŠ¤ë¡œ ë“±ë¡í•œë‹¤&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ì¸ë±ìŠ¤ ë ˆì¸ì§€ ìŠ¤ìº”ì€ ì¸ë±ìŠ¤ë¥¼ íƒìƒ‰(Index Seek)í•˜ëŠ” ë‹¨ê³„ì™€ ì¸ë±ìŠ¤ë¥¼ ìŠ¤ìº”(Index Scan)í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ êµ¬ë¶„í•´ ë³¼ ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ì„œ ì¿¼ë¦¬í•˜ëŠ” ì‘ì—…ì—ì„œëŠ” ê°€ì ¸ì˜¤ëŠ” ë ˆì½”ë“œì˜ ê±´ìˆ˜ê°€ ì†ŒëŸ‰(ì „ì²´ ë°ì´í„° í¬ê¸°ì˜ 20% ì´ë‚´)ì´ê¸° ë•Œë¬¸ì— ì¸ë±ìŠ¤ ìŠ¤ìº” ì‘ì—…ì€ ë¶€í•˜ê°€ ì‘ê³ , íŠ¹ì • ì¸ë±ìŠ¤ í‚¤ë¥¼ ì°¾ëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ì´ ë¶€í•˜ê°€ ë†’ì€ í¸ì´ë‹¤.&lt;/p&gt;

&lt;p&gt;JOIN ì‘ì—…ì—ì„œ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì„ ì½ì„ ë•ŒëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ì„ ë‹¨ í•œ ë²ˆë§Œ ìˆ˜í–‰í•˜ê³ , ê·¸ ì´í›„ë¶€í„°ëŠ” ìŠ¤ìº”ë§Œ ì‹¤í–‰í•˜ë©´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;í•˜ì§€ë§Œ ë“œë¦¬ë¸ í…Œì´ë¸”ì—ì„œëŠ” ì¸ë±ìŠ¤ íƒìƒ‰ ì‘ì—…ê³¼ ìŠ¤ìº” ì‘ì—…ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì—ì„œ ì½ì€ ë ˆì½”ë“œ ê±´ìˆ˜ë§Œí¼ ë°˜ë³µí•œë‹¤.&lt;/p&gt;

&lt;p&gt;ë“œë¼ì´ë¹™ í…Œì´ë¸”ê³¼ ë“œë¦¬ë¸ í…Œì´ë¸”ì´ 1:1 ì¡°ì¸ë˜ë”ë¼ë„ &lt;strong&gt;ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ì½ëŠ” ê²ƒì´ í›¨ì”¬ ë” í° ë¶€í•˜ë¥¼ ì°¨ì§€&lt;/strong&gt;í•œë‹¤.&lt;/p&gt;

&lt;p&gt;ê·¸ë˜ì„œ ì˜µí‹°ë§ˆì´ì €ëŠ” í•­ìƒ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ì•„ë‹ˆë¼ ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ìµœì ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆê²Œ ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•œë‹¤.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-SQL&quot;&gt;SELECT *
FROM employees e, dept_emp de
WHERE e.emp_no=de.emp_no
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;ì—¬ê¸°ì„œ ê° í…Œì´ë¸”ì˜ emp_no ì»¬ëŸ¼ì— ì¸ë±ìŠ¤ê°€ ìˆì„ ë•Œì™€ ì—†ì„ ë•Œ ì¡°ì¸ ìˆœì„œê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ” í•œ ë²ˆ ì‚´í´ë³´ì.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ë‘ ì»¬ëŸ¼ ëª¨ë‘ ì¸ë±ìŠ¤ê°€ ìˆëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì–´ëŠ í…Œì´ë¸”ì„ ë“œë¼ì´ë¹™ìœ¼ë¡œ ì„ íƒí•˜ë“  ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ë“œë¦¬ë¸ í…Œì´ë¸”ì˜ ê²€ìƒ‰ ì‘ì—…ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆë‹¤&lt;/li&gt;
      &lt;li&gt;ë³´í†µì˜ ê²½ìš° ì–´ëŠ ìª½ í…Œì´ë¸”ì´ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ë˜ë“  ì˜µí‹°ë§ˆì´ì €ê°€ ì„ íƒí•˜ëŠ” ë°©ë²•ì´ ìµœì ì¼ ë•Œê°€ ë§ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;employees í…Œì´ë¸”ì—ë§Œ ì¸ë±ìŠ¤ê°€ ìˆëŠ”ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì´ ë•ŒëŠ” employees í…Œì´ë¸”ì„ ë“œë¦¬ë¸ í…Œì´ë¸”ë¡œ ì„ íƒí•œë‹¤&lt;/li&gt;
      &lt;li&gt;ë“œë¦¬ë¸ í…Œì´ë¸”ì„ ì½ëŠ” ê²ƒì´ í›¨ì”¬ ë” í° ë¶€í•˜ë¥¼ ì°¨ì§€í•˜ê¸° ë•Œë¬¸ì— ë“œë¦¬ë¸ í…Œì´ë¸”ì—ì„œ ì¸ë±ìŠ¤ë¥¼ í™œìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;INNER JOINì€ ì¡°ì¸ ëŒ€ìƒ í…Œì´ë¸” ëª¨ë‘ì— í•´ë‹¹í•˜ëŠ” ë ˆì½”ë“œë§Œ ë°˜í™˜í•œë‹¤. ì´ê°™ì€ íŠ¹ì„± ë•Œë¬¸ì— OUTER JOINìœ¼ë¡œë§Œ ì¡°ì¸ì„ ì‹¤í–‰í•˜ëŠ” ì¿¼ë¦¬ë“¤ë„ ìì£¼ ë³´ì¸ë‹¤. í•˜ì§€ë§Œ ëŒ€ê°œì˜ ê²½ìš° OUTER JOINì€ ëŒ€ìƒ í…Œì´ë¸”ë“¤ì˜ ë°ì´í„°ê°€ ì¼ê´€ë˜ì§€ ì•Šì€ ê²½ìš°ì—ë§Œ í•„ìš”í•˜ë‹¤.&lt;/p&gt;

&lt;p&gt;MySQL ì˜µí‹°ë§ˆì´ì €ëŠ” OUTER JOINì‹œ ì¡°ì¸ ë˜ëŠ” í…Œì´ë¸”(FROM A LEFT JOIN Bì—ì„œ B)ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ë¡œ ì„ íƒí•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ë¬´ì¡°ê±´ ì•ì— ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì„ ë“œë¼ì´ë¹™ í…Œì´ë¸”ë¡œ ì„ íƒí•œë‹¤. ê·¸ ê²°ê³¼ ì¸ë±ìŠ¤ ìœ ë¬´ì— ë”°ë¼ ì¡°ì¸ ìˆœì„œë¥¼ ë³€ê²½í•¨ìœ¼ë¡œì¨ ì–»ê²Œ ë˜ëŠ” ìµœì í™”ì˜ ì´ì ì„ ì–»ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì— ì¿¼ë¦¬ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§ˆ ìˆ˜ ìˆë‹¤. ê·¸ë˜ì„œ ê¼­ í•„ìš”í•œ ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´ INNER JOINì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ì— ë„ì›€ì´ ëœë‹¤.&lt;/p&gt;

&lt;p&gt;JOINì˜ ìˆœì„œ&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;INNER JOINì¸ ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ì–´ì°¨í”¼ A and B and C ì´ê¸° ë•Œë¬¸ì— A JOIN B JOIN Cì´ë“  B JOIN A JOIN Cì´ë“  ê°™ë‹¤.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;LEFT JOINì˜ ê²½ìš° ê²°ê³¼ë„ ì„±ëŠ¥ë„ ë‹¬ë¼ì§„ë‹¤.
    &lt;ul&gt;
      &lt;li&gt;ì¼ë‹¨ ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì´ ë“œë¼ì´ë¹™ í…Œì´ë¸”ì´ ëœë‹¤ -&amp;gt; ì´ ë§ì€ ë’¤ì— ë”°ë¼ì˜¤ëŠ” í…Œì´ë¸”ì€ ë“œë¦¬ë¸ í…Œì´ë¸”ì´ ëœë‹¤ëŠ” ë§ì´ë‹¤ -&amp;gt; ë“œë¦¬ë¸ í…Œì´ë¸”ì€ ì¸ë±ìŠ¤ê°€ ì—†ìœ¼ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§„ë‹¤ -&amp;gt; ë’¤ì— ì¡°ì¸ë˜ëŠ” í…Œì´ë¸”ì˜ ì¸ë±ìŠ¤ ìœ ë¬´ì— ë”°ë¼ ì¿¼ë¦¬ ì„±ëŠ¥ì´ ë‹¬ë¼ì§„ë‹¤&lt;/li&gt;
      &lt;li&gt;ê²°ê³¼ ìì²´ë„ ë§¨ ì•ì— ë“±ì¥í•˜ëŠ” í…Œì´ë¸”ì˜ ëª¨ë“  ë ˆì½”ë“œê°€ ê¸°ì¤€ì´ ë˜ê¸° ë•Œë¬¸ì— ìˆœì„œì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;INNER JOINê³¼ OUTER JOINì´ ê²°í•©ë˜ëŠ” ê²½ìš°
    &lt;ul&gt;
      &lt;li&gt;ê°€ëŠ¥í•˜ë‹¤ë©´ INNER JOINì´ ì•ì— ì˜¤ë„ë¡ í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;subquery&quot;&gt;Subquery&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Avoid correlated sub queries as it searches row by row, impacting the speed of SQL query processing&lt;/li&gt;
  &lt;li&gt;JOINìœ¼ë¡œ í•´ê²°ë˜ë©´ ì„œë¸Œì¿¼ë¦¬ ëŒ€ì‹  JOINì„ ì‚¬ìš©í•˜ì&lt;/li&gt;
  &lt;li&gt;ì„œë¸Œì¿¼ë¦¬ ì•ˆì— whereì ˆê³¼ group byë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¤ëŠ” ë°ì´í„°ì–‘ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤&lt;/li&gt;
  &lt;li&gt;ì„œë¸Œì¿¼ë¦¬ëŠ” ì¸ë±ìŠ¤ ë˜ëŠ” ì œì•½ ì •ë³´ë¥¼ ê°€ì§€ì§€ ì•Šê¸° ë•Œë¬¸ì— ìµœì í™”ë˜ì§€ ëª»í•œë‹¤&lt;/li&gt;
  &lt;li&gt;ìœˆë„ìš° í•¨ìˆ˜ë¥¼ ê³ ë ¤í•´ë³´ì&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;temporary-table&quot;&gt;Temporary Table&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Use a temporary table to handle bulk data&lt;/li&gt;
  &lt;li&gt;Temporary table vs Using index access&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;order-by&quot;&gt;ORDER BY&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;ëŒ€ë¶€ë¶„ì˜ SELECT ì¿¼ë¦¬ì—ì„œ ì •ë ¬ì€ í•„ìˆ˜ì &lt;/li&gt;
  &lt;li&gt;ì •ë ¬ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ &lt;strong&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•&lt;/strong&gt;ê³¼ &lt;strong&gt;Filesort&lt;/strong&gt;ë¼ëŠ” ë³„ë„ì˜ ì²˜ë¦¬ë¥¼ ì´ìš©í•˜ëŠ” ë°©ë²•&lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ë°©ë²•&lt;/td&gt;
      &lt;td&gt;ì¥ì &lt;/td&gt;
      &lt;td&gt;ë‹¨ì &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ì¸ë±ìŠ¤ ì´ìš©&lt;/td&gt;
      &lt;td&gt;SELECT ë¬¸ì„ ì‹¤í–‰í•  ë•Œ ì´ë¯¸ ì¸ë±ìŠ¤ê°€ ì •ë ¬ë¼ ìˆì–´ ìˆœì„œëŒ€ë¡œ ì½ê¸°ë§Œ í•˜ë©´ ë˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¥´ë‹¤&lt;/td&gt;
      &lt;td&gt;INSERT, UPDATE, DELETE ì‘ì—…ì‹œ ë¶€ê°€ì ì¸ ì¸ë±ìŠ¤ ì¶”ê°€/ì‚­ì œ ì‘ì—…ì´ í•„ìš”í•˜ë¯€ë¡œ ëŠë¦¬ë‹¤&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Filesort ì´ìš©&lt;/td&gt;
      &lt;td&gt;ì¸ë±ìŠ¤ ì´ìš©ê³¼ ë°˜ëŒ€ë¡œ INSERT, UPDATE, DELETE ì‘ì—…ì´ ë¹ ë¥´ë‹¤&lt;/td&gt;
      &lt;td&gt;ì •ë ¬ ì‘ì—…ì´ ì¿¼ë¦¬ ì‹¤í–‰ ì‹œ ì²˜ë¦¬ë˜ì–´ ì¿¼ë¦¬ì˜ ì‘ë‹µ ì†ë„ê°€ ëŠë ¤ì§„ë‹¤&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Filesortë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ê²½ìš°&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì •ë ¬ ê¸°ì¤€ì´ ë„ˆë¬´ ë§ì•„ì„œ ëª¨ë“  ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°&lt;/li&gt;
  &lt;li&gt;ì–´ë–¤ ì²˜ë¦¬ì˜ ê²°ê³¼ë¥¼ ì •ë ¬í•´ì•¼ í•˜ëŠ” ê²½ìš°&lt;/li&gt;
  &lt;li&gt;ëœë¤í•˜ê²Œ ê²°ê³¼ ë ˆì½”ë“œë¥¼ ê°€ì ¸ì™€ì•¼ í•˜ëŠ” ê²½ìš°&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ì†ŒíŠ¸ ë²„í¼&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MySQLì€ ì •ë ¬ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ë³„ë„ì˜ ë©”ëª¨ë¦¬ ê³µê°„ì„ í• ë‹¹ë°›ì•„ì„œ ì‚¬ìš©í•˜ëŠ”ë° ì´ ë©”ëª¨ë¦¬ ê³µê°„ì„ ì†ŒíŠ¸ ë²„í¼ë¼ê³  í•œë‹¤&lt;/li&gt;
  &lt;li&gt;ì •ë ¬í•´ì•¼ í•  ë ˆì½”ë“œì˜ ê±´ìˆ˜ê°€ ì†ŒíŠ¸ ë²„í¼ì˜ í¬ê¸°ë³´ë‹¤ í¬ë‹¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í• ê¹Œ?
    &lt;ul&gt;
      &lt;li&gt;ì •ë ¬í•´ì•¼ í•  ë ˆì½”ë“œë¥¼ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬í•˜ê²Œ ë¨. ì´ ê³¼ì •ì—ì„œ ì„ì‹œ ì €ì¥ì„ ìœ„í•´ ë””ìŠ¤í¬ë¥¼ ì‚¬ìš©&lt;/li&gt;
      &lt;li&gt;ì¼ë¶€ë¥¼ ì²˜ë¦¬í•˜ê³  ë””ìŠ¤í¬ì— ì €ì¥í•˜ê¸°ë¥¼ ë°˜ë³µ ìˆ˜í–‰í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;ì •ë ¬ ì•Œê³ ë¦¬ì¦˜&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ë§Œ ê°€ì ¸ì™€ì„œ ì •ë ¬í•˜ëŠ” ë°©ì‹
    &lt;ul&gt;
      &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ ê°’ë§Œ ì†ŒíŠ¸ ë²„í¼ì— ë‹´ì•„ ì •ë ¬ì„ ìˆ˜í–‰&lt;/li&gt;
      &lt;li&gt;ê·¸ë¦¬ê³  ë‹¤ì‹œ ì •ë ¬ ìˆœì„œëŒ€ë¡œ í”„ë¼ì´ë¨¸ë¦¬ í‚¤ë¡œ í…Œì´ë¸”ì„ ì½ì–´ì„œ SELECTí•  ì»¬ëŸ¼ì„ ê°€ì ¸ì˜´&lt;/li&gt;
      &lt;li&gt;ê°€ì ¸ì˜¤ëŠ” ì»¬ëŸ¼ì´ ë‘ ê°œ ë¿ì´ë¼ ì†ŒíŠ¸ ë²„í¼ì— ë§ì€ ë ˆì½”ë“œë¥¼ í•œ ë²ˆì— ì½ì–´ì˜¬ ìˆ˜ ìˆìŒ&lt;/li&gt;
      &lt;li&gt;ë‹¨ì ì€ í…Œì´ë¸”ì„ ë‘ ë²ˆ ì½ì–´ì•¼ í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ì •ë ¬ ëŒ€ìƒ ì»¬ëŸ¼ê³¼ SELECTë¬¸ìœ¼ë¡œ ìš”ì²­í•œ ì»¬ëŸ¼ì„ ëª¨ë‘ ê°€ì ¸ì™€ì„œ ì •ë ¬í•˜ëŠ” ë°©ì‹
    &lt;ul&gt;
      &lt;li&gt;ìµœì‹  ë²„ì „ì˜ MySQLì—ì„œ ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹&lt;/li&gt;
      &lt;li&gt;SELECT ë¬¸ì—ì„œ ìš”ì²­í•œ ì»¬ëŸ¼ì˜ ê°œìˆ˜ê°€ ë§ì•„ì§€ë©´ ê³„ì† ë¶„í• í•´ì„œ ì†ŒíŠ¸ ë²„í¼ì— ì½ì–´ì™€ì•¼í•¨&lt;/li&gt;
      &lt;li&gt;ë ˆì½”ë“œì˜ í¬ê¸°ë‚˜ ê±´ìˆ˜ê°€ ì‘ì€ ê²½ìš° ì„±ëŠ¥ì´ ì¢‹ìŒ&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ì •ë ¬-ì²˜ë¦¬-ë°©ë²•&quot;&gt;ì •ë ¬ ì²˜ë¦¬ ë°©ë²•&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ì •ë ¬
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ì •ë ¬ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ë°˜ë“œì‹œ ORDER BYì˜ ìˆœì„œëŒ€ë¡œ ìƒì„±ëœ ì¸ë±ìŠ¤ê°€ ìˆì–´ì•¼ í•¨&lt;/li&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì´ìš©í•´ ì •ë ¬ì´ ê°€ëŠ¥í•œ ì´ìœ ëŠ” B-Tree ì¸ë±ìŠ¤ê°€ í‚¤ ê°’ìœ¼ë¡œ ì •ë ¬ë˜ì–´ ìˆê¸° ë•Œë¬¸&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Filesortë¥¼ ì‚¬ìš©í•œ ì •ë ¬
    &lt;ul&gt;
      &lt;li&gt;ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš°, WHERE ì¡°ê±´ì— ì¼ì¹˜í•˜ëŠ” ë ˆì½”ë“œë¥¼ ê²€ìƒ‰í•´ ì •ë ¬ ë²„í¼ì— ì €ì¥í•˜ë©´ì„œ ì •ë ¬ì„ ì²˜ë¦¬(FIlesort)í•¨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;ì°¸ê³ &quot;&gt;ì°¸ê³ &lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://dev.mysql.com/doc/refman/8.0/en/select-optimization.html&quot; target=&quot;_blank&quot;&gt;MySQL ê³µì‹ë¬¸ì„œ: Optimizing SELECT Statements&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://phoenixnap.com/kb/improve-mysql-performance-tuning-optimization&quot; target=&quot;_blank&quot;&gt;MySQL Performance Tuning and Optimization Tips&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://danbi-ncsoft.github.io/works/2021/11/05/etl-performace-tips.html&quot; target=&quot;_blank&quot;&gt;ETL ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ëª‡ ê°€ì§€ íŒë“¤&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://til.songyunseop.com/mysql/group-by-optimization.html&quot; target=&quot;_blank&quot;&gt;ì „ì§€ì  ì†¡ìœ¤ì„­ì‹œì  TIL, GROUP BY ìµœì í™”&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://otsteam.tistory.com/136&quot; target=&quot;_blank&quot;&gt;SQL ì„±ëŠ¥ì„ ìœ„í•œ 25ê°€ì§€ ê·œì¹™&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://jojoldu.tistory.com/173?category=761883&quot; target=&quot;_blank&quot;&gt;íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ SQLíŠœë‹ìº í”„ 4ì¼ì°¨ - ì¡°ì¸ì˜ ê¸°ë³¸ ì›ë¦¬ì™€ í™œìš©&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://brightestbulb.tistory.com/147&quot; target=&quot;_blank&quot;&gt;ì·¨ë¯¸ëŠ” ê³µë¶€ íŠ¹ê¸°ëŠ” ê¸°ë¡, Nested Loop Join, Driving Table&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://stackoverflow.com/questions/9614922/does-the-join-order-matter-in-sql&quot; target=&quot;_blank&quot;&gt;stackoverflow, Does the join order matter in SQL?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://coding-factory.tistory.com/756&quot; target=&quot;_blank&quot;&gt;ì½”ë”©íŒ©í† ë¦¬, [DB] ë°ì´í„°ë² ì´ìŠ¤ NESTED LOOPS JOIN (ì¤‘ì²© ë£¨í”„ ì¡°ì¸)ì— ëŒ€í•˜ì—¬&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://schatz37.tistory.com/2&quot; target=&quot;_blank&quot;&gt;ê³ ë™ì˜ ë°ì´í„° ë¶„ì„, [SQL] â€œì„±ëŠ¥ ê´€ì â€ì—ì„œ ë³´ëŠ” ê²°í•©(Join)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://schatz37.tistory.com/3?category=878798&quot; target=&quot;_blank&quot;&gt;ê³ ë™ì˜ ë°ì´í„° ë¶„ì„, [SQL] ì„±ëŠ¥ ê´€ì ì—ì„œì˜ ì„œë¸Œì¿¼ë¦¬(Subquery)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
                <pubDate>Fri, 29 Jul 2022 21:01:35 +0900</pubDate>
                <link>http://localhost:4000/mysql-series14</link>
                <guid isPermaLink="true">http://localhost:4000/mysql-series14</guid>
                
                <category>MySQL</category>
                
                
                <category>DE</category>
                
            </item>
        
    </channel>
</rss>
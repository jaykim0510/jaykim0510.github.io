I"8<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#스파크-설치" id="markdown-toc-스파크-설치">스파크 설치</a>    <ul>
      <li><a href="#pyspark" id="markdown-toc-pyspark">pyspark</a>        <ul>
          <li><a href="#자바-파이썬-설치" id="markdown-toc-자바-파이썬-설치">자바, 파이썬 설치</a></li>
          <li><a href="#pyspark-설치" id="markdown-toc-pyspark-설치">pyspark 설치</a></li>
        </ul>
      </li>
      <li><a href="#spark" id="markdown-toc-spark">Spark</a></li>
    </ul>
  </li>
  <li><a href="#로컬-개발-환경" id="markdown-toc-로컬-개발-환경">로컬 개발 환경</a></li>
  <li><a href="#클러스터-환경" id="markdown-toc-클러스터-환경">클러스터 환경</a></li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<h1 id="스파크-설치">스파크 설치</h1>
<p>스파크를 설치하는 과정 자체는 크게 복잡하지 않습니다. <strong>자바와 스파크만 설치</strong>하면 스파크를 사용할 수 있습니다. 자바가 필요한 이유는 스파크가 JVM 위에서 실행되기 때문입니다.</p>

<p>하지만 실무에서는 대부분의 빅데이터 소프트웨어들이 클러스터 환경에서 동작하기 때문에 제대로 활용하기 위해서는 여러 가지 준비할 것도 많고 설정해야 할 것들도 많습니다. 그래서 스파크는 <strong>개발/테스트를 위한 용도로 간단하게 사용할 때에는 단독 서버에서 동작하는 로컬 모드를, 배포를 위한 용도로 클라이언트, 클러스터 모드를 지원</strong>합니다.</p>

<p>스파크 애플리케이션 코드는 <strong>자바, 스칼라, 파이썬, R</strong>언어로 작성할 수 있습니다.</p>

<h2 id="pyspark">pyspark</h2>
<p>우선 저는 파이썬을 주언어로 사용하기 때문에 pyspark를 이용해 파이썬으로 스파크 애플리케이션 코드를 작성할 예정입니다. pyspark의 장점은 만약 개발/테스트를 위한 목적으로만 스파크를 사용할 예정이라면 스파크를 설치할 필요가 없다는 것입니다. 스파크를 사용하는데 스파크를 설치할 필요가 없다? 무슨 뜻이냐면 pyspark를 설치하기만 해도 스파크를 실행하기 위해 필요한 최소한의 파일을 함께 설치해줍니다.</p>

<p>하지만 여전히 자바는 설치해주어야 합니다.</p>

<blockquote>
  <p>To run Spark, you only require a Java runtime environment (JRE) but you may also download the Java development kit (JDK) which includes the JRE.</p>
</blockquote>

<p>저는 <strong>파이썬이 설치되어 있는 도커 이미지를 이용해 컨테이너 안에서 실습</strong>을 진행해 보았습니다.</p>

<h3 id="자바-파이썬-설치">자바, 파이썬 설치</h3>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 파이썬이 설치된 컨테이너 생성</span>
docker run <span class="nt">-it</span> python:3.8-buster
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># JDK 설치</span>
apt-get update
apt-get <span class="nb">install </span>openjdk-11-jdk
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># JAVA_HOME 변수 설정, 경로 추가</span>


<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/etc/openjdk-11-jdk     <span class="c"># 본인의 자바 설치 경로</span>
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$JAVA_HOME</span>/bin:<span class="nv">$PATH</span>

<span class="nb">.</span> /etc/profile <span class="c"># bash쉘이면 source /etc/profile</span>
</code></pre></div></div>
<h3 id="pyspark-설치">pyspark 설치</h3>
<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pyspark 설치</span>
pip <span class="nb">install </span>pyspark
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 잘 설치되었는지 확인
</span><span class="kn">import</span> <span class="nn">pyspark</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">SparkContext</span><span class="p">(</span><span class="n">appName</span><span class="o">=</span><span class="s">"SparkContext"</span><span class="p">)</span>

<span class="n">sc</span>
<span class="o">--------------------------------</span>
<span class="n">SparkContext</span>

<span class="n">Version</span>
<span class="n">v3</span><span class="p">.</span><span class="mf">2.1</span>
<span class="n">Master</span>
<span class="n">local</span><span class="p">[</span><span class="o">*</span><span class="p">]</span>
<span class="n">AppName</span>
<span class="n">SparkContext</span>
</code></pre></div></div>

<h2 id="spark">Spark</h2>
<p>이번에는 파이썬에 국한되지 않는 조금 더 일반적인 방법으로 스파크를 설치해보겠습니다. 이번에는 리눅스 운영체제만 가지는 컨테이너 위에서 실습을 진행하도록 하겠습니다.</p>

<p>처음에는 우분투 이미지를 바로 컨테이너로 띄우고 그 위에서 자바를 설치하려 했지만, 오라클에서 다운받는 방법을 제한하고 있어서 아래의 방법으로 진행했습니다. (우분투 이미지에 로컬에서 다운받은 자바를 하나의 이미지로 새로 빌드)</p>

<p>그래서 사실 위에서 진행한 pyspark만 설치하는 방법에서도 python이미지에 로컬 자바로 한 번 이미지를 빌드한 후 사용하는 것이 좋을 것 같습니다. 저도 아직 본격적으로 사용해보지는 않아서 에러가 있는지는 확인해보지 않았지만 로컬에서 자바를 다운 받고 빌드하는 방법은 확실히 안전합니다.</p>

<p>🦊 <strong>자바 설치</strong><br />
자바 라이센스를 소유하고 있는 오라클에서 2019년 4월부터 자바를 외부의 허용하지 않은 방법으로 다운받는 것을 금지시켰습니다. 그래서 wget과 같은 방식으로 자바8 버전을 더이상 다운받을 수 없게 되고 무조건 오라클에 로그인을 한 후 로컬에 먼저 다운을 받아야합니다. <del>자바 17은 가능한데 스파크에서 자바 17로 설치하니까 오류가 난다. 구글링에서는 자바를 다운그레이드 하라고 나와있다. 자바8로 해보니까 된다. 그래서 자바8을 지금 다운 받으려고 하는 것이다.</del>그래서 저같은 경우에는 왠만한 작업들은 무조건 도커 컨테이너에서 진행하는 편이라 처음에는 도커허브에서 자바8이 설치되어 있는 이미지를 찾아봤지만 뭔가 세부설정들이 제 마음에 들지 않게 되어 있어서 이미지를 직접 빌드하기로 결정했습니다. 제가 사용한 방법의 과정은 다음과 같습니다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 자바를 로컬에 다운로드</span>
<span class="c"># 다운로드 페이지 접속</span>
https://www.oracle.com/java/technologies/javase/javase8u211-later-archive-downloads.html

<span class="c"># 저는 M1 칩을 사용하고 있어서 ARM64 전용 파일을 다운로드 받았습니다.</span>
jdk-8u311-linux-aarch64.tar.gz

<span class="c"># 다운로드 받은 폴더에서 압축해제</span>
<span class="nb">tar</span> <span class="nt">-xzvf</span> jdk-8u311-linux-aarch64.tar.gz
</code></pre></div></div>
<div class="language-Dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Dockerfile 작성</span>
<span class="c"># 로컬에 설치한 자바를 컨테이너로 옮기고 스파크까지 설치해주었습니다</span>
<span class="k">FROM</span><span class="s"> ubuntu:latest</span>

<span class="k">COPY</span><span class="s"> jdk1.8.0_321 ./jdk1.8.0_321</span>

<span class="k">RUN </span>apt-get <span class="nt">-y</span> update <span class="se">\
</span><span class="o">&amp;&amp;</span> apt-get <span class="nt">-y</span> <span class="nb">install </span>vim <span class="se">\
</span><span class="o">&amp;&amp;</span> apt-get <span class="nt">-y</span> <span class="nb">install </span>wget <span class="se">\
</span><span class="o">&amp;&amp;</span> wget https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz <span class="se">\
</span><span class="o">&amp;&amp;</span> <span class="nb">tar</span> <span class="nt">-xzvf</span> spark-3.2.1-bin-hadoop3.2.tgz
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>vim /etc/profile

<span class="c"># 환경 변수설정해줍니다. 이부분은 Dockerfile에서 ENV로 설정해 줄 수도 있습니다</span>
<span class="nb">export </span><span class="nv">JAVA_HOME</span><span class="o">=</span>/jdk1.8.0_321
<span class="nb">export </span><span class="nv">PATH</span><span class="o">=</span><span class="nv">$JAVA_HOME</span>/bin:<span class="nv">$PATH</span>

<span class="nb">.</span> /etc/profile
</code></pre></div></div>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>spark-3.2.1-bin-hadoop3.2
<span class="nb">ls</span>
<span class="nt">--------------------------------------------------------------------------------------------------------------</span>
LICENSE  NOTICE  R  README.md  RELEASE  bin  conf  data  examples  jars  kubernetes  licenses  python  sbin  yarn
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 스파크에서 제공하는 실행 파일
cd bin
ls
</code></pre></div></div>

<p><img src="../images/../../images/spark_9.png" alt="" /></p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 스파크 셸 실행</span>
./bin/spark-shell

<span class="c"># 셸 종료</span>
:q
</code></pre></div></div>

<p><img src="../images/../../images/spark_8.png" alt="" /></p>

<p>위의 과정은 이미지에서 매번 스파크를 다운받는 방식이기 때문에, 스파크를 다운받은 컨테이너를 다시 한 번 이미지로 만들면 그 다음부터는 새로 만든 이미지를 이용하면 컨테이너를 띄우는 속도가 더 빨라지게 됩니다. 그래서 <code class="language-plaintext highlighter-rouge">docker commit</code> 명령어를 이용해 한번 더 이미지를 빌드하는 것을 권장드립니다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 로컬 터미널에서 docker commit 명령어로 이미지 생성</span>
<span class="c"># dockere commit &lt;원하는 컨테이너 이름&gt; &lt;생성할 이미지 이름&gt;</span>
docker commit thirsty_galois spark_container
</code></pre></div></div>

<h1 id="로컬-개발-환경">로컬 개발 환경</h1>
<p>위의 설치과정을 완료한 후 스파크의 설정 정보를 확인해 보겠습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./bin/spark-shell --verbose
</code></pre></div></div>

<p><img src="../images/../../images/spark_11.png" alt="" /></p>

<p>다른 부분은 일단 신경쓰지 말고 master 부분만 보도록 하겠습니다. 현재 master가 <code class="language-plaintext highlighter-rouge">local[*]</code>로 설정되어 있습니다. 이는 현재 드라이버 프로그램을 실행하는 서버를 포함해 워커 노드까지 모두 로컬 서버를 이용하고 있다는 뜻입니다. <code class="language-plaintext highlighter-rouge">*</code>는 로컬 서버의 모든 스레드를 사용하겠다는 뜻입니다.</p>

<p>따라서 여기까지만 설정하게 되면 로컬에서 테스트 목적으로 사용하기 위한 최소한의 준비는 끝난 것입니다. 이 외에도 여러 가지 설정들을 직접하고 싶을 때에는 <code class="language-plaintext highlighter-rouge">./conf</code>에 설정을 위한 여러가지 파일의 템플릿을 이용할 수 있습니다.</p>

<p><img src="../images/../../images/spark_12.png" alt="" /></p>

<h1 id="클러스터-환경">클러스터 환경</h1>

<h1 id="참고">참고</h1>
<ul>
  <li><a href="https://stackoverflow.com/questions/61816236/does-pyspark-code-run-in-jvm-or-python-subprocess" target="_blank">Pyspark 코드는 어디서 실행되는가?</a></li>
  <li><a href="https://stackoverflow.com/questions/51728177/can-pyspark-work-without-spark" target="_blank">Pyspark만으로 스파크 애플리케이션 실행할 수 있나?</a></li>
  <li><a href="https://stackoverflow.com/questions/58479357/pyspark-from-spark-installation-vs-pyspark-python-package" target="_blank">Pyspark의 한계</a></li>
  <li><a href="https://askubuntu.com/questions/1363992/bin-sh-1-source-not-found" target="_blank">bin/sh: 1: source: not found</a></li>
  <li><a href="https://unit-15.tistory.com/114?category=521121#recentComments" target="_blank">[Linux] 우분투에 자바 설치</a></li>
  <li><a href="https://superuser.com/questions/1466580/unable-to-download-oracle-jdk-8-using-wget-command" target="_blank">Unable to download Oracle JDK 8 using Wget command</a></li>
  <li><a href="https://nhj12311.tistory.com/37" target="_blank">자바(JDK, JRE) 모든 버전 다운로드( 6,7,8,9,10,11,12,13,14,15, 16, 17..)</a></li>
  <li><a href="https://www.quobyte.com/enterprise-analytics/howto-spark-quobyte-multinode" target="_blank">How to Set Up a Multi Node Apache Spark Cluster with Quobyte</a></li>
  <li><a href="https://www.ibm.com/docs/en/zpfas/1.1.0?topic=structure-updating-apache-spark-configuration-files" target="_blank">Updating the Apache Spark configuration files</a></li>
  <li><a href="https://velog.io/@somnode/spark-cluster-install" target="_blank">[spark] Spark 3 클러스터 설치</a></li>
</ul>
:ET
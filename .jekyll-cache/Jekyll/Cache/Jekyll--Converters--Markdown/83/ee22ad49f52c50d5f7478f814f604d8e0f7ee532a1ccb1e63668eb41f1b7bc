I"Y<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#pyflink" id="markdown-toc-pyflink">PyFlink</a></li>
  <li><a href="#datastream-api" id="markdown-toc-datastream-api">DataStream API</a>    <ul>
      <li><a href="#datastream-api-ë§›ë³´ê¸°" id="markdown-toc-datastream-api-ë§›ë³´ê¸°">DataStream API ë§›ë³´ê¸°</a></li>
    </ul>
  </li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="pyflink">PyFlink</h1>

<p><img src="/images/flink_30.png" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip install apache-flink
</code></pre></div></div>

<ul>
  <li>PyFlinkëŠ” <strong>ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ”ë° í•„ìš”í•œ ê³ ìˆ˜ì¤€ APIë¥¼ ì œê³µ</strong></li>
  <li>í¬ê²Œ ë‘ ê°€ì§€ì˜ <strong>Table API, DataStream API</strong>ë¥¼ ì œê³µ</li>
  <li>
    <p>ì œê³µë°›ì€ APIë¥¼ ì´ìš©í•´ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ë¥¼ ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ <strong>íŒŒì´ì¬</strong> ì–¸ì–´ë¡œ ì‘ì„±í•  ìˆ˜ ìˆìŒ</p>
  </li>
  <li>Table APIëŠ” SQLê³¼ ìœ ì‚¬í•œ í˜•íƒœì˜ ê°•ë ¥í•œ ê´€ê³„í˜• ì¿¼ë¦¬ë¥¼ ì‘ì„±í•˜ëŠ”ë° í•„ìš”í•œ ê¸°ëŠ¥ì„ ì œê³µ</li>
</ul>

<h1 id="datastream-api">DataStream API</h1>

<ul>
  <li>DataStream APIëŠ” ì‹œê°„, ìƒíƒœì™€ ê°™ì€ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬ì˜ í•µì‹¬ì´ ë˜ëŠ” ê°œë…ë“¤ì„ ë‹¤ë£¨ëŠ”ë° í•„ìš”í•œ ê¸°ëŠ¥ì„ ì œê³µ</li>
  <li><strong>Filtering, Update state, Defining window, Aggregating</strong>ê³¼ ê°™ì€ ìŠ¤íŠ¸ë¦¼ ë°ì´í„° ë³€í™˜ ê¸°ëŠ¥ì„ ì œê³µ</li>
</ul>

<h2 id="datastream-api-ë§›ë³´ê¸°">DataStream API ë§›ë³´ê¸°</h2>

<p>DataStream API applications begin by declaring an execution environment (StreamExecutionEnvironment), the context in which a streaming program is executed. This is what you will use to set the properties of your job (e.g. default parallelism, restart strategy), create your sources and finally trigger the execution of the job.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># ìŠ¤íŠ¸ë¦¬ë° í”„ë¡œê·¸ë¨ì´ ì‹¤í–‰ë˜ëŠ” ì‹¤í–‰í™˜ê²½
# ì‘ì—…ì˜ íŠ¹ì„±ì„ ì„¤ì •
# ì†ŒìŠ¤ ìƒì„±
# ì‘ì—…ì˜ ì‹¤í–‰ íŠ¸ë¦¬ê±°
</span><span class="n">env</span> <span class="o">=</span> <span class="n">StreamExecutionEnvironment</span><span class="p">.</span><span class="n">get_execution_environment</span><span class="p">()</span>
<span class="n">env</span><span class="p">.</span><span class="n">set_runtime_mode</span><span class="p">(</span><span class="n">RuntimeExecutionMode</span><span class="p">.</span><span class="n">BATCH</span><span class="p">)</span>
<span class="n">env</span><span class="p">.</span><span class="n">set_parallelism</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>Once a StreamExecutionEnvironment is created, you can use it to declare your source. Sources ingest data from external systems, such as Apache Kafka, Rabbit MQ, or Apache Pulsar, into Flink Jobs.</p>

<p>To keep things simple, this walkthrough uses a source which reads data from a file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ds = env.from_source(
    source=FileSource.for_record_stream_format(StreamFormat.text_line_format(),
                                               input_path)
                     .process_static_file_set().build(),
    watermark_strategy=WatermarkStrategy.for_monotonous_timestamps(),
    source_name="file_source"
)
</code></pre></div></div>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>
<ul>
  <li><a href="https://nightlies.apache.org/flink/flink-docs-master/docs/dev/python/overview/" target="_blank">Apache Flink: pyflink ê³µì‹ë¬¸ì„œ</a></li>
  <li><a href="https://nightlies.apache.org/flink/flink-docs-master/api/python/" target="_blank">Apache Flinkâ€ pyflink Docs ê³µì‹ë¬¸ì„œ</a></li>
</ul>

:ET
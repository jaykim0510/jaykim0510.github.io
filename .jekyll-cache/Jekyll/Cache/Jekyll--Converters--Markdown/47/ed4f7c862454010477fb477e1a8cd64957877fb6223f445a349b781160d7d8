I"WY<hr />
<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹" id="markdown-toc-ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹">ì¹´í”„ì¹´ì˜ ë°ì´í„° ì €ì¥ ë°©ì‹</a>    <ul>
      <li><a href="#partition" id="markdown-toc-partition">Partition</a></li>
      <li><a href="#segment" id="markdown-toc-segment">Segment</a></li>
    </ul>
  </li>
  <li><a href="#ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes" id="markdown-toc-ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes">ì €ì¥ëœ ë°ì´í„°ì˜ í¬ë§·(Kafka messages are just bytes)</a></li>
  <li><a href="#ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜" id="markdown-toc-ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜">ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜</a></li>
  <li><a href="#ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ" id="markdown-toc-ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ">ì¥ì•  ë³µêµ¬ë¥¼ ìœ„í•œ ë³µì œ</a></li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹">ì¹´í”„ì¹´ì˜ ë°ì´í„° ì €ì¥ ë°©ì‹</h1>

<p>Kafka is everywhere these days. With the advent of Microservices and distributed computing, Kafka has become a regular occurrence in the architecture of every product. In this article, Iâ€™ll try to explain how Kafkaâ€™s internal storage mechanism works.</p>

<p>Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a Distributed, Replicated Commit Log. This, I think, clearly represents what Kafka does, as all of us understand how logs are written to disk. And in this case, it is the messages pushed into Kafka that are stored to disk.</p>

<p>Regarding storage in Kafka, youâ€™ll always hear two terms - Partition and Topic. Partitions are the units of storage in Kafka for messages. And Topic can be thought of as being a container in which these partitions lie.</p>

<h2 id="partition">Partition</h2>

<p>I am going to start by creating a topic in Kafka with three partitions.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-topics.sh <span class="nt">--create</span> <span class="nt">--topic</span> freblogg <span class="nt">--partitions</span> 3 <span class="nt">--replication-factor</span> 1 <span class="nt">--zookeeper</span> localhost:2181
</code></pre></div></div>

<p>If I go into Kafkaâ€™s log directory, I see three directories created as follows.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ tree freblogg*
freblogg-0
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-1
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-2
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
</code></pre></div></div>

<p>We have three directories created because weâ€™ve given three partitions for our topic, which means that each partition gets a directory on the file system. You also see some files like index, log etc. Weâ€™ll get to them shortly.</p>

<p>One more thing that you should be able to see from here is that in Kafka, the topic is more of a logical grouping than anything else and that the Partition is the actual unit of storage in Kafka. That is what is physically stored on the disk. Letâ€™s understand partitions in some more detail.</p>

<p>Now let us send a couple of messages and see what happens. To send the messages Iâ€™m using the console producer as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-console-producer.sh --topic freblogg --broker-list localhost:9092
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -lh freblogg*
freblogg-0:
total 20M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpoint

freblogg-1:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpoint

freblogg-2:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint
</code></pre></div></div>

<p>Our two messages went into two of the partitions where you can see that the log files have a non zero size. This is because the messages in the partition are stored in the â€˜xxxx.logâ€™ file. To confirm that the messages are indeed stored in the log file, we can just see whatâ€™s inside that log file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat freblogg-2/*.log
@^@^BÃ‚Â°Ã‚Â£ÃƒÂ¦ÃƒÆ’^@^K^XÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿^@^@^@^A"^@^@^A^VHello World^@
</code></pre></div></div>

<p>The file format of the â€˜logâ€™ file is not conducive for textual representation but, you should see the â€˜Hello Worldâ€™ at the end indicating that this file got updated when we have sent the message into the topic. The second message we have sent went into the other partition.</p>

<p>Notice that the first message we sent, went into the third partition (freblogg-2) and the second message went into the second partition (freblogg-1). This is because Kafka arbitrarily picks the partition for the first message and then distributes the messages to partitions in a round-robin fashion. If a third message comes now, it would go into freblogg-0 and this order of partition continues for any new message that comes in. We can also make Kafka choose the same partition for our messages by adding a key to the message. Kafka stores all the messages with the same key into a single partition.</p>

<p>Each new message in the partition gets an Id which is one more than the previous Id number. This Id number is also called the Offset. So, the first message is at â€˜offsetâ€™ 0, the second message is at offset 1 and so on. These offset Idâ€™s are always incremented from the previous value.</p>

<p><img src="/images/kafka_75.png" alt="" /></p>

<p>We can understand those random characters in the log file, using a Kafka tool.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-run-class.bat kafka.tools.DumpLogSegments <span class="nt">--deep-iteration</span> <span class="nt">--print-data-log</span> <span class="nt">--files</span> logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
</code></pre></div></div>

<p>This gives the output</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dumping logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
Starting offset: 0

offset: 0 position: 0 CreateTime: 1533443377944 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 11 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: Hello World

offset: 1 position: 79 CreateTime: 1533462689974 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 6 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: amazon
</code></pre></div></div>

<p>CreateTimeê³¼ ê°™ì€ ê°’ì€ ì»¨ìŠˆë¨¸ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°’ì´ ì•„ë‹ˆë‹¤. ì¹´í”„ì¹´ ë‚´ë¶€ì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ë©”íƒ€ë°ì´í„°ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„°ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í•„ìš”í•˜ë‹¤ë©´, ë°ì´í„°ë¥¼ ìƒì„±í•  ë•Œ ë‚´ë¶€ì ìœ¼ë¡œ ë©”ì„¸ì§€ì— ëª…ì‹œì ìœ¼ë¡œ ë‹´ì•„ì„œ ë¸Œë¡œì»¤ì— ë‹´ì•„ì•¼ í•œë‹¤.</p>

<p>You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.</p>

<p>It is also important to note that a partition is tied to a broker. In other words, If we have three brokers and if the folder freblogg-0 exists on broker-1, you can be sure that it will not appear in any of the other brokers. Partitions of a topic can be spread out to multiple brokers but a partition is always present on one single Kafka broker (When the replication factor has its default value, which is 1. Replication is mentioned further below).</p>

<h2 id="segment">Segment</h2>

<p>Weâ€™ll finally talk about those index and log files weâ€™ve seen in the partition directory. Partition might be the standard unit of storage in Kafka, but it is not the lowest level of abstraction provided. Each partition is divided into segments.</p>

<p>A segment is simply a collection of messages of a partition. Instead of storing all the messages of a partition in a single file (think of the log file analogy again), Kafka splits them into chunks called segments. Doing this provides several advantages. Divide and Conquer FTW!</p>

<p>Most importantly, it makes purging data easy. As previously introduced partition is immutable from a consumer perspective. But Kafka can still remove the messages based on the â€œRetention policyâ€ of the topic. Deleting segments is much simpler than deleting things from a single file, especially when a producer might be pushing data into it.</p>

<p>Each segment file has <code class="language-plaintext highlighter-rouge">segment.log</code>, <code class="language-plaintext highlighter-rouge">segment.index</code> and `` fisegment.timeindexles.</p>

<p>Kafka always writes the messages into these segment files under a partition. There is always an active segment to which Kafka writes to. Once the segmentâ€™s size limit is reached, a new segment file is created and that becomes the active segment.</p>

<p>One of the common operations in Kafka is to read the message at a particular offset. For this, if it has to go to the log file to find the offset, it becomes an expensive task especially because the log file can grow to huge sizes (Defaultâ€”1G). This is where the .index file becomes useful. Index file stores the offsets and physical position of the message in the log file.</p>

<p>An index file for the log file Iâ€™ve showed in the â€˜Quick detourâ€™ above would look something like this:</p>

<p><img src="/images/kafka_76.png" alt="" /></p>

<p>If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.</p>

<h1 id="ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes">ì €ì¥ëœ ë°ì´í„°ì˜ í¬ë§·(Kafka messages are just bytes)</h1>

<p>Kafka messages are just bytes. Kafka messages are organized into topics. Each message is a key/value, but that is all that Kafka requires. Both key and value are just bytes when they are stored in Kafka. This makes Kafka applicable to a wide range of use cases, but it also means that developers have the responsibility of deciding how to serialize the data.</p>

<p>There are various serialization formats with common ones including:</p>

<ul>
  <li>JSON</li>
  <li>Avro</li>
  <li>Protobuf</li>
  <li>String delimited (e.g., CSV</li>
</ul>

<p>There are advantages and disadvantages to each of theseâ€”well, except delimited, in which case itâ€™s only disadvantages ğŸ˜‰</p>

<p>Choosing a serialization format</p>

<ul>
  <li><strong>Schema</strong>: A lot of the time your data will have a schema to it. You may not like the fact, but itâ€™s your responsibility as a developer to preserve and propagate this schema. The schema provides the contract between your services. Some message formats (such as Avro and Protobuf) have strong schema support, whilst others have lesser support (JSON) or none at all (delimited string).</li>
  <li><strong>Ecosystem compatibility</strong>: Avro, Protobuf, and JSON are first-class citizens in the Confluent Platform, with native support from the  Confluent Schema Registry, Kafka Connect, ksqlDB, and more.</li>
  <li><strong>Message size</strong>: Whilst JSON is plain text and relies on any compression configured in Kafka itself, Avro and Protobuf are both binary formats and thus provide smaller message sizes.</li>
  <li><strong>Language support</strong>: For example, support for Avro is strong in the Java space, whilst if youâ€™re using Go, chances are youâ€™ll be expecting to use Protobuf.</li>
</ul>

<p>ë°ì´í„°ë¥¼ ë¸Œë¡œì»¤ì— ì €ì¥í•  ë•ŒëŠ” ì „ì†¡ëœ ë°ì´í„°ì˜ í¬ë§·ê³¼ëŠ” ìƒê´€ì—†ì´ ì›í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë¸Œë¡œì»¤ì— ì €ì¥í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í”„ë¡œë“€ì„œê°€ JSONìœ¼ë¡œ ë³´ëƒˆë‹¤ê³  í•˜ë”ë¼ë„ ë¸Œë¡œì»¤ì— ì €ì¥í•  ë•Œ í¬ë§·ì€ Avro, Parquet, String ë­˜ í•˜ë“  ìƒê´€ì—†ë‹¤. ë‹¤ë§Œ ì¤‘ìš”í•œ ê²ƒì€ Serializerë¡œ Avroë¥¼ ì„ íƒí–ˆë‹¤ë©´, Deserializerë„ ë°˜ë“œì‹œ Avroë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. ê·¸ëŸ¬ê³  ë‚˜ë©´ ì»¨ìŠˆë¨¸ì—ì„œ ì „ë‹¬ ë°›ëŠ” ë°ì´í„°ì˜ í¬ë§·ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ì‹œ JSON í˜•íƒœë¥¼ ì–»ê²Œ ëœë‹¤.</p>

<p><img src="/images/kafka_78.png" alt="" /></p>

<p>Remember, Kafka messages are just pairs of key/value bytes, and you need to specify the converter for both keys and value, using the <code class="language-plaintext highlighter-rouge">key.converter</code> and <code class="language-plaintext highlighter-rouge">value.converter</code> configuration setting. In some situations, you may use different converters for the key and the value.</p>

<p>Hereâ€™s an example of using the String converter. Since itâ€™s just a string, thereâ€™s no schema to the data, and thus itâ€™s not so useful to use for the value:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"key.converter"</span>: <span class="s2">"org.apache.kafka.connect.storage.StringConverter"</span>,
</code></pre></div></div>

<p>Some converters have additional configuration. For Avro, you need to specify the Schema Registry. For JSON, you need to specify if you want Kafka Connect to embed the schema in the JSON itself. When you specify converter-specific configurations, always use the <code class="language-plaintext highlighter-rouge">key.converter</code>. or <code class="language-plaintext highlighter-rouge">value.converter</code>. prefix. For example, to use Avro for the message payload, youâ€™d specify the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"value.converter": "io.confluent.connect.avro.AvroConverter",
"value.converter.schema.registry.url": "http://schema-registry:8081",
</code></pre></div></div>

<p>Common converters include:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Avro</span>
io.confluent.connect.avro.AvroConverter

<span class="c"># Protobuf</span>
io.confluent.connect.protobuf.ProtobufConverter

<span class="c"># String</span>
org.apache.kafka.connect.storage.StringConverter

<span class="c"># JSON</span>
org.apache.kafka.connect.json.JsonConverter

<span class="c"># JSON schema</span>
io.confluent.connect.json.JsonSchemaConverter

<span class="c"># ByteArray</span>
org.apache.kafka.connect.converters.ByteArrayConverter
</code></pre></div></div>

<p>JSONì˜ ê²½ìš° ìŠ¤í‚¤ë§ˆê°€ ì„¤ì •ì„ ì•ˆí•˜ëŠ” ê²ƒì´ ë””í´íŠ¸ë‹¤. í•˜ì§€ë§Œ ìŠ¤í‚¤ë§ˆë¥¼ ê³ ì •í•˜ê³  ì‹¶ì€ ê²½ìš° ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>

<ol>
  <li>JSON schema <code class="language-plaintext highlighter-rouge">io.confluent.connect.json.JsonSchemaConverter</code>ë¥¼ ì“´ë‹¤ (with ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"value.converter": "io.confluent.connect.json.JsonSchemaConverter",
"value.converter.schema.registry.url": "http://schema-registry:8081",
</code></pre></div>    </div>
  </li>
  <li>ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ë§¤ë²ˆ ë©”ì‹œì§€ì— ìŠ¤í‚¤ë§ˆë¥¼ ë‹´ì•„ì„œ ì „ì†¡/ì €ì¥í•œë‹¤.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
</code></pre></div>    </div>
  </li>
</ol>

<p>2ë²ˆ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ë©”ì„¸ì§€ê°€ ë‹¤ìŒê³¼ ê°™ì´ schema ë¶€ë¶„ê³¼, payload ë¶€ë¶„ì´ í•¨ê»˜ ì €ì¥ëœë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "int64",
        "optional": false,
        "field": "registertime"
      },
      {
        "type": "string",
        "optional": false,
        "field": "userid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "regionid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "gender"
      }
    ],
    "optional": false,
    "name": "ksql.users"
  },
  "payload": "Hello World"
}
</code></pre></div></div>

<p>ì´ë ‡ê²Œ í•˜ë©´ ë©”ì„¸ì§€ ì‚¬ì´ì¦ˆê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— ë¹„íš¨ìœ¨ì ì´ë‹¤. ê·¸ë˜ì„œ ìŠ¤í‚¤ë§ˆê°€ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤.</p>

<p>ë§Œì•½ ì»¨ë²„í„°ì— JSON serializerë¥¼ ì‚¬ìš©í–ˆê³  ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¡œ ì„¤ì •í•˜ì§€ ì•Šì„ê±°ë¼ë©´,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
</code></pre></div></div>

<p>ì´ë ‡ê²Œ schemaë¥¼ ì°¾ì„ í•„ìš” ì—†ë‹¤ê³  ëª…ì‹œí•´ì£¼ì. (ë””í´íŠ¸ê°€ falseì¸ë° ì™œ í•´ì¤˜ì•¼í•˜ëŠ”ê±°ì§€..?)</p>

<p>ì•„ë˜ í‘œëŠ” serializerì™€ deserializerì˜ ì‹±í¬ë¥¼ ì–´ë–»ê²Œ ë§ì¶°ì•¼ ì—ëŸ¬ê°€ ì•ˆë‚˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ serializerëŠ” ë©”ì„¸ì§€ë‚˜ ìƒí™©ì— ë§ê²Œ ì›í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ê³ , deserializerëŠ” serializerì™€ ê°™ì€ í¬ë§·ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ë©´ ëœë‹¤.</p>

<p><img src="/images/kafka_79.png" alt="" /></p>

<h1 id="ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜">ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜</h1>

<p>To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.</p>

<p>Just increasing the number of consumers wonâ€™t increase the parallelism. You need to scale your partitions accordingly. To read data from a topic in parallel with two consumers, you create two partitions so that each consumer can read from its own partition. Also since partitions of a topic can be on different brokers, two consumers of a topic can read the data from two different brokers.</p>

<h1 id="ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ">ì¥ì•  ë³µêµ¬ë¥¼ ìœ„í•œ ë³µì œ</h1>
<p>Letâ€™s talk about replication. Whenever weâ€™re creating a topic in Kafka, we need to specify the replication factor we need for that topic. Letâ€™s say weâ€™ve two brokers and so weâ€™ve given the replication-factor as 2. What this means is that Kafka will try to always ensure that each partition of this topic has a backup/replica. The way Kafka distributes the partitions is quite similar to how HDFS distributes its data blocks across nodes.</p>

<p>Say for the freblogg topic that weâ€™ve been using so far, weâ€™ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this.</p>

<p><img src="/images/kafka_77.png" alt="" /></p>

<p>Even when you have a replicated partition on a different broker, Kafka wouldnâ€™t let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition.</p>

<p>A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.</p>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>
<ul>
  <li><a href="https://docs.cloudera.com/csa/1.2.0/flink-sql-table-api/topics/csa-kafka-sql-datatypes.html" target="_blank">Data types for Kafka connector</a></li>
  <li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/" target="_blank">Kafka Connect Deep Dive â€“ Converters and Serialization Explained</a></li>
  <li><a href="https://dol9.tistory.com/274" target="_blank">dol9, Kafka ìŠ¤í‚¤ë§ˆ ê´€ë¦¬, Schema Registry</a></li>
  <li><a href="https://www.freblogg.com/kafka-storage-internals" target="_blank">A Practical Introduction to Kafka Storage Internals</a></li>
  <li><a href="https://www.freecodecamp.org/news/what-makes-apache-kafka-so-fast-a8d4f94ab145/" target="_blank">Hereâ€™s what makes Apache Kafka so fast</a></li>
  <li><a href="https://stackoverflow.com/questions/40369238/which-directory-does-apache-kafka-store-the-data-in-broker-nodes#" target="_blank">stackoverflow: Which directory does apache kafka store the data in broker nodes</a></li>
  <li><a href="https://medium.com/@abhisheksharma_59226/how-kafka-stores-data-37ee611c89a2" target="_blank">Abhishek Sharma, How kafka stores data</a></li>
  <li><a href="https://rohithsankepally.github.io/Kafka-Storage-Internals/" target="_blank">Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals</a></li>
  <li><a href="https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7" target="_blank">towardsdatascience, Log Compacted Topics in Apache Kafka</a></li>
  <li><a href="https://www.conduktor.io/understanding-kafkas-internal-storage-and-log-retention" target="_blank">conduktor, Understanding Kafkaâ€™s Internal Storage and Log Retention</a></li>
  <li><a href="https://dev.to/heroku/what-is-a-commit-log-and-why-should-you-care-pib" target="_blank">What is a commit log and why should you care?</a></li>
</ul>
:ET
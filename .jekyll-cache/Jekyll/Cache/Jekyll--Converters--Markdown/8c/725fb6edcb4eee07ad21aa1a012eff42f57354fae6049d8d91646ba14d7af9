I"ބ<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#overview" id="markdown-toc-overview">Overview</a>    <ul>
      <li><a href="#output-modes" id="markdown-toc-output-modes">Output modes</a></li>
      <li><a href="#example" id="markdown-toc-example">Example</a></li>
    </ul>
  </li>
  <li><a href="#input-sources" id="markdown-toc-input-sources">Input Sources</a>    <ul>
      <li><a href="#socket" id="markdown-toc-socket">Socket</a></li>
      <li><a href="#file" id="markdown-toc-file">File</a></li>
      <li><a href="#kafka" id="markdown-toc-kafka">Kafka</a></li>
    </ul>
  </li>
  <li><a href="#output-sinks" id="markdown-toc-output-sinks">Output Sinks</a>    <ul>
      <li><a href="#file-1" id="markdown-toc-file-1">File</a></li>
      <li><a href="#kafka-1" id="markdown-toc-kafka-1">Kafka</a></li>
      <li><a href="#foreachbatch" id="markdown-toc-foreachbatch">foreachBatch</a></li>
      <li><a href="#foreach" id="markdown-toc-foreach">foreach</a></li>
    </ul>
  </li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<h1 id="overview">Overview</h1>

<p>Working with streaming data is a little different from working with batch data. With streaming data, we will never have complete data for analysis, as data is continuously coming in. <strong>Apache Spark provides a streaming API to analyze streaming data in pretty much the same way we work with batch data.</strong> Apache Spark Structured Streaming is built on top of the Spark-SQL API to leverage its optimization. Spark Streaming is a processing engine to process data in real-time from sources and output data to external storage systems.</p>

<p>Spark Streaming has 3 major components: input sources, streaming engine, and sink. Input sources generate data like Kafka, Flume, HDFS/S3, etc. Spark Streaming engine processes incoming data from various input sources. Sinks store processed data from Spark Streaming engine like HDFS, relational databases, or NoSQL datastores.</p>

<p>Let’s conceptualise Spark Streaming data as an unbounded table where new data will always be appended at the end of the table.</p>

<p><img src="/images/spark_stream_1.png" alt="" /></p>

<p>Spark will process data in micro-batches which can be defined by triggers. For example, let’s say we define a trigger as <code class="language-plaintext highlighter-rouge">1 second</code>, this means Spark will create micro-batches every second and process them accordingly.</p>

<h2 id="output-modes">Output modes</h2>

<p>After processing the streaming data, Spark needs to store it somewhere on persistent storage. Spark uses various output modes to store the streaming data.</p>

<ul>
  <li><strong>Append Mode</strong>: In this mode, Spark will output only newly processed rows since the last trigger.</li>
  <li><strong>Update Mode</strong>: In this mode, Spark will output only updated rows since the last trigger. If we are not using aggregation on streaming data (meaning previous records can’t be updated) then it will behave similarly to append mode.</li>
  <li><strong>Complete Mode</strong>: In this mode, Spark will output all the rows it has processed so far.</li>
</ul>

<h2 id="example">Example</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="kn">import</span> <span class="nn">pyspark.sql.functions</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"test"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"spark://spark-master:7077"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">setLogLevel</span><span class="p">(</span><span class="s">"ERROR"</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">readStream</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"rate"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"rowsPerSecond"</span><span class="p">,</span> <span class="mi">3</span><span class="p">).</span><span class="n">load</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">df</span><span class="p">.</span><span class="n">isStreaming</span> <span class="o">==</span> <span class="bp">True</span>

<span class="n">result_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">withColumn</span><span class="p">(</span><span class="s">"result"</span><span class="p">,</span> <span class="n">F</span><span class="p">.</span><span class="n">col</span><span class="p">(</span><span class="s">"value"</span><span class="p">)</span> <span class="o">+</span> <span class="n">F</span><span class="p">.</span><span class="n">lit</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>

<span class="n">result_df</span><span class="p">.</span><span class="n">writeStream</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"console"</span><span class="p">).</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">).</span><span class="n">start</span><span class="p">().</span><span class="n">awaitTermination</span><span class="p">()</span>
<span class="o">------------------------------------------------------------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">1</span>
<span class="o">-------------------------------------------</span>
<span class="o">+--------------------+-----+------+</span>
<span class="o">|</span>           <span class="n">timestamp</span><span class="o">|</span><span class="n">value</span><span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------+-----+------+</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">0</span><span class="o">|</span>     <span class="mi">1</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">2</span><span class="o">|</span>     <span class="mi">3</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">1</span><span class="o">|</span>     <span class="mi">2</span><span class="o">|</span>
<span class="o">+--------------------+-----+------+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">2</span>
<span class="o">-------------------------------------------</span>
<span class="o">+--------------------+-----+------+</span>
<span class="o">|</span>           <span class="n">timestamp</span><span class="o">|</span><span class="n">value</span><span class="o">|</span><span class="n">result</span><span class="o">|</span>
<span class="o">+--------------------+-----+------+</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">3</span><span class="o">|</span>     <span class="mi">4</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">5</span><span class="o">|</span>     <span class="mi">6</span><span class="o">|</span>
<span class="o">|</span><span class="mi">2023</span><span class="o">-</span><span class="mi">01</span><span class="o">-</span><span class="mi">01</span> <span class="mi">11</span><span class="p">:</span><span class="mi">35</span><span class="p">:...</span><span class="o">|</span>    <span class="mi">4</span><span class="o">|</span>     <span class="mi">5</span><span class="o">|</span>
<span class="o">+--------------------+-----+------+</span>
</code></pre></div></div>

<h1 id="input-sources">Input Sources</h1>

<p>Spark Streaming ingests data from different types of input sources for processing in real-time.</p>

<ul>
  <li><strong>Rate</strong> (for Testing): It will automatically generate data including 2 columns timestamp and value . This is generally used for testing purposes.</li>
  <li><strong>Socket</strong> (for Testing): This data source will listen to the specified socket and ingest any data into Spark Streaming. It is also used only for testing purposes.</li>
  <li><strong>File</strong>: This will listen to a particular directory as streaming data. It supports file formats like CSV, JSON, ORC, and Parquet. You can find the latest supported file format list here.</li>
  <li><strong>Kafka</strong>: This will read data from Apache Kafka and is compatible with Kafka broker versions 0.10.0 or higher</li>
</ul>

<h2 id="socket">Socket</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>apt update
apt install netcat

nc -lk 9999
</code></pre></div></div>

<p><img src="/images/spark_stream_2.png" alt="" /></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Socket Source
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">readStream</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"socket"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"host"</span><span class="p">,</span> <span class="s">"127.0.0.1"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"port"</span><span class="p">,</span> <span class="mi">9999</span><span class="p">).</span><span class="n">load</span><span class="p">()</span>

<span class="k">assert</span> <span class="n">df</span><span class="p">.</span><span class="n">isStreaming</span> <span class="o">==</span> <span class="bp">True</span>

<span class="n">df</span><span class="p">.</span><span class="n">writeStream</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"console"</span><span class="p">).</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">).</span><span class="n">start</span><span class="p">().</span><span class="n">awaitTermination</span><span class="p">()</span>
<span class="o">------------------------------------------------------------------------------------------------------------------</span>
<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">0</span>
<span class="o">-------------------------------------------</span>
<span class="o">+-----+</span>
<span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+-----+</span>
<span class="o">+-----+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">1</span>
<span class="o">-------------------------------------------</span>
<span class="o">+------+</span>
<span class="o">|</span> <span class="n">value</span><span class="o">|</span>
<span class="o">+------+</span>
<span class="o">|</span><span class="n">London</span><span class="o">|</span>
<span class="o">+------+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">2</span>
<span class="o">-------------------------------------------</span>
<span class="o">+-----+</span>
<span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+-----+</span>
<span class="o">|</span><span class="n">Paris</span><span class="o">|</span>
<span class="o">+-----+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">3</span>
<span class="o">-------------------------------------------</span>
<span class="o">+-----+</span>
<span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+-----+</span>
<span class="o">|</span><span class="n">Seoul</span><span class="o">|</span>
<span class="o">+-----+</span>
</code></pre></div></div>

<h2 id="file">File</h2>

<p>With file input source, our application will wait for available data in the specified directory. We will use some of the stock data available here. For example, Apple stock data present in this file: <a href="https://github.com/szrlee/Stock-Time-Series-Analysis/blob/master/data/AAPL_2006-01-01_to_2018-01-01.csv">AAPL_2006–01–01_to_2018–01–01.csv</a>. We will take the data for a few years like 2015, 2016, and 2017 and manually save it to a different file like AAPL_2015.csv, AAPL_2016.csvand AAPL_2017.csv respectively. Similarly, we will create the sample data for Google, Amazon, and Microsoft as well. We will keep all the CSV files locally under data/stocks folder. Also, create another folder data/stream which we will use to simulate the streaming data.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">//</span><span class="w"> </span><span class="err">a.json</span><span class="w">
</span><span class="p">{</span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Paris"</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">//</span><span class="w"> </span><span class="err">b.json</span><span class="w">
</span><span class="p">{</span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"Seoul"</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">//</span><span class="w"> </span><span class="err">c.json</span><span class="w">
</span><span class="p">{</span><span class="nl">"message"</span><span class="p">:</span><span class="w"> </span><span class="s2">"London"</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># File Source
</span>
<span class="n">ACCESS_KEY</span> <span class="o">=</span> <span class="s">"&lt;AWS ACCEESS KEY&gt;"</span>
<span class="n">SECRET_KEY</span> <span class="o">=</span> <span class="s">"&lt;AWS SECRET KEY&gt;"</span>
<span class="n">HADOOP_VERSION</span> <span class="o">=</span> <span class="s">"3.2.4"</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"test"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"spark://spark-master:7077"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.jars.packages"</span><span class="p">,</span> <span class="sa">f</span><span class="s">"org.apache.hadoop:hadoop-aws:</span><span class="si">{</span><span class="n">HADOOP_VERSION</span><span class="si">}</span><span class="s">,org.apache.hadoop:hadoop-client:</span><span class="si">{</span><span class="n">HADOOP_VERSION</span><span class="si">}</span><span class="s">"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.jars.excludes"</span><span class="p">,</span> <span class="s">"com.google.guava:guava"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.hadoop.fs.s3.impl"</span><span class="p">,</span> <span class="s">"org.apache.hadoop.fs.s3a.S3AFileSystem"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">'spark.hadoop.fs.s3a.aws.credentials.provider'</span><span class="p">,</span> <span class="s">'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider'</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.hadoop.fs.s3a.access.key"</span><span class="p">,</span> <span class="n">ACCESS_KEY</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.hadoop.fs.s3a.secret.key"</span><span class="p">,</span> <span class="n">SECRET_KEY</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">setLogLevel</span><span class="p">(</span><span class="s">"ERROR"</span><span class="p">)</span>

<span class="n">schema</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">([</span><span class="n">StructField</span><span class="p">(</span><span class="s">"message"</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="n">nullable</span><span class="o">=</span><span class="bp">False</span><span class="p">)])</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">readStream</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"json"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"path"</span><span class="p">,</span> <span class="sa">f</span><span class="s">"s3a://jay-ml-models/test/"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"truncate"</span><span class="p">,</span> <span class="bp">False</span><span class="p">).</span><span class="n">schema</span><span class="p">(</span><span class="n">schema</span><span class="p">).</span><span class="n">load</span><span class="p">()</span>
<span class="o">-----------------------------------------------------------------------------------------------------------------------------</span>
<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">0</span>
<span class="o">-------------------------------------------</span>
<span class="o">+-------+</span>
<span class="o">|</span><span class="n">message</span><span class="o">|</span>
<span class="o">+-------+</span>
<span class="o">|</span>  <span class="n">Paris</span><span class="o">|</span>
<span class="o">|</span>  <span class="n">Seoul</span><span class="o">|</span>
<span class="o">+-------+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">2</span>
<span class="o">-------------------------------------------</span>
<span class="o">+-------+</span>
<span class="o">|</span><span class="n">message</span><span class="o">|</span>
<span class="o">+-------+</span>
<span class="o">|</span> <span class="n">London</span><span class="o">|</span>
<span class="o">+-------+</span>
</code></pre></div></div>

<h2 id="kafka">Kafka</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Kafka Source
</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"test"</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">'spark://spark-master:7077'</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">'spark.jars'</span><span class="p">,</span> <span class="s">'/opt/spark/jars/kafka-clients-3.3.1.jar,/opt/spark/jars/spark-sql-kafka-0-10_2.13-3.2.2.jar,/opt/spark/jars/spark-token-provider-kafka-0-10_2.13-3.2.2.jar,/opt/spark/jars/commons-pool2-2.11.1.jar'</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">result_df</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">selectExpr</span><span class="p">(</span><span class="s">"CAST(key AS STRING)"</span><span class="p">,</span> <span class="s">"CAST(value AS STRING)"</span><span class="p">)</span>

<span class="n">result_df</span><span class="p">.</span><span class="n">writeStream</span> \
          <span class="p">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">)</span> \
          <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"console"</span><span class="p">)</span> \
          <span class="p">.</span><span class="n">start</span><span class="p">().</span><span class="n">awaitTermination</span><span class="p">()</span>
<span class="o">--------------------------------------------------------------------------------</span>
<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">1</span>
<span class="o">-------------------------------------------</span>
<span class="o">+----+-----+</span>
<span class="o">|</span> <span class="n">key</span><span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+----+-----+</span>
<span class="o">|</span><span class="n">null</span><span class="o">|</span><span class="n">paris</span><span class="o">|</span>
<span class="o">+----+-----+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">2</span>
<span class="o">-------------------------------------------</span>
<span class="o">+----+-----+</span>
<span class="o">|</span> <span class="n">key</span><span class="o">|</span><span class="n">value</span><span class="o">|</span>
<span class="o">+----+-----+</span>
<span class="o">|</span><span class="n">null</span><span class="o">|</span><span class="n">seoul</span><span class="o">|</span>
<span class="o">+----+-----+</span>

<span class="o">-------------------------------------------</span>
<span class="n">Batch</span><span class="p">:</span> <span class="mi">3</span>
<span class="o">-------------------------------------------</span>
<span class="o">+----+------+</span>
<span class="o">|</span> <span class="n">key</span><span class="o">|</span> <span class="n">value</span><span class="o">|</span>
<span class="o">+----+------+</span>
<span class="o">|</span><span class="n">null</span><span class="o">|</span><span class="n">london</span><span class="o">|</span>
<span class="o">+----+------+</span>

</code></pre></div></div>

<h1 id="output-sinks">Output Sinks</h1>

<p>In Spark Streaming, output sinks store results into external storage.</p>

<ul>
  <li><strong>Console sink</strong>: Displays the content of the DataFrame to console.</li>
  <li><strong>File sink</strong>: Stores the contents of a DataFrame in a file within a directory. Supported file formats are <code class="language-plaintext highlighter-rouge">csv</code>, <code class="language-plaintext highlighter-rouge">json</code>, <code class="language-plaintext highlighter-rouge">orc</code>, and <code class="language-plaintext highlighter-rouge">parquet</code>.</li>
  <li><strong>Kafka sink</strong>: Publishes data to a Kafka topic.</li>
  <li><strong>Foreach sink</strong>: Applies to each row of a DataFrame and can be used when writing custom logic to store data.</li>
  <li><strong>ForeachBatch sink</strong>: Applies to each micro-batch of a DataFrame and also can be used when writing custom logic to store data.</li>
</ul>

<h2 id="file-1">File</h2>

<p>The file sink stores the contents of a streaming DataFrame to a specified directory and format.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">resultDf</span>
  <span class="p">.</span><span class="n">writeStream</span>
  <span class="p">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">)</span> <span class="o">//</span> <span class="n">Filesink</span> <span class="n">only</span> <span class="n">support</span> <span class="n">Append</span> <span class="n">mode</span><span class="p">.</span>
  <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"csv"</span><span class="p">)</span> <span class="o">//</span> <span class="n">supports</span> <span class="n">these</span> <span class="n">formats</span> <span class="p">:</span> <span class="n">csv</span><span class="p">,</span> <span class="n">json</span><span class="p">,</span> <span class="n">orc</span><span class="p">,</span> <span class="n">parquet</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"path"</span><span class="p">,</span> <span class="s">"output/filesink_output"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"header"</span><span class="p">,</span> <span class="n">true</span><span class="p">)</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"checkpointLocation"</span><span class="p">,</span> <span class="s">"checkpoint/filesink_checkpoint"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">start</span><span class="p">()</span>
  <span class="p">.</span><span class="n">awaitTermination</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="kafka-1">Kafka</h2>

<p>With the kafka sink, we publish the content of our streaming DataFrame to a Kafka topic.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">resultDf</span>
  <span class="p">.</span><span class="n">writeStream</span>
  <span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"kafka"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"kafka.bootstrap.servers"</span><span class="p">,</span> <span class="s">"localhost:9092"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"topic"</span><span class="p">,</span> <span class="s">"testConsumer"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">option</span><span class="p">(</span><span class="s">"checkpointLocation"</span><span class="p">,</span> <span class="s">"checkpoint/kafka_checkpoint"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">start</span><span class="p">()</span>
  <span class="p">.</span><span class="n">awaitTermination</span><span class="p">()</span>

</code></pre></div></div>

<h2 id="foreachbatch">foreachBatch</h2>

<p>So far we discussed sinks where the output system was already defined like <code class="language-plaintext highlighter-rouge">file</code>, <code class="language-plaintext highlighter-rouge">kafka</code>, or <code class="language-plaintext highlighter-rouge">console</code>. What if we would like to store data in any arbitrary storage like a NoSQL DB (for example MongoDB) or a Relational DB (like MySQL). By using <code class="language-plaintext highlighter-rouge">foreach</code> and <code class="language-plaintext highlighter-rouge">foreachBatch</code> we can write custom logic to store data. <code class="language-plaintext highlighter-rouge">foreach</code> performs custom write logic on each row and <code class="language-plaintext highlighter-rouge">foreachBatch</code> performs custom write logic on each micro-batch.</p>

<p>Use the <code class="language-plaintext highlighter-rouge">savetoMySQL()</code> function to save our streaming DataFrame to MySQL</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">resultDf</span>
  <span class="p">.</span><span class="n">writeStream</span>
  <span class="p">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">foreachBatch</span><span class="p">(</span><span class="n">saveToMySql</span><span class="p">)</span>
  <span class="p">.</span><span class="n">start</span><span class="p">()</span>
  <span class="p">.</span><span class="n">awaitTermination</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="foreach">foreach</h2>

<p>The foreach output sink performs custom write logic to <strong>each record</strong> in a streaming DataFrame. If <code class="language-plaintext highlighter-rouge">foreachBatch</code> is not an option, e.g. in continuous processing mode or if a batch data writer does not exist, then we can use the foreach sink for custom write logic.</p>

<p>To use foreach we need to implement 3 methods (open, process, and close).</p>

<ul>
  <li><strong>open</strong>: function to open connection</li>
  <li><strong>process</strong>: write data to the specified connection</li>
  <li><strong>close</strong>: function to close connection</li>
</ul>

<p>Then we create an instance of the <code class="language-plaintext highlighter-rouge">ForeachWriter</code> class and implement the <code class="language-plaintext highlighter-rouge">open()</code>, <code class="language-plaintext highlighter-rouge">process()</code>, and <code class="language-plaintext highlighter-rouge">close()</code> methods.</p>

<p>Use the <code class="language-plaintext highlighter-rouge">ForeachWriter</code> instance defined above to write data using the foreach sink.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">initDF</span>
  <span class="p">.</span><span class="n">writeStream</span>
  <span class="p">.</span><span class="n">outputMode</span><span class="p">(</span><span class="s">"append"</span><span class="p">)</span>
  <span class="p">.</span><span class="n">foreach</span><span class="p">(</span><span class="n">customWriter</span><span class="p">)</span>
  <span class="p">.</span><span class="n">start</span><span class="p">()</span>
  <span class="p">.</span><span class="n">awaitTermination</span><span class="p">()</span>
</code></pre></div></div>

<h1 id="참고">참고</h1>

<ul>
  <li><a href="https://medium.com/analytics-vidhya/apache-spark-structured-streaming-with-pyspark-b4a054a7947d" target="_blank">Apache Spark Structured Streaming with Pyspark</a></li>
  <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.ss/index.html" target="_blank">pyspark, Structured Streaming</a></li>
  <li><a href="https://sparkbyexamples.com/spark/spark-streaming-outputmode/" target="_blank">Spark Streaming – Different Output modes explained</a></li>
  <li><a href="https://medium.com/expedia-group-tech/apache-spark-structured-streaming-first-streaming-example-1-of-6-e8f3219748ef" target="_blank">Apache Spark Structured Streaming — First Streaming Example (1 of 6)</a></li>
  <li><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank">Spark 공식문서, Pyspark Structured Streaming Guide</a></li>
</ul>
:ET
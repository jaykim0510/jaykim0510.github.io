I"(<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#rdd-dataframe-dataset" id="markdown-toc-rdd-dataframe-dataset">RDD, Dataframe, Dataset</a>    <ul>
      <li><a href="#rdd" id="markdown-toc-rdd">RDD</a></li>
      <li><a href="#dataframe" id="markdown-toc-dataframe">Dataframe</a></li>
      <li><a href="#dataset" id="markdown-toc-dataset">Dataset</a></li>
    </ul>
  </li>
  <li><a href="#dataframe-1" id="markdown-toc-dataframe-1">Dataframe</a>    <ul>
      <li><a href="#dataframe-생성" id="markdown-toc-dataframe-생성">Dataframe 생성</a></li>
      <li><a href="#기본-연산" id="markdown-toc-기본-연산">기본 연산</a></li>
      <li><a href="#액션-연산" id="markdown-toc-액션-연산">액션 연산</a></li>
      <li><a href="#비타입-트랜스포메이션-연산" id="markdown-toc-비타입-트랜스포메이션-연산">비타입 트랜스포메이션 연산</a></li>
    </ul>
  </li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<h1 id="rdd-dataframe-dataset">RDD, Dataframe, Dataset</h1>

<h2 id="rdd">RDD</h2>

<p><img src="../images/../../images/spark_15.jpg" alt="" /></p>

<p>Spark Core에 RDD가 있다면 Spark SQL에는 Dataframe과 Dataset이 있습니다. 기존의 RDD를 이용해 스파크 애플리케이션 코드를 작성할 때에는 RDD가 가지고 있는 메서드나 특성을 알아야지만 코드를 작성할 수 있었습니다. 그래서 RDD에 대한 이해도가 높아야 분산 환경에서 높은 처리 성능을 이끌어 낼 수 있었습니다.</p>

<h2 id="dataframe">Dataframe</h2>

<p><img src="../images/../../images/spark_16.jpg" alt="" /></p>

<p>그러던 중 Spark 1.3버전에서 Dataframe이라는 새로운 데이터 모델이 공개되었습니다. Dataframe은 개발자들에게 친숙한 SQL과 비슷한 방식으로 작성할 수 있도록하는 API를 제공해 진입 장벽을 낮췄으며 코드의 가독성 또한 높여주었습니다. Dataframe도 마찬가지로 low-level에서는 RDD로 코드가 동작하는데 Spark SQL은 내부적으로 Catalyst Optimizer를 통해 최적의 RDD 코드로 변환됩니다. 따라서 쉬운 코드 작성과 높은 성능을 모두 얻게되었습니다.</p>

<p>그러나 Dataframe에도 아쉬운 점이 있었는데, 바로 RDD에서 가능했던 컴파일 타임 오류 체크 기능을 사용할 수 없다는 점이었습니다.</p>

<h2 id="dataset">Dataset</h2>
<p>Spark 1.6버전에서 RDD의 장점과 Dataframe의 장점을 합친 새로운 데이터 모델인 Dataset이 등장했습니다.</p>

<p>그리고 Spark 2.0 이후부터는 Dataframe이 Dataset 안에 포함되었습니다.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 데이터셋은 데이터를 처리할 때 데이터의 타입을 있는 그대로 활용할 수 있습니다.</span>
데이터셋: Dataset[String], Dataset[Int]

<span class="c"># 데이터프레임은 데이터를 처리할 때 데이터 타입을 무조건 org.apache.spark.sql.Row로 감싸줘야 합니다.</span>
데이터프레임: Dataset[Row]
</code></pre></div></div>

<p><img src="../images/../../images/spark_17.png" alt="" /></p>

<p>이렇게 Dataframe은 원래 데이터가 가지고 있던 타입의 특성은 사용하지 않기 때문에 Dataframe API은 비타입 트랜스포메이션 연산(untyped operations)으로 분류됩니다.</p>

<table>
  <tbody>
    <tr>
      <td><strong>데이터 모델</strong></td>
      <td><strong>사용 가능한 연산</strong></td>
    </tr>
    <tr>
      <td>Dataframe</td>
      <td>기본 연산, 액션 연산, 비타입 트랜스포메이션 연산</td>
    </tr>
    <tr>
      <td>Dataset</td>
      <td>기본 연산, 액션 연산, 타입 트랜스포메이션 연산</td>
    </tr>
  </tbody>
</table>

<p>저는 파이썬을 주언어로 사용하고 있으며 파이썬 언어는 Dataframe API만 제공하기 때문에 이번 포스트에서는 액션 연산과 비타입 트랜스포메이션 연산에 대해서만 다루도록 하겠습니다.</p>

<h1 id="dataframe-1">Dataframe</h1>

<h2 id="dataframe-생성">Dataframe 생성</h2>
<p>Dataframe은 SparkSession을 이용해 생성합니다. 생성 방법은 <strong>파일이나 데이터베이스와 같은 스파크 외부</strong>에 저장된 데이터를 이용할 수도 있고, <strong>스파크 내에서의 RDD나 Dataframe</strong>을 이용해 새로운 Dataframe을 생성할 수도 있습니다.</p>

<p><strong>외부 데이터 소스</strong></p>

<p>파일이나 데이터베이스같은 외부 저장소의 데이터를 읽어와서 Dataframe을 생성할 때는 SparkSession의 <code class="language-plaintext highlighter-rouge">read()</code>메소드를 이용하면 됩니다. <code class="language-plaintext highlighter-rouge">read()</code>메소드는 DataFrameReader 인스턴스를 생성하고 이를 이용해 다양한 유형의 데이터를 읽고 Dataframe을 생성할 수 있습니다.</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"sample"</span><span class="p">).</span><span class="n">master</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="s">"json"</span><span class="p">).</span><span class="n">option</span><span class="p">(</span><span class="s">"allowComments"</span><span class="p">,</span> <span class="s">"true"</span><span class="p">).</span><span class="n">load</span><span class="p">(</span><span class="s">"&lt;spark_home_dir&gt;/test.json"</span><span class="p">)</span>  
</code></pre></div></div>

<p>전체적인 생성 과정은 크게 다음과 같습니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. Spark Session의 read() 메소드를 호출해 DataFrameReader 인스턴스 생성
2. format() 메소드로 데이터소스의 유형을 지정
3. option() 메소드로 데이터소스 처리에 필요한 옵션을 지정
4. load() 메소드로 대상 파일을 읽고 데이터프레임을 생성
</code></pre></div></div>

<p>다음은 DataFrameReader가 제공하는 주요 메소드입니다.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- format()
    읽어들이고자 하는 데이터 소스의 유형을 문자열로 지정("kafka", "csv", "json", "parquet", "text" 등)
    이 밖에도 지원하지 않는 데이터소스는 라이브러리를 클래스패스에 추가해서 사용할 수 있습니다

- option/options()
    데이터소스에 사용할 설정 정보를 지정
    데이터소스에  따라 다름

- load()
    데이터소스로부터 실제 데이터를 읽어서 Dataframe을 생성

- json()
    JSON 형식을 따르는 문자열로 구성된 파일이나 RDD로부터 Dataframe 생성

- parquet()
    파케이 형식응로 작성된 파일을 읽어서 Dataframe 생성

- text()
    일반 텍스트 형식으로 작성된 파일을 읽어서 Dataframe 생성

- csv()
    CSV 파일을 읽어 Dataframe 생성 
</code></pre></div></div>

<p><strong>RDD, Dataframe</strong></p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"sample"</span><span class="p">).</span><span class="n">master</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">row1</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"kim"</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">job</span><span class="o">=</span><span class="s">"student"</span><span class="p">)</span>
<span class="n">row2</span> <span class="o">=</span> <span class="n">Row</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">"mike"</span><span class="p">,</span> <span class="n">age</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">job</span><span class="o">=</span><span class="s">"student"</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">row1</span><span class="p">,</span> <span class="n">row2</span><span class="p">]</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div>
<h2 id="기본-연산">기본 연산</h2>
<h2 id="액션-연산">액션 연산</h2>

<h2 id="비타입-트랜스포메이션-연산">비타입 트랜스포메이션 연산</h2>

<h1 id="참고">참고</h1>
<ul>
  <li><a href="http://www.kyobobook.co.kr/product/detailViewKor.laf?ejkGb=KOR&amp;mallGb=KOR&amp;barcode=9791158391034&amp;orderClick=LEa&amp;Kc=" target="_blank">빅데이터 분석을 위한 스파크2 프로그래밍 책</a></li>
  <li><a href="https://loustler.io/data_eng/spark-rdd-dataframe-and-dataset/" target="_blank">loustler, [Apache Spark] Spark RDD, Dataframe and DataSet</a></li>
</ul>
:ET
I"C<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€" id="markdown-toc-ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€">ë¶„ì‚° ì‹œìŠ¤í…œì´ë€</a></li>
  <li><a href="#ì¥ì " id="markdown-toc-ì¥ì ">ì¥ì </a>    <ul>
      <li><a href="#performance" id="markdown-toc-performance">Performance</a></li>
      <li><a href="#scalability" id="markdown-toc-scalability">Scalability</a></li>
      <li><a href="#availability" id="markdown-toc-availability">Availability</a></li>
    </ul>
  </li>
  <li><a href="#high-levelì—ì„œì˜-ë™ì‘" id="markdown-toc-high-levelì—ì„œì˜-ë™ì‘">High-levelì—ì„œì˜ ë™ì‘</a>    <ul>
      <li><a href="#partitioning" id="markdown-toc-partitioning">Partitioning</a></li>
      <li><a href="#replication" id="markdown-toc-replication">Replication</a></li>
    </ul>
  </li>
  <li><a href="#ì–´ë ¤ìš´-ì " id="markdown-toc-ì–´ë ¤ìš´-ì ">ì–´ë ¤ìš´ ì </a>    <ul>
      <li><a href="#synchronization" id="markdown-toc-synchronization">Synchronization</a>        <ul>
          <li><a href="#clock-synchronization" id="markdown-toc-clock-synchronization">Clock Synchronization</a></li>
          <li><a href="#data-synchronization" id="markdown-toc-data-synchronization">Data Synchronization</a></li>
        </ul>
      </li>
      <li><a href="#network-asynchrony" id="markdown-toc-network-asynchrony">Network Asynchrony</a></li>
      <li><a href="#partial-failures" id="markdown-toc-partial-failures">Partial Failures</a></li>
    </ul>
  </li>
  <li><a href="#low-levelì—ì„œì˜-ë™ì‘" id="markdown-toc-low-levelì—ì„œì˜-ë™ì‘">Low-levelì—ì„œì˜ ë™ì‘</a>    <ul>
      <li><a href="#consensus-algorithm" id="markdown-toc-consensus-algorithm">Consensus Algorithm</a>        <ul>
          <li><a href="#raft" id="markdown-toc-raft">RAFT</a></li>
        </ul>
      </li>
      <li><a href="#failure-detector-timeout" id="markdown-toc-failure-detector-timeout">Failure Detector (Timeout)</a></li>
      <li><a href="#de-duplication-algorithm" id="markdown-toc-de-duplication-algorithm">De-duplication Algorithm</a></li>
    </ul>
  </li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€">ë¶„ì‚° ì‹œìŠ¤í…œì´ë€</h1>

<blockquote>
  <p>A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another</p>
</blockquote>

<p><img src="/images/dis_sys_2.png" alt="" /></p>

<p>ë‚˜ëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì„ ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ìœ¼ë¡œ ì •ë¦¬í•˜ë ¤ê³  í•œë‹¤.</p>

<p><img src="/images/dis_sys_1.png" alt="" /></p>

<h1 id="ì¥ì ">ì¥ì </h1>

<h2 id="performance">Performance</h2>

<p>ì—¬ê¸°ì„œ PerformanceëŠ” ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œì˜ Performanceì™€ ë¹„êµí•´ ê°€ê²©ëŒ€ë¹„ ë” ë‚«ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. Performanceì˜ ì ˆëŒ€ì ì¸ ìˆ˜ì¹˜ ìì²´ê°€ ë” ì˜¤ë¥¼ ì´ìœ ëŠ” ì—†ë‹¤. ì˜¤íˆë ¤ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ìœ¼ë¡œ ê°ì†Œí•  ê°€ëŠ¥ì„±ì€ ìˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶„ì‚° ì‹œìŠ¤í…œì„ ì“°ëŠ” ì´ìœ ëŠ” ê°€ê²©ì ì¸ ì¸¡ë©´ì—ì„œ ê·¸ë§Œí¼ ê°’ì‹¼ ì¥ë¹„ë¥¼ ì—¬ëŸ¬ ëŒ€ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‚«ê³ , ì„±ëŠ¥ì ì¸ ì¸¡ë©´ ì´ì™¸ì—ë„ ë¶„ì‚° ì‹œìŠ¤í…œì´ ì£¼ëŠ” ì¥ì ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h2 id="scalability">Scalability</h2>

<p>ë¶„ì‚° ì‹œìŠ¤í…œì€ ì„œë¹„ìŠ¤ ê·œëª¨, íŠ¸ë˜í”½ëŸ‰, ì‘ì—…ëŸ‰ì— ë”°ë¼ ì‹œìŠ¤í…œì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•´ ì´ë¥¼ í•¸ë“¤ë§í•  ëŠ¥ë ¥ì´ ìˆë‹¤. ë¬¼ë¡  ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œë„ Vertical-scalingì´ ê°€ëŠ¥í•˜ë‹¤. í•˜ì§€ë§Œ Horizontal-scalingì´ ê°€ê²©ì ì¸ ì¸¡ë©´ê³¼ í™•ì¥ì´ ìš©ì´í•˜ë‹¤ëŠ” ì ì—ì„œ ì´ì ì´ ìˆë‹¤.</p>

<h2 id="availability">Availability</h2>

<p>ë¶„ì‚° ì‹œìŠ¤í…œì€ ë…¸ë“œ ì¼ë¶€ì— ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ê³„ì† ê°™ì€ ê¸°ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” 24ì‹œê°„ ë‚´ë‚´ ì¥ì• ì—†ëŠ” ì„œë¹„ìŠ¤ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë§ì´ë‹¤. ë¬¼ë¡  ì´ë¥¼ ìœ„í•´ ìš”êµ¬ë˜ëŠ” ì¡°ê±´ë“¤ì´ ìˆëŠ”ë° ì´ ë¶€ë¶„ì€ ë’¤ì—ì„œ ë” ìì„¸íˆ ë‹¤ë£° ê²ƒì´ë‹¤.</p>

<p>ì§€ê¸ˆê¹Œì§€ ë¶„ì‚° ì‹œìŠ¤í…œì˜ ì¥ì ì— ëŒ€í•´ì„œ ì–˜ê¸°í–ˆë‹¤. ì´ëŸ¬í•œ ì¥ì ì„ ì–»ê¸°ìœ„í•´ í•´ì•¼í•  ì¼ì´ ìˆë‹¤. ìš°ì„  High-levelì—ì„œ ì´ì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤.</p>

<h1 id="high-levelì—ì„œì˜-ë™ì‘">High-levelì—ì„œì˜ ë™ì‘</h1>

<h2 id="partitioning">Partitioning</h2>

<p>íŒŒí‹°ì…”ë‹ì€ ë¶„ì‚° ì‹œìŠ¤í…œì˜ ì¥ì  ì¤‘ì—ì„œë„ Scalability, Performanceë¥¼ ì–»ê¸° ìœ„í•´ í•„ìš”í•œ í•µì‹¬ì´ë‹¤. íŒŒí‹°ì…”ë‹ì€ ì²˜ë¦¬(ë˜ëŠ” ì €ì¥)í•´ì•¼ í•  ë°ì´í„°ë¥¼ ì‘ê²Œ ë‚˜ëˆ„ì–´ì„œ ì´ë¥¼ ì²˜ë¦¬(ë˜ëŠ” ì €ì¥)í•´ì•¼ í•  ë…¸ë“œì—ê²Œ í• ë‹¹í•´ì£¼ëŠ” ì‘ì—…ì´ë‹¤.</p>

<p>í•˜ì§€ë§Œ ì˜¤íˆë ¤ í•˜ë‚˜ì˜ ì‘ì—…ì„ ìœ„í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ê±°ì³ ì—¬ëŸ¬ ë…¸ë“œì— ì ‘ê·¼í•´ì•¼ í•œë‹¤ëŠ” ì ì—ì„œ ë‹¨ì ì´ ë˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•ŒëŠ” ìµœëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ì„ ì¤„ì´ëŠ” ê²ƒì´ ê´€ê±´ì´ë‹¤.</p>

<p>íŒŒí‹°ì…”ë‹ì€ í¬ê²Œ range partitioning, hash partitioning, consistent hashingê°€ ìˆë‹¤. Apache HBaseëŠ” range partitioningì„ ì“°ê³ , Apache CassandraëŠ” consistent hasingì„ ì“´ë‹¤.</p>

<h2 id="replication">Replication</h2>

<p>ë³µì œ(Replication)ëŠ” Availabilityë¥¼ ìœ„í•´ í•„ìš”í•œ í•µì‹¬ì´ë‹¤. ë³µì œëŠ” ê°™ì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë³µìˆ˜ ì €ì¥í•¨ìœ¼ë¡œì¨, ë…¸ë“œ ì¤‘ ì¼ë¶€ì— ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ê³„ì† ì—­í• ì„ ìœ ì§€í•  ìˆ˜ ìˆê²Œ í•œë‹¤.</p>

<p>ë³µì œí•˜ëŠ” ê²ƒì´ ë‹¨ìˆœí•œ ì¼ì€ ì•„ë‹ˆë‹¤. ìš°ì„  ë³µì œ ìˆ˜ ë§Œí¼ ë” ë§ì€ ì €ì¥ìš©ëŸ‰ì´ í•„ìš”í•˜ë‹¤. ë˜í•œ ë³µì‚¬ëœ í›„ì—ë„ ì›ë³¸ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ ë  ë•Œë§ˆë‹¤ ë™ê¸°í™”í•´ì•¼ í•œë‹¤.</p>

<p>ë³µì œ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ primary-backup replicationì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<p>We commonly refer to the remaining replicas as followers or secondaries. These can only handle read requests. Every time the leader receives an update, it executes it locally and also propagates the update to the other nodes. This ensures that all the replicas maintain a consistent view of the data.</p>

<p><img src="/images/dis_sys_4.png" alt="" /></p>

<p>ë¦¬ë”ëŠ” ì—…ë°ì´íŠ¸ë¥¼ ì–´ë–»ê²Œ íŒ”ë¡œì›Œë“¤ì—ê²Œ ì „íŒŒí• ê¹Œ</p>

<p>There are two ways to propagate the updates: synchronously and asynchronously.</p>

<p>Synchronous replication</p>

<p>In synchronous replication, the node replies to the client to indicate the update is completeâ€”only after receiving acknowledgments from the other replicas that theyâ€™ve also performed the update on their local storage. This guarantees that the client is able to view the update in a subsequent read after acknowledging it, no matter which replica the client reads from.</p>

<p>Furthermore, synchronous replication provides increased durability. This is because the update is not lost even if the leader crashes right after it acknowledges the update.</p>

<p>However, this technique can make writing requests slower. This is because the leader has to wait until it receives responses from all the replicas.</p>

<p><img src="/images/dis_sys_3.png" alt="" /></p>

<p>Asynchronous replication</p>

<p>In asynchronous replication, the node replies to the client as soon as it performs the update in its local storage, without waiting for responses from the other replicas.</p>

<p>This technique increases performance significantly for write requests. This is because the client no longer pays the penalty of the network requests to the other replicas.</p>

<p>However, this comes at the cost of reduced consistency and decreased durability. After a client receives a response for an update request, the client might read older (stale) values in a subsequent read. This is only possible if the operation happens in one of the replicas that have not yet performed the update. Moreover, if the leader node crashes right after it acknowledges an update, and the propagation requests to the other replicas are lost, any acknowledged update is eventually lost.</p>

<p><img src="/images/dis_sys_5.png" alt="" /></p>

<p>Most widely used databases, such as PostgreSQL or MySQL, use a primary-backup replication technique that supports both asynchronous and synchronous replication.</p>

<p>primary-backup replicationì—ëŠ” ì¥ì ê³¼ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì¥ì </p>

<ul>
  <li>It is simple to understand and implement</li>
  <li>Concurrent operations serialized in the leader node, remove the need for more complicated, distributed concurrency protocols. In general, this property also makes it easier to support transactional operations</li>
  <li>It is scalable for read-heavy workloads, because the capacity for reading requests can be increased by adding more read replicas</li>
</ul>

<p>ë‹¨ì </p>

<ul>
  <li>It is not very scalable for write-heavy workloads, because a single node (the leader)â€™s capacity determines the capacity for writes</li>
  <li>It imposes an obvious trade-off between performance, durability, and consistency</li>
  <li>Scaling the read capacity by adding more follower nodes can create a bottleneck in the network bandwidth of the leader node, if thereâ€™s a large number of followers listening for updates</li>
  <li>The process of failing over to a follower node when the leader node crashes, is not instant. This may create some downtime and also introduce the risk of errors</li>
</ul>

<p>ë˜í•œ primary-backup replicationì€ í•­ìƒ ë¦¬ë”ê°€ ì¡´ì¬í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë¦¬ë”ê°€ ì˜ ì‚´ì•„ìˆëŠ”ì§€ ì²´í¬í•˜ê³ , ë¦¬ë”ê°€ ì£½ì—ˆë‹¤ë©´ ë¦¬ë”ë¥¼ ìƒˆë¡œ ì„ ì¶œí•˜ëŠ” Leader election ë¬¸ì œë„ ê³ ë ¤í•´ì•¼ í•œë‹¤.</p>

<h1 id="ì–´ë ¤ìš´-ì ">ì–´ë ¤ìš´ ì </h1>

<h2 id="synchronization">Synchronization</h2>

<h3 id="clock-synchronization">Clock Synchronization</h3>

<p>Distributed System is a collection of computers connected via the high speed communication network. In the distributed system, the hardware and software components communicate and coordinate their actions by message passing. Each node in distributed systems can share their resources with other nodes. So, there is need of proper allocation of resources to preserve the state of resources and help coordinate between the several processes. To resolve such conflicts, synchronization is used. Synchronization in distributed systems is achieved via clocks.</p>

<p>The physical clocks are used to adjust the time of nodes.Each node in the system can share its local time with other nodes in the system. The time is set based on UTC (Universal Time Coordination). UTC is used as a reference time clock for the nodes in the system.</p>

<p>The clock synchronization can be achieved by 2 ways: External and Internal Clock Synchronization.</p>

<ul>
  <li>External clock synchronization is the one in which an external reference clock is present. It is used as a reference and the nodes in the system can set and adjust their time accordingly.</li>
  <li>Internal clock synchronization is the one in which each node shares its time with other nodes and all the nodes set and adjust their times accordingly.</li>
</ul>

<p>There are 2 types of clock synchronization algorithms: Centralized and Distributed.</p>

<ul>
  <li>Centralized is the one in which a time server is used as a reference. The single time server propagates its time to the nodes and all the nodes adjust the time accordingly. It is dependent on single time server so if that node fails, the whole system will lose synchronization. Examples of centralized are- Berkeley Algorithm, Passive Time Server, Active Time Server etc.</li>
  <li>Distributed is the one in which there is no centralized time server present. Instead the nodes adjust their time by using their local time and then, taking the average of the differences of time with other nodes. Distributed algorithms overcome the issue of centralized algorithms like the scalability and single point failure. Examples of Distributed algorithms are â€“ Global Averaging Algorithm, Localized Averaging Algorithm, NTP (Network time protocol) etc.</li>
</ul>

<h3 id="data-synchronization">Data Synchronization</h3>

<p>ë°ì´í„° ë™ê¸°í™” ë¬¸ì œëŠ” ìœ„ì—ì„œ ë§í–ˆë˜ ë°ì´í„° ë³µì œ ê³¼ì •ì—ì„œ, ê·¸ë¦¬ê³  ì—…ë°ì´íŠ¸ì‹œ ë³µì œëœ ë°ì´í„°ë“¤ê°„ì˜ ë™ê¸°í™”ë¥¼ ë§í•œë‹¤. ë˜í•œ íŠ¸ëœì­ì…˜ê³¼ ê°™ì´ ACID íŠ¹ì„±ì´ ìš”êµ¬ë  ë•Œ, ë°ì´í„°ë“¤ì´ Atomicí•˜ê²Œ ì²˜ë¦¬ë˜ëŠ”ì§€ì™€ë„ ê´€ë ¨ëœë‹¤.</p>

<h2 id="network-asynchrony">Network Asynchrony</h2>

<p>Network asynchrony is a property of communication networks that cannot provide strong guarantees around delivering events, e.g., a maximum amount of time a message requires for delivery. This can create a lot of counter-intuitive behaviors that are not present in non-distributed systems. This contrasts to memory operations that provide much stricter guarantees. For instance, messages might take extremely long to deliver in a distributed system. They may even deliver out of orderâ€”or not at all.</p>

<ul>
  <li>ë¶„ì‚° ì‹œìŠ¤í…œì„ ì´ìš©í•œ ì¸í„°ë„· ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì˜ ì•„í‚¤í…ì²˜ëŠ” ëŒ€ë¶€ë¶„ ë¹„ê³µìœ  ì•„í‚¤í…ì²˜(Shared-nothing)
    <ul>
      <li>ê° ë…¸ë“œëŠ” CPU, RAM, ë””ìŠ¤í¬ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì‚¬ìš©(ë””ìŠ¤í¬ë„ ë…ë¦½ëœë‹¤ëŠ” ì ì—ì„œ Shared Disk Architectureì™€ ë‹¤ë¦„)</li>
    </ul>
  </li>
  <li>ë¹„ê³µìœ  ì‹œìŠ¤í…œì—ì„œ ì¥ë¹„ë“¤ì´ í†µì‹ í•˜ëŠ” ìœ ì¼í•œ ìˆ˜ë‹¨ì€ ë„¤íŠ¸ì›Œí¬</li>
  <li>ì „ì†¡ ì¸¡ì€ íŒ¨í‚·ì´ ì „ì†¡ ëœê±´ì§€, ë„¤íŠ¸ì›Œí¬ ë¬¸ì œì¸ì§€, ìˆ˜ì‹  ì¸¡ ì„œë²„ ë¬¸ì œì¸ì§€ ì•Œ ìˆ˜ ì—†ìœ¼ë©° ì‹¬ì§€ì–´ ì „ë‹¬ì´ ì‹¤íŒ¨í•œê±´ì§€ ì•„ë‹ˆë©´ ì§€ì—°ëœê±´ì§€ ì¡°ì°¨ ì•Œ ìˆ˜ ì—†ìŒ</li>
</ul>

<p><img src="/images/dist_3.png" alt="" /></p>

<h2 id="partial-failures">Partial Failures</h2>

<p>Partial failures are the cases where only some components of a distributed system fail. This behavior can contrast with certain kinds of applications a single server deploys. These applications work under the assumption that either everything is working fine, or there has been a server crash. It introduces significant complexity when it requires atomicity across components in a distributed system. Thus, we must ensure that we either apply an operation to all the nodes of a system, or to none of them.</p>

<p><img src="/images/dis_sys_7.png" alt="" /></p>

<h1 id="low-levelì—ì„œì˜-ë™ì‘">Low-levelì—ì„œì˜ ë™ì‘</h1>

<h2 id="consensus-algorithm">Consensus Algorithm</h2>

<p><img src="/images/dis_sys_8.png" alt="" /></p>

<ul>
  <li>whether a transaction has been committed or not</li>
  <li>whether a message has been delivered or not</li>
</ul>

<h3 id="raft">RAFT</h3>
<p>Raft is a protocol for implementing distributed consensus.</p>

<ul>
  <li>Leader Election</li>
  <li>Log Replication</li>
</ul>

<h2 id="failure-detector-timeout">Failure Detector (Timeout)</h2>

<p>The asynchronous nature of the network in a distributed system can make it very hard for us to differentiate between a crashed node and a node that is just really slow to respond to requests.</p>

<p>Timeouts is the main mechanism we can use to detect failures in distributed systems. Since an asynchronous network can infinitely delay messages, timeouts impose an artificial upper bound on these delays. As a result, we can assume that a node fails when it is slower than this bound.</p>

<p>However, a timeout does not represent an actual limit. Thus, it creates the following trade-off.</p>

<p>ë‚´ê°€ ì •í•œ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì´ ë°œìƒí•œ ë¬¸ì œì˜ ê·¼ë³¸ì ì¸ ì›ì¸ì„ í•´ê²°í•˜ëŠ” ì ì€ ì•„ë‹ˆë‹¤. ë°œìƒí•œ ë¬¸ì œë¡œ ìƒê²Œë˜ëŠ” ì„±ëŠ¥ì´ ì‹¤ì œ ì´ìƒìœ¼ë¡œ ë–¨ì–´ì§ˆ ìˆ˜ ë°–ì— ì—†ë‹¤. íƒ€ì„ì•„ì›ƒ ê°’ì— ë”°ë¼ Completenessì™€ Accruacy ì‚¬ì´ì— íŠ¸ë ˆì´ë“œ ì˜¤í”„ ë°œìƒ</p>

<p>Completeness corresponds to the percentage of crashed nodes a failure detector successfully identifies in a certain period.</p>

<p>Accuracy corresponds to the number of mistakes a failure detector makes in a certain period.</p>

<h2 id="de-duplication-algorithm">De-duplication Algorithm</h2>

<p>In the de-duplication approach, we give every message a unique identifier, and every retried message contains the same identifier as the original. In this way, the recipient can remember the set of identifiers it received and executed already. It will also avoid executing operations that are executed.</p>

<p>It is important to note that in order to do this, we must have control on both sides of the system: sender and receiver. This is because the ID generation occurs on the sender side, but the de-duplication process occurs on the receiver side.</p>

<p>ë¦¬ë”ëŠ” ë°ì´í„°ë¥¼ ì „ë‹¬í•  ë•Œë§ˆë‹¤ ë°ì´í„°ì— ê³ ìœ í•œ ì‹ë³„ìë¥¼ ë¶™ì¸ë‹¤. íŒ”ë¡œì›Œë“¤ì€ ì‹ë³„ìë¥¼ ë³´ê³  ìê¸°ê°€ ê°€ì§€ê³  ìˆëŠ” ì‹ë³„ìë³´ë‹¤ í° ê°’ì¸ ê²½ìš°ì—ë§Œ ë°›ì•„ì„œ ì €ì¥í•œë‹¤. ë§Œì•½ ê°™ì€ ì‹ë³„ìì˜ ë°ì´í„°ë¥¼ ë°›ì•˜ë‹¤ë©´ ë¦¬ë”ì—ê²Œ ACK ë©”ì‹œì§€ë§Œ ë³´ë‚´ê³ , ì €ì¥ì€ í•˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>

<ul>
  <li><a href="http://thesecretlivesofdata.com/raft/" target="_blank">Video Demo,  Raft: Understandable Distributed Consensus</a></li>
  <li><a href="https://www.freecodecamp.org/news/in-search-of-an-understandable-consensus-algorithm-a-summary-4bc294c97e0d/" target="_blank">Understanding the Raft consensus algorithm: an academic article summary</a></li>
  <li><a href="https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-2-data-replication/" target="_blank">Building a Distributed Log from Scratch, Part 2: Data Replication</a></li>
</ul>
:ET
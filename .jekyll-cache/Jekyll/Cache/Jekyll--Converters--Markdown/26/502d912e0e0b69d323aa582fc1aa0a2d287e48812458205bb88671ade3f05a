I"Ò$<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€" id="markdown-toc-ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€">ë¶„ì‚° ì‹œìŠ¤í…œì´ë€</a></li>
  <li><a href="#ì¥ì " id="markdown-toc-ì¥ì ">ì¥ì </a>    <ul>
      <li><a href="#performance" id="markdown-toc-performance">Performance</a></li>
      <li><a href="#scalability" id="markdown-toc-scalability">Scalability</a></li>
      <li><a href="#availability" id="markdown-toc-availability">Availability</a></li>
    </ul>
  </li>
  <li><a href="#í•„ìš”í•œ-ê²ƒ" id="markdown-toc-í•„ìš”í•œ-ê²ƒ">í•„ìš”í•œ ê²ƒ</a>    <ul>
      <li><a href="#partitioning" id="markdown-toc-partitioning">Partitioning</a></li>
      <li><a href="#replication" id="markdown-toc-replication">Replication</a></li>
    </ul>
  </li>
  <li><a href="#ì–´ë ¤ìš´-ì " id="markdown-toc-ì–´ë ¤ìš´-ì ">ì–´ë ¤ìš´ ì </a>    <ul>
      <li><a href="#synchronization" id="markdown-toc-synchronization">Synchronization</a></li>
      <li><a href="#network-asynchrony" id="markdown-toc-network-asynchrony">Network Asynchrony</a></li>
      <li><a href="#partial-failures" id="markdown-toc-partial-failures">Partial Failures</a></li>
    </ul>
  </li>
  <li><a href="#ê·¹ë³µ" id="markdown-toc-ê·¹ë³µ">ê·¹ë³µ</a></li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="ë¶„ì‚°-ì‹œìŠ¤í…œì´ë€">ë¶„ì‚° ì‹œìŠ¤í…œì´ë€</h1>

<blockquote>
  <p>A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another</p>
</blockquote>

<p><img src="/images/dis_sys_2.png" alt="" /></p>

<p>ë‚˜ëŠ” ë¶„ì‚° ì‹œìŠ¤í…œì„ ë‹¤ìŒê³¼ ê°™ì€ ë§¥ë½ìœ¼ë¡œ ì •ë¦¬í•˜ë ¤ê³  í•œë‹¤.</p>

<p><img src="/images/dis_sys_1.png" alt="" /></p>

<h1 id="ì¥ì ">ì¥ì </h1>

<h2 id="performance">Performance</h2>

<p>ì—¬ê¸°ì„œ PerformanceëŠ” ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œì˜ Performanceì™€ ë¹„êµí•´ ê°€ê²©ëŒ€ë¹„ ë” ë‚«ë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. Performanceì˜ ì ˆëŒ€ì ì¸ ìˆ˜ì¹˜ ìì²´ê°€ ë” ì˜¤ë¥¼ ì´ìœ ëŠ” ì—†ë‹¤. ì˜¤íˆë ¤ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ìœ¼ë¡œ ê°ì†Œí•  ê°€ëŠ¥ì„±ì€ ìˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶„ì‚° ì‹œìŠ¤í…œì„ ì“°ëŠ” ì´ìœ ëŠ” ê°€ê²©ì ì¸ ì¸¡ë©´ì—ì„œ ê·¸ë§Œí¼ ê°’ì‹¼ ì¥ë¹„ë¥¼ ì—¬ëŸ¬ ëŒ€ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë‚«ê³ , ì„±ëŠ¥ì ì¸ ì¸¡ë©´ ì´ì™¸ì—ë„ ë¶„ì‚° ì‹œìŠ¤í…œì´ ì£¼ëŠ” ì¥ì ì´ ìˆê¸° ë•Œë¬¸ì´ë‹¤.</p>

<h2 id="scalability">Scalability</h2>

<p>ë¶„ì‚° ì‹œìŠ¤í…œì€ ì„œë¹„ìŠ¤ ê·œëª¨, íŠ¸ë˜í”½ëŸ‰, ì‘ì—…ëŸ‰ì— ë”°ë¼ ì‹œìŠ¤í…œì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•´ ì´ë¥¼ í•¸ë“¤ë§í•  ëŠ¥ë ¥ì´ ìˆë‹¤. ë¬¼ë¡  ë‹¨ì¼ ì‹œìŠ¤í…œì—ì„œë„ Vertical-scalingì´ ê°€ëŠ¥í•˜ë‹¤. í•˜ì§€ë§Œ Horizontal-scalingì´ ê°€ê²©ì ì¸ ì¸¡ë©´ê³¼ í™•ì¥ì´ ìš©ì´í•˜ë‹¤ëŠ” ì ì—ì„œ ì´ì ì´ ìˆë‹¤.</p>

<h2 id="availability">Availability</h2>

<p>ë¶„ì‚° ì‹œìŠ¤í…œì€ ë…¸ë“œ ì¼ë¶€ì— ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ê³„ì† ê°™ì€ ê¸°ëŠ¥ì„ ìœ ì§€í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” 24ì‹œê°„ ë‚´ë‚´ ì¥ì• ì—†ëŠ” ì„œë¹„ìŠ¤ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ë§ì´ë‹¤. ë¬¼ë¡  ì´ë¥¼ ìœ„í•´ ìš”êµ¬ë˜ëŠ” ì¡°ê±´ë“¤ì´ ìˆëŠ”ë° ì´ ë¶€ë¶„ì€ ë’¤ì—ì„œ ë” ìì„¸íˆ ë‹¤ë£° ê²ƒì´ë‹¤.</p>

<p>ì§€ê¸ˆê¹Œì§€ ë¶„ì‚° ì‹œìŠ¤í…œì˜ ì¥ì ì— ëŒ€í•´ì„œ ì–˜ê¸°í–ˆë‹¤. ì´ëŸ¬í•œ ì¥ì ì„ ì–»ê¸°ìœ„í•´ í•´ì•¼í•  ì¼ì´ ìˆë‹¤. ìš°ì„  High-levelì—ì„œ ì´ì— ëŒ€í•´ ì•Œì•„ë³´ê² ë‹¤.</p>

<h1 id="í•„ìš”í•œ-ê²ƒ">í•„ìš”í•œ ê²ƒ</h1>

<h2 id="partitioning">Partitioning</h2>

<p>íŒŒí‹°ì…”ë‹ì€ ë¶„ì‚° ì‹œìŠ¤í…œì˜ ì¥ì  ì¤‘ì—ì„œë„ Scalability, Performanceë¥¼ ì–»ê¸° ìœ„í•´ í•„ìš”í•œ í•µì‹¬ì´ë‹¤. íŒŒí‹°ì…”ë‹ì€ ì²˜ë¦¬(ë˜ëŠ” ì €ì¥)í•´ì•¼ í•  ë°ì´í„°ë¥¼ ì‘ê²Œ ë‚˜ëˆ„ì–´ì„œ ì´ë¥¼ ì²˜ë¦¬(ë˜ëŠ” ì €ì¥)í•´ì•¼ í•  ë…¸ë“œì—ê²Œ í• ë‹¹í•´ì£¼ëŠ” ì‘ì—…ì´ë‹¤.</p>

<p>í•˜ì§€ë§Œ ì˜¤íˆë ¤ í•˜ë‚˜ì˜ ì‘ì—…ì„ ìœ„í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ê±°ì³ ì—¬ëŸ¬ ë…¸ë“œì— ì ‘ê·¼í•´ì•¼ í•œë‹¤ëŠ” ì ì—ì„œ ë‹¨ì ì´ ë˜ëŠ” ê²½ìš°ë„ ìˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•ŒëŠ” ìµœëŒ€í•œ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ì„ ì¤„ì´ëŠ” ê²ƒì´ ê´€ê±´ì´ë‹¤.</p>

<p>íŒŒí‹°ì…”ë‹ì€ í¬ê²Œ range partitioning, hash partitioning, consistent hashingê°€ ìˆë‹¤. Apache HBaseëŠ” range partitioningì„ ì“°ê³ , Apache CassandraëŠ” consistent hasingì„ ì“´ë‹¤.</p>

<h2 id="replication">Replication</h2>

<p>ë³µì œ(Replication)ëŠ” Availabilityë¥¼ ìœ„í•´ í•„ìš”í•œ í•µì‹¬ì´ë‹¤. ë³µì œëŠ” ê°™ì€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë³µìˆ˜ ì €ì¥í•¨ìœ¼ë¡œì¨, ë…¸ë“œ ì¤‘ ì¼ë¶€ì— ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ê³„ì† ì—­í• ì„ ìœ ì§€í•  ìˆ˜ ìˆê²Œ í•œë‹¤.</p>

<p>ë³µì œí•˜ëŠ” ê²ƒì´ ë‹¨ìˆœí•œ ì¼ì€ ì•„ë‹ˆë‹¤. ìš°ì„  ë³µì œ ìˆ˜ ë§Œí¼ ë” ë§ì€ ì €ì¥ìš©ëŸ‰ì´ í•„ìš”í•˜ë‹¤. ë˜í•œ ë³µì‚¬ëœ í›„ì—ë„ ì›ë³¸ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ ë  ë•Œë§ˆë‹¤ ë™ê¸°í™”í•´ì•¼ í•œë‹¤.</p>

<p>ë³µì œ ë°©ë²• ì¤‘ í•˜ë‚˜ì¸ primary-backup replicationì— ëŒ€í•´ ì•Œì•„ë³´ì.</p>

<p>We commonly refer to the remaining replicas as followers or secondaries. These can only handle read requests. Every time the leader receives an update, it executes it locally and also propagates the update to the other nodes. This ensures that all the replicas maintain a consistent view of the data.</p>

<p><img src="/images/dis_sys_4.png" alt="" /></p>

<p>ë¦¬ë”ëŠ” ì—…ë°ì´íŠ¸ë¥¼ ì–´ë–»ê²Œ íŒ”ë¡œì›Œë“¤ì—ê²Œ ì „íŒŒí• ê¹Œ</p>

<p>There are two ways to propagate the updates: synchronously and asynchronously.</p>

<p>Synchronous replication</p>

<p>In synchronous replication, the node replies to the client to indicate the update is completeâ€”only after receiving acknowledgments from the other replicas that theyâ€™ve also performed the update on their local storage. This guarantees that the client is able to view the update in a subsequent read after acknowledging it, no matter which replica the client reads from.</p>

<p>Furthermore, synchronous replication provides increased durability. This is because the update is not lost even if the leader crashes right after it acknowledges the update.</p>

<p>However, this technique can make writing requests slower. This is because the leader has to wait until it receives responses from all the replicas.</p>

<p><img src="/images/dis_sys_3.png" alt="" /></p>

<p>Asynchronous replication</p>

<p>In asynchronous replication, the node replies to the client as soon as it performs the update in its local storage, without waiting for responses from the other replicas.</p>

<p>This technique increases performance significantly for write requests. This is because the client no longer pays the penalty of the network requests to the other replicas.</p>

<p>However, this comes at the cost of reduced consistency and decreased durability. After a client receives a response for an update request, the client might read older (stale) values in a subsequent read. This is only possible if the operation happens in one of the replicas that have not yet performed the update. Moreover, if the leader node crashes right after it acknowledges an update, and the propagation requests to the other replicas are lost, any acknowledged update is eventually lost.</p>

<p><img src="/images/dis_sys_5.png" alt="" /></p>

<p>Most widely used databases, such as PostgreSQL or MySQL, use a primary-backup replication technique that supports both asynchronous and synchronous replication.</p>

<p>primary-backup replicationì—ëŠ” ì¥ì ê³¼ ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.</p>

<p>ì¥ì </p>

<ul>
  <li>It is simple to understand and implement</li>
  <li>Concurrent operations serialized in the leader node, remove the need for more complicated, distributed concurrency protocols. In general, this property also makes it easier to support transactional operations</li>
  <li>It is scalable for read-heavy workloads, because the capacity for reading requests can be increased by adding more read replicas</li>
</ul>

<p>ë‹¨ì </p>

<ul>
  <li>It is not very scalable for write-heavy workloads, because a single node (the leader)â€™s capacity determines the capacity for writes</li>
  <li>It imposes an obvious trade-off between performance, durability, and consistency</li>
  <li>Scaling the read capacity by adding more follower nodes can create a bottleneck in the network bandwidth of the leader node, if thereâ€™s a large number of followers listening for updates</li>
  <li>The process of failing over to a follower node when the leader node crashes, is not instant. This may create some downtime and also introduce the risk of errors</li>
</ul>

<p>ë˜í•œ primary-backup replicationì€ í•­ìƒ ë¦¬ë”ê°€ ì¡´ì¬í•´ì•¼ í•œë‹¤. ë”°ë¼ì„œ ë¦¬ë”ê°€ ì˜ ì‚´ì•„ìˆëŠ”ì§€ ì²´í¬í•˜ê³ , ë¦¬ë”ê°€ ì£½ì—ˆë‹¤ë©´ ë¦¬ë”ë¥¼ ìƒˆë¡œ ì„ ì¶œí•˜ëŠ” Leader election ë¬¸ì œë„ ê³ ë ¤í•´ì•¼ í•œë‹¤.</p>

<h1 id="ì–´ë ¤ìš´-ì ">ì–´ë ¤ìš´ ì </h1>

<h2 id="synchronization">Synchronization</h2>

<h2 id="network-asynchrony">Network Asynchrony</h2>

<p><img src="/images/dist_3.png" alt="" /></p>

<h2 id="partial-failures">Partial Failures</h2>

<h1 id="ê·¹ë³µ">ê·¹ë³µ</h1>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>

<ul>
  <li><a href="http://thesecretlivesofdata.com/raft/" target="_blank">Video Demo,  Raft: Understandable Distributed Consensus</a></li>
  <li><a href="https://www.freecodecamp.org/news/in-search-of-an-understandable-consensus-algorithm-a-summary-4bc294c97e0d/" target="_blank">Understanding the Raft consensus algorithm: an academic article summary</a></li>
  <li><a href="https://bravenewgeek.com/building-a-distributed-log-from-scratch-part-2-data-replication/" target="_blank">Building a Distributed Log from Scratch, Part 2: Data Replication</a></li>
</ul>
:ET
I"€!<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#rdd" id="markdown-toc-rdd">RDD</a></li>
  <li><a href="#pyspark-rdd-benefits" id="markdown-toc-pyspark-rdd-benefits">PySpark RDD Benefits</a></li>
  <li><a href="#creating-rdd" id="markdown-toc-creating-rdd">Creating RDD</a>    <ul>
      <li><a href="#create-rdd-using-sparkcontextparallelize" id="markdown-toc-create-rdd-using-sparkcontextparallelize">Create RDD using sparkContext.parallelize()</a></li>
      <li><a href="#create-rdd-using-sparkcontexttextfile" id="markdown-toc-create-rdd-using-sparkcontexttextfile">Create RDD using sparkContext.textFile()</a></li>
      <li><a href="#create-empty-rdd" id="markdown-toc-create-empty-rdd">Create empty RDD</a></li>
    </ul>
  </li>
  <li><a href="#rdd-operations" id="markdown-toc-rdd-operations">RDD Operations</a>    <ul>
      <li><a href="#rdd-transformation" id="markdown-toc-rdd-transformation">RDD Transformation</a></li>
      <li><a href="#rdd-action" id="markdown-toc-rdd-action">RDD Action</a></li>
    </ul>
  </li>
  <li><a href="#ì‹¤ìŠµ-ìë£Œ" id="markdown-toc-ì‹¤ìŠµ-ìë£Œ">ì‹¤ìŠµ ìë£Œ</a></li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="rdd">RDD</h1>

<ul>
  <li>RDDëŠ” ìŠ¤íŒŒí¬ì˜ ê¸°ë³¸ì ì¸ ë°ì´í„° ëª¨ë¸</li>
  <li>RDDëŠ” ì¥ì• ì— ê°•ì¸, ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ì„ ìµœì†Œí™” ì‹œì¼œì£¼ëŠ” ì—°ì‚° ìµœì í™”, íŒŒí‹°ì…”ë‹ì„ í†µí•œ ë¶„ì‚°ì²˜ë¦¬ë¥¼ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤</li>
  <li>ë¶„ì‚°ì²˜ë¦¬ë¥¼ ìœ„í•´ RDDë¥¼ íŒŒí‹°ì…”ë‹í•˜ê³  ë¶„ì‚°í•˜ëŠ” ê³¼ì •ì„ ê±±ì •í•˜ì§€ ì•Šì•„ë„ ëœë‹¤. ìŠ¤íŒŒí¬ê°€ ê¸°ë³¸ì ìœ¼ë¡œ ì œê³µí•´ì¤€ë‹¤</li>
</ul>

<h1 id="pyspark-rdd-benefits">PySpark RDD Benefits</h1>

<ul>
  <li>
    <p>ìŠ¤íŒŒí¬ë¥¼ ì´ìš©í•˜ë©´ ê¸°ì¡´ì˜ ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ì—ì„œì˜ ë°ì´í„° ì²˜ë¦¬ì™€ ë¹„êµí•´ ì•„ë˜ì™€ ê°™ì€ ì¥ì ì´ ìˆë‹¤</p>
  </li>
  <li><strong>In-Memory Processing</strong>
    <ul>
      <li>ìŠ¤íŒŒí¬ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ë¨¼ì € ì˜¬ë ¤ë†“ê³ , ì„ íƒì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•œë‹¤. ê·¸ë˜ì„œ êµ‰ì¥íˆ ë¹ ë¥´ë‹¤</li>
    </ul>
  </li>
  <li><strong>Fault Tolerance</strong>
    <ul>
      <li>RDDëŠ” transformation(RDD1 -&gt; RDD2) ì—°ì‚°ì„ í•˜ë©´ í•­ìƒ ìƒˆë¡œìš´ RDDë¥¼ ë§Œë“¤ì–´ ë‚¸ë‹¤</li>
      <li>ì¤‘ê°„ì— ë„¤íŠ¸ì›Œí¬, í•˜ë“œì›¨ì–´ ë“±ì˜ ì¥ì• ë¡œ ì—°ì‚°ì— ì‹¤íŒ¨í•˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ì´ì „ì˜ RDDë¡œ ëŒì•„ê°€ ì¬ì—°ì‚° í•œë‹¤</li>
    </ul>
  </li>
  <li><strong>Lazy Evolution</strong>
    <ul>
      <li>RDDëŠ” transformation ì—°ì‚°ì„ ë°”ë¡œ ì‹¤í–‰í•˜ì§€ ì•Šê³ , action ì—°ì‚°ì´ ì¼ì–´ë‚  ë•Œ ì§€ê¸ˆê¹Œì§€ì˜ transformationì„ ë„¤íŠ¸ì›Œí¬ ë¹„ìš©ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì‹¤í–‰ ê³„íšì„ ì„¸ì›Œ ì‹¤í–‰í•œë‹¤</li>
    </ul>
  </li>
  <li><strong>Partitioning</strong>
    <ul>
      <li>ìŠ¤íŒŒí¬ëŠ” ìë™ìœ¼ë¡œ RDDë¥¼ ì½”ì–´ ìˆ˜ë§Œí¼ íŒŒí‹°ì…”ë‹í•œë‹¤</li>
    </ul>
  </li>
</ul>

<h1 id="creating-rdd">Creating RDD</h1>

<h2 id="create-rdd-using-sparkcontextparallelize">Create RDD using sparkContext.parallelize()</h2>

<ul>
  <li>ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë°ì´í„°ë¥¼ RDDë¡œ ë§Œë“¤ê³  íŒŒí‹°ì…”ë‹í•´ì¤€ë‹¤</li>
  <li>ìƒìš© ì„œë¹„ìŠ¤ì—ì„œëŠ” ëŒ€ê°œ HDFS, S3, HBaseì™€ ê°™ì€ ì™¸ë¶€ ì €ì¥ì†Œì˜ ë°ì´í„°ë¥¼ ì´ìš©í•´ RDDë¥¼ ë§Œë“ ë‹¤</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Create RDD from parallelize    
</span><span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">11</span><span class="p">,</span><span class="mi">12</span><span class="p">]</span>
<span class="n">rdd</span><span class="o">=</span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="create-rdd-using-sparkcontexttextfile">Create RDD using sparkContext.textFile()</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">#Create RDD from external Data source
</span><span class="n">rdd2</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">"/path/textFile.txt"</span><span class="p">)</span>

</code></pre></div></div>

<h2 id="create-empty-rdd">Create empty RDD</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creates empty RDD with no partition    
</span><span class="n">rdd</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">emptyRDD</span> 
<span class="c1"># rddString = spark.sparkContext.emptyRDD[String]
</span>


<span class="c1">#Create empty RDD with partition
</span><span class="n">rdd2</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">parallelize</span><span class="p">([],</span><span class="mi">10</span><span class="p">)</span> <span class="c1">#This creates 10 partitions
</span>
</code></pre></div></div>

<h1 id="rdd-operations">RDD Operations</h1>

<ul>
  <li>RDD transformations: Transformations are lazy operations, instead of updating an RDD, these operations return another RDD</li>
  <li>RDD actions: operations that trigger computation and return RDD values</li>
</ul>

<h2 id="rdd-transformation">RDD Transformation</h2>

<ul>
  <li>Transformations on PySpark RDD returns another RDD and transformations are lazy meaning they donâ€™t execute until you call an action on RDD</li>
  <li>ex. <code class="language-plaintext highlighter-rouge">flatMap()</code>, <code class="language-plaintext highlighter-rouge">map()</code>, <code class="language-plaintext highlighter-rouge">reduceByKey()</code>, <code class="language-plaintext highlighter-rouge">filter()</code>, <code class="language-plaintext highlighter-rouge">sortByKey()</code></li>
  <li>Shuffle Operation
    <ul>
      <li>Shuffling is a mechanism PySpark uses to redistribute the data across different executors and even across machines. PySpark shuffling triggers when we perform certain transformation operations like <code class="language-plaintext highlighter-rouge">groupByKey()</code>, <code class="language-plaintext highlighter-rouge">reduceByKey()</code>, <code class="language-plaintext highlighter-rouge">join()</code></li>
      <li>PySpark Shuffle is an expensive operation since it involves the Disk I/O, Involves data serialization and deserialization, Network I/O</li>
    </ul>
  </li>
</ul>

<h2 id="rdd-action">RDD Action</h2>

<ul>
  <li>RDD Action operations return the values from an RDD to a driver program. In other words, any RDD function that returns non-RDD is considered as an action.</li>
  <li>ex. <code class="language-plaintext highlighter-rouge">count()</code>, <code class="language-plaintext highlighter-rouge">first()</code>, <code class="language-plaintext highlighter-rouge">max()</code>, <code class="language-plaintext highlighter-rouge">reduce()</code>, <code class="language-plaintext highlighter-rouge">take()</code>, <code class="language-plaintext highlighter-rouge">collect()</code>, <code class="language-plaintext highlighter-rouge">saveAsTextFile()</code></li>
</ul>

<h1 id="ì‹¤ìŠµ-ìë£Œ">ì‹¤ìŠµ ìë£Œ</h1>

<ul>
  <li><a href="https://github.com/kimziont/pyspark_train" target="_blank"><strong>ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì‹¤ìŠµ ìë£Œ</strong></a></li>
</ul>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>

<ul>
  <li><a href="https://sparkbyexamples.com/" target="_blank">Spark By Example, ìŠ¤íŒŒí¬ ë°°ìš°ê¸° ì¢‹ì€ ë¸”ë¡œê·¸</a></li>
  <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html" target="_blank">PySpark ê³µì‹ë¬¸ì„œ, Spark Session</a></li>
</ul>
:ET
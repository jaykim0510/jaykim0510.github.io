I"Sp<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#intuition" id="markdown-toc-intuition">Intuition</a></li>
  <li><a href="#initialization" id="markdown-toc-initialization">Initialization</a></li>
  <li><a href="#experiment-tracking" id="markdown-toc-experiment-tracking">Experiment Tracking</a></li>
  <li><a href="#artifact" id="markdown-toc-artifact">Artifact</a></li>
  <li><a href="#versioning" id="markdown-toc-versioning">Versioning</a></li>
  <li><a href="#hyperparameter-tuning" id="markdown-toc-hyperparameter-tuning">Hyperparameter Tuning</a></li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<h1 id="intuition">Intuition</h1>

<ul>
  <li>
    <p>Machine learning experiment tracking, dataset versioning, and model evaluation</p>
  </li>
  <li>
    <p>Weights &amp; Biases is the machine learning platform for developers to build better models faster. Use W&amp;B’s lightweight, interoperable tools to quickly track experiments, version and iterate on datasets, evaluate model performance, reproduce models, visualize results and spot regressions, and share findings with colleagues.</p>
  </li>
  <li>
    <p>Set up W&amp;B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your datasets and models are tracked and versioned in a reliable system of record.</p>
  </li>
  <li><a href="https://docs.wandb.ai/guides/track">Experiment Tracking</a></li>
  <li><a href="https://docs.wandb.ai/guides/sweeps">Hyperparameter Tuning</a></li>
  <li><a href="https://docs.wandb.ai/guides/data-and-model-versioning">Data and Model Versioning</a></li>
  <li><a href="https://docs.wandb.ai/guides/artifacts">Artifact</a></li>
  <li><a href="https://docs.wandb.ai/guides/models">Model Management</a></li>
  <li><a href="https://docs.wandb.ai/guides/data-vis">Data Visualization</a></li>
</ul>

<h1 id="initialization">Initialization</h1>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pip <span class="nb">install </span>wandb
wandb login
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project_name</span><span class="o">=</span><span class="s">"my-project"</span><span class="p">)</span> 
</code></pre></div></div>

<h1 id="experiment-tracking">Experiment Tracking</h1>

<ul>
  <li>
    <p>Track and visualize experiments in real time, compare baselines, and iterate quickly on ML projects</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">wandb.init()</code>: Launch Experiments with wandb.init</p>
    <ul>
      <li>Initialize a new run at the top of your script. This returns a Run object and creates a local directory where all logs and files are saved, then streamed asynchronously to a W&amp;B server. If you want to use a private server instead of our hosted cloud server, we offer Self-Hosting.</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">wandb</span>

<span class="n">config</span> <span class="o">=</span> <span class="nb">dict</span> <span class="p">(</span>
  <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
  <span class="n">momentum</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
  <span class="n">architecture</span> <span class="o">=</span> <span class="s">"CNN"</span><span class="p">,</span>
  <span class="n">dataset_id</span> <span class="o">=</span> <span class="s">"peds-0192"</span><span class="p">,</span>
  <span class="n">infra</span> <span class="o">=</span> <span class="s">"AWS"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Config: Track hyperparameters, architecture, dataset, and anything else you'd like to use to reproduce your model.
# we can group, sort, and filter runs dynamically in the app through Config
</span>
<span class="c1"># Project: A project is a set of experiments you can compare together.
</span>
<span class="c1"># Notes: A quick commit message to yourself, the note can be set from your script and is editable in the table.
</span>
<span class="c1"># Tags: Identify baseline runs and favorite runs. You can filter runs using tags, and they're editable in the table.
</span>
<span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span>
  <span class="n">project</span><span class="o">=</span><span class="s">"detect-pedestrians"</span><span class="p">,</span>
  <span class="n">notes</span><span class="o">=</span><span class="s">"tweak baseline"</span><span class="p">,</span>
  <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s">"baseline"</span><span class="p">,</span> <span class="s">"paper1"</span><span class="p">],</span>
  <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">wandb.config</code>: Configure Experiments with wandb.config
    <ul>
      <li>Set the wandb.config object in your script to save your training configuration: hyperparameters, input settings like dataset name or model type, and any other independent variables for your experiments.</li>
      <li>The model settings (you capture in config) are useful later to organize and query your results.</li>
      <li>(Note that output metrics or dependent variables (like loss and accuracy) should be saved with <code class="language-plaintext highlighter-rouge">wandb.log</code> instead.)</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span>
  <span class="n">project</span><span class="o">=</span><span class="s">"detect-pedestrians"</span><span class="p">,</span>
  <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">update</span><span class="p">({</span><span class="s">"lr"</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s">"channels"</span><span class="p">:</span> <span class="mi">16</span><span class="p">})</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">wandb.log()</code>: Log Data with wandb.log
    <ul>
      <li>Keep track of metrics, videos, custom plots, and more</li>
      <li>Each time you log, we increment the step by default, so you can see how your models and data evolve over time.</li>
      <li>Log metrics over time in a training loop, such as accuracy and loss. By default, when you call wandb.log it appends a new step to the history object and updates the summary object.</li>
      <li>history: An array of dictionary-like objects that tracks metrics over time. These time series values are shown as default line plots in the UI.</li>
      <li>summary: By default, the final value of a metric logged with wandb.log(). You can set the summary for a metric manually to capture the highest accuracy or lowest loss instead of the final value. These values are used in the table, and plots that compare runs — for example, you could visualize at the final accuracy for all runs in your project.</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">wandb</span><span class="p">.</span><span class="n">log</span><span class="p">({</span><span class="s">"loss"</span><span class="p">:</span> <span class="mf">0.314</span><span class="p">,</span> <span class="s">"epoch"</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
           <span class="s">"inputs"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Image</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span>
           <span class="s">"logits"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Histogram</span><span class="p">(</span><span class="n">ouputs</span><span class="p">),</span>
           <span class="s">"captions"</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Html</span><span class="p">(</span><span class="n">captions</span><span class="p">)})</span>

<span class="n">my_table</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Table</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">"a"</span><span class="p">,</span> <span class="s">"b"</span><span class="p">],</span> <span class="n">data</span><span class="o">=</span><span class="p">[[</span><span class="s">"a1"</span><span class="p">,</span> <span class="s">"b1"</span><span class="p">],</span> <span class="p">[</span><span class="s">"a2"</span><span class="p">,</span> <span class="s">"b2"</span><span class="p">]])</span>
<span class="n">run</span><span class="p">.</span><span class="n">log</span><span class="p">({</span><span class="s">"Table Name"</span><span class="p">:</span> <span class="n">my_table</span><span class="p">})</span>

<span class="n">example_images</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">wandb</span><span class="p">.</span><span class="n">Image</span><span class="p">(</span>
                <span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">caption</span><span class="o">=</span><span class="s">"Pred: {} Truth: {}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">item</span><span class="p">(),</span> <span class="n">target</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span class="n">run</span><span class="p">.</span><span class="n">log</span><span class="p">({</span><span class="s">"Image"</span><span class="p">:</span> <span class="n">example_images</span><span class="p">})</span>

<span class="n">run</span><span class="p">.</span><span class="n">log</span><span class="p">({</span><span class="s">'roc'</span><span class="p">:</span> <span class="n">wandb</span><span class="p">.</span><span class="n">plots</span><span class="p">.</span><span class="n">ROC</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_prob_pred</span><span class="p">,</span> <span class="n">cnb</span><span class="p">.</span><span class="n">classes_</span><span class="p">)})</span>
</code></pre></div></div>

<h1 id="artifact">Artifact</h1>

<ul>
  <li><code class="language-plaintext highlighter-rouge">wandb.log_artifact</code>:
    <ul>
      <li>Artifacts to track datasets, models, dependencies, and results through each step of your machine learning pipeline. Artifacts make it easy to get a complete and auditable history of changes to your files.</li>
      <li>Artifacts can be thought of as a versioned directory. Artifacts are either an input of a run or an output of a run. Common artifacts include entire training sets and models. Store datasets directly into artifacts, or use artifact references to point to data in other systems like Amazon S3, GCP, or your own system.</li>
      <li>Save outputs of a run, like the model weights or a table of predictions. This lets you track not just model training, but all the pipeline steps that affect the final model.</li>
      <li>An artifact is like a directory of data. Each entry is either an actual file stored in the artifact, or a reference to an external URI. You can nest folders inside an artifact just like a regular filesystem. You can store any data, including: datasets, models, images, HTML, code, audio, raw binary data and more.</li>
      <li>Every time you change the contents of this directory, Weights &amp; Biases will create a new version of your artifact instead of overwriting the previous contents.</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"artifacts-example"</span><span class="p">,</span> <span class="n">job_type</span><span class="o">=</span><span class="s">'dataset'</span><span class="p">)</span>

<span class="c1"># artifact 오브젝트 생성
</span><span class="n">artifact</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Artifact</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s">'bicycle-dataset'</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">'dataset'</span><span class="p">)</span>

<span class="c1"># artifact에 넣고싶은 것 추가
</span><span class="n">artifact</span><span class="p">.</span><span class="n">add_file</span><span class="p">(</span><span class="n">local_path</span><span class="o">=</span><span class="s">'dataset.h5'</span><span class="p">)</span>

<span class="c1"># run에 artifact 로그
</span><span class="n">run</span><span class="p">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">artifact</span><span class="p">)</span>

<span class="o">------</span>

<span class="c1"># artifact를 사용
</span><span class="n">artifact</span> <span class="o">=</span> <span class="n">run</span><span class="p">.</span><span class="n">use_artifact</span><span class="p">(</span><span class="s">'bicycle-dataset:latest'</span><span class="p">)</span>

<span class="c1"># artifact를 다운
</span><span class="n">artifact_dir</span> <span class="o">=</span> <span class="n">artifact</span><span class="p">.</span><span class="n">download</span><span class="p">()</span>

</code></pre></div></div>

<h1 id="versioning">Versioning</h1>

<ul>
  <li>Use Artifacts for dataset versioning, model versioning, and tracking dependencies and results across machine learning pipelines</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">"my_project"</span><span class="p">)</span>
<span class="n">artifact</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">Artifact</span><span class="p">(</span><span class="s">"new_dataset"</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="s">"ver1"</span><span class="p">)</span>

<span class="c1"># model: model이 저장된 경로
</span><span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">"model"</span><span class="p">)</span>

<span class="c1"># train: train 데이터가 저장된 경로
</span><span class="k">for</span> <span class="nb">dir</span> <span class="ow">in</span> <span class="p">[</span><span class="s">"train"</span><span class="p">,</span> <span class="s">"val"</span><span class="p">,</span> <span class="s">"test"</span><span class="p">]:</span>
	<span class="n">artifact</span><span class="p">.</span><span class="n">add_dir</span><span class="p">(</span><span class="nb">dir</span><span class="p">)</span>

<span class="c1"># 모델이 저장된 경로인 model을 artifact에 추가
</span><span class="n">artifact</span><span class="p">.</span><span class="n">add_dir</span><span class="p">(</span><span class="s">"model"</span><span class="p">)</span>

<span class="n">run</span><span class="p">.</span><span class="n">log_artifact</span><span class="p">(</span><span class="n">artifact</span><span class="p">)</span>

</code></pre></div></div>

<h1 id="hyperparameter-tuning">Hyperparameter Tuning</h1>

<ul>
  <li>Hyperparameter search and model optimization with W&amp;B Sweeps</li>
  <li>Use Weights &amp; Biases Sweeps to automate hyperparameter search and explore the space of possible models.</li>
</ul>

<p><img src="/images/wandb_1.png" alt="" /></p>

<p>There are two components to Weights &amp; Biases Sweeps: a controller and one or more agents.</p>

<ul>
  <li>Controller picks out new hyperparameter combinations.</li>
  <li>Agents query the Weights &amp; Biases server for hyperparameters and use them to run model training. The training results are then reported back to the Sweep server. Agents can run one or more processes on one or more machines. The flexibility of agents to run multiples processes across multiples machines makes it easy to parallelize and scale Sweeps.</li>
</ul>

<p>Create a W&amp;B Sweep with the following steps:</p>

<ul>
  <li><strong>Add W&amp;B to your code</strong>: In your Python script, add a couple lines of code to log hyperparameters and output metrics from your script. See Add W&amp;B to your code for more information.</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># 🐝 Step 1: Define training function that takes in hyperparameter 
# values from `wandb.config` and uses them to train a model and return metric
</span><span class="k">def</span> <span class="nf">train_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">bs</span><span class="p">):</span> 
  <span class="n">acc</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">+</span> <span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="mi">30</span><span class="p">)</span> <span class="o">+</span>  <span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.2</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">((</span><span class="n">epoch</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span> <span class="o">+</span>  <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">/</span><span class="mi">5</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">acc</span><span class="p">,</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">evaluate_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span> 
  <span class="n">acc</span> <span class="o">=</span> <span class="mf">0.1</span> <span class="o">+</span> <span class="p">((</span><span class="n">epoch</span><span class="o">/</span><span class="mi">20</span><span class="p">)</span> <span class="o">+</span>  <span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="p">((</span><span class="n">epoch</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">/</span><span class="mi">10</span> <span class="o">+</span>  <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span><span class="o">/</span><span class="mi">6</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">acc</span><span class="p">,</span> <span class="n">loss</span>

<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="c1"># Use the wandb.init() API to generate a background process 
</span>    <span class="c1"># to sync and log data as a Weights and Biases run.
</span>    <span class="c1"># Optionally provide the name of the project. 
</span>    <span class="n">run</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">project</span><span class="o">=</span><span class="s">'my-first-sweep'</span><span class="p">)</span>

    <span class="c1"># note that we define values from `wandb.config` instead of 
</span>    <span class="c1"># defining hard values
</span>    <span class="n">lr</span>  <span class="o">=</span>  <span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">lr</span>
    <span class="n">bs</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">batch_size</span>
    <span class="n">epochs</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">config</span><span class="p">.</span><span class="n">epochs</span>

    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>
      <span class="n">train_acc</span><span class="p">,</span> <span class="n">train_loss</span> <span class="o">=</span> <span class="n">train_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">bs</span><span class="p">)</span>
      <span class="n">val_acc</span><span class="p">,</span> <span class="n">val_loss</span> <span class="o">=</span> <span class="n">evaluate_one_epoch</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>

      <span class="n">wandb</span><span class="p">.</span><span class="n">log</span><span class="p">({</span>
        <span class="s">'epoch'</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span> 
        <span class="s">'train_acc'</span><span class="p">:</span> <span class="n">train_acc</span><span class="p">,</span>
        <span class="s">'train_loss'</span><span class="p">:</span> <span class="n">train_loss</span><span class="p">,</span> 
        <span class="s">'val_acc'</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">,</span> 
        <span class="s">'val_loss'</span><span class="p">:</span> <span class="n">val_loss</span>
      <span class="p">})</span>
</code></pre></div></div>

<ul>
  <li><strong>Define the sweep configuration</strong>: Define the variables and ranges to sweep over. Pick a search strategy— we support grid, random, and Bayesian search, plus techniques for faster iterations like early stopping. See Define sweep configuration for more information.</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 🐝 Step 2: Define sweep config
</span><span class="n">sweep_configuration</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'method'</span><span class="p">:</span> <span class="s">'random'</span><span class="p">,</span>
    <span class="s">'name'</span><span class="p">:</span> <span class="s">'sweep'</span><span class="p">,</span>
    <span class="s">'metric'</span><span class="p">:</span> <span class="p">{</span><span class="s">'goal'</span><span class="p">:</span> <span class="s">'maximize'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">:</span> <span class="s">'val_acc'</span><span class="p">},</span>
    <span class="s">'parameters'</span><span class="p">:</span> 
    <span class="p">{</span>
        <span class="s">'batch_size'</span><span class="p">:</span> <span class="p">{</span><span class="s">'values'</span><span class="p">:</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">]},</span>
        <span class="s">'epochs'</span><span class="p">:</span> <span class="p">{</span><span class="s">'values'</span><span class="p">:</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">15</span><span class="p">]},</span>
        <span class="s">'lr'</span><span class="p">:</span> <span class="p">{</span><span class="s">'max'</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s">'min'</span><span class="p">:</span> <span class="mf">0.0001</span><span class="p">}</span>
     <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<ul>
  <li><strong>Initialize sweep</strong>: Start the Sweep server. We host this central controller and coordinate between the agents that execute the sweep. See Initialize sweeps for more information.</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 🐝 Step 3: Initialize sweep by passing in config
</span><span class="n">sweep_id</span> <span class="o">=</span> <span class="n">wandb</span><span class="p">.</span><span class="n">sweep</span><span class="p">(</span><span class="n">sweep</span><span class="o">=</span><span class="n">sweep_configuration</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="s">'my-first-sweep'</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Start sweep</strong>: Run a single-line command on each machine you’d like to use to train models in the sweep. The agents ask the central sweep server what hyperparameters to try next, and then they execute the runs. See Start sweep agents for more information.</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 🐝 Step 4: Call to `wandb.agent` to start a sweep
</span><span class="n">wandb</span><span class="p">.</span><span class="n">agent</span><span class="p">(</span><span class="n">sweep_id</span><span class="p">,</span> <span class="n">function</span><span class="o">=</span><span class="n">main</span><span class="p">,</span> <span class="n">count</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><strong>Visualize results (optional)</strong>: Open our live dashboard to see all your results in one central place.</li>
</ul>

<p><img src="/images/wandb_2.png" alt="" /></p>

<h1 id="참고">참고</h1>

<ul>
  <li><a href="https://docs.wandb.ai/" target="_blank">wandb 공식문서</a></li>
  <li><a href="https://neptune.ai/blog/best-tools-for-model-tuning-and-hyperparameter-optimization" target="_blank">neptune, Best Tools for Model Tuning and Hyperparameter Optimization</a></li>
  <li><a href="https://pebpung.github.io/wandb/2021/10/10/WandB-2.html" target="_blank">pebpung, Sweep이란? - Hyper Parameter 최적화 Tool</a></li>
  <li><a href="https://pebpung.github.io/wandb/2021/10/17/WandB-3.html">pebpung, WandB의 다양한 시각화방법 (feat. Confusion Matrices)</a></li>
</ul>
:ET
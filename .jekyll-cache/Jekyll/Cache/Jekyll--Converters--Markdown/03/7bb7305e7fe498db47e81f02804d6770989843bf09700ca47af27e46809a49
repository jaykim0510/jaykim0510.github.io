I"y<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#인공신경망" id="markdown-toc-인공신경망">인공신경망</a>    <ul>
      <li><a href="#퍼셉트론" id="markdown-toc-퍼셉트론">퍼셉트론</a></li>
      <li><a href="#dnn" id="markdown-toc-dnn">DNN</a></li>
      <li><a href="#비선형-활성-함수" id="markdown-toc-비선형-활성-함수">비선형 활성 함수</a></li>
    </ul>
  </li>
  <li><a href="#결론" id="markdown-toc-결론">결론</a></li>
</ul>

<hr />

<h1 id="인공신경망">인공신경망</h1>

<ul>
  <li>Artificial Neural Network</li>
  <li>인공신경망은 딥러닝을 이루는 핵심 개념이다</li>
  <li>딥러닝은 결국 인공신경망 레이어를 깊이(deep) 쌓아서 딥러닝인 것이다</li>
</ul>

<h2 id="퍼셉트론">퍼셉트론</h2>

<ul>
  <li><strong>퍼셉트론은 가장 간단한 인공 신경망 구조 중 하나</strong>로 1957년 프랑크 로젠블라트가 제안했다</li>
  <li>퍼셉트론은 현대의 인공 신경망과 비교하면 상당히 원시적인 신경망이지만, 노드, 가중치, 레이어와 같은 인공 신경망의 중요한 구성요소들을 공부하는데에 큰 의미가 있다</li>
</ul>

<p><img src="/images/ai_basic_5.png" alt="" />
<em>Photo by <a href="https://compmath.korea.ac.kr/deeplearning/Perceptron.html">Daeki Yoon</a></em></p>

<ul>
  <li>퍼셉트론 (혹은 인공신경망)은 위 그림과 같이, <span class="very__important">입력을 받아 각각의 가중치를 곱해 더한 후 활성 함수를 통과시켜 값을 출력하는 일종의 함수</span>다</li>
</ul>

<p><img src="/images/ai_basic_10.png" alt="" /></p>

<ul>
  <li>하지만 퍼셉트론은 비선형 분류 문제를 풀 수 없다는 치명적인 약점이 있다</li>
  <li>이 문제는 1970년대 민스키의 『Perceptrons』에서 지적되었고, 이 후 한동안 신경망 연구가 정체기를 겪었다</li>
</ul>

<p><img src="/images/ai_basic_6.png" alt="" width="50%" /></p>

<ul>
  <li>이 문제를 해결하기 위해 <span class="very__important">은닉층</span> 이라는 개념이 도입되었다</li>
  <li>다시 XOR 문제로 돌아와 보면, XOR 문제는 주어진 <code class="language-plaintext highlighter-rouge">x1</code>, <code class="language-plaintext highlighter-rouge">x2</code> 공간에서는 데이터를 분류하는 모델을 만들 수 없다. 따라서 분류가 가능하도록 해주는 특징공간으로 옮겨야 하는데, 이를 가능하게 해주는 것이 바로 은닉층의 역할이다.<br />
<img src="/images/ai_basic_7.png" alt="" width="80%" /></li>
  <li>밑에 그림과 같이 두 개의 퍼셉트론을 이용해 새로운 특징공간 <code class="language-plaintext highlighter-rouge">z1</code>, <code class="language-plaintext highlighter-rouge">z2</code>로 옮기면 우리의 데이터를 분류할 수 있게 된다(<code class="language-plaintext highlighter-rouge">z1</code>, <code class="language-plaintext highlighter-rouge">z2</code>층이 바로 은닉층)<br />
<img src="/images/ai_basic_8.png" alt="" width="80%" /></li>
</ul>

<p><img src="/images/ai_basic_11.png" alt="" /></p>

<h2 id="dnn">DNN</h2>

<ul>
  <li>Deep Neural Network</li>
  <li>인공신경망은 일종의 함수인데, 이러한 인공신경망을 깊이 쌓으면 그것이 바로 <strong>깊은 인공 신경망(DNN)</strong>이 된다</li>
  <li>이것은 여러 개의 함수가 적용된 합성 함수와 같다</li>
</ul>

<p><img src="/images/ai_basic_12.png" alt="" /></p>

<p><img src="/images/ai_basic_9.png" alt="" width="60%" /></p>

<ul>
  <li>깊은 층을 통해 비선형 분류 문제를 해결하게 되며 이를 계기로 다양한 문제에 층을 깊이 쌓은 신경망 구조가 주목을 받기 시작했다</li>
  <li>신경망을 깊게 쌓음으로써 기계가 여러 가지 특징 공간에서 데이터를 볼 수 있게 되었고 이 방법은 실제로 비정형 데이터(음성, 사진 등)를 다루는 데에 굉장한 성능을 보여주었다</li>
</ul>

<h2 id="비선형-활성-함수">비선형 활성 함수</h2>

<ul>
  <li><span class="very__important">깊은 인공신경망</span>. 이것이 바로 딥러닝의 핵심이다</li>
  <li>은닉층을 깊게 쌓으면 모델이 데이터를 더 다양한 특징 공간에서 볼 수 있고 이것은 마치 사람의 생각이 더 깊어지고 더 다양한 관점에서 사물을 바라보는 것과 비슷하다</li>
  <li>하지만 은닉층을 깊이 쌓으려면 은닉층에 반드시 포함되어야 하는 요소가 있는데 바로 비선형 활성 함수(Non linear activation function)이다</li>
  <li>깊은 인공 신경망은 여러 개의 함수가 적용된 합성 함수와 같은데, 합성 함수가 linear한 관계라면, 결국은 하나의 linear function과 다를게 없고, 이는 신경망을 깊어지는 효과를 만들어내지 못한다</li>
  <li>그래서 <span class="very__important">인공신경망을 깊게 만들려면 하나로 합쳐지지 않는 비선형 활성 함수가 필요</span>한 것이다</li>
</ul>

<h1 id="결론">결론</h1>

<ul>
  <li>인공신경망은 일종의 함수다</li>
  <li>인공신경망을 깊게 쌓으면 비선형 문제를 비롯한 복잡한 문제를 풀 수 있게 된다</li>
  <li>인공신경망을 깊게 쌓기 위해서는 각 은닉층마다 비선형 활성 함수가 있어야 한다</li>
</ul>
:ET
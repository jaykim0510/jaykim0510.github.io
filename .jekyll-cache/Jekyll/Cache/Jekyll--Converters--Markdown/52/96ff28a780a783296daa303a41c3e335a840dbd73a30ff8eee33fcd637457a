I"H7<hr />
<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#partition" id="markdown-toc-partition">Partition</a></li>
  <li><a href="#segment" id="markdown-toc-segment">Segment</a></li>
  <li><a href="#parallelism-with-partitions" id="markdown-toc-parallelism-with-partitions">Parallelism with Partitions</a></li>
  <li><a href="#replication" id="markdown-toc-replication">Replication</a></li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<p>Kafka is everywhere these days. With the advent of Microservices and distributed computing, Kafka has become a regular occurrence in the architecture of every product. In this article, I’ll try to explain how Kafka’s internal storage mechanism works.</p>

<p>Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a Distributed, Replicated Commit Log. This, I think, clearly represents what Kafka does, as all of us understand how logs are written to disk. And in this case, it is the messages pushed into Kafka that are stored to disk.</p>

<p>Regarding storage in Kafka, you’ll always hear two terms - Partition and Topic. Partitions are the units of storage in Kafka for messages. And Topic can be thought of as being a container in which these partitions lie.</p>

<h2 id="partition">Partition</h2>

<p>I am going to start by creating a topic in Kafka with three partitions.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-topics.sh <span class="nt">--create</span> <span class="nt">--topic</span> freblogg <span class="nt">--partitions</span> 3 <span class="nt">--replication-factor</span> 1 <span class="nt">--zookeeper</span> localhost:2181
</code></pre></div></div>

<p>If I go into Kafka’s log directory, I see three directories created as follows.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ tree freblogg*
freblogg-0
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-1
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-2
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
</code></pre></div></div>

<p>We have three directories created because we’ve given three partitions for our topic, which means that each partition gets a directory on the file system. You also see some files like index, log etc. We’ll get to them shortly.</p>

<p>One more thing that you should be able to see from here is that in Kafka, the topic is more of a logical grouping than anything else and that the Partition is the actual unit of storage in Kafka. That is what is physically stored on the disk. Let’s understand partitions in some more detail.</p>

<p>Now let us send a couple of messages and see what happens. To send the messages I’m using the console producer as follows:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-console-producer.sh --topic freblogg --broker-list localhost:9092
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -lh freblogg*
freblogg-0:
total 20M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpoint

freblogg-1:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpoint

freblogg-2:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint
</code></pre></div></div>

<p>Our two messages went into two of the partitions where you can see that the log files have a non zero size. This is because the messages in the partition are stored in the ‘xxxx.log’ file. To confirm that the messages are indeed stored in the log file, we can just see what’s inside that log file.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat freblogg-2/*.log
@^@^BÂ°Â£Ã¦Ãƒ^@^K^XÃ¿Ã¿Ã¿Ã¿Ã¿Ã¿^@^@^@^A"^@^@^A^VHello World^@
</code></pre></div></div>

<p>The file format of the ‘log’ file is not conducive for textual representation but, you should see the ‘Hello World’ at the end indicating that this file got updated when we have sent the message into the topic. The second message we have sent went into the other partition.</p>

<p>Notice that the first message we sent, went into the third partition (freblogg-2) and the second message went into the second partition (freblogg-1). This is because Kafka arbitrarily picks the partition for the first message and then distributes the messages to partitions in a round-robin fashion. If a third message comes now, it would go into freblogg-0 and this order of partition continues for any new message that comes in. We can also make Kafka choose the same partition for our messages by adding a key to the message. Kafka stores all the messages with the same key into a single partition.</p>

<p>Each new message in the partition gets an Id which is one more than the previous Id number. This Id number is also called the Offset. So, the first message is at ‘offset’ 0, the second message is at offset 1 and so on. These offset Id’s are always incremented from the previous value.</p>

<p><img src="/images/kafka_75.png" alt="" /></p>

<p>We can understand those random characters in the log file, using a Kafka tool.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-run-class.bat kafka.tools.DumpLogSegments <span class="nt">--deep-iteration</span> <span class="nt">--print-data-log</span> <span class="nt">--files</span> logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
</code></pre></div></div>

<p>This gives the output</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dumping logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
Starting offset: 0

offset: 0 position: 0 CreateTime: 1533443377944 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 11 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: Hello World

offset: 1 position: 79 CreateTime: 1533462689974 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 6 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: amazon
</code></pre></div></div>

<p>CreateTime과 같은 값은 컨슈머로 가져와서 사용할 수 있는 값이 아니다. 카프카 내부적으로 가지고 있는 메타데이터이다. 그렇기 때문에 데이터의 타임스탬프가 필요하다면, 데이터를 생성할 때 내부적으로 메세지에 명시적으로 담아서 브로커에 담아야 한다.</p>

<p>You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.</p>

<p>It is also important to note that a partition is tied to a broker. In other words, If we have three brokers and if the folder freblogg-0 exists on broker-1, you can be sure that it will not appear in any of the other brokers. Partitions of a topic can be spread out to multiple brokers but a partition is always present on one single Kafka broker (When the replication factor has its default value, which is 1. Replication is mentioned further below).</p>

<h2 id="segment">Segment</h2>

<p>We’ll finally talk about those index and log files we’ve seen in the partition directory. Partition might be the standard unit of storage in Kafka, but it is not the lowest level of abstraction provided. Each partition is divided into segments.</p>

<p>A segment is simply a collection of messages of a partition. Instead of storing all the messages of a partition in a single file (think of the log file analogy again), Kafka splits them into chunks called segments. Doing this provides several advantages. Divide and Conquer FTW!</p>

<p>Most importantly, it makes purging data easy. As previously introduced partition is immutable from a consumer perspective. But Kafka can still remove the messages based on the “Retention policy” of the topic. Deleting segments is much simpler than deleting things from a single file, especially when a producer might be pushing data into it.</p>

<p>Each segment file has <code class="language-plaintext highlighter-rouge">segment.log</code>, <code class="language-plaintext highlighter-rouge">segment.index</code> and `` fisegment.timeindexles.</p>

<p>Kafka always writes the messages into these segment files under a partition. There is always an active segment to which Kafka writes to. Once the segment’s size limit is reached, a new segment file is created and that becomes the active segment.</p>

<p>One of the common operations in Kafka is to read the message at a particular offset. For this, if it has to go to the log file to find the offset, it becomes an expensive task especially because the log file can grow to huge sizes (Default—1G). This is where the .index file becomes useful. Index file stores the offsets and physical position of the message in the log file.</p>

<p>An index file for the log file I’ve showed in the ‘Quick detour’ above would look something like this:</p>

<p><img src="/images/kafka_76.png" alt="" /></p>

<p>If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.</p>

<h2 id="parallelism-with-partitions">Parallelism with Partitions</h2>

<p>To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.</p>

<p>Just increasing the number of consumers won’t increase the parallelism. You need to scale your partitions accordingly. To read data from a topic in parallel with two consumers, you create two partitions so that each consumer can read from its own partition. Also since partitions of a topic can be on different brokers, two consumers of a topic can read the data from two different brokers.</p>

<h2 id="replication">Replication</h2>
<p>Let’s talk about replication. Whenever we’re creating a topic in Kafka, we need to specify the replication factor we need for that topic. Let’s say we’ve two brokers and so we’ve given the replication-factor as 2. What this means is that Kafka will try to always ensure that each partition of this topic has a backup/replica. The way Kafka distributes the partitions is quite similar to how HDFS distributes its data blocks across nodes.</p>

<p>Say for the freblogg topic that we’ve been using so far, we’ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this.</p>

<p><img src="/images/kafka_77.png" alt="" /></p>

<h1 id="참고">참고</h1>
<ul>
  <li><a href="https://docs.cloudera.com/csa/1.2.0/flink-sql-table-api/topics/csa-kafka-sql-datatypes.html" target="_blank">Data types for Kafka connector</a></li>
  <li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/" target="_blank">Kafka Connect Deep Dive – Converters and Serialization Explained</a></li>
  <li><a href="https://dol9.tistory.com/274" target="_blank">dol9, Kafka 스키마 관리, Schema Registry</a></li>
  <li><a href="https://www.freblogg.com/kafka-storage-internals" target="_blank">A Practical Introduction to Kafka Storage Internals</a></li>
  <li><a href="https://www.freecodecamp.org/news/what-makes-apache-kafka-so-fast-a8d4f94ab145/" target="_blank">Here’s what makes Apache Kafka so fast</a></li>
  <li><a href="https://stackoverflow.com/questions/40369238/which-directory-does-apache-kafka-store-the-data-in-broker-nodes#" target="_blank">stackoverflow: Which directory does apache kafka store the data in broker nodes</a></li>
  <li><a href="https://medium.com/@abhisheksharma_59226/how-kafka-stores-data-37ee611c89a2" target="_blank">Abhishek Sharma, How kafka stores data</a></li>
  <li><a href="https://rohithsankepally.github.io/Kafka-Storage-Internals/" target="_blank">Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals</a></li>
  <li><a href="https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7" target="_blank">towardsdatascience, Log Compacted Topics in Apache Kafka</a></li>
  <li><a href="https://www.conduktor.io/understanding-kafkas-internal-storage-and-log-retention" target="_blank">conduktor, Understanding Kafka’s Internal Storage and Log Retention</a></li>
  <li><a href="https://dev.to/heroku/what-is-a-commit-log-and-why-should-you-care-pib" target="_blank">What is a commit log and why should you care?</a></li>
</ul>
:ET
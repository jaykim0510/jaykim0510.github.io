I"<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#신경망-학습" id="markdown-toc-신경망-학습">신경망 학습</a>    <ul>
      <li><a href="#손실-함수" id="markdown-toc-손실-함수">손실 함수</a>        <ul>
          <li><a href="#mse" id="markdown-toc-mse">MSE</a></li>
        </ul>
      </li>
      <li><a href="#가중치-업데이트" id="markdown-toc-가중치-업데이트">가중치 업데이트</a></li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="신경망-학습">신경망 학습</h1>

<ul>
  <li>앞에서 살펴본 인공 신경망은 여러 층의 은닉층을 포함하고 있었다</li>
  <li>각각의 은닉층에는 가중치라는 매개변수가 있다</li>
  <li>학습이란 <span class="very__important">훈련 데이터로부터 가중치 매개변수의 최적값을 찾아내는 것</span>을 말한다</li>
  <li>가중치 매개변수의 최적값은 <span class="very__important">신경망 모델의 출력이 실제 값에 최대한 가까워질 때의 값</span>을 의미한다</li>
</ul>

<p><img src="/images/ai_basic_13.png" alt="" width="80%" /></p>

<h2 id="손실-함수">손실 함수</h2>

<ul>
  <li>손실 함수는 <span class="very__important">신경망 모델의 출력과 실제 값의 차이를 수학적으로 정의한 함수</span>를 의미한다</li>
  <li>모델의 출력과 실제 값이 가까워지려면 두 값의 차이가 최소가 되어야 하므로, 우리는 <span class="very__important">손실 함수가 최소값이 되도록 학습</span>을 하면 되는 것이다</li>
  <li>손실 함수의 종류는 대표적으로 평균 제곱 오차(mean squared error, MSE)와 크로스 엔트로피 오차(cross entropy error)가 있다</li>
</ul>

<h3 id="mse">MSE</h3>

<ul>
  <li><strong>Mean Squared Error</strong></li>
  <li><strong>회귀 문제</strong>에서 대표적으로 많이 사용되는 손실 함수이다</li>
  <li>(크로스 엔트로피 오차는 분류 문제에서 대표적으로 사용된다. 크로스 엔트로피는 나중에 분류 문제를 다루는 글에서 따로 다루도록 하겠다)</li>
</ul>

<p><img src="/images/ai_basic_14.png" alt="" width="80%" /></p>

<ul>
  <li>데이터 (<code class="language-plaintext highlighter-rouge">x</code>(입력), <code class="language-plaintext highlighter-rouge">y</code>(정답레이블)) 쌍이 총 <code class="language-plaintext highlighter-rouge">100</code>개 있다고 해보자</li>
  <li>입력 데이터 <code class="language-plaintext highlighter-rouge">1</code>개마다 신경말 모델의 출력 <code class="language-plaintext highlighter-rouge">y_hat</code>이 있고 이 값이 정답 레이블 <code class="language-plaintext highlighter-rouge">y</code> 1개와의 차이가 있다</li>
  <li>하지만 입력 데이터 <code class="language-plaintext highlighter-rouge">1</code>개에만 잘 맞는 모델이 아니라, 데이터 <code class="language-plaintext highlighter-rouge">100</code>개에 대해 평균적으로 잘 맞는 모델의 가중치를 찾아야 한다</li>
  <li>그래서 전체 데이터 개수만큼 발생한 <strong>오차의 평균</strong>을 구해야 한다</li>
  <li>또한 첫 번째 쌍에서 오차가 <code class="language-plaintext highlighter-rouge">-2</code>이고, 두 번째 쌍에서 오차가 <code class="language-plaintext highlighter-rouge">+2</code> 였다면 둘을 더하면 <code class="language-plaintext highlighter-rouge">0</code>이 되어서 오차가 상쇄되는 문제가 생긴다</li>
  <li>그래서 단순히 오차가 아니라, <strong>제곱 오차</strong>를 구해야 한다</li>
  <li>(절대값이 아니라 제곱을 하는 이유는 제곱이 미분이 가능하기 때문이다. 미분이 되는 것이 나중에 배울 경사 하강법에서 이점이 있다)</li>
</ul>

<h2 id="가중치-업데이트">가중치 업데이트</h2>

<ul>
  <li>학습은 최적의 가중치 매개변수를 찾아내는 것이다</li>
  <li>그러려면 학습 데이터(<code class="language-plaintext highlighter-rouge">(x, y)</code>)를 통과시킬 때마다 구해지는 오차를 최소화시키도록 가중치(<code class="language-plaintext highlighter-rouge">w</code>)를 업데이트 해야한다</li>
  <li>은닉층에 있는 수많은 가중치들을 각각 어느정도 더하고 빼야 할까?</li>
</ul>

<p><img src="/images/ai_basic_16.png" alt="" width="80%" /></p>

<ul>
  <li>딥러닝에서는 이를 위해 <strong>경사 하강법(Gradient descent)</strong> 방법을 사용한다</li>
  <li>그래디언트는 함수값을 가장 빠르게 증가시키는 방향이기 때문에 마이너스(-)를 취해주면 가장 빠르게 감소시키는 방향을 가르킨다</li>
  <li>
    <p>그래서 각 가중치 값마다 손실 함수의 미분을 구한 후, 상수값을 곱해서 이를 원래의 가중치에서 빼주면, 가중치는 점점 최적의 가중치에 가까워진다</p>
  </li>
  <li>가중치가 하나만 있다고 생각하고 경사 하강법을 시각적으로 단순화 시켜보면 아래와 같다</li>
</ul>

<p><img src="/images/ai_basic_18.png" alt="" /></p>

<ul>
  <li>따라서 가중치가 여러개인 일반적인 상황에서는 손실 함수를 각 가중치에 대해 편미분하여, 각각의 가중치에 대해 업데이트 해주면 된다</li>
</ul>

<p><img src="/images/ai_basic_19.png" alt="" /></p>

<ul>
  <li>놀라운 사실은 MSE 함수의 미분 값은 결국 모델의 출력과 정답과의 차이인 오차(error)와 입력값(x)의 곱과 같다는 사실이다</li>
  <li>마지막으로 이를 행렬로 나타내면 아래와 같다</li>
</ul>

<p><img src="/images/ai_basic_20.png" alt="" width="70%" /></p>
:ET
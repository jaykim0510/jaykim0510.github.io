I"±‰<hr />
<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹" id="markdown-toc-ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹">ì¹´í”„ì¹´ì˜ ë°ì´í„° ì €ì¥ ë°©ì‹</a>    <ul>
      <li><a href="#partition" id="markdown-toc-partition">Partition</a></li>
      <li><a href="#segment" id="markdown-toc-segment">Segment</a></li>
    </ul>
  </li>
  <li><a href="#ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes" id="markdown-toc-ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes">ì €ì¥ëœ ë°ì´í„°ì˜ í¬ë§·(Kafka messages are just bytes)</a></li>
  <li><a href="#ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜" id="markdown-toc-ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜">ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜</a></li>
  <li><a href="#ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ" id="markdown-toc-ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ">ì¥ì•  ë³µêµ¬ë¥¼ ìœ„í•œ ë³µì œ</a></li>
  <li><a href="#ë¡œê·¸-ì„¤ì •ì„-í†µí•´-íš¨ìœ¨ì ìœ¼ë¡œ-ë³´ê´€í•˜ê¸°log-retention" id="markdown-toc-ë¡œê·¸-ì„¤ì •ì„-í†µí•´-íš¨ìœ¨ì ìœ¼ë¡œ-ë³´ê´€í•˜ê¸°log-retention">ë¡œê·¸ ì„¤ì •ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ê´€í•˜ê¸°(Log Retention)</a>    <ul>
      <li><a href="#role-of-indexing-within-the-partition" id="markdown-toc-role-of-indexing-within-the-partition">Role of Indexing within the Partition</a></li>
      <li><a href="#rolling-segments" id="markdown-toc-rolling-segments">Rolling segments</a></li>
      <li><a href="#impact-of-increasingdecreasing-the-segment-size" id="markdown-toc-impact-of-increasingdecreasing-the-segment-size">Impact of increasing/decreasing the segment size</a></li>
      <li><a href="#log-retention---the-records-may-persist-longer-than-the-retention-time" id="markdown-toc-log-retention---the-records-may-persist-longer-than-the-retention-time">Log retention - The records may persist longer than the retention time</a></li>
      <li><a href="#conclusion" id="markdown-toc-conclusion">Conclusion</a></li>
    </ul>
  </li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<h1 id="ì¹´í”„ì¹´ì˜-ë°ì´í„°-ì €ì¥-ë°©ì‹">ì¹´í”„ì¹´ì˜ ë°ì´í„° ì €ì¥ ë°©ì‹</h1>

<p>Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a <strong>Distributed, Replicated Commit Log</strong>. This, I think, clearly represents what Kafka does, as all of us understand how <strong>logs</strong> are written to disk. And in this case, it is the <strong>messages pushed into Kafka that are stored to disk</strong>.</p>

<ul>
  <li>KafkaëŠ” ì»¤ë°‹ ë¡œê·¸ë¥¼ ë¶„ì‚° ë³µì œí•˜ëŠ” ì‹œìŠ¤í…œ</li>
  <li>ì—¬ê¸°ì„œ ë¡œê·¸ëŠ” ìš°ë¦¬ê°€ ë””ìŠ¤í¬ì— ì €ì¥í•œ ë©”ì„¸ì§€ë¥¼ ì˜ë¯¸</li>
  <li>(ìš°ë¦¬ì˜ ë©”ì„¸ì§€ë¥¼ ë¡œê·¸ë¡œ í‘œí˜„í•˜ë ¤ê³  í•˜ëŠ” ì´ìœ ëŠ” ì•„ë§ˆ ë©”ì„¸ì§€ ì•ˆì— ë³´í†µ ë°ì´í„° ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ë¥¸ ë©”íƒ€ë°ì´í„°ë„ ë“¤ì–´ ìˆì–´ì„œ?)</li>
</ul>

<p>ì¹´í”„ì¹´ì˜ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.</p>

<p><img src="/images/kafka_80.png" alt="" /></p>

<ul>
  <li>Topic: namespaceì²˜ëŸ¼ ë…¼ë¦¬ì ìœ¼ë¡œ êµ¬ë¶„í•˜ëŠ” ê¸°ì¤€. ë°ì´í„°ë¥¼ êµ¬ë¶„í•˜ëŠ” ê°€ì¥ í° êµ¬ë¶„ ê¸°ì¤€</li>
  <li>Partition: ì‹¤ì œë¡œ ì»¨ìŠˆë¨¸ê°€ ë‹´ë‹¹í•˜ëŠ” ì‘ì—… ë‹¨ìœ„(ì»¨ìŠˆë¨¸ ê·¸ë£¹ë‚´ì—ì„œ íŒŒí‹°ì…˜ì€ í•˜ë‚˜ì˜ ì»¨ìŠˆë¨¸ì—ê²Œë§Œ í• ë‹¹ ê°€ëŠ¥). í´ë”ë¡œ êµ¬ë¶„</li>
  <li>Segment: ì—¬ëŸ¬ ë©”ì„¸ì§€ë¥¼ ë¬¶ì–´ë†“ì€ í•˜ë‚˜ì˜ íŒŒì¼. íŒŒí‹°ì…˜ í•œ ê°œì— ì—¬ëŸ¬ ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ê°€ ì €ì¥ë˜ì–´ ìˆìŒ.</li>
  <li>Message: ìš°ë¦¬ê°€ ì‹¤ì œë¡œ ë³´ë‚´ëŠ” ë°ì´í„° + ìƒì„±ëœ íƒ€ì„ìŠ¤íƒ¬í”„ + í”„ë¡œë“€ì„œ ID + â€¦ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŒ</li>
</ul>

<h2 id="partition">Partition</h2>

<p>3ê°œì˜ íŒŒí‹°ì…˜ì„ ê°€ì§€ëŠ” í† í”½ì„ ìš°ì„  í•œ ê°œ ë§Œë“¤ì–´ë³´ì.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-topics.sh <span class="nt">--create</span> <span class="nt">--topic</span> freblogg <span class="nt">--partitions</span> 3 <span class="nt">--replication-factor</span> 1 <span class="nt">--zookeeper</span> localhost:2181
</code></pre></div></div>

<p>íŒŒí‹°ì…˜ì´ ì €ì¥ë˜ëŠ” ìœ„ì¹˜ë¡œ ì´ë™í•´ í† í”½ ì´ë¦„ìœ¼ë¡œ ì‹œì‘í•˜ëŠ” íŒŒí‹°ì…˜ì„ ê²€ìƒ‰í•´ë³´ë©´ 3ê°œì˜ í´ë”ê°€ ë³´ì¸ë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ tree freblogg*
freblogg-0
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-1
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-2
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
</code></pre></div></div>

<p>ë‹¤ìŒê³¼ ê°™ì€ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•´ ë¸Œë¡œì»¤ë¡œ ë©”ì„¸ì§€ë¥¼ ë³´ë‚´ë³´ì.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-console-producer.sh --topic freblogg --broker-list localhost:9092
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ ls -lh freblogg*
freblogg-0:
total 20M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpoint

freblogg-1:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpoint

freblogg-2:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint
</code></pre></div></div>

<p>ë‘ ê°œì˜ ë©”ì„¸ì§€ë¥¼ ë³´ëƒˆë‹¤. ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ë©´ ë‘ ê°œì˜ íŒŒí‹°ì…˜ì´ ê°€ì§€ëŠ” 00000000000000000000.log ë¼ëŠ” ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì˜ ìš©ëŸ‰ì´ ì¦ê°€í–ˆë‹¤. íŒŒì¼ì„ ì—´ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì´ ì í˜€ìˆë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cat freblogg-2/*.log
@^@^BÃ‚Â°Ã‚Â£ÃƒÂ¦ÃƒÆ’^@^K^XÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿ÃƒÂ¿^@^@^@^A"^@^@^A^VHello World^@
</code></pre></div></div>

<p>ë¸Œë¡œì»¤ì— ì €ì¥ëœ ë©”ì„¸ì§€ëŠ” ë°”ì´íŠ¸ í˜•íƒœë¡œ ì €ì¥ë˜ê¸° ë•Œë¬¸ì— ì œëŒ€ë¡œ ë””ì½”ë”©í•˜ì§€ ì•Šìœ¼ë©´ ì´ìƒí•˜ê²Œ ì½íŒë‹¤. í•˜ì§€ë§Œ Hello Worldë¼ê³  ì íŒ ê²ƒì„ ë³´ì•„ .logë¼ëŠ” íŒŒì¼ì— ìš°ë¦¬ê°€ ë³´ë‚¸ ë©”ì„¸ì§€ê°€ ì €ì¥ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</p>

<p>ë©”ì„¸ì§€ê°€ íŒŒí‹°ì…˜ì— í•˜ë‚˜ì”© ì €ì¥ëœ ì´ìœ ëŠ” ë¼ìš´ë“œ ë¡œë¹ˆ ë°©ì‹ìœ¼ë¡œ ë©”ì„¸ì§€ë¥¼ íŒŒí‹°ì…˜ì— í• ë‹¹í•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë©”ì„¸ì§€ í• ë‹¹ ë°©ì‹ì€ ì¹´í”„ì¹´ì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ë¥¸ ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆê³ , ë§Œì•½ ë©”ì„¸ì§€ì— í‚¤ë¥¼ ì„¤ì •í•´ì¤¬ë‹¤ë©´ í‚¤ë§ˆë‹¤ íŒŒí‹°ì…˜ì„ ë‹¤ë¥´ê²Œ í• ë‹¹í•˜ë„ë¡ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•  ìˆ˜ë„ ìˆë‹¤.</p>

<p>ì„¸ê·¸ë¨¼íŠ¸ëŠ” ì—¬ëŸ¬ ë©”ì„¸ì§€ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ ì €ì¥í•˜ê³  ìˆê³ , ê°ê°ì˜ ë©”ì„¸ì§€ëŠ” 1ì”© ì¦ê°€í•˜ëŠ” offsetì„ ê°€ì§„ë‹¤. ê° ì„¸ê·¸ë¨¼íŠ¸ëŠ” ìì‹ ì´ ê°€ì§€ê³  ìˆëŠ” ë©”ì„¸ì§€ì˜ ê°€ì¥ ì²˜ìŒ ì˜¤í”„ì…‹ì„ ì´ë¦„ìœ¼ë¡œ í•œë‹¤.</p>

<p><img src="/images/kafka_75.png" alt="" /></p>

<p>ìœ„ì™€ ê°™ì€ ëœë¤í•œ ë¬¸ìì—´ë“¤ì„ ì½ê³  ì‹¶ìœ¼ë©´ Kafka íˆ´ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kafka-run-class.bat kafka.tools.DumpLogSegments <span class="nt">--deep-iteration</span> <span class="nt">--print-data-log</span> <span class="nt">--files</span> logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
</code></pre></div></div>

<p>This gives the output</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Dumping logs<span class="se">\f</span>reblogg-2<span class="se">\0</span>0000000000000000000.log
Starting offset: 0

offset: 0 position: 0 CreateTime: 1533443377944 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 11 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: Hello World

offset: 1 position: 79 CreateTime: 1533462689974 isvalid: <span class="nb">true </span>keysize: <span class="nt">-1</span> valuesize: 6 producerId: <span class="nt">-1</span> headerKeys: <span class="o">[]</span> payload: amazon
</code></pre></div></div>

<p>CreateTimeê³¼ ê°™ì€ ê°’ì€ ì»¨ìŠˆë¨¸ë¡œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê°’ì´ ì•„ë‹ˆë‹¤. ì¹´í”„ì¹´ ë‚´ë¶€ì ìœ¼ë¡œ ê°€ì§€ê³  ìˆëŠ” ë©”íƒ€ë°ì´í„°ì´ë‹¤. ê·¸ë ‡ê¸° ë•Œë¬¸ì— ë°ì´í„°ì˜ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ í•„ìš”í•˜ë‹¤ë©´, ë°ì´í„°ë¥¼ ìƒì„±í•  ë•Œ ë‚´ë¶€ì ìœ¼ë¡œ ë©”ì„¸ì§€ì— ëª…ì‹œì ìœ¼ë¡œ ë‹´ì•„ì„œ ë¸Œë¡œì»¤ì— ë‹´ì•„ì•¼ í•œë‹¤.</p>

<p>You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.</p>

<h2 id="segment">Segment</h2>

<p>ìœ„ì—ì„œ ë´¤ë˜ <code class="language-plaintext highlighter-rouge">.log</code>, <code class="language-plaintext highlighter-rouge">.index</code>, <code class="language-plaintext highlighter-rouge">.timeindex</code>ì„ ëª¨ë‘ ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì´ë¼ê³  í•œë‹¤. ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì„ í•˜ë‚˜ë¡œ í•˜ì§€ ì•Šê³ , ë‚˜ëˆ„ì–´ ì €ì¥í•˜ëŠ” ì´ìœ ëŠ” ì—¬ëŸ¬ê°€ì§€ê°€ ìˆë‹¤.</p>

<p>ê·¸ì¤‘ì—ì„œë„ ë°ì´í„°ë¥¼ ì‚­ì œí•  ë•Œ ì´ì ì´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. KafkaëŠ” êµ¬ì¡°ì  íŠ¹ì„±ìœ¼ë¡œ ë©”ì„¸ì§€ë§ˆë‹¤ ë°ì´í„°ë¥¼ ì‚­ì œí•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ìœ ì¼í•˜ê²Œ ë©”ì„¸ì§€ë¥¼ ì‚­ì œí•˜ëŠ” ë°©ë²•ì€ ë°”ë¡œ ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ê²ƒì´ë‹¤. ë³´í†µ ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ ì‚­ì œëŠ” ì¹´í”„ì¹´ configurationì„ í†µí•´ ì‚­ì œí•˜ëŠ” <strong>Retention policy</strong> ë°©ë²•ì„ ì‚¬ìš©í•œë‹¤. (ì •ì±…ì„ í†µí•´ ì£¼ê¸°ì ìœ¼ë¡œ ì‚­ì œ)</p>

<p>ì„¸ê·¸ë¨¼íŠ¸ íŒŒì¼ì˜ ì˜ë¯¸ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">.index</code> file: This contains the mapping of message offset to its physical position in .log file.</li>
  <li><code class="language-plaintext highlighter-rouge">.log</code> file: This file contains the actual records and maintains the records up to a specific offset. The name of the file depicts the starting offset added to this file.</li>
  <li>.index file: This file has an index that maps a record offset to the byte offset of the record within the** .log **file. This mapping is used to read the record from any specific offset.</li>
  <li><code class="language-plaintext highlighter-rouge">.timeindex</code> file: This file contains the mapping of the timestamp to record offset, which internally maps to the byte offset of the record using the .index file. This helps in accessing the records from the specific timestamp.</li>
  <li><code class="language-plaintext highlighter-rouge">.snapshot</code> file: contains a snapshot of the producer state regarding sequence IDs used to avoid duplicate records. It is used when, after a new leader is elected, the preferred one comes back and needs such a state to become a leader again. This is only available for the active segment (log file)</li>
  <li><code class="language-plaintext highlighter-rouge">.leader-epoch-checkpoint</code>: It refers to the number of leaders previously assigned by the controller. The replicas use the leader epoch as a means of verifying the current leader. The leader-epoch-checkpoint file contains two columns: epochs and offsets. Each row is a checkpoint for the latest recorded leader epoch and the leaderâ€™s latest offset upon becoming leader</li>
</ul>

<p>An index file for the log file Iâ€™ve showed in the â€˜Quick detourâ€™ above would look something like this:</p>

<p><img src="/images/kafka_76.png" alt="" /></p>

<p>If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.</p>

<p><img src="/images/kafka_81.png" alt="" /></p>

<h1 id="ì €ì¥ëœ-ë°ì´í„°ì˜-í¬ë§·kafka-messages-are-just-bytes">ì €ì¥ëœ ë°ì´í„°ì˜ í¬ë§·(Kafka messages are just bytes)</h1>

<p><strong>Kafka messages are just bytes</strong>. Kafka messages are organized into topics. Each message is a key/value, but that is all that Kafka requires. Both key and value are just bytes when they are stored in Kafka. This makes Kafka applicable to a <strong>wide range of use cases</strong>, but it also means that developers have the <strong>responsibility of deciding how to serialize the data.</strong></p>

<p>There are various <strong>serialization formats</strong> with common ones including:</p>

<ul>
  <li>JSON</li>
  <li>Avro</li>
  <li>Protobuf</li>
  <li>String delimited (e.g., CSV</li>
</ul>

<p>There are advantages and disadvantages to each of theseâ€”well, except delimited, in which case itâ€™s only disadvantages ğŸ˜‰</p>

<p>Choosing a serialization format</p>

<ul>
  <li><strong>Schema</strong>: A lot of the time your data will have a schema to it. You may not like the fact, but itâ€™s your responsibility as a developer to preserve and propagate this schema. The schema provides the <strong>contract between your services</strong>. Some message formats (such as Avro and Protobuf) have strong schema support, whilst others have lesser support (JSON) or none at all (delimited string).</li>
  <li><strong>Ecosystem compatibility</strong>: Avro, Protobuf, and JSON are first-class citizens in the Confluent Platform, with native support from the  Confluent Schema Registry, Kafka Connect, ksqlDB, and more.</li>
  <li><strong>Message size</strong>: Whilst JSON is plain text and relies on any compression configured in Kafka itself, Avro and Protobuf are both binary formats and thus provide smaller message sizes.</li>
  <li><strong>Language support</strong>: For example, support for Avro is strong in the Java space, whilst if youâ€™re using Go, chances are youâ€™ll be expecting to use Protobuf.</li>
</ul>

<p>ë°ì´í„°ë¥¼ ë¸Œë¡œì»¤ì— ì €ì¥í•  ë•ŒëŠ” ì „ì†¡ëœ ë°ì´í„°ì˜ í¬ë§·ê³¼ëŠ” ìƒê´€ì—†ì´ ì›í•˜ëŠ” í¬ë§·ìœ¼ë¡œ ë¸Œë¡œì»¤ì— ì €ì¥í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í”„ë¡œë“€ì„œê°€ JSONìœ¼ë¡œ ë³´ëƒˆë‹¤ê³  í•˜ë”ë¼ë„ ë¸Œë¡œì»¤ì— ì €ì¥í•  ë•Œ í¬ë§·ì€ Avro, Parquet, String ë­˜ í•˜ë“  ìƒê´€ì—†ë‹¤. ë‹¤ë§Œ ì¤‘ìš”í•œ ê²ƒì€ Serializerë¡œ Avroë¥¼ ì„ íƒí–ˆë‹¤ë©´, Deserializerë„ ë°˜ë“œì‹œ Avroë¥¼ ì„ íƒí•´ì•¼ í•œë‹¤. ê·¸ëŸ¬ê³  ë‚˜ë©´ ì»¨ìŠˆë¨¸ì—ì„œ ì „ë‹¬ ë°›ëŠ” ë°ì´í„°ì˜ í¬ë§·ì€ ìì—°ìŠ¤ëŸ½ê²Œ ë‹¤ì‹œ JSON í˜•íƒœë¥¼ ì–»ê²Œ ëœë‹¤.</p>

<p><img src="/images/kafka_78.png" alt="" /></p>

<p>Remember, Kafka messages are just pairs of key/value bytes, and you need to specify the converter for both keys and value, using the <code class="language-plaintext highlighter-rouge">key.converter</code> and <code class="language-plaintext highlighter-rouge">value.converter</code> configuration setting. In some situations, you may use different converters for the key and the value.</p>

<p>Hereâ€™s an example of using the String converter. Since itâ€™s just a string, thereâ€™s no schema to the data, and thus itâ€™s not so useful to use for the value:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"key.converter"</span>: <span class="s2">"org.apache.kafka.connect.storage.StringConverter"</span>,
</code></pre></div></div>

<p>Some converters have additional configuration. For Avro, you need to specify the Schema Registry. For JSON, you need to specify if you want Kafka Connect to embed the schema in the JSON itself. When you specify converter-specific configurations, always use the <code class="language-plaintext highlighter-rouge">key.converter</code>. or <code class="language-plaintext highlighter-rouge">value.converter</code>. prefix. For example, to use Avro for the message payload, youâ€™d specify the following:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"value.converter": "io.confluent.connect.avro.AvroConverter",
"value.converter.schema.registry.url": "http://schema-registry:8081",
</code></pre></div></div>

<p>Common converters include:</p>

<div class="language-sh highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Avro</span>
io.confluent.connect.avro.AvroConverter

<span class="c"># Protobuf</span>
io.confluent.connect.protobuf.ProtobufConverter

<span class="c"># String</span>
org.apache.kafka.connect.storage.StringConverter

<span class="c"># JSON</span>
org.apache.kafka.connect.json.JsonConverter

<span class="c"># JSON schema</span>
io.confluent.connect.json.JsonSchemaConverter

<span class="c"># ByteArray</span>
org.apache.kafka.connect.converters.ByteArrayConverter
</code></pre></div></div>

<p>JSONì˜ ê²½ìš° ìŠ¤í‚¤ë§ˆê°€ ì„¤ì •ì„ ì•ˆí•˜ëŠ” ê²ƒì´ ë””í´íŠ¸ë‹¤. í•˜ì§€ë§Œ ìŠ¤í‚¤ë§ˆë¥¼ ê³ ì •í•˜ê³  ì‹¶ì€ ê²½ìš° ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.</p>

<ol>
  <li>JSON schema <code class="language-plaintext highlighter-rouge">io.confluent.connect.json.JsonSchemaConverter</code>ë¥¼ ì“´ë‹¤ (with ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬)
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"value.converter": "io.confluent.connect.json.JsonSchemaConverter",
"value.converter.schema.registry.url": "http://schema-registry:8081",
</code></pre></div>    </div>
  </li>
  <li>ë¹„íš¨ìœ¨ì ì´ì§€ë§Œ ë§¤ë²ˆ ë©”ì‹œì§€ì— ìŠ¤í‚¤ë§ˆë¥¼ ë‹´ì•„ì„œ ì „ì†¡/ì €ì¥í•œë‹¤.
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=true
</code></pre></div>    </div>
  </li>
</ol>

<p>2ë²ˆ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ ë©”ì„¸ì§€ê°€ ë‹¤ìŒê³¼ ê°™ì´ schema ë¶€ë¶„ê³¼, payload ë¶€ë¶„ì´ í•¨ê»˜ ì €ì¥ëœë‹¤.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "int64",
        "optional": false,
        "field": "registertime"
      },
      {
        "type": "string",
        "optional": false,
        "field": "userid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "regionid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "gender"
      }
    ],
    "optional": false,
    "name": "ksql.users"
  },
  "payload": "Hello World"
}
</code></pre></div></div>

<p>ì´ë ‡ê²Œ í•˜ë©´ ë©”ì„¸ì§€ ì‚¬ì´ì¦ˆê°€ ì»¤ì§€ê¸° ë•Œë¬¸ì— ë¹„íš¨ìœ¨ì ì´ë‹¤. ê·¸ë˜ì„œ ìŠ¤í‚¤ë§ˆê°€ í•„ìš”í•œ ê²½ìš°ì—ëŠ” ìŠ¤í‚¤ë§ˆ ë ˆì§€ìŠ¤íŠ¸ë¦¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ íš¨ìœ¨ì ì´ë‹¤.</p>

<p>ë§Œì•½ ì»¨ë²„í„°ì— JSON serializerë¥¼ ì‚¬ìš©í–ˆê³  ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¡œ ì„¤ì •í•˜ì§€ ì•Šì„ê±°ë¼ë©´,</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
</code></pre></div></div>

<p>ì´ë ‡ê²Œ schemaë¥¼ ì°¾ì„ í•„ìš” ì—†ë‹¤ê³  ëª…ì‹œí•´ì£¼ì. (ë””í´íŠ¸ê°€ falseì¸ë° ì™œ í•´ì¤˜ì•¼í•˜ëŠ”ê±°ì§€..?)</p>

<p>ì•„ë˜ í‘œëŠ” serializerì™€ deserializerì˜ ì‹±í¬ë¥¼ ì–´ë–»ê²Œ ë§ì¶°ì•¼ ì—ëŸ¬ê°€ ì•ˆë‚˜ëŠ”ì§€ ì•Œë ¤ì¤€ë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ serializerëŠ” ë©”ì„¸ì§€ë‚˜ ìƒí™©ì— ë§ê²Œ ì›í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ê³ , deserializerëŠ” serializerì™€ ê°™ì€ í¬ë§·ì„ ì‚¬ìš©í•˜ë„ë¡ í•˜ë©´ ëœë‹¤.</p>

<p><img src="/images/kafka_79.png" alt="" /></p>

<h1 id="ì„±ëŠ¥-í–¥ìƒì„-ìœ„í•œ-íŒŒí‹°ì…˜-ìˆ˜">ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ íŒŒí‹°ì…˜ ìˆ˜</h1>

<p>To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.</p>

<p>íŒŒí‹°ì…˜ ë‚´ì—ì„œëŠ” ë©”ì„¸ì§€ì˜ ìˆœì„œê°€ ì§€ì¼œì§„ë‹¤. ê·¸ë˜ì„œ í† í”½ì„ ì´ë£¨ëŠ” íŒŒí‹°ì…˜ì´ 1ê°œë¼ë©´ ë©”ì„¸ì§€ì˜ ìˆœì„œë¥¼ ê±±ì •í•  í•„ìš”ê°€ ì—†ë‹¤. í•˜ì§€ë§Œ íŒŒí‹°ì…˜ì˜ ê°œìˆ˜ë¥¼ 2ê°œ ì´ìƒìœ¼ë¡œ í•˜ë©´ ë©”ì„¸ì§€ì˜ ìˆœì„œê°€ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤.</p>

<p>ë³‘ë ¬ ì²˜ë¦¬ë¥¼ í†µí•´ ì„±ëŠ¥ì„ ë†’ì´ê³ ì í•  ë•Œ, íŒŒí‹°ì…˜ì˜ ê°œìˆ˜ì™€ ì»¨ìŠˆë¨¸ì˜ ê°œìˆ˜ë¥¼ ëŠ˜ë ¤ì¤€ë‹¤.</p>

<ul>
  <li>íŒŒí‹°ì…˜ì˜ ìˆ˜ &gt;= ì»¨ìŠˆë¨¸ ìˆ˜</li>
  <li>ë³‘ë ¬ ì •ë„ = MIN(íŒŒí‹°ì…˜ì˜ ìˆ˜, ì»¨ìŠˆë¨¸ ìˆ˜)</li>
  <li>íŒŒí‹°ì…˜ì˜ ê°œìˆ˜ëŠ” ëŠ˜ë¦´ìˆ˜ë§Œ ìˆê³  ì¤„ì¼ ìˆ˜ëŠ” ì—†ìŒ</li>
</ul>

<h1 id="ì¥ì• -ë³µêµ¬ë¥¼-ìœ„í•œ-ë³µì œ">ì¥ì•  ë³µêµ¬ë¥¼ ìœ„í•œ ë³µì œ</h1>
<p>ë³µì œëŠ” íŠ¹ì • ë¸Œë¡œì»¤ ì„œë²„ì— ì¥ì• ê°€ ë‚¬ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ê¸° ìœ„í•œ ìš©ë„ë‹¤. ë§Œì•½ ë¸Œë¡œì»¤ê°€ 1ëŒ€ë¼ë©´ ë³µì œëŠ” ì•„ë¬´ ì˜ë¯¸ê°€ ì—†ë‹¤. ë³µì œëŠ” ë¸Œë¡œì»¤ì˜ ê°œìˆ˜ë§Œí¼ ì„¤ì •í•˜ë©´ ëœë‹¤. ë” í¬ê²Œ ë” ì ê²Œ í•´ë„ ë˜ì§€ë§Œ, ê°™ê²Œ í•˜ëŠ” ê²ƒì´ ì œì¼ í•©ë‹¹í•œ ì„ íƒì´ë‹¤.</p>

<p>ë³µì œìˆ˜ëŠ” í† í”½ë§ˆë‹¤ ë‹¤ë¥´ê²Œ ì„¤ì •í•  ìˆ˜ ìˆë‹¤. ë³µì œ ìˆ˜ëŠ” ëŠ˜ë¦¬ëŠ” ë§Œí¼ ì„±ëŠ¥ì´ ì•½ê°„ ë–¨ì–´ì§„ë‹¤. ê·¸ë˜ì„œ í† í”½ì˜ ì¤‘ìš”ë„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤.</p>

<p>ë³µì œì— ê´€í•´ ì´í•´í•˜ë ¤ë©´ ë¦¬ë”/íŒ”ë¡œì›Œ, ì»¤ë°‹ê³¼ ê°™ì€ ê²ƒë“¤ì„ ë°°ì›Œì•¼ í•œë‹¤. ì»¨ìŠˆë¨¸ëŠ” ë¦¬ë” íŒŒí‹°ì…˜ë§Œ ê°€ì ¸ê°ˆ ìˆ˜ ìˆë‹¤. ë³µì œëŠ” ë¦¬ë”ê°€ ì¥ì• ê°€ ë‚¬ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„í•˜ê¸° ìœ„í•œ ìš©ë„ë‹¤.</p>

<p>Say for the freblogg topic that weâ€™ve been using so far, weâ€™ve given the replication factor as 2. The resulting distribution of its three partitions will look something like this.</p>

<p><img src="/images/kafka_77.png" alt="" /></p>

<p>Even when you have a replicated partition on a different broker, Kafka wouldnâ€™t let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition.</p>

<p>A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.</p>

<h1 id="ë¡œê·¸-ì„¤ì •ì„-í†µí•´-íš¨ìœ¨ì ìœ¼ë¡œ-ë³´ê´€í•˜ê¸°log-retention">ë¡œê·¸ ì„¤ì •ì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ ë³´ê´€í•˜ê¸°(Log Retention)</h1>

<p>Apache Kafka is a commit-log system. The records are appended at the end of each Partition, and each Partition is also split into segments. Segments help delete older records through Compaction, improve performance, and much more.</p>

<p>Kafka allows us to optimize the log-related configurations, we can control the rolling of segments, log retention, etc. <strong>These configurations determine how long the record will be stored</strong> and weâ€™ll see how it impacts the brokerâ€™s performance, especially when the cleanup policy is set to Delete.</p>

<p>For better performance and maintainability, multiple segments get created, and rather than reading from one huge Partition, Consumers can now read faster from a smaller segment file. A directory with the partition name gets created and maintains all the segments for that partition as various files.</p>

<p><img src="/images/kafka_81.png" alt="" /></p>

<p>The active segment is the only file available for reading and writing while consumers can use other log segments (non-active) to read data. When the active segment becomes full (configured by <code class="language-plaintext highlighter-rouge">log.segment.bytes</code>, default 1 GB) or the configured time (<code class="language-plaintext highlighter-rouge">log.roll.hours</code> or <code class="language-plaintext highlighter-rouge">log.roll.ms</code>, default 7 days) passes, the segment gets rolled. This means that the <strong>active segment gets closed and re-opens with read-only mode and a new segment file</strong> (active segment) will be created in read-write mode.</p>

<h2 id="role-of-indexing-within-the-partition">Role of Indexing within the Partition</h2>
<p>Indexing helps consumers to read data starting from any specific offset or using any time range. As mentioned previously, the <code class="language-plaintext highlighter-rouge">.index</code> file contains an index that maps the logical offset to the byte offset of the record within the <code class="language-plaintext highlighter-rouge">.log</code> file. <strong>You might expect that this mapping is available for each record, but it doesnâ€™t work this way.</strong></p>

<p><strong>How these entries are added inside the index file is defined by the <code class="language-plaintext highlighter-rouge">log.index.interval.bytes</code> parameter, which is 4096 bytes by default.</strong> This means that after every 4096 bytes added to the <code class="language-plaintext highlighter-rouge">.log</code> file, an entry gets added to the <code class="language-plaintext highlighter-rouge">.index</code> file. Suppose the producer is sending records of 100 bytes each to a Kafka topic. In this case, a new index entry will be added to the <code class="language-plaintext highlighter-rouge">.index</code> file after every 41 records (41*100 = 4100 bytes) appended to the log file.</p>

<p>(ëª¨ë“  ë ˆì½”ë“œê°€ ì¸ë±ì‹±ë˜ê¸°ëŠ” í•˜ëŠ”ë°, ë ˆì½”ë“œ í•œ ê°œ ë„£ì„ë•Œë§ˆë‹¤ ì¸ë±ì‹±ë˜ëŠ” ê²ƒì€ ì•„ë‹ˆê³  <code class="language-plaintext highlighter-rouge">.log</code> íŒŒì¼ í•˜ë‚˜ê°€ ë‹¤ ì°¨ê³ ë‚˜ë©´ í•´ë‹¹ <code class="language-plaintext highlighter-rouge">.log</code> íŒŒì¼ì˜ ë ˆì½”ë“œë¥¼ ì¸ë±ì‹±í•´ì„œ <code class="language-plaintext highlighter-rouge">.index</code> íŒŒì¼ì„ ë§Œë“ ë‹¤)</p>

<p><img src="/images/kafka_82.png" alt="" /></p>

<p>If a consumer wants to read starting at a specific offset, a search for the record is made as follows:</p>

<ul>
  <li>Search for the <code class="language-plaintext highlighter-rouge">.index</code> file based on its name. For e.g. If the offset is 1191, the index file will be searched whose name has a value less than 1191. The naming convention for the index file is the same as that of the log file</li>
  <li>Search for an entry in the <code class="language-plaintext highlighter-rouge">.index</code> file where the requested offset falls.</li>
  <li>Use the mapped byte offset to access the <code class="language-plaintext highlighter-rouge">.log</code> file and start consuming the records from that byte offset.</li>
</ul>

<p>As we mentioned, consumers may also want to read the records from a specific timestamp. This is where the <code class="language-plaintext highlighter-rouge">.timeindex</code> file comes into the picture. It maintains a timestamp and offset mapping (which maps to the corresponding entry in the <code class="language-plaintext highlighter-rouge">.index</code> file), which maps to the actual byte offset in the <code class="language-plaintext highlighter-rouge">.log</code> file. (íŠ¹ì • íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë ˆì½”ë“œ ì½ëŠ” ë°©ë²•: <code class="language-plaintext highlighter-rouge">.timeindex</code> -&gt; <code class="language-plaintext highlighter-rouge">.index</code> -&gt; <code class="language-plaintext highlighter-rouge">.log</code>)</p>

<p><img src="/images/kafka_83.png" alt="" /></p>

<h2 id="rolling-segments">Rolling segments</h2>
<p>As discussed in the above sections, the active segment gets rolled once any of these conditions are met-</p>

<ol>
  <li>Maximum segment size - configured by <code class="language-plaintext highlighter-rouge">log.segment.bytes</code>, defaults to 1 Gb</li>
  <li>Rolling segment time - configured by <code class="language-plaintext highlighter-rouge">log.roll.ms</code> or <code class="language-plaintext highlighter-rouge">log.roll.hours</code>, defaults to 7 days</li>
  <li>Index/timeindex is full - The index and timeindex share the same maximum size, which is defined by the <code class="language-plaintext highlighter-rouge">log.index.size.max.bytes</code>, defaults to 10 MB</li>
</ol>

<p>(ë³´í†µ 1ë²ˆ í¬ê¸°ë¥¼ ëŠ˜ë¦¬ë©´, 3ë²ˆ í¬ê¸°ë„ ëŠ˜ë ¤ì•¼ í•œë‹¤)</p>

<h2 id="impact-of-increasingdecreasing-the-segment-size">Impact of increasing/decreasing the segment size</h2>
<p>Generally you donâ€™t want to increase/decrease the <code class="language-plaintext highlighter-rouge">log.segment.bytes</code> and keep it as default. But letâ€™s discuss the impact of changing this value so that you can make an informed decision if thereâ€™s a need.</p>

<h2 id="log-retention---the-records-may-persist-longer-than-the-retention-time">Log retention - The records may persist longer than the retention time</h2>
<p>Kafka, with its feature of retaining the log for a longer duration rather than deleting it like traditional messaging queues once consumed, provides many added advantages. Multiple consumers can read the same data, apart from reading the data it can also be sent to data warehouses for further analytics.</p>

<p>How long is the data retained in Kafka? This is configurable using the maximum number of bytes to retain by using the <code class="language-plaintext highlighter-rouge">log.retention.bytes</code> parameter. If you want to set a retention period, you can use the <code class="language-plaintext highlighter-rouge">log.retention.ms</code>, <code class="language-plaintext highlighter-rouge">log.retention.minutes</code>, or <code class="language-plaintext highlighter-rouge">log.retention.hours</code> (7 days by default) parameters.</p>

<p>Suppose you configure the topic by specifying a retention time of 600000 ms (10 mins) and a segment size of 16384 bytes, the expectation would be to roll the segment once its size reaches 16 Kb but this is the max size if the record to be inserted is of more size than available in the active segment, the segment will be rolled and the record will get saved in the new segment.</p>

<p>Regarding the log retention, the expectation would be that the record will be persisted for 10 mins and after that, it should get deleted. A segment, together with the records it contains, can be deleted only when it is closed. So the following things may impact when the records get deleted-</p>

<ul>
  <li>If the producer is slow and the maximum size of 16 Kb is not reached within 10 minutes, older records wonâ€™t be deleted. In this case, the log retention would be higher than 10 mins.</li>
  <li>If the active segment is filled quickly, it will be closed but only get deleted once the last inserted record persists for 10 mins. So in this case as well, the latest inserted record would be persisted for more than 10 mins. - Suppose the segment is getting filled in 7 mins and getting closed, the last inserted record will stay for 10 mins so the actual retention time for the first record inserted into the segment would be 17 mins.</li>
  <li>The log can be persisted for an even longer duration than the last added record in the segment. How? Because the thread which gets executed and checks which log segments need to be deleted runs every 5 mins. This is configurable using log.retention.check.interval.ms configurations. - Depending on the last added record to the segment, this cleanup thread can miss the 10 min retention deadline. So in our example above instead of persisting the segment for 17 mins, it could be persisted for 22 mins.</li>
  <li>Do you think that this would be the maximum time the record is persisted in Kafka? No, the cleaner thread checks and just marks the segment to be deleted. The log.segment.delete.delay.ms broker parameter defines when the file will actually be removed from the file system when itâ€™s marked as â€œdeletedâ€ (default, 1 min) - Going back to our example the log is still available even after 23 mins, which is way longer than the retention time of 10 mins.</li>
</ul>

<p>So The usual retention limits are set by using log.retention.ms defines a kind of minimum time the record will be persisted in the file system.</p>

<p>Consumers get records from closed segments but not from deleted ones, even if they are just marked as â€œdeletedâ€ but not actually removed from the file system.</p>

<p>Note: I have described a single record getting appended to the segment for simplicity and to let you understand the concept clearly but in actuality multiple records (record batch) get appended to the segment file.</p>

<h2 id="conclusion">Conclusion</h2>
<p>As discussed in this blog, configuration parameters can have a surprisingly big influence on how long your data is retained. Understanding these parameters and how you can adjust them gives you a lot more control over how you handle your data. Letâ€™s summarize what parameters we have discussed here-</p>

<p><img src="/images/kafka_84.png" alt="" /></p>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>
<ul>
  <li><a href="https://docs.cloudera.com/csa/1.2.0/flink-sql-table-api/topics/csa-kafka-sql-datatypes.html" target="_blank">Data types for Kafka connector</a></li>
  <li><a href="https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/" target="_blank">Kafka Connect Deep Dive â€“ Converters and Serialization Explained</a></li>
  <li><a href="https://dol9.tistory.com/274" target="_blank">dol9, Kafka ìŠ¤í‚¤ë§ˆ ê´€ë¦¬, Schema Registry</a></li>
  <li><a href="https://www.freblogg.com/kafka-storage-internals" target="_blank">A Practical Introduction to Kafka Storage Internals</a></li>
  <li><a href="https://www.freecodecamp.org/news/what-makes-apache-kafka-so-fast-a8d4f94ab145/" target="_blank">Hereâ€™s what makes Apache Kafka so fast</a></li>
  <li><a href="https://stackoverflow.com/questions/40369238/which-directory-does-apache-kafka-store-the-data-in-broker-nodes#" target="_blank">stackoverflow: Which directory does apache kafka store the data in broker nodes</a></li>
  <li><a href="https://medium.com/@abhisheksharma_59226/how-kafka-stores-data-37ee611c89a2" target="_blank">Abhishek Sharma, How kafka stores data</a></li>
  <li><a href="https://rohithsankepally.github.io/Kafka-Storage-Internals/" target="_blank">Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals</a></li>
  <li><a href="https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7" target="_blank">towardsdatascience, Log Compacted Topics in Apache Kafka</a></li>
  <li><a href="https://www.conduktor.io/understanding-kafkas-internal-storage-and-log-retention" target="_blank">conduktor, Understanding Kafkaâ€™s Internal Storage and Log Retention</a></li>
  <li><a href="https://dev.to/heroku/what-is-a-commit-log-and-why-should-you-care-pib" target="_blank">What is a commit log and why should you care?</a></li>
</ul>
:ET
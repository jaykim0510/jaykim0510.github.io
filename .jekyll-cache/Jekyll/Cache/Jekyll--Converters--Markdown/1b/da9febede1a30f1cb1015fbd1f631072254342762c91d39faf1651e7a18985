I"><hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#배치-경사-하강법" id="markdown-toc-배치-경사-하강법">배치 경사 하강법</a>    <ul>
      <li><a href="#1-확률적-경사-하강법stochastic-gradient-descent" id="markdown-toc-1-확률적-경사-하강법stochastic-gradient-descent">1. 확률적 경사 하강법(Stochastic Gradient Descent)</a></li>
      <li><a href="#2-배치-경사-하강법batch-gradient-descent" id="markdown-toc-2-배치-경사-하강법batch-gradient-descent">2. 배치 경사 하강법(Batch Gradient Descent)</a>        <ul>
          <li><a href="#배치-경사-하강법-수식-과정" id="markdown-toc-배치-경사-하강법-수식-과정">배치 경사 하강법 수식 과정</a>            <ul>
              <li><a href="#1-데이터를-batch-sizeeg-64-128만큼-forward-propagation시킨다" id="markdown-toc-1-데이터를-batch-sizeeg-64-128만큼-forward-propagation시킨다">(1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다</a></li>
              <li><a href="#2-error를-구한다" id="markdown-toc-2-error를-구한다">(2) Error를 구한다</a></li>
              <li><a href="#3-각-특성노드의-가중치를-업데이트-하기-위한-평균-그래디언트를-구한다" id="markdown-toc-3-각-특성노드의-가중치를-업데이트-하기-위한-평균-그래디언트를-구한다">(3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다</a></li>
              <li><a href="#4-가중치를-업데이트-한다" id="markdown-toc-4-가중치를-업데이트-한다">(4) 가중치를 업데이트 한다</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<hr />

<h1 id="배치-경사-하강법">배치 경사 하강법</h1>

<h2 id="1-확률적-경사-하강법stochastic-gradient-descent">1. 확률적 경사 하강법(Stochastic Gradient Descent)</h2>
<p>확률적 경사 하강법은 데이터 세트에서 무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산합니다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용합니다. 그렇기 때문에 굉장히 빠른 속도로 가중치를 업데이트 할 수 있게 됩니다. 하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌이 납니다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것입니다. 그래서 느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법입니다.</p>

<h2 id="2-배치-경사-하강법batch-gradient-descent">2. 배치 경사 하강법(Batch Gradient Descent)</h2>

<p>배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 64, 128개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용합니다. 다시 말해 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 64개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 겁니다.<br />
또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정을 겪지 않았습니다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능합니다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어입니다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 됩니다.</p>

<p>확률적 경사 하강법과 배치 경사 하강법  <br />
<img src="/images/sgd_bgd.png" alt="" width="100%" /></p>

<h3 id="배치-경사-하강법-수식-과정">배치 경사 하강법 수식 과정</h3>

<h4 id="1-데이터를-batch-sizeeg-64-128만큼-forward-propagation시킨다">(1) 데이터를 Batch size(e.g. 64, 128)만큼 Forward propagation시킨다</h4>
<p>Forward propagation은 앞에서 했던 데이터와 가중치를 곱하고 합하는 과정들을 일컫는 말입니다.</p>

<p><img src="/images/batch_1.png" alt="" width="100%" /></p>

<p><img src="/images/batch_2.png" alt="" width="100%" /></p>

<h4 id="2-error를-구한다">(2) Error를 구한다</h4>

<p><img src="/images/batch_3.png" alt="" width="50%" /></p>

<h4 id="3-각-특성노드의-가중치를-업데이트-하기-위한-평균-그래디언트를-구한다">(3) 각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다</h4>

<p><img src="/images/batch_4.png" alt="" width="100%" /></p>

<p><img src="/images/batch_5.png" alt="" width="50%" /></p>

<h4 id="4-가중치를-업데이트-한다">(4) 가중치를 업데이트 한다</h4>

<p><img src="/images/batch_6.png" alt="" width="50%" /></p>
:ET
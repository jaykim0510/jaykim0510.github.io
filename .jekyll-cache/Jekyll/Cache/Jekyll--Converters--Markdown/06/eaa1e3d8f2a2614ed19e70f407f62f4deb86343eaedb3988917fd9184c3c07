I"h<<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#what-is-pyspark" id="markdown-toc-what-is-pyspark">What is PySpark</a></li>
  <li><a href="#pyspark-installation" id="markdown-toc-pyspark-installation">PySpark Installation</a></li>
  <li><a href="#sparksession" id="markdown-toc-sparksession">SparkSession</a>    <ul>
      <li><a href="#create-sparksession" id="markdown-toc-create-sparksession">Create SparkSession</a></li>
      <li><a href="#create-another-sparksession" id="markdown-toc-create-another-sparksession">Create Another SparkSession</a></li>
      <li><a href="#using-spark-config" id="markdown-toc-using-spark-config">Using Spark Config</a></li>
      <li><a href="#create-pyspark-dataframe" id="markdown-toc-create-pyspark-dataframe">Create PySpark DataFrame</a></li>
      <li><a href="#working-with-spark-sql" id="markdown-toc-working-with-spark-sql">Working with Spark SQL</a></li>
      <li><a href="#sparksession-commonly-used-methods" id="markdown-toc-sparksession-commonly-used-methods">SparkSession Commonly Used Methods</a></li>
      <li><a href="#sparkcontext" id="markdown-toc-sparkcontext">SparkContext</a></li>
    </ul>
  </li>
  <li><a href="#참고" id="markdown-toc-참고">참고</a></li>
</ul>

<hr />

<h1 id="what-is-pyspark">What is PySpark</h1>

<p>PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities, using PySpark we can run applications parallelly on the distributed cluster (multiple nodes).</p>

<p>In other words, PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine for large scale powerful distributed data processing and machine learning applications.</p>

<p>Spark basically written in Scala and later on due to its industry adaptation it’s API PySpark released for Python using Py4J. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interface with JVM objects, hence to run PySpark you also need Java to be installed along with Python, and Apache Spark.</p>

<h1 id="pyspark-installation">PySpark Installation</h1>

<ul>
  <li>JVM을 포함하는 패키지 (JRE 또는 JDK)</li>
  <li>Python 설치</li>
  <li>Spark 설치</li>
  <li>PySpark 라이브러리 설치 (Spark 버전과 일치시키는 것이 좋다 아니면 <code class="language-plaintext highlighter-rouge">Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist</code>에러 발생할 수 있음)</li>
</ul>

<h1 id="sparksession">SparkSession</h1>

<ul>
  <li>Spark 2.0 이후로 SparkSession이 RDD와 DataFrame을 다루기 위한 엔트리 포인트가 되었다</li>
  <li>(이전에는 SparkContext)</li>
  <li><code class="language-plaintext highlighter-rouge">from pyspark.sql import SparkSession</code></li>
  <li><code class="language-plaintext highlighter-rouge">SparkSession.builder</code> 를 통해 <code class="language-plaintext highlighter-rouge">SparkSession</code> 객체를 만든다</li>
  <li>SparkSession internally creates SparkConfig and SparkContext with the configuration provided with SparkSession</li>
  <li>내가 사용할 테이블 수만큼 많은 SparkSession 객체를 하나의 애플리케이션 내에서 생성할 수 있다</li>
  <li>(<code class="language-plaintext highlighter-rouge">SparkSession.builder()</code> or <code class="language-plaintext highlighter-rouge">SparkSession.newSession()</code>를 이용해서)</li>
  <li>(<code class="language-plaintext highlighter-rouge">SparkContext</code>는 애플리케이션당 1개. <code class="language-plaintext highlighter-rouge">SparkSession</code> 많이 생성해도 S<code class="language-plaintext highlighter-rouge">parkContext</code>는 공유)</li>
</ul>

<h2 id="create-sparksession">Create SparkSession</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"local[1]"</span><span class="p">)</span> \ 
                    <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">'SparkByExamples.com'</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">master()</code>
    <ul>
      <li>Sets the Spark master URL to connect to, such as <code class="language-plaintext highlighter-rouge">local</code> to run locally, <code class="language-plaintext highlighter-rouge">local[4]</code> to run locally with 4 cores, or <code class="language-plaintext highlighter-rouge">spark://master:7077</code> to run on a Spark standalone cluster</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">appName()</code>
    <ul>
      <li>Sets a name for the application, which will be shown in the Spark web UI</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">getOrCreate()</code>
    <ul>
      <li>Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder</li>
    </ul>
  </li>
</ul>

<h2 id="create-another-sparksession">Create Another SparkSession</h2>

<ul>
  <li>이미 존재하고 있는 master, appName, SparkContext 공유</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Create new SparkSession
</span><span class="n">spark2</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">newSession</span>

</code></pre></div></div>

<h2 id="using-spark-config">Using Spark Config</h2>

<ul>
  <li>set some configs to SparkSession
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1"># Usage of config()
</span>  <span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
      <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"local[1]"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">"SparkByExamples.com"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.some.config.option"</span><span class="p">,</span> <span class="s">"config-value"</span><span class="p">)</span> \
      <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

</code></pre></div>    </div>
  </li>
  <li>런타임 도중에 설정값을 바꾸거나 읽어올 수 있다
    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Set Config
</span>  <span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"spark.executor.memory"</span><span class="p">,</span> <span class="s">"5g"</span><span class="p">)</span>

  <span class="c1"># Get a Spark Config
</span>  <span class="n">partitions</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">conf</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="s">"spark.sql.shuffle.partitions"</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">partitions</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="create-pyspark-dataframe">Create PySpark DataFrame</h2>

<ul>
  <li>SparkSession also provides several methods to create a Spark DataFrame and DataSet</li>
  <li>리스트 또는 pandas의 DataFrame을 인자로 받을 수 있다</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create DataFrame
</span><span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">createDataFrame</span><span class="p">(</span>
    <span class="p">[(</span><span class="s">"Scala"</span><span class="p">,</span> <span class="mi">25000</span><span class="p">),</span> <span class="p">(</span><span class="s">"Spark"</span><span class="p">,</span> <span class="mi">35000</span><span class="p">),</span> <span class="p">(</span><span class="s">"PHP"</span><span class="p">,</span> <span class="mi">21000</span><span class="p">)])</span>
<span class="n">df</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Output
#+-----+-----+
#|   _1|   _2|
#+-----+-----+
#|Scala|25000|
#|Spark|35000|
#|  PHP|21000|
#+-----+-----+
</span>
</code></pre></div></div>

<h2 id="working-with-spark-sql">Working with Spark SQL</h2>

<ul>
  <li>Using <code class="language-plaintext highlighter-rouge">SparkSession</code> you can access PySpark/Spark SQL capabilities in PySpark. In order to use SQL features first, you need to create a temporary view in PySpark.</li>
  <li>Once you have a temporary view you can run any ANSI SQL queries using <code class="language-plaintext highlighter-rouge">spark.sql()</code> method.</li>
  <li>PySpark SQL temporary views are session-scoped and will not be available if the session that creates it terminates. If you want to have a temporary view that is shared among all sessions and keep alive until the Spark application terminates, you can create a global temporary view using <code class="language-plaintext highlighter-rouge">createGlobalTempView()</code></li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Spark SQL
</span><span class="n">df</span><span class="p">.</span><span class="n">createOrReplaceTempView</span><span class="p">(</span><span class="s">"sample_table"</span><span class="p">)</span>
<span class="n">df2</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sql</span><span class="p">(</span><span class="s">"SELECT _1,_2 FROM sample_table"</span><span class="p">)</span>
<span class="n">df2</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="sparksession-commonly-used-methods">SparkSession Commonly Used Methods</h2>

<ul>
  <li>
    <p><code class="language-plaintext highlighter-rouge">version()</code>: Returns the Spark version where your application is running, probably the Spark version your cluster is configured with.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">createDataFrame()</code>: This creates a DataFrame from a collection and an RDD</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">getActiveSession()</code>: returns an active Spark session.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">read()</code>: Returns an instance of DataFrameReader class, this is used to read records from csv, parquet, avro, and more file formats into DataFrame.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">readStream()</code>: Returns an instance of DataStreamReader class, this is used to read streaming data. that can be used to read streaming data into DataFrame.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sparkContext()</code>: Returns a SparkContext.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sql()</code>: Returns a DataFrame after executing the SQL mentioned.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">sqlContext()</code>: Returns SQLContext.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">stop()</code>: Stop the current SparkContext.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">table()</code>: Returns a DataFrame of a table or view.</p>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">udf()</code>: Creates a PySpark UDF to use it on DataFrame, Dataset, and SQL.</p>
  </li>
</ul>

<h2 id="sparkcontext">SparkContext</h2>

<ul>
  <li>The Spark driver program creates and uses SparkContext to connect to the cluster manager to submit PySpark jobs, and know what resource manager (YARN, Mesos, or Standalone) to communicate to. It is the heart of the PySpark application.</li>
  <li>Since PySpark 2.0, Creating a SparkSession creates a SparkContext internally and exposes the sparkContext variable to use.</li>
  <li>At any given time only one SparkContext instance should be active per JVM. In case you want to create another you should stop existing SparkContext using stop() before creating a new one.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># Create SparkSession from builder
</span><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>
<span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"local[1]"</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">appName</span><span class="p">(</span><span class="s">'SparkByExamples.com'</span><span class="p">)</span> \
                    <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Spark App Name : "</span><span class="o">+</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">appName</span><span class="p">)</span>

<span class="c1"># Outputs
#&lt;SparkContext master=local[1] appName=SparkByExamples.com&gt;
#Spark App Name : SparkByExamples.com
</span>


<span class="c1"># SparkContext stop() method
</span><span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span><span class="p">.</span><span class="n">stop</span><span class="p">()</span>

</code></pre></div></div>

<h1 id="참고">참고</h1>

<ul>
  <li><a href="https://sparkbyexamples.com/" target="_blank">Spark By Example, 스파크 배우기 좋은 블로그</a></li>
  <li><a href="https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/spark_session.html" target="_blank">PySpark 공식문서, Spark Session</a></li>
</ul>
:ET
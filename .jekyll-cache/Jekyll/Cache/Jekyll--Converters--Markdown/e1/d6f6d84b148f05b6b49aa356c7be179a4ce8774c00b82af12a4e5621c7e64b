I"û<<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#ì¥ì• -ëŒ€ì‘fault-tolerance" id="markdown-toc-ì¥ì• -ëŒ€ì‘fault-tolerance">ì¥ì•  ëŒ€ì‘(Fault Tolerance)</a>    <ul>
      <li><a href="#ì¥ì• -ê°ì§€ì™€-íšŒë³µ" id="markdown-toc-ì¥ì• -ê°ì§€ì™€-íšŒë³µ">ì¥ì•  ê°ì§€ì™€ íšŒë³µ</a></li>
      <li><a href="#ë¦¬ë”ì™€-íŒ”ë¡œì›Œ" id="markdown-toc-ë¦¬ë”ì™€-íŒ”ë¡œì›Œ">ë¦¬ë”ì™€ íŒ”ë¡œì›Œ</a></li>
      <li><a href="#ë³µì œ" id="markdown-toc-ë³µì œ">ë³µì œ</a></li>
      <li><a href="#ì¼ê´€ì„±-ë³´ì¥" id="markdown-toc-ì¼ê´€ì„±-ë³´ì¥">ì¼ê´€ì„± ë³´ì¥</a></li>
      <li><a href="#í•©ì˜-ì•Œê³ ë¦¬ì¦˜" id="markdown-toc-í•©ì˜-ì•Œê³ ë¦¬ì¦˜">í•©ì˜ ì•Œê³ ë¦¬ì¦˜</a></li>
    </ul>
  </li>
  <li><a href="#ë¶„ì‚°-ì²˜ë¦¬distributed-computing" id="markdown-toc-ë¶„ì‚°-ì²˜ë¦¬distributed-computing">ë¶„ì‚° ì²˜ë¦¬(Distributed Computing)</a></li>
  <li><a href="#ì‹œìŠ¤í…œ-í™•ì¥scale-out" id="markdown-toc-ì‹œìŠ¤í…œ-í™•ì¥scale-out">ì‹œìŠ¤í…œ í™•ì¥(Scale-out)</a></li>
  <li><a href="#ì°¸ê³ " id="markdown-toc-ì°¸ê³ ">ì°¸ê³ </a></li>
</ul>

<hr />

<p>distributed systemì€ fault tolerance í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ì„œë²„ì— replication. replicationí•  ë•Œ ìœ ì˜í•  ì ì´ consistency (ì–´ë–¤ ì„œë²„ê°€ ì„ íƒë˜ì–´ë„ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì¼ê´€ëœ ê²°ê³¼ë¥¼ ëŒë ¤ ì£¼ëŠ”ê°€). consistencyë¥¼ ì œê³µí•˜ê¸° ìœ„í•œ ê²ƒì´ í•©ì˜ ì•Œê³ ë¦¬ì¦˜?</p>

<h1 id="ì¥ì• -ëŒ€ì‘fault-tolerance">ì¥ì•  ëŒ€ì‘(Fault Tolerance)</h1>

<h2 id="ì¥ì• -ê°ì§€ì™€-íšŒë³µ">ì¥ì•  ê°ì§€ì™€ íšŒë³µ</h2>

<p><strong>Heartbeats and Ping</strong></p>

<h2 id="ë¦¬ë”ì™€-íŒ”ë¡œì›Œ">ë¦¬ë”ì™€ íŒ”ë¡œì›Œ</h2>

<p>ë™ê¸°í™”ëŠ” ë¹„ìš©ì´ ë§ì´ ë“¤ ìˆ˜ ìˆë‹¤. ë™ê¸°í™” ì˜¤ë²„í—¤ë“œë¥¼ ì¤„ì´ê¸° ìœ„í•´ ì¼ë¶€ ì•Œê³ ë¦¬ì¦˜ì—ëŠ” ë¶„ì‚° ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¨ê³„ë¥¼ ìˆ˜í–‰ ë° ì¡°ì •í•˜ëŠ” ë¦¬ë” í”„ë¡œì„¸ìŠ¤ê°€ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë¶„ì‚° ì‹œìŠ¤í…œì˜ í”„ë¡œì„¸ìŠ¤ëŠ” ê· ì¼í•˜ê³  ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ë¦¬ë” ì—­í• ì„ ë§¡ì„ ìˆ˜ ìˆë‹¤. ì¥ì• ê°€ ë°œìƒí•˜ë©´ ì–´ë–¤ í”„ë¡œì„¸ìŠ¤ë¼ë„ ë¦¬ë” ì„ ì¶œ ê³¼ì •ì„ ì‹œì‘í•  ìˆ˜ ìˆê³  ì„ ì¶œëœ í”„ë¡œì„¸ìŠ¤ëŠ” ì´ì „ ë¦¬ë”ì˜ ì‘ì—…ì„ ì´ì–´ì„œ ìˆ˜í–‰í•œë‹¤.</p>

<p>ì´ìƒì ìœ¼ë¡œëŠ” í•œ ë²ˆì— í•˜ë‚˜ì˜ ë¦¬ë”ë§Œ ì¡´ì¬í•˜ê³ , ì—¬ëŸ¬ ë¦¬ë”ê°€ ì„ ì¶œë˜ê³  ì„œë¡œ ì¡´ì¬ë¥¼ ëª¨ë¥´ëŠ” ìƒí™©(split brain í˜„ìƒ)ì€ ì ˆëŒ€ë¡œ ë°œìƒí•˜ì§€ ì•Šì•„ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ë§ì€ ë¦¬ë” ì„ ì¶œ ì•Œê³ ë¦¬ì¦˜ì´ ì´ ì¡°ê±´ì„ ìœ„ë°˜í•œë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œëŠ” í´ëŸ¬ìŠ¤í„° êµ¬ì„±ì› ê³¼ë°˜ìˆ˜ì˜ ë™ì˜ê°€ í•„ìš”í•˜ë‹¤. ë©€í‹° íŒì†ŒìŠ¤ì™€ ë˜í”„íŠ¸ë¥¼ ë¹„ë¡¯í•œ ì—¬ëŸ¬ í•©ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ë¦¬ë” í”„ë¡œì„¸ìŠ¤ê°€ ì¡°ìœ¨ì„ ë‹´ë‹¹í•œë‹¤.</p>

<p>ì„ ì¶œ ì‘ì—…ì€ ê²°ì •ë¡ ì ì´ì–´ì•¼ í•œë‹¤. ì •í™•íˆ í•˜ë‚˜ì˜ ë¦¬ë”ë¥¼ ì„ ì¶œí•˜ê³  ëª¨ë“  ì°¸ê°€ìê°€ ê²°ê³¼ë¥¼ ì¸ì •í•´ì•¼ í•œë‹¤. ì„ ì¶œëœ ìƒˆë¡œìš´ ë¦¬ë”ëŠ” ëª¨ë“  êµ¬ì„±ì›ì—ê²Œ ìì‹ ì˜ ì¡´ì¬ë¥¼ ì•Œë ¤ì•¼ í•œë‹¤.</p>

<p>ë¦¬ë” í”„ë¡œì„¸ìŠ¤ê°€ ìˆëŠ” ì‹œìŠ¤í…œì˜ ê°€ì¥ í° ë¬¸ì œëŠ” ë¦¬ë”ê°€ ë³‘ëª© ì§€ì ì´ ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë§ì€ ì‹œìŠ¤í…œì´ ë°ì´í„°ë¥¼ ë…ë¦½ì ì¸ íŒŒí‹°ì…˜ìœ¼ë¡œ ë‚˜ëˆ„ê³  ê° íŒŒí‹°ì…˜ë³„ë¡œ ë¦¬ë”ë¥¼ ì„ ì¶œí•œë‹¤. ìŠ¤íŒ¨ë„ˆ(Spanner)ê°€ ì´ ë°©ì‹ì„ ì‚¬ìš©í•œë‹¤.</p>

<p>ì„ ì¶œ ì‘ì—…ì€ ë¹„ìš©ì´ ë†’ì€ ì‘ì—…ì´ì§€ë§Œ ìì£¼ ìˆ˜í–‰ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì‹œìŠ¤í…œ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ”ë‹¤.</p>

<p>ë¦¬ë”ì˜ ì •ì²´ëŠ” í”„ë¡œì„¸ìŠ¤ê°€ ëª¨ë¥´ëŠ” ì‚¬ì´ì— ë°”ë€” ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ê° í”„ë¡œì„¸ìŠ¤ê°€ ê°œë³„ì ìœ¼ë¡œ ì•Œê³  ìˆëŠ” ë¦¬ë”ì— ëŒ€í•œ ì •ë³´ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ì•¼ í•œë‹¤. ì´ ë¬¸ì œëŠ” ë¦¬ë” ì„ ì¶œ ì•Œê³ ë¦¬ì¦˜ê³¼ ì¥ì•  ê°ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ê°™ì´ ì‚¬ìš©í•´ í•´ê²°í•  ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•ˆì •ì ì¸ ë¦¬ë” ì„ ì¶œ ì•Œê³ ë¦¬ì¦˜ì€ ì•ˆì •ì ì¸ í”„ë¡œì„¸ìŠ¤ì— ë¦¬ë”ì˜ ê¸°íšŒë¥¼ ì£¼ê³  íƒ€ì„ì•„ì›ƒ ê¸°ë°˜ì˜ ì¥ì•  ê°ì§€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ í•´ë‹¹ í”„ë¡œì„¸ìŠ¤ì— ì¥ì• ê°€ ë°œìƒí•˜ì§€ ì•Šê³  ì ‘ê·¼ì´ ê°€ëŠ¥í•œ ë™ì•ˆ ê³„ì†í•´ì„œ ë¦¬ë” ì—­í• ì„ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë³´ì¥í•œë‹¤.</p>

<p>ë¦¬ë”ì— ì˜ì¡´í•˜ëŠ” ëŒ€ë¶€ë¶„ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ì—¬ëŸ¬ ë¦¬ë”ê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ í—ˆìš©í•˜ê³  ë¦¬ë” ì‚¬ì´ì˜ ì¶©ëŒì„ ìµœëŒ€í•œ ë¹ ë¥´ê²Œ í•´ê²°í•œë‹¤.</p>

<h2 id="ë³µì œ">ë³µì œ</h2>

<p>In a distributed system data is stored is over different computers in a network. Therefore, we need to make sure that data is readily available for the users. Availability of the data is an important factor often accomplished by data replication. Replication is the practice of keeping several copies of data in different places.</p>

<p>Why do we require replication?<br />
The first and foremost thing is that it makes our system more stable because of node replication. It is good to have replicas of a node in a network due to following reasons:</p>

<ul>
  <li>If a node stops working, the distributed network will still work fine due to its replicas which will be there. Thus it increases the fault tolerance of the system.</li>
  <li>It also helps in load sharing where loads on a server are shared among different replicas.</li>
  <li>It enhances the availability of the data. If the replicas are created and data is stored near to the consumers, it would be easier and faster to fetch data.</li>
</ul>

<p><strong>Types of Replication</strong></p>
<ul>
  <li>Active Replication</li>
  <li>Passive Replication</li>
</ul>

<p><strong>Active Replication</strong></p>
<ul>
  <li>The request of the client goes to all the replicas.</li>
  <li>It is to be made sure that every replica receives the client request in the same order else the system will get inconsistent.</li>
  <li>There is no need for coordination because each copy processes the same request in the same sequence.</li>
  <li>All replicas respond to the clientâ€™s request.</li>
</ul>

<p><strong>Advantages</strong></p>

<ul>
  <li>It is really simple. The codes in active replication are the same throughout.</li>
  <li>It is transparent.</li>
  <li>Even if a node fails, it will be easily handled by replicas of that node.</li>
</ul>

<p><strong>Disadvantages</strong></p>

<ul>
  <li>It increases resource consumption. The greater the number of replicas, the greater the memory needed.</li>
  <li>It increases the time complexity. If some change is done on one replica it should also be done in all others.</li>
</ul>

<p><strong>Passive Replication</strong></p>

<ul>
  <li>The client request goes to the primary replica, also called the main replica.</li>
  <li>There are more replicas that act as backup for the primary replica.</li>
  <li>Primary replica informs all other backup replicas about any modification done.</li>
  <li>The response is returned to the client by a primary replica.</li>
  <li>Periodically primary replica sends some signal to backup replicas to let them know that it is working perfectly fine.</li>
  <li>In case of failure of a primary replica, a backup replica becomes the primary replica.</li>
</ul>

<p><strong>Advantages</strong></p>

<ul>
  <li>The resource consumption is less as backup servers only come into play when the primary server fails.</li>
  <li>The time complexity of this is also less as thereâ€™s no need for updating in all the nodes replicas, unlike active replication.</li>
</ul>

<p><strong>Disadvantages</strong></p>

<ul>
  <li>If some failure occurs, the response time is delayed.</li>
</ul>

<h2 id="ì¼ê´€ì„±-ë³´ì¥">ì¼ê´€ì„± ë³´ì¥</h2>

<p><strong>CAP Theorem</strong></p>

<p><img src="/images/cap.png" alt="" /></p>

<p><a href="https://data-science-blog.com/blog/2021/10/14/cap-theorem/" target="_blank">Understanding NoSQL Databases by the CAP Theorem</a></p>

<p>The CAP Theorem (as put forth in a presentation by Eric Brewer in 2000) stated that distributed shared-data systems had three properties but systems could only choose to adhere to two of those properties:</p>

<ul>
  <li>Consistency</li>
  <li>Availability</li>
  <li>Partition-tolerance</li>
</ul>

<p>Distributed systems designed for fault tolerance are not much use if they cannot operate in a partitioned state (a state where one or more nodes are unreachable). Thus, partition-tolerance is always a requirement, so the two basic modes that most systems use are either Availability-Partition-tolerant (â€œAPâ€) or Consistency-Partition-tolerant (â€œCPâ€).</p>

<p>An â€œAPâ€-oriented database remains available even if it was partitioned in some way. For instance, if one or more nodes went down, or two or more parts of the cluster were separated by a network outage (a so-called â€œsplit-brainâ€ situation), the remaining database nodes would remain available and continue to respond to requests for data (reads) or even accept new data (writes). However, its data would become inconsistent across the cluster during the partitioned state. Transactions (reads and writes) in an â€œAPâ€-mode database are considered to be â€œeventually consistentâ€ because they are allowed to write to some portion of nodes; inconsistencies across nodes are settled over time using various anti-entropy methods.</p>

<p>A â€œCPâ€-oriented database would instead err on the side of consistency in the case of a partition, even if it meant that the database became unavailable in order to maintain its consistency. For example, a database for a bank might disallow transactions to prevent it from becoming inconsistent and allowing withdrawals of more money than were actually available in an account. Transactions on such systems are referred to as â€œstrongly consistentâ€ because all nodes on a system need to reflect the change before the transaction is considered complete or successful.</p>

<p><strong>Eventual Consistency</strong></p>

<p>Eventual consistency is a consistency model that enables the data store to be highly available. It is also known as optimistic replication &amp; is key to distributed systems. So, how exactly does it work? Letâ€™s Understand this with the help of a use case.</p>

<p>Real World Use Case :</p>

<ul>
  <li>Think of a popular microblogging site deployed across the world in different geographical regions like Asia, America, and Europe. Moreover, each geographical region has multiple data center zones: North, East, West, and South.</li>
  <li>Furthermore, each zone has multiple clusters which have multiple server nodes running. So, we have many datastore nodes spread across the world that micro-blogging site uses for persisting data. Since there are so many nodes running, there is no single point of failure.</li>
  <li>The data store service is highly available. Even if a few nodes go down persistence service is still up. Letâ€™s say a celebrity makes a post on the website that everybody starts liking around the world.</li>
  <li>At a point in time, a user in Japan likes a post which increases the â€œLikeâ€ count of the post from say 100 to 101. At the same point in time, a user in America, in a different geographical zone, clicks on the post, and he sees â€œLikeâ€ count as 100, not 101.</li>
</ul>

<p>Reason for the above Use case :</p>

<ul>
  <li>Simply, because the new updated value of the Post â€œLikeâ€ counter needs some time to move from Japan to America and update server nodes running there. Though the value of the counter at that point in time was 101, the user in America sees old inconsistent values.</li>
  <li>But when he refreshes his web page after a few seconds â€œLikeâ€ counter value shows as 101. So, data was initially inconsistent but eventually got consistent across server nodes deployed around the world. This is what eventual consistency is.</li>
</ul>

<p><strong>Strong Consistency</strong></p>

<p>Strong Consistency simply means the data must be strongly consistent at all times. All the server nodes across the world should contain the same value as an entity at any point in time. And the only way to implement this behavior is by locking down the nodes when being updated.</p>

<p>Real World Use Case :</p>

<ul>
  <li>Letâ€™s continue the same Eventual Consistency example from the previous lesson. To ensure Strong Consistency in the system, when a user in Japan likes posts, all nodes across different geographical zones must be locked down to prevent any concurrent updates.</li>
  <li>This means at one point in time, only one user can update the post â€œLikeâ€ counter value. So, once a user in Japan updates the â€œLikeâ€ counter from 100 to 101. The value gets replicated globally across all nodes. Once all nodes reach consensus, locks get lifted. Now, other users can Like posts.</li>
  <li>If the nodes take a while to reach a consensus, they must wait until then. Well, this is surely not desired in the case of social applications. But think of a stock market application where the users are seeing different prices of the same stock at one point in time and updating it concurrently. This would create chaos. Therefore, to avoid this confusion we need our systems to be Strongly Consistent.</li>
  <li>The nodes must be locked down for updates. Queuing all requests is one good way of making a system Strongly Consistent. The strong Consistency model hits the capability of the system to be Highly Available &amp; perform concurrent updates. This is how strongly consistent ACID transactions are implemented.</li>
</ul>

<p><strong>ACID Transaction Support</strong></p>

<p>Distributed systems like NoSQL databases which scale horizontally on the fly donâ€™t support ACID transactions globally &amp; this is due to their design. The whole reason for the development of NoSQL tech is the ability to be Highly Available and Scalable. If we must lock down nodes every time, it becomes just like SQL. So, NoSQL databases donâ€™t support ACID transactions and those that claim to, have terms and conditions applied to them. Generally, transaction support is limited to a geographic zone or an entity hierarchy. Developers of tech make sure that all the Strongly consistent entity nodes reside in the same geographic zone to make ACID transactions possible.</p>

<p>Conclusion: For transactional things go for MySQL because it provides a lock-in feature and supports ACID transactions.</p>

<h2 id="í•©ì˜-ì•Œê³ ë¦¬ì¦˜">í•©ì˜ ì•Œê³ ë¦¬ì¦˜</h2>

<p>Consensus is a general agreement on a decision made by the majority of those involved. For example, the problem may be as simple as friends trying to decide which restaurant has multiple options to choose from or complex as decisions on distributed systems.</p>

<p><strong>Need of consensus in a distributed system</strong></p>

<p>In a distributed system, nodes are distributed across the network. Some of these nodes might get failed(crash fault) or starts behaving abnormally (Byzantine Fault). In such a scenario, it becomes difficult to come to a common decision. More concisely,</p>

<ul>
  <li>There are n processes, m of which may be faulty.</li>
  <li>The task is to make all the Nonfaulty processes agree on some value(s) even in the presence of the faulty processes.</li>
</ul>

<p>So we can remove these problems by given below solutions:</p>

<ul>
  <li>Consensus Without Any Fault</li>
  <li>Consensus With at most m Crash Faults</li>
  <li>Consensus With at most m Byzantine Faults</li>
</ul>

<p><strong>ë©€í‹° íŒì†ŒìŠ¤</strong></p>

<p><strong>ë˜í”„íŠ¸</strong></p>

<h1 id="ë¶„ì‚°-ì²˜ë¦¬distributed-computing">ë¶„ì‚° ì²˜ë¦¬(Distributed Computing)</h1>

<h1 id="ì‹œìŠ¤í…œ-í™•ì¥scale-out">ì‹œìŠ¤í…œ í™•ì¥(Scale-out)</h1>

<h1 id="ì°¸ê³ ">ì°¸ê³ </h1>

<ul>
  <li><a href="https://www.geeksforgeeks.org/consensus-problem-of-distributed-systems/?ref=gcse" target="_blank">GeeksforGeeks: Consensus Problem of Distributed Systems</a></li>
</ul>
:ET
I":@<hr />

<p id="toc"><strong>Table of Contents</strong></p>
<ul id="markdown-toc">
  <li><a href="#파이토치" id="markdown-toc-파이토치">파이토치</a></li>
  <li><a href="#디바이스-cpu-gpu-mps-등" id="markdown-toc-디바이스-cpu-gpu-mps-등">디바이스 (CPU, GPU, MPS 등)</a>    <ul>
      <li><a href="#디바이스-확인하기" id="markdown-toc-디바이스-확인하기">디바이스 확인하기</a></li>
      <li><a href="#디바이스-설정하기" id="markdown-toc-디바이스-설정하기">디바이스 설정하기</a></li>
      <li><a href="#텐서에-디바이스-할당하기" id="markdown-toc-텐서에-디바이스-할당하기">텐서에 디바이스 할당하기</a></li>
    </ul>
  </li>
  <li><a href="#텐서-속성" id="markdown-toc-텐서-속성">텐서 속성</a></li>
  <li><a href="#텐서-생성" id="markdown-toc-텐서-생성">텐서 생성</a>    <ul>
      <li><a href="#torchtensor" id="markdown-toc-torchtensor">torch.Tensor</a></li>
      <li><a href="#torchtensor-1" id="markdown-toc-torchtensor-1">torch.tensor</a></li>
      <li><a href="#torchas_tensor" id="markdown-toc-torchas_tensor">torch.as_tensor</a></li>
      <li><a href="#torchzeros-torchzeros_like-torchones-torchones_like" id="markdown-toc-torchzeros-torchzeros_like-torchones-torchones_like">torch.zeros, torch.zeros_like, torch.ones, torch.ones_like</a></li>
      <li><a href="#torchempty-torchempty_like-torchfull-torchfull_like" id="markdown-toc-torchempty-torchempty_like-torchfull-torchfull_like">torch.empty, torch.empty_like, torch.full, torch.full_like</a></li>
      <li><a href="#torcharange-torchlinspace" id="markdown-toc-torcharange-torchlinspace">torch.arange, torch.linspace</a></li>
      <li><a href="#torchrand-torchrandn-torchrandint" id="markdown-toc-torchrand-torchrandn-torchrandint">torch.rand, torch.randn, torch.randint</a></li>
    </ul>
  </li>
  <li><a href="#텐서-조작" id="markdown-toc-텐서-조작">텐서 조작</a>    <ul>
      <li><a href="#텐서-이어붙이기" id="markdown-toc-텐서-이어붙이기">텐서 이어붙이기</a>        <ul>
          <li><a href="#torchcat" id="markdown-toc-torchcat">torch.cat</a></li>
          <li><a href="#torchstack-torchvstack-torchhstack-torchdstack" id="markdown-toc-torchstack-torchvstack-torchhstack-torchdstack">torch.stack, torch.vstack, torch.hstack, torch.dstack</a></li>
        </ul>
      </li>
      <li><a href="#텐서-쪼개기" id="markdown-toc-텐서-쪼개기">텐서 쪼개기</a>        <ul>
          <li><a href="#torchsplit" id="markdown-toc-torchsplit">torch.split</a></li>
        </ul>
      </li>
      <li><a href="#텐서-형변환" id="markdown-toc-텐서-형변환">텐서 형변환</a></li>
      <li><a href="#텐서-인덱싱" id="markdown-toc-텐서-인덱싱">텐서 인덱싱</a>        <ul>
          <li><a href="#torchindex_select" id="markdown-toc-torchindex_select">torch.index_select</a></li>
        </ul>
      </li>
      <li><a href="#텐서-모양-바꾸기" id="markdown-toc-텐서-모양-바꾸기">텐서 모양 바꾸기</a>        <ul>
          <li><a href="#torchreshape" id="markdown-toc-torchreshape">torch.reshape</a></li>
          <li><a href="#tensorview" id="markdown-toc-tensorview">Tensor.view()</a></li>
          <li><a href="#torchtranspose" id="markdown-toc-torchtranspose">torch.transpose</a></li>
          <li><a href="#torchsqueeze-torchunsqueeze" id="markdown-toc-torchsqueeze-torchunsqueeze">torch.squeeze(), torch.unsqueeze()</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#텐서-연산과-함수" id="markdown-toc-텐서-연산과-함수">텐서 연산과 함수</a></li>
  <li><a href="#텐서-곱" id="markdown-toc-텐서-곱">텐서 곱</a>    <ul>
      <li><a href="#행렬곱" id="markdown-toc-행렬곱">행렬곱</a></li>
      <li><a href="#행렬-원소간-곱" id="markdown-toc-행렬-원소간-곱">행렬 원소간 곱</a></li>
      <li><a href="#벡터-내적" id="markdown-toc-벡터-내적">벡터 내적</a></li>
      <li><a href="#배치-행렬곱" id="markdown-toc-배치-행렬곱">배치 행렬곱</a></li>
    </ul>
  </li>
  <li><a href="#자동-미분과-그래디언트" id="markdown-toc-자동-미분과-그래디언트">자동 미분과 그래디언트</a>    <ul>
      <li><a href="#autograd" id="markdown-toc-autograd">Autograd</a></li>
    </ul>
  </li>
</ul>

<hr />
<h1 id="파이토치">파이토치</h1>

<ul>
  <li>파이토치는 <strong>딥러닝 프레임워크</strong>중 하나다</li>
  <li>파이토치는 넘파이(Numpy) 배열과 유사한 텐서(Tensor)를 사용한다</li>
  <li>파이토치는 GPU를 활용한 딥러닝 코드를 작성하는데 편리한 기능을 제공한다</li>
  <li>텐서는 기본적으로 다차원 배열을 처리하기에 적합한 자료구조로 이해할 수 있다.</li>
  <li>텐서는 “자동 미분” 기능을 제공한다.</li>
</ul>

<h1 id="디바이스-cpu-gpu-mps-등">디바이스 (CPU, GPU, MPS 등)</h1>

<ul>
  <li>파이토치는 텐서간의 연산을 할 때, <span class="very__important">텐서들이 서로 같은 디바이스(Device)안에 있어야 한다</span></li>
  <li>따라서 가능하면, 연산을 수행하는 텐서들을 모두 GPU에 올린 뒤에 연산을 수행하는 것이 좋다</li>
  <li>(GPU가 없다면 다른 장비(CPU, MPS)도 된다)</li>
</ul>

<h2 id="디바이스-확인하기">디바이스 확인하기</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="c1"># GPU 사용 가능 여부
</span><span class="n">torch</span><span class="p">.</span><span class="n">cpu</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="c1"># CPU 사용 가능 여부
</span><span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="c1"># MPS 사용 가능 여부
</span></code></pre></div></div>

<h2 id="디바이스-설정하기">디바이스 설정하기</h2>

<ul>
  <li>만약 <code class="language-plaintext highlighter-rouge">device</code>를 지금 사용중인 디바이스가 아닌 다른 디바이스로 설정하려는 경우 <code class="language-plaintext highlighter-rouge">'cuda:X'</code> 처럼 <code class="language-plaintext highlighter-rouge">X</code>에 디바이스 ordinal을 표기해야 한다. 하지만 지금 현재 장비 사용하려는 경우 <code class="language-plaintext highlighter-rouge">0</code> 붙이거나 아니면 아예 생략해도 된다. 그래서 나는 그냥 생략할 예정이다</li>
  <li><code class="language-plaintext highlighter-rouge">device</code>로 문자열 <code class="language-plaintext highlighter-rouge">'cuda'</code> 이런식으로 전달해도 되고, <code class="language-plaintext highlighter-rouge">torch.device('cuda')</code> 이런 객체를 전달해도 된다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device</span> <span class="o">=</span> <span class="s">"cpu"</span>
<span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">"cuda:0"</span>
<span class="k">elif</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">"mps"</span>
<span class="k">print</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="o">--------------------</span>
<span class="s">"mps"</span>

<span class="c1"># 이렇게 한 줄로 쓸 수도 있음
</span><span class="n">device</span> <span class="o">=</span> <span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'mps'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">backends</span><span class="p">.</span><span class="n">mps</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span>
</code></pre></div></div>

<h2 id="텐서에-디바이스-할당하기">텐서에 디바이스 할당하기</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Tensor.cpu()</code>, <code class="language-plaintext highlighter-rouge">Tensor.cuda()</code> 써도 되지만,</li>
  <li><code class="language-plaintext highlighter-rouge">device</code>를 정의했으면 <code class="language-plaintext highlighter-rouge">Tensor.to(device)</code> 를 쓰는게 더 괜찮은 것 같다</li>
  <li>(게다가 mps는 <code class="language-plaintext highlighter-rouge">.mps()</code> 따로 없어서 무조건 <code class="language-plaintext highlighter-rouge">.to()</code> 써야함)</li>
  <li>텐서를 디바이스에 옮기는 과정은, 원본 텐서를 새로운 디바이스에 복사한다. 그래서 새로운 디바이스에 할당된 텐서를 다루기 위해서는 변수에 재할당 해야한다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># 이렇게 x.to(device)는 device로 x를 복사한 후 복사한 값을 반환한다
</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># cpu
</span><span class="k">print</span><span class="p">(</span><span class="n">z</span><span class="p">.</span><span class="n">device</span><span class="p">)</span> <span class="c1"># mps: 0
</span>
<span class="c1"># 보통은 x = x.to(device) 이렇게 쓰면 된다
</span>
<span class="c1"># 참고로 .to() 는 device 뿐만 아니라 타입까지 변환할 수 있다
</span><span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>

<span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>


<span class="c1"># 결론,
# to()를 쓰자
# x = x.to(device) 처럼 device로 옮겨진 텐서를 재할당하자
</span></code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 텐서를 연산할 때는 서로 같은 디바이스에 있어야 한다
</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]</span>
<span class="p">]).</span><span class="n">to</span><span class="p">(</span><span class="s">"mps"</span><span class="p">)</span>

<span class="c1"># CPU 장치의 텐서
</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">],</span>
    <span class="p">[</span><span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">]</span>
<span class="p">])</span>

<span class="k">print</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="c1"># 오류 발생
</span><span class="o">--------------------------------</span>
<span class="nb">RuntimeError</span><span class="p">:</span> <span class="n">Tensor</span> <span class="k">for</span> <span class="n">argument</span> <span class="c1">#2 'mat2' is on CPU, but expected it to be on GPU (while checking arguments for mm)
</span></code></pre></div></div>

<h1 id="텐서-속성">텐서 속성</h1>

<ul>
  <li>텐서의 <strong>기본 속성</strong>으로는 다음과 같은 것들이 있다.
    <ul>
      <li>모양(shape)</li>
      <li>자료형(data type)</li>
      <li>저장된 장치(device)</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"shape: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"type: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">dtype</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"device: </span><span class="si">{</span><span class="n">tensor</span><span class="p">.</span><span class="n">device</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="o">--------------------------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.0271</span><span class="p">,</span> <span class="mf">0.0495</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.0183</span><span class="p">,</span> <span class="mf">0.3877</span><span class="p">]])</span>
<span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="nb">type</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
<span class="n">device</span><span class="p">:</span> <span class="n">cpu</span>
</code></pre></div></div>

<h1 id="텐서-생성">텐서 생성</h1>

<ul>
  <li>텐서를 생성하는 데 있어 아래에서 설명한 것 말고도 훨씬 더 많은 방법들이 있다 <a href="https://pytorch.org/docs/stable/torch.html#creation-ops">(파이토치 공식문서 참고)</a></li>
</ul>

<h2 id="torchtensor">torch.Tensor</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.Tensor()</code> (= <code class="language-plaintext highlighter-rouge">torch.FloatTensor()</code>) 말고도, <code class="language-plaintext highlighter-rouge">torch.DoubleTensor()</code>, <code class="language-plaintext highlighter-rouge">torch.IntTensor()</code>, <code class="language-plaintext highlighter-rouge">torch.LongTensor()</code> 등 다양한 데이터 타입을 이름으로 가지는 생성자가 있다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]]</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">l</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
<span class="o">-----------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">4.</span><span class="p">]])</span>
</code></pre></div></div>

<h2 id="torchtensor-1">torch.tensor</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.tensor()</code>의 가장 큰 특징은 입력으로 받은 <strong>data를 항상 새로 복사해 텐서로 만든다</strong>는 점이다</li>
  <li>복사는 메모리 낭비를 유발하기 때문에, 상황에 맞게 대처 방법을 사용하는게 좋다
    <ul>
      <li>데이터를 공유하지만 메모리 효율적인 방법으로 텐서를 만들고 싶은 경우: <code class="language-plaintext highlighter-rouge">torch.as_tensor(data)</code></li>
      <li>텐서의 단순 <code class="language-plaintext highlighter-rouge">requires_grad()</code> 플래그만 바꾸고 싶은 경우: <code class="language-plaintext highlighter-rouge">Tensor.requires_grad(bool)</code></li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">torch.tensor()</code>는 새로운 메모리에 텐서를 만들고 autograph에서 떼어낸다 (creates a new leaf tensor)
    <ul>
      <li>그래서 텐서를 복사할 때는 <code class="language-plaintext highlighter-rouge">torch.tensor()</code> 보다는 같은 역할이지만 더 명시적인, <code class="language-plaintext highlighter-rouge">Tensor.clone().detach()</code> 를 권장하기도 한다</li>
    </ul>
  </li>
</ul>

<h2 id="torchas_tensor">torch.as_tensor</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.as_tensor()</code>는 데이터를 공유하며, autograph에 붙어있다 (preserves autograd history if possible)</li>
  <li>이미 텐서인 경우 같은 타입, 디바이스면 그냥 원본 반환하고, 타입이 달라지거나 디바이스가 달라지면 아예 새로 복사한다</li>
  <li>넘파이 배열의 경우 <code class="language-plaintext highlighter-rouge">torch.from_numpy()</code> 가 조금 더 빠르다</li>
</ul>

<h2 id="torchzeros-torchzeros_like-torchones-torchones_like">torch.zeros, torch.zeros_like, torch.ones, torch.ones_like</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.zeros()</code>는 <code class="language-plaintext highlighter-rouge">size</code> 형태의 텐서를 <code class="language-plaintext highlighter-rouge">0</code>으로, <code class="language-plaintext highlighter-rouge">torch.ones()</code>는 <code class="language-plaintext highlighter-rouge">size</code> 형태의 텐서를 <code class="language-plaintext highlighter-rouge">1</code>로 채운다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.zeros_like()</code>는 <code class="language-plaintext highlighter-rouge">input</code>과 같은 형태의 텐서를 <code class="language-plaintext highlighter-rouge">0</code>으로, <code class="language-plaintext highlighter-rouge">torch.ones_like()</code>는 <code class="language-plaintext highlighter-rouge">input</code>과 같은 형태의 텐서를 <code class="language-plaintext highlighter-rouge">1</code>로 채운다</li>
  <li>이 때 <code class="language-plaintext highlighter-rouge">input</code>은 텐서여야 한다. 넘파이 배열 같은 다른 타입을 쓰려면 <code class="language-plaintext highlighter-rouge">torch.ones(input.shape)</code> 이렇게 써야 한다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="torchempty-torchempty_like-torchfull-torchfull_like">torch.empty, torch.empty_like, torch.full, torch.full_like</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.empty()</code>, <code class="language-plaintext highlighter-rouge">torch.empty_like()</code>는 비어있는 것은 아니고, <code class="language-plaintext highlighter-rouge">0</code>에 가까운 초기화되지 않은 값을 가진다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span> <span class="c1"># [2, 3] 이렇게 넣어줘도 된다
</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
</code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.full()</code>, <code class="language-plaintext highlighter-rouge">torch.full_like()</code>는 <code class="language-plaintext highlighter-rouge">fill_value</code>로 텐서를 채워준다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float64</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">fill_value</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="torcharange-torchlinspace">torch.arange, torch.linspace</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.arange()</code>는 간격을 지정할 수 있다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.linspace()</code>는 개수를 지정할 수 있다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span> <span class="c1"># tensor([0, 1, 2, 3, 4])
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span> <span class="c1"># tensor([1, 2, 3])
</span><span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span> <span class="c1"># tensor([0.0000, 0.5000, 1.0000, 1.5000, 2.0000])
</span>
<span class="n">torch</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span> <span class="c1"># tensor([ 3.0000,  4.7500,  6.5000,  8.2500, 10.0000])
</span></code></pre></div></div>

<h2 id="torchrand-torchrandn-torchrandint">torch.rand, torch.randn, torch.randint</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.rand()</code>는 <code class="language-plaintext highlighter-rouge">0</code>과 <code class="language-plaintext highlighter-rouge">1</code>사이의 uniform distribution 에서 <code class="language-plaintext highlighter-rouge">size</code> 만큼 샘플링한 텐서를 반환한다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.randn()</code>은 <code class="language-plaintext highlighter-rouge">0</code>과 <code class="language-plaintext highlighter-rouge">1</code>사이의 normal distribution 에서 <code class="language-plaintext highlighter-rouge">size</code> 만큼 샘플링한 텐서를 반환한다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.randint()</code>는 <code class="language-plaintext highlighter-rouge">low</code>와 <code class="language-plaintext highlighter-rouge">high-1</code> 사이의 정수 값을 uniform distribution 에서 <code class="language-plaintext highlighter-rouge">size</code> 만큼 샘플링한 텐서를 반환한다</li>
  <li>(<code class="language-plaintext highlighter-rouge">rand_like()</code>, <code class="language-plaintext highlighter-rouge">randn_like()</code>, <code class="language-plaintext highlighter-rouge">randint_like()</code>도 있다)</li>
  <li>더 많은 샘플링 방법들이 있다 <a href="https://pytorch.org/docs/stable/torch.html#random-sampling">(파이토치 공식문서 참고)</a></li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># [2, 3] 형태의 텐서에 uniform distribution으로 샘플링한 값을 채운다
</span><span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># [2, 3] 형태의 텐서에 normal distribution으로 샘플링한 값을 채운다
</span><span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1"># [2, 2] 형태의 텐서에 3과 9 사이의 정수 값을 uniform distribution으로 샘플링한 값을 채운다
</span></code></pre></div></div>

<h1 id="텐서-조작">텐서 조작</h1>

<h2 id="텐서-이어붙이기">텐서 이어붙이기</h2>

<h3 id="torchcat">torch.cat</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.cat(tensors, dim)</code>은 <code class="language-plaintext highlighter-rouge">dim</code> 방향으로 텐서를 이어 붙인다. <code class="language-plaintext highlighter-rouge">dim</code> 방향 제외한 나머지 부분은 shape이 서로 같아야 한다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">--------------------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">--------------------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">tensors</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">--------------------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]])</span>
</code></pre></div></div>

<h3 id="torchstack-torchvstack-torchhstack-torchdstack">torch.stack, torch.vstack, torch.hstack, torch.dstack</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.stack(tensors, dim)</code>은 <strong>새로운 차원</strong>으로 텐서를 이어 붙인다.</li>
  <li>텐서들의 사이즈는 모두 같아야 한다</li>
  <li>새로운 차원은 크기는 쌓은 <code class="language-plaintext highlighter-rouge">len(tensors)</code>가 된다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 예를 들어, 3 X 4 짜리 행렬을 5개를 새로운 차원에 이어붙인다고 하면,
# 3 X 4 는 2차원 행렬이기 때문에 사이즈가 [3, 4] 이렇게 되어있다.
# 여기서 새로운 차원이 생길 수 있는 자리는 [new, 3, 4], [3, new, 4] 또는 [3, 4, new] 이렇게 있다
# 우리는 5개를 이어붙인다고 했기 때문에 new에 5가 들어간다
</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">).</span><span class="n">shape</span> <span class="c1"># [5, 3, 4]
</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="n">shape</span> <span class="c1"># [3, 5, 4]
</span><span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">a</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="n">shape</span> <span class="c1"># [3, 4, 5]
</span></code></pre></div></div>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.vstack(tensors)</code>은 dim=0 (세로)방향으로 쌓는다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.hstack(tensors)</code>은 dim=1 (가로)방향으로 쌓는다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.dstack(tensors)</code>은 dim=2 (3차원)방향으로 쌓는다</li>
  <li><code class="language-plaintext highlighter-rouge">dim</code> 방향 제외한 다른 차원의 사이즈는 같아야 한다</li>
  <li>쌓으면 결과는 다른 차원의 사이즈는 그대로고, <code class="language-plaintext highlighter-rouge">dim</code> 방향의 차원만 모두 더해진다
    <ul>
      <li>vstack의 경우: (2, 3), (3, 3) -&gt; (5, 3)</li>
      <li>hstack의 경우: (1, 2, 3), (1, 5, 3) -&gt; (1, 7, 3)</li>
      <li>dstack의 경우: (1, 2), (1, 2) -&gt; (1, 2, 2)</li>
    </ul>
  </li>
</ul>

<h2 id="텐서-쪼개기">텐서 쪼개기</h2>

<h3 id="torchsplit">torch.split</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.split(tensor, split_size or selections, dim)</code></li>
  <li><code class="language-plaintext highlighter-rouge">split_size(int)</code>인 경우 <code class="language-plaintext highlighter-rouge">split_size</code> 값 만큼의 <code class="language-plaintext highlighter-rouge">size</code> 갖는 텐서로 나눈다 (batch size같은 개념)</li>
  <li><code class="language-plaintext highlighter-rouge">selections(list)</code>인 경우 <code class="language-plaintext highlighter-rouge">selections</code>의 element값 만큼을 <code class="language-plaintext highlighter-rouge">split_size</code>로 가진다 -&gt; <code class="language-plaintext highlighter-rouge">selections</code> 원소의 합이 split하고자 하는 차원의 값과 같아야 함
    <ul>
      <li>(3, 16) -&gt; selections: [1, 5, 10] ok</li>
      <li>(3, 16) -&gt; selections: [1, 5, 11] X</li>
    </ul>
  </li>
  <li>쪼개기도 <code class="language-plaintext highlighter-rouge">vsplit</code>, <code class="language-plaintext highlighter-rouge">hsplit</code>, <code class="language-plaintext highlighter-rouge">dsplit</code> 있음</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">y</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">------------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>

<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">z</span><span class="p">:</span>
    <span class="k">print</span><span class="p">(</span><span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">------------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">5</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="텐서-형변환">텐서 형변환</h2>

<ul>
  <li>텐서의 자료형(정수, 실수 등)을 변환할 수 있다</li>
  <li><code class="language-plaintext highlighter-rouge">Tensor.type()</code></li>
  <li><code class="language-plaintext highlighter-rouge">Tensor.int()</code>, <code class="language-plaintext highlighter-rouge">Tensor.float()</code></li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="o">---------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">float32</span>



<span class="n">x_int</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nb">type</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_int</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="o">---------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
<span class="n">torch</span><span class="p">.</span><span class="n">int32</span>

<span class="n">x_float</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nb">float</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">x_float</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="o">---------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">float32</span>
</code></pre></div></div>

<h2 id="텐서-인덱싱">텐서 인덱싱</h2>

<h3 id="torchindex_select">torch.index_select</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.index_select(input, dim, index)</code></li>
  <li><code class="language-plaintext highlighter-rouge">index</code>를 통해 특정 index의 tensor를 가져온다</li>
  <li><code class="language-plaintext highlighter-rouge">index</code>는 IntTensor 또는 LongTensor type의 1-D tensor이다</li>
  <li><code class="language-plaintext highlighter-rouge">input.dim()</code>은 변하지 않는다 (3차원이면 계속 3차원)</li>
  <li><code class="language-plaintext highlighter-rouge">input</code>의 <code class="language-plaintext highlighter-rouge">dim</code> 사이즈는 <code class="language-plaintext highlighter-rouge">len(index)</code>와 같아진다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">IntTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">],</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span>

<span class="n">indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">index_select</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">indices</span><span class="p">)</span>
<span class="o">----------------------------------------------</span>

<span class="o">------------------------</span>
<span class="n">x</span>
<span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="o">------------------------</span>
<span class="n">y0</span>
<span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">2</span><span class="p">,</span>  <span class="mi">3</span><span class="p">,</span>  <span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="o">------------------------</span>
<span class="n">y1</span>
<span class="n">shape</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">1</span><span class="p">,</span>  <span class="mi">3</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">5</span><span class="p">,</span>  <span class="mi">7</span><span class="p">],</span>
        <span class="p">[</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">11</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="o">------------------------</span>
</code></pre></div></div>

<h2 id="텐서-모양-바꾸기">텐서 모양 바꾸기</h2>

<h3 id="torchreshape">torch.reshape</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.reshape(input, shape)</code></li>
  <li><code class="language-plaintext highlighter-rouge">input</code>과 data는 같다, shape만 다르다</li>
  <li>가능하면 view of input 아니면 copy</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">8.</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">z</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="o">----------------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">8</span><span class="p">])</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">8</span><span class="p">])</span> <span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</code></pre></div></div>

<h3 id="tensorview">Tensor.view()</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Tensor.view(shape)</code></li>
  <li><code class="language-plaintext highlighter-rouge">torch.reshape()</code>과 비슷하지만 기존 텐서와 메모리를 공유하기 때문에 메모리를 낭비하지 않는 방식이다</li>
  <li>텐서의 형태를 바꿀 때 인덱스는 바뀌었으나 실제 메모리 상에서는 배열을 바꾸지 않는다(위치 바꾸는 연산이 잦으면 성능을 떨어트리므로)</li>
  <li>contiguous한 텐서만 지원한다
    <ul>
      <li>(contiguous한 텐서인지 확인: <code class="language-plaintext highlighter-rouge">Tensor.is_contiguous()</code>)</li>
      <li>(contiguous한 텐서 만들고 싶을 때: <code class="language-plaintext highlighter-rouge">Tensor.contiguous()</code>)</li>
      <li><code class="language-plaintext highlighter-rouge">Tensor.contiguous().view()</code> 이렇게 쓰면 된다</li>
    </ul>
  </li>
</ul>

<h3 id="torchtranspose">torch.transpose</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.transpose(input, dim0, dim1)</code></li>
  <li><code class="language-plaintext highlighter-rouge">dim0</code>과 <code class="language-plaintext highlighter-rouge">dim1</code>이 바뀐 tensor를 리턴</li>
  <li><code class="language-plaintext highlighter-rouge">input</code>과 memory를 공유</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="n">y</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># (4, 3, 2)
</span></code></pre></div></div>

<h3 id="torchsqueeze-torchunsqueeze">torch.squeeze(), torch.unsqueeze()</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">torch.squeeze(tensor, dim)</code>: 특정(<code class="language-plaintext highlighter-rouge">dim</code>) 차원의 값이 1이면 차원을 없애버린다</li>
  <li><code class="language-plaintext highlighter-rouge">torch.unsqueeze(tensor, dim)</code>: 특정(<code class="language-plaintext highlighter-rouge">dim</code>) 차원을 추가한다</li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="n">y0</span><span class="p">.</span><span class="n">shape</span>
<span class="n">y1</span><span class="p">.</span><span class="n">shape</span>
<span class="n">y2</span><span class="p">.</span><span class="n">shape</span>
<span class="o">--------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="n">y0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>

<span class="n">y0</span><span class="p">.</span><span class="n">shape</span>
<span class="n">y1</span><span class="p">.</span><span class="n">shape</span>
<span class="n">y2</span><span class="p">.</span><span class="n">shape</span>
<span class="n">y3</span><span class="p">.</span><span class="n">shape</span>
<span class="o">---------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">])</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<h1 id="텐서-연산과-함수">텐서 연산과 함수</h1>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 같은 크기를 가진 두 개의 텐서에 대하여 사칙연산 가능
# 기본적으로 요소별(element-wise) 연산
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">7</span><span class="p">,</span> <span class="mi">8</span><span class="p">]</span>
<span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">-</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">a</span> <span class="o">/</span> <span class="n">b</span><span class="p">)</span>
<span class="o">---------------------------</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">6</span><span class="p">,</span>  <span class="mi">8</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">12</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">4</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">21</span><span class="p">,</span> <span class="mi">32</span><span class="p">]])</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2000</span><span class="p">,</span> <span class="mf">0.3333</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">0.4286</span><span class="p">,</span> <span class="mf">0.5000</span><span class="p">]])</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>


<span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">3.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">])</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">])</span>



<span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>



<span class="n">torch</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>



<span class="n">torch</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">return_types</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span>
<span class="n">values</span><span class="o">=</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">]]),</span>
<span class="n">indices</span><span class="o">=</span><span class="n">tensor</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>



<span class="n">torch</span><span class="p">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>



<span class="n">torch</span><span class="p">.</span><span class="nb">any</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nb">all</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>

<span class="n">t</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>



<span class="n">t</span><span class="p">[(</span><span class="n">t</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">t</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">)]</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">7</span><span class="p">])</span>



<span class="n">t</span><span class="p">[</span><span class="n">t</span><span class="p">.</span><span class="n">remainder</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">2</span><span class="p">,</span>  <span class="mi">4</span><span class="p">,</span>  <span class="mi">6</span><span class="p">,</span>  <span class="mi">8</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>



<span class="n">torch</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">t</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">,</span> <span class="n">t</span><span class="o">*</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">,</span> <span class="mi">20</span><span class="p">])</span>



<span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]).</span><span class="n">unique</span><span class="p">()</span>
<span class="o">-------------------------------------</span>
<span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>


</code></pre></div></div>

<h1 id="텐서-곱">텐서 곱</h1>

<h2 id="행렬곱">행렬곱</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모두 같다
</span>
<span class="n">A</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="n">A</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span> <span class="c1"># This function does not broadcast. For broadcasting matrix products, see torch.matmul()
</span><span class="n">A</span> <span class="o">@</span> <span class="n">B</span>
<span class="o">-----------------</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">],</span>
        <span class="p">[</span><span class="mf">14.</span><span class="p">,</span> <span class="mf">32.</span><span class="p">]])</span>
</code></pre></div></div>

<h2 id="행렬-원소간-곱">행렬 원소간 곱</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모두 같다
</span>
<span class="n">A</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
<span class="n">A</span> <span class="o">*</span> <span class="n">B</span>
<span class="o">------------</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.</span><span class="p">,</span>  <span class="mf">9.</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">8.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">]])</span>
</code></pre></div></div>

<h2 id="벡터-내적">벡터 내적</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 모두 같다
</span>
<span class="n">vec_A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">vec_B</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>

<span class="n">vec_A</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec_B</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">vec_A</span><span class="p">,</span> <span class="n">vec_B</span><span class="p">)</span>
<span class="o">----------</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">39.</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">39.</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="배치-행렬곱">배치 행렬곱</h2>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">m</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">p</span> <span class="o">=</span> <span class="mi">30</span>

<span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">))</span>
<span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">rand</span><span class="p">((</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">))</span>

<span class="n">torch</span><span class="p">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">).</span><span class="n">shape</span>
<span class="o">--------------------------</span>
<span class="n">torch</span><span class="p">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">32</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
</code></pre></div></div>

<h1 id="자동-미분과-그래디언트">자동 미분과 그래디언트</h1>

<ul>
  <li>신경망 학습은 크게 두 단계로 나뉘어진다
    <ul>
      <li>순전파: forward propagation을 통해prediction을 구하고 이를 이용해 loss 계산</li>
      <li>역전파: back prop을 통해 gradients를 구하고 이를 이용해 parameter들을 adjust</li>
    </ul>
  </li>
</ul>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># forward prop
</span><span class="n">prediction</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># backward prop
</span><span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
</code></pre></div></div>

<h2 id="autograd">Autograd</h2>

<ul>
  <li>https://tutorials.pytorch.kr/beginner/basics/autogradqs_tutorial.html</li>
  <li>https://tutorials.pytorch.kr/beginner/blitz/autograd_tutorial.html</li>
  <li>텐서에 requires_grad=True를 적용하면 이는 autograd가 이 tensor의 모든 연산들을 추적하도록 합니다</li>
  <li>loss가 계산되고 loss.backward()를 호출하면 autograd는 gradient를 계산하고 이를 텐서의 .grad 속성(attribute)에 저장합니다</li>
  <li>autograd는 실행 시점에 정의되는(define-by-run) 프레임워크입니다</li>
</ul>
:ET
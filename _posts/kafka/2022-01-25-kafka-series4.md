---
layout: post
title:  'Kafka Series [Part4]: Ïπ¥ÌîÑÏπ¥Ïùò Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû•'
description: 
date:   2022-01-25 15:01:35 +0300
image:  '/images/kafka_78.png'
logo_image:  '/images/kafka_logo.png'
categories:   DE
tags: Kafka
---

---
**Table of Contents**
{: #toc }
*  TOC
{:toc}

---  

# Ïπ¥ÌîÑÏπ¥Ïùò Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Î∞©Ïãù

Kafka is typically referred to as a Distributed, Replicated Messaging Queue, which although technically true, usually leads to some confusion depending on your definition of a messaging queue. Instead, I prefer to call it a **Distributed, Replicated Commit Log**. This, I think, clearly represents what Kafka does, as all of us understand how **logs** are written to disk. And in this case, it is the **messages pushed into Kafka that are stored to disk**.  

- KafkaÎäî Ïª§Î∞ã Î°úÍ∑∏Î•º Î∂ÑÏÇ∞ Î≥µÏ†úÌïòÎäî ÏãúÏä§ÌÖú
- Ïó¨Í∏∞ÏÑú Î°úÍ∑∏Îäî Ïö∞Î¶¨Í∞Ä ÎîîÏä§ÌÅ¨Ïóê Ï†ÄÏû•Ìïú Î©îÏÑ∏ÏßÄÎ•º ÏùòÎØ∏
- (Ïö∞Î¶¨Ïùò Î©îÏÑ∏ÏßÄÎ•º Î°úÍ∑∏Î°ú ÌëúÌòÑÌïòÎ†§Í≥† ÌïòÎäî Ïù¥Ïú†Îäî ÏïÑÎßà Î©îÏÑ∏ÏßÄ ÏïàÏóê Î≥¥ÌÜµ Îç∞Ïù¥ÌÑ∞ ÎøêÎßå ÏïÑÎãàÎùº Îã§Î•∏ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ÎèÑ Îì§Ïñ¥ ÏûàÏñ¥ÏÑú?)

Ïπ¥ÌîÑÏπ¥Ïùò Îç∞Ïù¥ÌÑ∞Îäî Îã§ÏùåÍ≥º Í∞ôÏùÄ Íµ¨Ï°∞Î°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÎã§.  

![](/images/kafka_80.png)

- Topic: namespaceÏ≤òÎüº ÎÖºÎ¶¨Ï†ÅÏúºÎ°ú Íµ¨Î∂ÑÌïòÎäî Í∏∞Ï§Ä. Îç∞Ïù¥ÌÑ∞Î•º Íµ¨Î∂ÑÌïòÎäî Í∞ÄÏû• ÌÅ∞ Íµ¨Î∂Ñ Í∏∞Ï§Ä
- Partition: Ïã§Ï†úÎ°ú Ïª®ÏäàÎ®∏Í∞Ä Îã¥ÎãπÌïòÎäî ÏûëÏóÖ Îã®ÏúÑ(Ïª®ÏäàÎ®∏ Í∑∏Î£πÎÇ¥ÏóêÏÑú ÌååÌã∞ÏÖòÏùÄ ÌïòÎÇòÏùò Ïª®ÏäàÎ®∏ÏóêÍ≤åÎßå Ìï†Îãπ Í∞ÄÎä•). Ìè¥ÎçîÎ°ú Íµ¨Î∂Ñ
- Segment: Ïó¨Îü¨ Î©îÏÑ∏ÏßÄÎ•º Î¨∂Ïñ¥ÎÜìÏùÄ ÌïòÎÇòÏùò ÌååÏùº. ÌååÌã∞ÏÖò Ìïú Í∞úÏóê Ïó¨Îü¨ Í∞úÏùò ÏÑ∏Í∑∏Î®ºÌä∏Í∞Ä Ï†ÄÏû•ÎêòÏñ¥ ÏûàÏùå.
- Message: Ïö∞Î¶¨Í∞Ä Ïã§Ï†úÎ°ú Î≥¥ÎÇ¥Îäî Îç∞Ïù¥ÌÑ∞ + ÏÉùÏÑ±Îêú ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑ + ÌîÑÎ°úÎìÄÏÑú ID + ...Î°ú Ïù¥Î£®Ïñ¥Ï†∏ ÏûàÏùå

## Partition

3Í∞úÏùò ÌååÌã∞ÏÖòÏùÑ Í∞ÄÏßÄÎäî ÌÜ†ÌîΩÏùÑ Ïö∞ÏÑ† Ìïú Í∞ú ÎßåÎì§Ïñ¥Î≥¥Ïûê.  

```sh
kafka-topics.sh --create --topic freblogg --partitions 3 --replication-factor 1 --zookeeper localhost:2181
```

ÌååÌã∞ÏÖòÏù¥ Ï†ÄÏû•ÎêòÎäî ÏúÑÏπòÎ°ú Ïù¥ÎèôÌï¥ ÌÜ†ÌîΩ Ïù¥Î¶ÑÏúºÎ°ú ÏãúÏûëÌïòÎäî ÌååÌã∞ÏÖòÏùÑ Í≤ÄÏÉâÌï¥Î≥¥Î©¥ 3Í∞úÏùò Ìè¥ÎçîÍ∞Ä Î≥¥Ïù∏Îã§.  

```
$ tree freblogg*
freblogg-0
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-1
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
freblogg-2
|-- 00000000000000000000.index
|-- 00000000000000000000.log
|-- 00000000000000000000.timeindex
`-- leader-epoch-checkpoint
```


Îã§ÏùåÍ≥º Í∞ôÏùÄ Î™ÖÎ†πÏñ¥Î•º Ïã§ÌñâÌï¥ Î∏åÎ°úÏª§Î°ú Î©îÏÑ∏ÏßÄÎ•º Î≥¥ÎÇ¥Î≥¥Ïûê.  

```
kafka-console-producer.sh --topic freblogg --broker-list localhost:9092
```

```
$ ls -lh freblogg*
freblogg-0:
total 20M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121   0 Aug  5 08:26 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121   0 Aug  5 08:26 leader-epoch-checkpoint

freblogg-1:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  68 Aug  5 10:15 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 10:15 leader-epoch-checkpoint

freblogg-2:
total 21M
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.index
- freblogg 197121  79 Aug  5 09:59 00000000000000000000.log
- freblogg 197121 10M Aug  5 08:26 00000000000000000000.timeindex
- freblogg 197121  11 Aug  5 09:59 leader-epoch-checkpoint
```

Îëê Í∞úÏùò Î©îÏÑ∏ÏßÄÎ•º Î≥¥ÎÉàÎã§. Í≤∞Í≥ºÎ•º ÌôïÏù∏Ìï¥Î≥¥Î©¥ Îëê Í∞úÏùò ÌååÌã∞ÏÖòÏù¥ Í∞ÄÏßÄÎäî 00000000000000000000.log ÎùºÎäî ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùºÏùò Ïö©ÎüâÏù¥ Ï¶ùÍ∞ÄÌñàÎã§. ÌååÏùºÏùÑ Ïó¥Ïñ¥Î≥¥Î©¥ Îã§ÏùåÍ≥º Í∞ôÏùÄ ÎÇ¥Ïö©Ïù¥ Ï†ÅÌòÄÏûàÎã§.  

```
$ cat freblogg-2/*.log
@^@^B√Ç¬∞√Ç¬£√É¬¶√É∆í^@^K^X√É¬ø√É¬ø√É¬ø√É¬ø√É¬ø√É¬ø^@^@^@^A"^@^@^A^VHello World^@
```

Î∏åÎ°úÏª§Ïóê Ï†ÄÏû•Îêú Î©îÏÑ∏ÏßÄÎäî Î∞îÏù¥Ìä∏ ÌòïÌÉúÎ°ú Ï†ÄÏû•ÎêòÍ∏∞ ÎïåÎ¨∏Ïóê Ï†úÎåÄÎ°ú ÎîîÏΩîÎî©ÌïòÏßÄ ÏïäÏúºÎ©¥ Ïù¥ÏÉÅÌïòÍ≤å ÏùΩÌûåÎã§. ÌïòÏßÄÎßå Hello WorldÎùºÍ≥† Ï†ÅÌûå Í≤ÉÏùÑ Î≥¥ÏïÑ .logÎùºÎäî ÌååÏùºÏóê Ïö∞Î¶¨Í∞Ä Î≥¥ÎÇ∏ Î©îÏÑ∏ÏßÄÍ∞Ä Ï†ÄÏû•ÎêúÎã§Îäî Í≤ÉÏùÑ Ïïå Ïàò ÏûàÎã§.  

Î©îÏÑ∏ÏßÄÍ∞Ä ÌååÌã∞ÏÖòÏóê ÌïòÎÇòÏî© Ï†ÄÏû•Îêú Ïù¥Ïú†Îäî ÎùºÏö¥Îìú Î°úÎπà Î∞©ÏãùÏúºÎ°ú Î©îÏÑ∏ÏßÄÎ•º ÌååÌã∞ÏÖòÏóê Ìï†ÎãπÌïòÍ∏∞ ÎïåÎ¨∏Ïù¥Îã§. Î©îÏÑ∏ÏßÄ Ìï†Îãπ Î∞©ÏãùÏùÄ Ïπ¥ÌîÑÏπ¥ÏóêÏÑú Ï†úÍ≥µÌïòÎäî Îã§Î•∏ Î∞©ÏãùÏùÑ ÏÇ¨Ïö©Ìï† ÏàòÎèÑ ÏûàÍ≥†, ÎßåÏïΩ Î©îÏÑ∏ÏßÄÏóê ÌÇ§Î•º ÏÑ§Ï†ïÌï¥Ï§¨Îã§Î©¥ ÌÇ§ÎßàÎã§ ÌååÌã∞ÏÖòÏùÑ Îã§Î•¥Í≤å Ìï†ÎãπÌïòÎèÑÎ°ù Ïª§Ïä§ÌÑ∞ÎßàÏù¥ÏßïÌï† ÏàòÎèÑ ÏûàÎã§.  

ÏÑ∏Í∑∏Î®ºÌä∏Îäî Ïó¨Îü¨ Î©îÏÑ∏ÏßÄÎ•º ÌïòÎÇòÎ°ú Î¨∂Ïñ¥ Ï†ÄÏû•ÌïòÍ≥† ÏûàÍ≥†, Í∞ÅÍ∞ÅÏùò Î©îÏÑ∏ÏßÄÎäî 1Ïî© Ï¶ùÍ∞ÄÌïòÎäî offsetÏùÑ Í∞ÄÏßÑÎã§. Í∞Å ÏÑ∏Í∑∏Î®ºÌä∏Îäî ÏûêÏã†Ïù¥ Í∞ÄÏßÄÍ≥† ÏûàÎäî Î©îÏÑ∏ÏßÄÏùò Í∞ÄÏû• Ï≤òÏùå Ïò§ÌîÑÏÖãÏùÑ Ïù¥Î¶ÑÏúºÎ°ú ÌïúÎã§.  

![](/images/kafka_75.png)

ÏúÑÏôÄ Í∞ôÏùÄ ÎûúÎç§Ìïú Î¨∏ÏûêÏó¥Îì§ÏùÑ ÏùΩÍ≥† Ïã∂ÏúºÎ©¥ Kafka Ìà¥ÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§.  

```sh
kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --print-data-log --files logs\freblogg-2\00000000000000000000.log
```

This gives the output

```sh
Dumping logs\freblogg-2\00000000000000000000.log
Starting offset: 0

offset: 0 position: 0 CreateTime: 1533443377944 isvalid: true keysize: -1 valuesize: 11 producerId: -1 headerKeys: [] payload: Hello World

offset: 1 position: 79 CreateTime: 1533462689974 isvalid: true keysize: -1 valuesize: 6 producerId: -1 headerKeys: [] payload: amazon
```

CreateTimeÍ≥º Í∞ôÏùÄ Í∞íÏùÄ Ïª®ÏäàÎ®∏Î°ú Í∞ÄÏ†∏ÏôÄÏÑú ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎäî Í∞íÏù¥ ÏïÑÎãàÎã§. Ïπ¥ÌîÑÏπ¥ ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Í∞ÄÏßÄÍ≥† ÏûàÎäî Î©îÌÉÄÎç∞Ïù¥ÌÑ∞Ïù¥Îã§. Í∑∏Î†áÍ∏∞ ÎïåÎ¨∏Ïóê Îç∞Ïù¥ÌÑ∞Ïùò ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑÍ∞Ä ÌïÑÏöîÌïòÎã§Î©¥, Îç∞Ïù¥ÌÑ∞Î•º ÏÉùÏÑ±Ìï† Îïå ÎÇ¥Î∂ÄÏ†ÅÏúºÎ°ú Î©îÏÑ∏ÏßÄÏóê Î™ÖÏãúÏ†ÅÏúºÎ°ú Îã¥ÏïÑÏÑú Î∏åÎ°úÏª§Ïóê Îã¥ÏïÑÏïº ÌïúÎã§.  

You can see that it stores information of the offset, time of creation, key and value sizes etc along with the actual message payload in the log file.  


## Segment

ÏúÑÏóêÏÑú Î¥§Îçò `.log`, `.index`, `.timeindex`ÏùÑ Î™®Îëê ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùºÏù¥ÎùºÍ≥† ÌïúÎã§. ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùºÏùÑ ÌïòÎÇòÎ°ú ÌïòÏßÄ ÏïäÍ≥†, ÎÇòÎàÑÏñ¥ Ï†ÄÏû•ÌïòÎäî Ïù¥Ïú†Îäî Ïó¨Îü¨Í∞ÄÏßÄÍ∞Ä ÏûàÎã§.  

Í∑∏Ï§ëÏóêÏÑúÎèÑ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ≠Ï†úÌï† Îïå Ïù¥Ï†êÏù¥ ÏûàÎã§Îäî Í≤ÉÏù¥Îã§. KafkaÎäî Íµ¨Ï°∞Ï†Å ÌäπÏÑ±ÏúºÎ°ú Î©îÏÑ∏ÏßÄÎßàÎã§ Îç∞Ïù¥ÌÑ∞Î•º ÏÇ≠Ï†úÌïòÎäî Í≤ÉÏù¥ Î∂àÍ∞ÄÎä•ÌïòÎã§. Ïú†ÏùºÌïòÍ≤å Î©îÏÑ∏ÏßÄÎ•º ÏÇ≠Ï†úÌïòÎäî Î∞©Î≤ïÏùÄ Î∞îÎ°ú ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùºÏùÑ ÏÇ≠Ï†úÌïòÎäî Í≤ÉÏù¥Îã§. Î≥¥ÌÜµ ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùº ÏÇ≠Ï†úÎäî Ïπ¥ÌîÑÏπ¥ configurationÏùÑ ÌÜµÌï¥ ÏÇ≠Ï†úÌïòÎäî **Retention policy** Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©ÌïúÎã§. (Ï†ïÏ±ÖÏùÑ ÌÜµÌï¥ Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú ÏÇ≠Ï†ú)   

ÏÑ∏Í∑∏Î®ºÌä∏ ÌååÏùºÏùò ÏùòÎØ∏Îäî Îã§ÏùåÍ≥º Í∞ôÎã§.   

- `.index` file: This contains the mapping of message offset to its physical position in .log file.
- `.log` file: This file contains the actual records and maintains the records up to a specific offset. The name of the file depicts the starting offset added to this file.
- .index file: This file has an index that maps a record offset to the byte offset of the record within the** .log **file. This mapping is used to read the record from any specific offset.
- `.timeindex` file: This file contains the mapping of the timestamp to record offset, which internally maps to the byte offset of the record using the .index file. This helps in accessing the records from the specific timestamp.
- `.snapshot` file: contains a snapshot of the producer state regarding sequence IDs used to avoid duplicate records. It is used when, after a new leader is elected, the preferred one comes back and needs such a state to become a leader again. This is only available for the active segment (log file)
- `.leader-epoch-checkpoint`: It refers to the number of leaders previously assigned by the controller. The replicas use the leader epoch as a means of verifying the current leader. The leader-epoch-checkpoint file contains two columns: epochs and offsets. Each row is a checkpoint for the latest recorded leader epoch and the leader's latest offset upon becoming leader

An index file for the log file I‚Äôve showed in the ‚ÄòQuick detour‚Äô above would look something like this:  

![](/images/kafka_76.png)  

If you need to read the message at offset 1, you first search for it in the index file and figure out that the message is in position 79. Then you directly go to position 79 in the log file and start reading. This makes it quite effective as you can use binary search to quickly get to the correct offset in the already sorted index file.  

![](/images/kafka_81.png)  

# Ï†ÄÏû•Îêú Îç∞Ïù¥ÌÑ∞Ïùò Ìè¨Îß∑(Kafka messages are just bytes)

**Kafka messages are just bytes**. Kafka messages are organized into topics. Each message is a key/value, but that is all that Kafka requires. Both key and value are just bytes when they are stored in Kafka. This makes Kafka applicable to a **wide range of use cases**, but it also means that developers have the **responsibility of deciding how to serialize the data.**

There are various **serialization formats** with common ones including:  

- JSON
- Avro
- Protobuf
- String delimited (e.g., CSV

There are advantages and disadvantages to each of these‚Äîwell, except delimited, in which case it‚Äôs only disadvantages üòâ  

Choosing a serialization format  

- **Schema**: A lot of the time your data will have a schema to it. You may not like the fact, but it‚Äôs your responsibility as a developer to preserve and propagate this schema. The schema provides the **contract between your services**. Some message formats (such as Avro and Protobuf) have strong schema support, whilst others have lesser support (JSON) or none at all (delimited string).
- **Ecosystem compatibility**: Avro, Protobuf, and JSON are first-class citizens in the Confluent Platform, with native support from the  Confluent Schema Registry, Kafka Connect, ksqlDB, and more.
- **Message size**: Whilst JSON is plain text and relies on any compression configured in Kafka itself, Avro and Protobuf are both binary formats and thus provide smaller message sizes.
- **Language support**: For example, support for Avro is strong in the Java space, whilst if you‚Äôre using Go, chances are you‚Äôll be expecting to use Protobuf.

Îç∞Ïù¥ÌÑ∞Î•º Î∏åÎ°úÏª§Ïóê Ï†ÄÏû•Ìï† ÎïåÎäî Ï†ÑÏÜ°Îêú Îç∞Ïù¥ÌÑ∞Ïùò Ìè¨Îß∑Í≥ºÎäî ÏÉÅÍ¥ÄÏóÜÏù¥ ÏõêÌïòÎäî Ìè¨Îß∑ÏúºÎ°ú Î∏åÎ°úÏª§Ïóê Ï†ÄÏû•Ìï† Ïàò ÏûàÎã§. ÏòàÎ•º Îì§Ïñ¥ ÌîÑÎ°úÎìÄÏÑúÍ∞Ä JSONÏúºÎ°ú Î≥¥ÎÉàÎã§Í≥† ÌïòÎçîÎùºÎèÑ Î∏åÎ°úÏª§Ïóê Ï†ÄÏû•Ìï† Îïå Ìè¨Îß∑ÏùÄ Avro, Parquet, String Î≠ò ÌïòÎì† ÏÉÅÍ¥ÄÏóÜÎã§. Îã§Îßå Ï§ëÏöîÌïú Í≤ÉÏùÄ SerializerÎ°ú AvroÎ•º ÏÑ†ÌÉùÌñàÎã§Î©¥, DeserializerÎèÑ Î∞òÎìúÏãú AvroÎ•º ÏÑ†ÌÉùÌï¥Ïïº ÌïúÎã§. Í∑∏Îü¨Í≥† ÎÇòÎ©¥ Ïª®ÏäàÎ®∏ÏóêÏÑú Ï†ÑÎã¨ Î∞õÎäî Îç∞Ïù¥ÌÑ∞Ïùò Ìè¨Îß∑ÏùÄ ÏûêÏó∞Ïä§ÎüΩÍ≤å Îã§Ïãú JSON ÌòïÌÉúÎ•º ÏñªÍ≤å ÎêúÎã§.  

![](/images/kafka_78.png)

Remember, Kafka messages are just pairs of key/value bytes, and you need to specify the converter for both keys and value, using the `key.converter` and `value.converter` configuration setting. In some situations, you may use different converters for the key and the value.   

Here‚Äôs an example of using the String converter. Since it‚Äôs just a string, there‚Äôs no schema to the data, and thus it‚Äôs not so useful to use for the value:  

```sh
"key.converter": "org.apache.kafka.connect.storage.StringConverter",
```

Some converters have additional configuration. For Avro, you need to specify the Schema Registry. For JSON, you need to specify if you want Kafka Connect to embed the schema in the JSON itself. When you specify converter-specific configurations, always use the `key.converter`. or `value.converter`. prefix. For example, to use Avro for the message payload, you‚Äôd specify the following:  

```
"value.converter": "io.confluent.connect.avro.AvroConverter",
"value.converter.schema.registry.url": "http://schema-registry:8081",
```

Common converters include:  

```sh
# Avro
io.confluent.connect.avro.AvroConverter

# Protobuf
io.confluent.connect.protobuf.ProtobufConverter

# String
org.apache.kafka.connect.storage.StringConverter

# JSON
org.apache.kafka.connect.json.JsonConverter

# JSON schema
io.confluent.connect.json.JsonSchemaConverter

# ByteArray
org.apache.kafka.connect.converters.ByteArrayConverter
```

JSONÏùò Í≤ΩÏö∞ Ïä§ÌÇ§ÎßàÍ∞Ä ÏÑ§Ï†ïÏùÑ ÏïàÌïòÎäî Í≤ÉÏù¥ ÎîîÌè¥Ìä∏Îã§. ÌïòÏßÄÎßå Ïä§ÌÇ§ÎßàÎ•º Í≥†Ï†ïÌïòÍ≥† Ïã∂ÏùÄ Í≤ΩÏö∞ Îëê Í∞ÄÏßÄ Î∞©Î≤ïÏùÑ ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎã§.  

1. JSON schema `io.confluent.connect.json.JsonSchemaConverter`Î•º Ïì¥Îã§ (with Ïä§ÌÇ§Îßà Î†àÏßÄÏä§Ìä∏Î¶¨)
   ```
   "value.converter": "io.confluent.connect.json.JsonSchemaConverter",
   "value.converter.schema.registry.url": "http://schema-registry:8081",
   ```
2. ÎπÑÌö®Ïú®Ï†ÅÏù¥ÏßÄÎßå Îß§Î≤à Î©îÏãúÏßÄÏóê Ïä§ÌÇ§ÎßàÎ•º Îã¥ÏïÑÏÑú Ï†ÑÏÜ°/Ï†ÄÏû•ÌïúÎã§.
   ```
   value.converter=org.apache.kafka.connect.json.JsonConverter
   value.converter.schemas.enable=true
   ```

2Î≤à Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ Î©îÏÑ∏ÏßÄÍ∞Ä Îã§ÏùåÍ≥º Í∞ôÏù¥ schema Î∂ÄÎ∂ÑÍ≥º, payload Î∂ÄÎ∂ÑÏù¥ Ìï®Íªò Ï†ÄÏû•ÎêúÎã§.  

```json
{
  "schema": {
    "type": "struct",
    "fields": [
      {
        "type": "int64",
        "optional": false,
        "field": "registertime"
      },
      {
        "type": "string",
        "optional": false,
        "field": "userid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "regionid"
      },
      {
        "type": "string",
        "optional": false,
        "field": "gender"
      }
    ],
    "optional": false,
    "name": "ksql.users"
  },
  "payload": "Hello World"
}
```

Ïù¥Î†áÍ≤å ÌïòÎ©¥ Î©îÏÑ∏ÏßÄ ÏÇ¨Ïù¥Ï¶àÍ∞Ä Ïª§ÏßÄÍ∏∞ ÎïåÎ¨∏Ïóê ÎπÑÌö®Ïú®Ï†ÅÏù¥Îã§. Í∑∏ÎûòÏÑú Ïä§ÌÇ§ÎßàÍ∞Ä ÌïÑÏöîÌïú Í≤ΩÏö∞ÏóêÎäî Ïä§ÌÇ§Îßà Î†àÏßÄÏä§Ìä∏Î¶¨Î•º ÏÇ¨Ïö©ÌïòÎäî Í≤ÉÏù¥ Ìö®Ïú®Ï†ÅÏù¥Îã§.  

ÎßåÏïΩ Ïª®Î≤ÑÌÑ∞Ïóê JSON serializerÎ•º ÏÇ¨Ïö©ÌñàÍ≥† Ïä§ÌÇ§ÎßàÎ•º Îî∞Î°ú ÏÑ§Ï†ïÌïòÏßÄ ÏïäÏùÑÍ±∞ÎùºÎ©¥,  

```
value.converter=org.apache.kafka.connect.json.JsonConverter
value.converter.schemas.enable=false
```

Ïù¥Î†áÍ≤å schemaÎ•º Ï∞æÏùÑ ÌïÑÏöî ÏóÜÎã§Í≥† Î™ÖÏãúÌï¥Ï£ºÏûê. (ÎîîÌè¥Ìä∏Í∞Ä falseÏù∏Îç∞ Ïôú Ìï¥Ï§òÏïºÌïòÎäîÍ±∞ÏßÄ..?)  

ÏïÑÎûò ÌëúÎäî serializerÏôÄ deserializerÏùò Ïã±ÌÅ¨Î•º Ïñ¥ÎñªÍ≤å ÎßûÏ∂∞Ïïº ÏóêÎü¨Í∞Ä ÏïàÎÇòÎäîÏßÄ ÏïåÎ†§Ï§ÄÎã§. Í∏∞Î≥∏Ï†ÅÏúºÎ°ú serializerÎäî Î©îÏÑ∏ÏßÄÎÇò ÏÉÅÌô©Ïóê ÎßûÍ≤å ÏõêÌïòÎäî Í≤ÉÏùÑ ÏÑ†ÌÉùÌïòÍ≥†, deserializerÎäî serializerÏôÄ Í∞ôÏùÄ Ìè¨Îß∑ÏùÑ ÏÇ¨Ïö©ÌïòÎèÑÎ°ù ÌïòÎ©¥ ÎêúÎã§.    

![](/images/kafka_79.png)

# ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ ÏúÑÌïú ÌååÌã∞ÏÖò Ïàò

To guarantee the order of reading messages from a partition, Kafka restricts to having only one consumer (from a consumer group) per partition. So, if a partition gets messages a,f and k, the consumer will also read them in the order a,f and k. This is an important thing to make a note of as the order of message consumption is not guaranteed at a topic level when you have multiple partitions.  

ÌååÌã∞ÏÖò ÎÇ¥ÏóêÏÑúÎäî Î©îÏÑ∏ÏßÄÏùò ÏàúÏÑúÍ∞Ä ÏßÄÏºúÏßÑÎã§. Í∑∏ÎûòÏÑú ÌÜ†ÌîΩÏùÑ Ïù¥Î£®Îäî ÌååÌã∞ÏÖòÏù¥ 1Í∞úÎùºÎ©¥ Î©îÏÑ∏ÏßÄÏùò ÏàúÏÑúÎ•º Í±±Ï†ïÌï† ÌïÑÏöîÍ∞Ä ÏóÜÎã§. ÌïòÏßÄÎßå ÌååÌã∞ÏÖòÏùò Í∞úÏàòÎ•º 2Í∞ú Ïù¥ÏÉÅÏúºÎ°ú ÌïòÎ©¥ Î©îÏÑ∏ÏßÄÏùò ÏàúÏÑúÍ∞Ä Î≥¥Ïû•ÎêòÏßÄ ÏïäÎäîÎã§.  

Î≥ëÎ†¨ Ï≤òÎ¶¨Î•º ÌÜµÌï¥ ÏÑ±Îä•ÏùÑ ÎÜíÏù¥Í≥†Ïûê Ìï† Îïå, ÌååÌã∞ÏÖòÏùò Í∞úÏàòÏôÄ Ïª®ÏäàÎ®∏Ïùò Í∞úÏàòÎ•º ÎäòÎ†§Ï§ÄÎã§.  

- ÌååÌã∞ÏÖòÏùò Ïàò >= Ïª®ÏäàÎ®∏ Ïàò
- Î≥ëÎ†¨ Ï†ïÎèÑ = MIN(ÌååÌã∞ÏÖòÏùò Ïàò, Ïª®ÏäàÎ®∏ Ïàò)
- ÌååÌã∞ÏÖòÏùò Í∞úÏàòÎäî ÎäòÎ¶¥ÏàòÎßå ÏûàÍ≥† Ï§ÑÏùº ÏàòÎäî ÏóÜÏùå

# Ïû•Ïï† Î≥µÍµ¨Î•º ÏúÑÌïú Î≥µÏ†ú
Î≥µÏ†úÎäî ÌäπÏ†ï Î∏åÎ°úÏª§ ÏÑúÎ≤ÑÏóê Ïû•Ïï†Í∞Ä ÎÇ¨ÏùÑ Í≤ΩÏö∞Î•º ÎåÄÎπÑÌïòÍ∏∞ ÏúÑÌïú Ïö©ÎèÑÎã§. ÎßåÏïΩ Î∏åÎ°úÏª§Í∞Ä 1ÎåÄÎùºÎ©¥ Î≥µÏ†úÎäî ÏïÑÎ¨¥ ÏùòÎØ∏Í∞Ä ÏóÜÎã§. Î≥µÏ†úÎäî Î∏åÎ°úÏª§Ïùò Í∞úÏàòÎßåÌÅº ÏÑ§Ï†ïÌïòÎ©¥ ÎêúÎã§. Îçî ÌÅ¨Í≤å Îçî Ï†ÅÍ≤å Ìï¥ÎèÑ ÎêòÏßÄÎßå, Í∞ôÍ≤å ÌïòÎäî Í≤ÉÏù¥ Ï†úÏùº Ìï©ÎãπÌïú ÏÑ†ÌÉùÏù¥Îã§.  

Î≥µÏ†úÏàòÎäî ÌÜ†ÌîΩÎßàÎã§ Îã§Î•¥Í≤å ÏÑ§Ï†ïÌï† Ïàò ÏûàÎã§. Î≥µÏ†ú ÏàòÎäî ÎäòÎ¶¨Îäî ÎßåÌÅº ÏÑ±Îä•Ïù¥ ÏïΩÍ∞Ñ Îñ®Ïñ¥ÏßÑÎã§. Í∑∏ÎûòÏÑú ÌÜ†ÌîΩÏùò Ï§ëÏöîÎèÑÏóê Îî∞Îùº Îã§Î•¥Í≤å ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏù¥ Ï¢ãÎã§.  

Î≥µÏ†úÏóê Í¥ÄÌï¥ Ïù¥Ìï¥ÌïòÎ†§Î©¥ Î¶¨Îçî/ÌåîÎ°úÏõå, Ïª§Î∞ãÍ≥º Í∞ôÏùÄ Í≤ÉÎì§ÏùÑ Î∞∞ÏõåÏïº ÌïúÎã§. Ïª®ÏäàÎ®∏Îäî Î¶¨Îçî ÌååÌã∞ÏÖòÎßå Í∞ÄÏ†∏Í∞à Ïàò ÏûàÎã§. Î≥µÏ†úÎäî Î¶¨ÎçîÍ∞Ä Ïû•Ïï†Í∞Ä ÎÇ¨ÏùÑ Í≤ΩÏö∞Î•º ÎåÄÎπÑÌïòÍ∏∞ ÏúÑÌïú Ïö©ÎèÑÎã§.  

Say for the freblogg topic that we've been using so far, we've given the replication factor as 2. The resulting distribution of its three partitions will look something like this.  

![](/images/kafka_77.png)

Even when you have a replicated partition on a different broker, Kafka wouldn‚Äôt let you read from it because in each replicated set of partitions, there is a LEADER and the rest of them are just mere FOLLOWERS serving as backup. The followers keep on syncing the data from the leader partition periodically, waiting for their chance to shine. When the leader goes down, one of the in-sync follower partitions is chosen as the new leader and now you can consume data from this partition.  

A Leader and a Follower of a single partition are never in a single broker. It should be quite obvious why that is so.  


# Î°úÍ∑∏ ÏÑ§Ï†ïÏùÑ ÌÜµÌï¥ Ìö®Ïú®Ï†ÅÏúºÎ°ú Î≥¥Í¥ÄÌïòÍ∏∞(Log Retention)

Apache Kafka is a commit-log system. The records are appended at the end of each Partition, and each Partition is also split into segments. Segments help delete older records through Compaction, improve performance, and much more.  

Kafka allows us to optimize the log-related configurations, we can control the rolling of segments, log retention, etc. **These configurations determine how long the record will be stored** and we‚Äôll see how it impacts the broker's performance, especially when the cleanup policy is set to Delete.  

For better performance and maintainability, multiple segments get created, and rather than reading from one huge Partition, Consumers can now read faster from a smaller segment file. A directory with the partition name gets created and maintains all the segments for that partition as various files.  

![](/images/kafka_81.png)

The active segment is the only file available for reading and writing while consumers can use other log segments (non-active) to read data. When the active segment becomes full (configured by `log.segment.bytes`, default 1 GB) or the configured time (`log.roll.hours` or `log.roll.ms`, default 7 days) passes, the segment gets rolled. This means that the **active segment gets closed and re-opens with read-only mode and a new segment file** (active segment) will be created in read-write mode.

## Role of Indexing within the Partition
Indexing helps consumers to read data starting from any specific offset or using any time range. As mentioned previously, the `.index` file contains an index that maps the logical offset to the byte offset of the record within the `.log` file. **You might expect that this mapping is available for each record, but it doesn‚Äôt work this way.**  

**How these entries are added inside the index file is defined by the `log.index.interval.bytes` parameter, which is 4096 bytes by default.** This means that after every 4096 bytes added to the `.log` file, an entry gets added to the `.index` file. Suppose the producer is sending records of 100 bytes each to a Kafka topic. In this case, a new index entry will be added to the `.index` file after every 41 records (41*100 = 4100 bytes) appended to the log file.  

(Î™®Îì† Î†àÏΩîÎìúÍ∞Ä Ïù∏Îç±Ïã±ÎêòÍ∏∞Îäî ÌïòÎäîÎç∞, Î†àÏΩîÎìú Ìïú Í∞ú ÎÑ£ÏùÑÎïåÎßàÎã§ Ïù∏Îç±Ïã±ÎêòÎäî Í≤ÉÏùÄ ÏïÑÎãàÍ≥† `.log` ÌååÏùº ÌïòÎÇòÍ∞Ä Îã§ Ï∞®Í≥†ÎÇòÎ©¥ Ìï¥Îãπ `.log` ÌååÏùºÏùò Î†àÏΩîÎìúÎ•º Ïù∏Îç±Ïã±Ìï¥ÏÑú `.index` ÌååÏùºÏùÑ ÎßåÎì†Îã§)  

![](/images/kafka_82.png)

If a consumer wants to read starting at a specific offset, a search for the record is made as follows:  

- Search for the `.index` file based on its name. For e.g. If the offset is 1191, the index file will be searched whose name has a value less than 1191. The naming convention for the index file is the same as that of the log file
- Search for an entry in the `.index` file where the requested offset falls.
- Use the mapped byte offset to access the `.log` file and start consuming the records from that byte offset.

As we mentioned, consumers may also want to read the records from a specific timestamp. This is where the `.timeindex` file comes into the picture. It maintains a timestamp and offset mapping (which maps to the corresponding entry in the `.index` file), which maps to the actual byte offset in the `.log` file. (ÌäπÏ†ï ÌÉÄÏûÑÏä§ÌÉ¨ÌîÑÎ°ú Î†àÏΩîÎìú ÏùΩÎäî Î∞©Î≤ï: `.timeindex` -> `.index` -> `.log`)  

![](/images/kafka_83.png)

## Rolling segments
As discussed in the above sections, the active segment gets rolled once any of these conditions are met-

1. Maximum segment size - configured by `log.segment.bytes`, defaults to 1 Gb
2. Rolling segment time - configured by `log.roll.ms` or `log.roll.hours`, defaults to 7 days
3. Index/timeindex is full - The index and timeindex share the same maximum size, which is defined by the `log.index.size.max.bytes`, defaults to 10 MB

(Î≥¥ÌÜµ 1Î≤à ÌÅ¨Í∏∞Î•º ÎäòÎ¶¨Î©¥, 3Î≤à ÌÅ¨Í∏∞ÎèÑ ÎäòÎ†§Ïïº ÌïúÎã§)  

## Impact of increasing/decreasing the segment size
Generally you don‚Äôt want to increase/decrease the `log.segment.bytes` and keep it as default. But let‚Äôs discuss the impact of changing this value so that you can make an informed decision if there‚Äôs a need.  

## Log retention - The records may persist longer than the retention time
Kafka, with its feature of retaining the log for a longer duration rather than deleting it like traditional messaging queues once consumed, provides many added advantages. Multiple consumers can read the same data, apart from reading the data it can also be sent to data warehouses for further analytics.  

How long is the data retained in Kafka? This is configurable using the maximum number of bytes to retain by using the `log.retention.bytes` parameter. If you want to set a retention period, you can use the `log.retention.ms`, `log.retention.minutes`, or `log.retention.hours` (7 days by default) parameters.  


The following things may impact when the records get deleted-

- If the producer is slow and the maximum size of 16 Kb is not reached within 10 minutes, older records won‚Äôt be deleted. In this case, the log retention would be higher than 10 mins.
- If the active segment is filled quickly, it will be closed but only get deleted once the last inserted record persists for 10 mins. So in this case as well, the latest inserted record would be persisted for more than 10 mins. - Suppose the segment is getting filled in 7 mins and getting closed, the last inserted record will stay for 10 mins so the actual retention time for the first record inserted into the segment would be 17 mins.
- The log can be persisted for an even longer duration than the last added record in the segment. How? Because the thread which gets executed and checks which log segments need to be deleted runs every 5 mins. This is configurable using log.retention.check.interval.ms configurations. - Depending on the last added record to the segment, this cleanup thread can miss the 10 min retention deadline. So in our example above instead of persisting the segment for 17 mins, it could be persisted for 22 mins.
- Do you think that this would be the maximum time the record is persisted in Kafka? No, the cleaner thread checks and just marks the segment to be deleted. The log.segment.delete.delay.ms broker parameter defines when the file will actually be removed from the file system when it‚Äôs marked as ‚Äúdeleted‚Äù (default, 1 min) - Going back to our example the log is still available even after 23 mins, which is way longer than the retention time of 10 mins.

So The usual retention limits are set by using `log.retention.ms` defines a kind of minimum time the record will be persisted in the file system.  

Consumers get records from closed segments but not from deleted ones, even if they are just marked as ‚Äúdeleted‚Äù but not actually removed from the file system.  

## Conclusion


![](/images/kafka_84.png)


# Ï∞∏Í≥†
- [Data types for Kafka connector](https://docs.cloudera.com/csa/1.2.0/flink-sql-table-api/topics/csa-kafka-sql-datatypes.html){:target="_blank"}
- [Kafka Connect Deep Dive ‚Äì Converters and Serialization Explained](https://www.confluent.io/blog/kafka-connect-deep-dive-converters-serialization-explained/){:target="_blank"}
- [dol9, Kafka Ïä§ÌÇ§Îßà Í¥ÄÎ¶¨, Schema Registry](https://dol9.tistory.com/274){:target="_blank"}
- [A Practical Introduction to Kafka Storage Internals](https://www.freblogg.com/kafka-storage-internals){:target="_blank"}
- [Here‚Äôs what makes Apache Kafka so fast](https://www.freecodecamp.org/news/what-makes-apache-kafka-so-fast-a8d4f94ab145/){:target="_blank"}
- [stackoverflow: Which directory does apache kafka store the data in broker nodes](https://stackoverflow.com/questions/40369238/which-directory-does-apache-kafka-store-the-data-in-broker-nodes#){:target="_blank"}
- [Abhishek Sharma, How kafka stores data](https://medium.com/@abhisheksharma_59226/how-kafka-stores-data-37ee611c89a2){:target="_blank"}
- [Rohith Sankepally:g Deep Dive Into Apache Kafka. Storage Internals](https://rohithsankepally.github.io/Kafka-Storage-Internals/){:target="_blank"}
- [towardsdatascience, Log Compacted Topics in Apache Kafka](https://towardsdatascience.com/log-compacted-topics-in-apache-kafka-b1aa1e4665a7){:target="_blank"}
- [conduktor, Understanding Kafka's Internal Storage and Log Retention](https://www.conduktor.io/understanding-kafkas-internal-storage-and-log-retention){:target="_blank"}
- [What is a commit log and why should you care?](https://dev.to/heroku/what-is-a-commit-log-and-why-should-you-care-pib){:target="_blank"}
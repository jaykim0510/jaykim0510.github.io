---
layout: post
title:  '[AI Basic] 경사 하강법'
description: 배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 m개씩 묶음(batch)해서 그 평균 그래디언트를 가중치 업데이트에 사용하는 것이다
date:   2024-01-11 15:01:35 +0300
image:  '/images/ai_basic_logo.png'
logo_image:  '/images/ai_basic_logo.png'
category: AI
tag: AI_basic
---
---

**Table of Contents**
{: #toc }
*  TOC
{:toc}

---


# 배치 경사 하강법


## 확률적 경사 하강법(Stochastic Gradient Descent)  

확률적 경사 하강법은 데이터 세트에서 **무작위로 균일하게 하나의 데이터를 추출해 그래디언트를 계산**한다. 즉 가중치를 한 번 업데이트 하기 위해 샘플을 1개씩만 사용한다. 그렇기 때문에 굉장히 **빠른 속도로 가중치를 업데이트** 할 수 있게 된다. **하지만 이러한 방법은 다소 가중치를 성급하게 바꾸는 듯한 느낌**이 난다. 자칫 노이즈가 많은 데이터에 대해 가중치를 업데이트를 하게되면 그러한 데이터를 만날 때 마다 가중치가 잘못된 방향으로 업데이트 될 것이다. 그래서 **느리지만 조금 더 신중하게 가중치를 업데이트 하기 위해 나온 방법이 바로 배치 경사 하강법**이다.


## 배치 경사 하강법(Batch Gradient Descent)  

배치 경사 하강법은 가중치를 한 번 업데이트 하기 위해 데이터 샘플을 m(ex. 16, 32)개 정도 사용해 각 샘플마다 그래디언트를 계산 후, 가중치를 업데이트 할 때는 그들의 평균을 구해 그 평균값을 가중치 업데이트에 사용한다. 다시 말해 **가중치를 한 번 업데이트 하기 위해 데이터 1개가 아닌 m개씩 묶음(batch)해서 그 평균 그래디언트를 사용하겠다는 것**이다.  

또한 생각해보면 우리는 데이터에 대해 그래디언트를 계산하기 위해 그렇게 어려운 계산 과정이 아닌 단순 합과 곱의 연산이었다. 따라서 그래디언트 계산은 GPU에 있는 코어로도 충분히 해결이 가능하다. GPU는 비교적 단순한 연산을 하는 코어가 수 십개에서 수 천개 있는 하드웨어이다. 따라서 우리는 GPU를 사용해 가중치 업데이트를 할 것이고, 이 때 배치 경사 하강법을 사용하게 되면 훨씬 더 GPU를 효율적으로 사용할 수 있게 된다.  

![](/images/sgd_bgd.png)


### 배치 경사 하강법 수식 과정

1. **데이터를 Batch size(m)만큼 Forward propagation시킨다**  

    ![](/images/batch_2.png)

2. **Error를 구한다**  

    ![](/images/batch_3.png){: width="50%"}  


3. **각 특성(노드)의 가중치를 업데이트 하기 위한 평균 그래디언트를 구한다**  

    ![](/images/batch_4.png) 

    ![](/images/batch_5.png){: width="70%"}  

4. **가중치를 업데이트 한다**  

    ![](/images/batch_6.png){: width="70%"}  